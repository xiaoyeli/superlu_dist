diff --git a/.gitignore b/.gitignore
index dd12eaa..adcaf5c 100644
--- a/.gitignore
+++ b/.gitignore
@@ -1,9 +1,12 @@
 *~
 
-# You have to ignore this genrated file or git will complain that it is an
+# You have to ignore this generated file or git will complain that it is an
 # unknown file!
 /make.inc
 
 # If the instructions are telling people to create this build dir under the
 # source tree, you had better put in an ignore for this.
-/build/*
+/build/
+
+# Ignore Testing/ folder
+Testing/
diff --git a/CMakeLists.txt b/CMakeLists.txt
index 95f101b..d870ea9 100644
--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -15,6 +15,7 @@ set(VERSION_BugFix "0")
 set(PROJECT_VERSION ${VERSION_MAJOR}.${VERSION_MINOR}.${VERSION_BugFix})
 
 list(APPEND CMAKE_MODULE_PATH "${PROJECT_SOURCE_DIR}/cmake")
+
 ######################################################################
 #
 # IDEAS: xSDK standards module
@@ -51,6 +52,7 @@ SET(CMAKE_INSTALL_RPATH_USE_LINK_PATH TRUE)
 #----
 
 SET(BUILD_STATIC_LIBS TRUE CACHE BOOL "Include static libs when building shared")
+
 if (BUILD_SHARED_LIBS)
   message("-- SuperLU_DIST will be built as a shared library.")
   set(PROJECT_NAME_LIB_EXPORT libsuperlu_dist.so)
@@ -63,8 +65,6 @@ else()
 endif()
 
 enable_language (C)
-enable_language (CXX)
-set(CMAKE_CXX_STANDARD 11)
 if (XSDK_ENABLE_Fortran)
   enable_language (Fortran)
   set(NOFORTRAN FALSE)
@@ -110,6 +110,7 @@ set(INSTALL_INC_DIR "${default_install_inc_dir}" CACHE STRING "The folder where
 set(INSTALL_LIB_DIR "${default_install_lib_dir}" CACHE STRING "The folder where libraries will be installed.")
 set(INSTALL_BIN_DIR "${default_install_bin_dir}" CACHE STRING "The folder where runtime files will be installed.")
 
+
 # Set up required compiler defines and options.
 ## get_directory_property( DirDefs COMPILE_DEFINITIONS )
 # set(CMAKE_C_FLAGS "-DDEBUGlevel=0 -DPRNTlevel=0 ${CMAKE_C_FLAGS}")
@@ -219,6 +220,7 @@ if(PARMETIS_FOUND)
   set(HAVE_PARMETIS TRUE)
 endif()
 
+
 ######################################################################
 #
 # Include directories
@@ -231,6 +233,7 @@ if (TPL_PARMETIS_INCLUDE_DIRS)
   include_directories(${TPL_PARMETIS_INCLUDE_DIRS})  ## parmetis
 endif ()
 include_directories(${MPI_C_INCLUDE_PATH})
+
 ######################################################################
 #
 # Add subdirectories
diff --git a/DoxyConfig b/DoxyConfig
index 5bbc5a0..36b49c5 100644
--- a/DoxyConfig
+++ b/DoxyConfig
@@ -31,7 +31,7 @@ PROJECT_NAME           = SuperLU Distributed
 # This could be handy for archiving the generated documentation or 
 # if some version control system is used.
 
-PROJECT_NUMBER         = 5.0.0
+PROJECT_NUMBER         = 5.3.0
 e
 # The OUTPUT_DIRECTORY tag is used to specify the (relative or absolute) 
 # base path where the generated documentation will be put. 
@@ -513,7 +513,7 @@ WARN_LOGFILE           =
 # directories like "/usr/src/myproject". Separate the files or directories 
 # with spaces.
 
-INPUT                  = SRC/ EXAMPLE/ FORTRAN/
+INPUT                  = SRC/ EXAMPLE/ FORTRAN/ TEST/
 
 # This tag can be used to specify the character encoding of the source files 
 # that doxygen parses. Internally doxygen uses the UTF-8 encoding, which is 
diff --git a/EXAMPLE/Makefile b/EXAMPLE/Makefile
index 7984801..79ece5a 100644
--- a/EXAMPLE/Makefile
+++ b/EXAMPLE/Makefile
@@ -133,8 +133,7 @@ pzdrive4_ABglobal: $(ZEXMG4) $(DSUPERLULIB)
 #	$(CC) $(CFLAGS) $(CDEFS) $(BLASDEF) $(INCLUDEDIR) -c pdgstrf.c $(VERBOSE)
 .c.o:
 	$(CC) $(CFLAGS) $(CDEFS) $(BLASDEF) $(INCLUDEDIR) -c $< $(VERBOSE)
-.cpp.o:
-	$(CPP) $(CPPFLAGS) $(CDEFS) $(BLASDEF) $(INCLUDEDIR) -c $< $(VERBOSE)
+
 .f.o:
 	$(FORTRAN) $(FFLAGS) -c $< $(VERBOSE)
 
diff --git a/EXAMPLE/dcreate_matrix.c b/EXAMPLE/dcreate_matrix.c
index d90185e..a622463 100644
--- a/EXAMPLE/dcreate_matrix.c
+++ b/EXAMPLE/dcreate_matrix.c
@@ -63,8 +63,7 @@ at the top-level directory.
  * </pre>
  */
 
- 
- int dcreate_matrix(SuperMatrix *A, int nrhs, double **rhs,
+int dcreate_matrix(SuperMatrix *A, int nrhs, double **rhs,
                    int *ldb, double **x, int *ldx,
                    FILE *fp, gridinfo_t *grid)
 {
@@ -90,203 +89,14 @@ at the top-level directory.
 #endif
 
     if ( !iam ) {
-    double t = SuperLU_timer_();
+        double t = SuperLU_timer_();
 
-	/* Read the matrix stored on disk in Harwell-Boeing format. */
-	dreadhb_dist(iam, fp, &m, &n, &nnz, &nzval, &rowind, &colptr);
-	
-	printf("Time to read and distribute matrix %.2f\n", 
-	        SuperLU_timer_() - t);  fflush(stdout);	
-		
-	/* Broadcast matrix A to the other PEs. */
-	MPI_Bcast( &m,     1,   mpi_int_t,  0, grid->comm );
-	MPI_Bcast( &n,     1,   mpi_int_t,  0, grid->comm );
-	MPI_Bcast( &nnz,   1,   mpi_int_t,  0, grid->comm );
-	MPI_Bcast( nzval,  nnz, MPI_DOUBLE, 0, grid->comm );
-	MPI_Bcast( rowind, nnz, mpi_int_t,  0, grid->comm );
-	MPI_Bcast( colptr, n+1, mpi_int_t,  0, grid->comm );
-    } else {
-	/* Receive matrix A from PE 0. */
-	MPI_Bcast( &m,   1,   mpi_int_t,  0, grid->comm );
-	MPI_Bcast( &n,   1,   mpi_int_t,  0, grid->comm );
-	MPI_Bcast( &nnz, 1,   mpi_int_t,  0, grid->comm );
-
-	/* Allocate storage for compressed column representation. */
-	dallocateA_dist(n, nnz, &nzval, &rowind, &colptr);
-
-	MPI_Bcast( nzval,   nnz, MPI_DOUBLE, 0, grid->comm );
-	MPI_Bcast( rowind,  nnz, mpi_int_t,  0, grid->comm );
-	MPI_Bcast( colptr,  n+1, mpi_int_t,  0, grid->comm );
-    }
-
-#if 0
-    nzval[0]=0.1;
-#endif
-
-    /* Compute the number of rows to be distributed to local process */
-    m_loc = m / (grid->nprow * grid->npcol); 
-    m_loc_fst = m_loc;
-    /* When m / procs is not an integer */
-    if ((m_loc * grid->nprow * grid->npcol) != m) {
-        /*m_loc = m_loc+1;
-          m_loc_fst = m_loc;*/
-      if (iam == (grid->nprow * grid->npcol - 1)) /* last proc. gets all*/
-	  m_loc = m - m_loc * (grid->nprow * grid->npcol - 1);
-    }
-
-    /* Create compressed column matrix for GA. */
-    dCreate_CompCol_Matrix_dist(&GA, m, n, nnz, nzval, rowind, colptr,
-				SLU_NC, SLU_D, SLU_GE);
-
-    /* Generate the exact solution and compute the right-hand side. */
-    if ( !(b_global = doubleMalloc_dist(m*nrhs)) )
-        ABORT("Malloc fails for b[]");
-    if ( !(xtrue_global = doubleMalloc_dist(n*nrhs)) )
-        ABORT("Malloc fails for xtrue[]");
-    *trans = 'N';
-
-    dGenXtrue_dist(n, nrhs, xtrue_global, n);
-    dFillRHS_dist(trans, nrhs, xtrue_global, n, &GA, b_global, m);
-
-    /*************************************************
-     * Change GA to a local A with NR_loc format     *
-     *************************************************/
-
-    rowptr = (int_t *) intMalloc_dist(m_loc+1);
-    marker = (int_t *) intCalloc_dist(n);
-
-    /* Get counts of each row of GA */
-    for (i = 0; i < n; ++i)
-      for (j = colptr[i]; j < colptr[i+1]; ++j) ++marker[rowind[j]];
-    /* Set up row pointers */
-    rowptr[0] = 0;
-    fst_row = iam * m_loc_fst;
-    nnz_loc = 0;
-    for (j = 0; j < m_loc; ++j) {
-      row = fst_row + j;
-      rowptr[j+1] = rowptr[j] + marker[row];
-      marker[j] = rowptr[j];
-    }
-    nnz_loc = rowptr[m_loc];
-
-    nzval_loc = (double *) doubleMalloc_dist(nnz_loc);
-    colind = (int_t *) intMalloc_dist(nnz_loc);
-
-    /* Transfer the matrix into the compressed row storage */
-    for (i = 0; i < n; ++i) {
-      for (j = colptr[i]; j < colptr[i+1]; ++j) {
-	row = rowind[j];
-	if ( (row>=fst_row) && (row<fst_row+m_loc) ) {
-	  row = row - fst_row;
-	  relpos = marker[row];
-	  colind[relpos] = i;
-	  nzval_loc[relpos] = nzval[j];
-	  ++marker[row];
-	}
-      }
-    }
-
-#if ( DEBUGlevel>=2 )
-    if ( !iam ) dPrint_CompCol_Matrix_dist(&GA);
-#endif   
-
-    /* Destroy GA */
-    Destroy_CompCol_Matrix_dist(&GA);
-
-    /******************************************************/
-    /* Change GA to a local A with NR_loc format */
-    /******************************************************/
-
-    /* Set up the local A in NR_loc format */
-    dCreate_CompRowLoc_Matrix_dist(A, m, n, nnz_loc, m_loc, fst_row,
-				   nzval_loc, colind, rowptr,
-				   SLU_NR_loc, SLU_D, SLU_GE);
-    
-    /* Get the local B */
-    if ( !((*rhs) = doubleMalloc_dist(m_loc*nrhs)) )
-        ABORT("Malloc fails for rhs[]");
-    for (j =0; j < nrhs; ++j) {
-	for (i = 0; i < m_loc; ++i) {
-	    row = fst_row + i;
-	    (*rhs)[j*m_loc+i] = b_global[j*n+row];
-	}
-    }
-    *ldb = m_loc;
-
-    /* Set the true X */    
-    *ldx = m_loc;
-    if ( !((*x) = doubleMalloc_dist(*ldx * nrhs)) )
-        ABORT("Malloc fails for x_loc[]");
-
-    /* Get the local part of xtrue_global */
-    for (j = 0; j < nrhs; ++j) {
-      for (i = 0; i < m_loc; ++i)
-	(*x)[i + j*(*ldx)] = xtrue_global[i + fst_row + j*n];
-    }
-
-    SUPERLU_FREE(b_global);
-    SUPERLU_FREE(xtrue_global);
-    SUPERLU_FREE(marker);
-
-#if ( DEBUGlevel>=1 )
-    printf("sizeof(NRforamt_loc) %lu\n", sizeof(NRformat_loc));
-    CHECK_MALLOC(iam, "Exit dcreate_matrix()");
-#endif
-    return 0;
-}
-
- 
- 
- 
-int dcreate_matrix_postfix(SuperMatrix *A, int nrhs, double **rhs,
-                   int *ldb, double **x, int *ldx,
-                   FILE *fp, char * postfix, gridinfo_t *grid)
-{
-    SuperMatrix GA;              /* global A */
-    double   *b_global, *xtrue_global;  /* replicated on all processes */
-    int_t    *rowind, *colptr;	 /* global */
-    double   *nzval;             /* global */
-    double   *nzval_loc;         /* local */
-    int_t    *colind, *rowptr;	 /* local */
-    int_t    m, n, nnz;
-    int_t    m_loc, fst_row, nnz_loc;
-    int_t    m_loc_fst; /* Record m_loc of the first p-1 processors,
-			   when mod(m, p) is not zero. */ 
-    int_t    row, col, i, j, relpos;
-    int      iam;
-    char     trans[1];
-    int_t      *marker;
-
-    iam = grid->iam;
-
-#if ( DEBUGlevel>=1 )
-    CHECK_MALLOC(iam, "Enter dcreate_matrix()");
-#endif
-
-    if ( !iam ) {
-    double t = SuperLU_timer_();       
-	if(!strcmp(postfix,"rua")){
-		/* Read the matrix stored on disk in Harwell-Boeing format. */
-		dreadhb_dist(iam, fp, &m, &n, &nnz, &nzval, &rowind, &colptr);
-	}else if(!strcmp(postfix,"mtx")){
-		/* Read the matrix stored on disk in Matrix Market format. */
-		dreadMM_dist(fp, &m, &n, &nnz, &nzval, &rowind, &colptr);
-	}else if(!strcmp(postfix,"rb")){
-		/* Read the matrix stored on disk in Rutherford-Boeing format. */
-		dreadrb_dist(iam, fp, &m, &n, &nnz, &nzval, &rowind, &colptr);		
-	}else if(!strcmp(postfix,"dat")){
-		/* Read the matrix stored on disk in triplet format. */
-		dreadtriple_dist(fp, &m, &n, &nnz, &nzval, &rowind, &colptr);
-	}else if(!strcmp(postfix,"bin")){
-		/* Read the matrix stored on disk in binary format. */
-		dread_binary(fp, &m, &n, &nnz, &nzval, &rowind, &colptr);		
-	}else {
-		ABORT("File format not known");
-	}
+        /* Read the matrix stored on disk in Harwell-Boeing format. */
+        dreadhb_dist(iam, fp, &m, &n, &nnz, &nzval, &rowind, &colptr);
 
 	printf("Time to read and distribute matrix %.2f\n", 
 	        SuperLU_timer_() - t);  fflush(stdout);
-		
+
 	/* Broadcast matrix A to the other PEs. */
 	MPI_Bcast( &m,     1,   mpi_int_t,  0, grid->comm );
 	MPI_Bcast( &n,     1,   mpi_int_t,  0, grid->comm );
diff --git a/EXAMPLE/dcreate_matrix_perturbed.c b/EXAMPLE/dcreate_matrix_perturbed.c
index 4c6ae01..d4ea5e1 100644
--- a/EXAMPLE/dcreate_matrix_perturbed.c
+++ b/EXAMPLE/dcreate_matrix_perturbed.c
@@ -228,192 +228,3 @@ int dcreate_matrix_perturbed(SuperMatrix *A, int nrhs, double **rhs,
 #endif
     return 0;
 }
-
-
-
-int dcreate_matrix_perturbed_postfix(SuperMatrix *A, int nrhs, double **rhs,
-                   int *ldb, double **x, int *ldx,
-                   FILE *fp, char *postfix, gridinfo_t *grid)
-{
-    SuperMatrix GA;              /* global A */
-    double   *b_global, *xtrue_global;  /* replicated on all processes */
-    int_t    *rowind, *colptr;	 /* global */
-    double   *nzval;             /* global */
-    double   *nzval_loc;         /* local */
-    int_t    *colind, *rowptr;	 /* local */
-    int_t    m, n, nnz;
-    int_t    m_loc, fst_row, nnz_loc;
-    int_t    m_loc_fst; /* Record m_loc of the first p-1 processors,
-			   when mod(m, p) is not zero. */ 
-    int_t    row, col, i, j, relpos;
-    int      iam;
-    char     trans[1];
-    int_t      *marker;
-
-    iam = grid->iam;
-
-#if ( DEBUGlevel>=1 )
-    CHECK_MALLOC(iam, "Enter dcreate_matrix()");
-#endif
-
-    if ( !iam ) {
-    double t = SuperLU_timer_();       
-	if(!strcmp(postfix,"rua")){
-		/* Read the matrix stored on disk in Harwell-Boeing format. */
-		dreadhb_dist(iam, fp, &m, &n, &nnz, &nzval, &rowind, &colptr);
-	}else if(!strcmp(postfix,"mtx")){
-		/* Read the matrix stored on disk in Matrix Market format. */
-		dreadMM_dist(fp, &m, &n, &nnz, &nzval, &rowind, &colptr);
-	}else if(!strcmp(postfix,"rb")){
-		/* Read the matrix stored on disk in Rutherford-Boeing format. */
-		dreadrb_dist(iam, fp, &m, &n, &nnz, &nzval, &rowind, &colptr);		
-	}else if(!strcmp(postfix,"dat")){
-		/* Read the matrix stored on disk in triplet format. */
-		dreadtriple_dist(fp, &m, &n, &nnz, &nzval, &rowind, &colptr);
-	}else if(!strcmp(postfix,"bin")){
-		/* Read the matrix stored on disk in binary format. */
-		dread_binary(fp, &m, &n, &nnz, &nzval, &rowind, &colptr);		
-	}else {
-		ABORT("File format not known");
-	}
-
-	printf("Time to read and distribute matrix %.2f\n", 
-	        SuperLU_timer_() - t);  fflush(stdout);
-
-	/* Broadcast matrix A to the other PEs. */
-	MPI_Bcast( &m,     1,   mpi_int_t,  0, grid->comm );
-	MPI_Bcast( &n,     1,   mpi_int_t,  0, grid->comm );
-	MPI_Bcast( &nnz,   1,   mpi_int_t,  0, grid->comm );
-	MPI_Bcast( nzval,  nnz, MPI_DOUBLE, 0, grid->comm );
-	MPI_Bcast( rowind, nnz, mpi_int_t,  0, grid->comm );
-	MPI_Bcast( colptr, n+1, mpi_int_t,  0, grid->comm );
-    } else {
-	/* Receive matrix A from PE 0. */
-	MPI_Bcast( &m,   1,   mpi_int_t,  0, grid->comm );
-	MPI_Bcast( &n,   1,   mpi_int_t,  0, grid->comm );
-	MPI_Bcast( &nnz, 1,   mpi_int_t,  0, grid->comm );
-
-	/* Allocate storage for compressed column representation. */
-	dallocateA_dist(n, nnz, &nzval, &rowind, &colptr);
-
-	MPI_Bcast( nzval,   nnz, MPI_DOUBLE, 0, grid->comm );
-	MPI_Bcast( rowind,  nnz, mpi_int_t,  0, grid->comm );
-	MPI_Bcast( colptr,  n+1, mpi_int_t,  0, grid->comm );
-    }
-
-    /* Perturbed the 1st and last diagonal of the matrix to lower
-       values. Intention is to change perm_r[].   */
-    nzval[0] *= 0.01;
-    nzval[nnz-1] *= 0.0001; 
-
-    /* Compute the number of rows to be distributed to local process */
-    m_loc = m / (grid->nprow * grid->npcol); 
-    m_loc_fst = m_loc;
-    /* When m / procs is not an integer */
-    if ((m_loc * grid->nprow * grid->npcol) != m) {
-        /*m_loc = m_loc+1;
-          m_loc_fst = m_loc;*/
-      if (iam == (grid->nprow * grid->npcol - 1)) /* last proc. gets all*/
-	  m_loc = m - m_loc * (grid->nprow * grid->npcol - 1);
-    }
-
-    /* Create compressed column matrix for GA. */
-    dCreate_CompCol_Matrix_dist(&GA, m, n, nnz, nzval, rowind, colptr,
-				SLU_NC, SLU_D, SLU_GE);
-
-    /* Generate the exact solution and compute the right-hand side. */
-    if ( !(b_global = doubleMalloc_dist(m*nrhs)) )
-        ABORT("Malloc fails for b[]");
-    if ( !(xtrue_global = doubleMalloc_dist(n*nrhs)) )
-        ABORT("Malloc fails for xtrue[]");
-    *trans = 'N';
-
-    dGenXtrue_dist(n, nrhs, xtrue_global, n);
-    dFillRHS_dist(trans, nrhs, xtrue_global, n, &GA, b_global, m);
-
-    /*************************************************
-     * Change GA to a local A with NR_loc format     *
-     *************************************************/
-
-    rowptr = (int_t *) intMalloc_dist(m_loc+1);
-    marker = (int_t *) intCalloc_dist(n);
-
-    /* Get counts of each row of GA */
-    for (i = 0; i < n; ++i)
-      for (j = colptr[i]; j < colptr[i+1]; ++j) ++marker[rowind[j]];
-    /* Set up row pointers */
-    rowptr[0] = 0;
-    fst_row = iam * m_loc_fst;
-    nnz_loc = 0;
-    for (j = 0; j < m_loc; ++j) {
-      row = fst_row + j;
-      rowptr[j+1] = rowptr[j] + marker[row];
-      marker[j] = rowptr[j];
-    }
-    nnz_loc = rowptr[m_loc];
-
-    nzval_loc = (double *) doubleMalloc_dist(nnz_loc);
-    colind = (int_t *) intMalloc_dist(nnz_loc);
-
-    /* Transfer the matrix into the compressed row storage */
-    for (i = 0; i < n; ++i) {
-      for (j = colptr[i]; j < colptr[i+1]; ++j) {
-	row = rowind[j];
-	if ( (row>=fst_row) && (row<fst_row+m_loc) ) {
-	  row = row - fst_row;
-	  relpos = marker[row];
-	  colind[relpos] = i;
-	  nzval_loc[relpos] = nzval[j];
-	  ++marker[row];
-	}
-      }
-    }
-
-#if ( DEBUGlevel>=2 )
-    if ( !iam ) dPrint_CompCol_Matrix_dist(&GA);
-#endif   
-
-    /* Destroy GA */
-    Destroy_CompCol_Matrix_dist(&GA);
-
-    /******************************************************/
-    /* Change GA to a local A with NR_loc format */
-    /******************************************************/
-
-    /* Set up the local A in NR_loc format */
-    dCreate_CompRowLoc_Matrix_dist(A, m, n, nnz_loc, m_loc, fst_row,
-				   nzval_loc, colind, rowptr,
-				   SLU_NR_loc, SLU_D, SLU_GE);
-    
-    /* Get the local B */
-    if ( !((*rhs) = doubleMalloc_dist(m_loc*nrhs)) )
-        ABORT("Malloc fails for rhs[]");
-    for (j =0; j < nrhs; ++j) {
-	for (i = 0; i < m_loc; ++i) {
-	    row = fst_row + i;
-	    (*rhs)[j*m_loc+i] = b_global[j*n+row];
-	}
-    }
-    *ldb = m_loc;
-
-    /* Set the true X */    
-    *ldx = m_loc;
-    if ( !((*x) = doubleMalloc_dist(*ldx * nrhs)) )
-        ABORT("Malloc fails for x_loc[]");
-
-    /* Get the local part of xtrue_global */
-    for (j = 0; j < nrhs; ++j) {
-      for (i = 0; i < m_loc; ++i)
-	(*x)[i + j*(*ldx)] = xtrue_global[i + fst_row + j*n];
-    }
-
-    SUPERLU_FREE(b_global);
-    SUPERLU_FREE(xtrue_global);
-    SUPERLU_FREE(marker);
-
-#if ( DEBUGlevel>=1 )
-    printf("sizeof(NRforamt_loc) %lu\n", sizeof(NRformat_loc));
-    CHECK_MALLOC(iam, "Exit dcreate_matrix()");
-#endif
-    return 0;
-}
\ No newline at end of file
diff --git a/EXAMPLE/pddrive.c b/EXAMPLE/pddrive.c
index a83ac9a..7d6073a 100644
--- a/EXAMPLE/pddrive.c
+++ b/EXAMPLE/pddrive.c
@@ -22,7 +22,7 @@ at the top-level directory.
  */
 
 #include <math.h>
-#include <superlu_dist_config.h>								
+#include <superlu_dist_config.h>
 #include "superlu_ddefs.h"
 
 /*! \brief
@@ -62,30 +62,18 @@ int main(int argc, char *argv[])
     int    m, n;
     int      nprow, npcol;
     int      iam, info, ldb, ldx, nrhs;
-    char     **cpp, c, *postfix;
+    char     **cpp, c;
     FILE *fp, *fopen();
     int cpp_defs();
-	int ii;
-	int omp_mpi_level;
-	
+
     nprow = 1;  /* Default process rows.      */
     npcol = 1;  /* Default process columns.   */
-    nrhs =1;   /* Number of right-hand side. */
- 
+    nrhs = 1;   /* Number of right-hand side. */
+
     /* ------------------------------------------------------------
        INITIALIZE MPI ENVIRONMENT. 
        ------------------------------------------------------------*/
     MPI_Init( &argc, &argv );
-    //MPI_Init_thread( &argc, &argv, MPI_THREAD_MULTIPLE, &omp_mpi_level); 
-	
-
-#if ( VAMPIR>=1 )
-    VT_traceoff(); 
-#endif
-
-#if ( VTUNE>=1 )
-	__itt_pause();
-#endif
 
     /* Parse command line argv[]. */
     for (cpp = argv+1; *cpp; ++cpp) {
@@ -117,28 +105,6 @@ int main(int argc, char *argv[])
        ------------------------------------------------------------*/
     superlu_gridinit(MPI_COMM_WORLD, nprow, npcol, &grid);
 
-	if(grid.iam==0){
-	MPI_Query_thread(&omp_mpi_level);
-    switch (omp_mpi_level) {
-      case MPI_THREAD_SINGLE:
-		printf("MPI_Query_thread with MPI_THREAD_SINGLE\n");
-		fflush(stdout);
-	break;
-      case MPI_THREAD_FUNNELED:
-		printf("MPI_Query_thread with MPI_THREAD_FUNNELED\n");
-		fflush(stdout);
-	break;
-      case MPI_THREAD_SERIALIZED:
-		printf("MPI_Query_thread with MPI_THREAD_SERIALIZED\n");
-		fflush(stdout);
-	break;
-      case MPI_THREAD_MULTIPLE:
-		printf("MPI_Query_thread with MPI_THREAD_MULTIPLE\n");
-		fflush(stdout);
-	break;
-    }
-	}
-
     /* Bail out if I do not belong in the grid. */
     iam = grid.iam;
     if ( iam >= nprow * npcol )	goto out;
@@ -160,18 +126,10 @@ int main(int argc, char *argv[])
     CHECK_MALLOC(iam, "Enter main()");
 #endif
 
-	for(ii = 0;ii<strlen(*cpp);ii++){
-		if((*cpp)[ii]=='.'){
-			postfix = &((*cpp)[ii+1]);
-		}
-	}
-	// printf("%s\n", postfix);
-	
-
     /* ------------------------------------------------------------
        GET THE MATRIX FROM FILE AND SETUP THE RIGHT HAND SIDE. 
        ------------------------------------------------------------*/
-    dcreate_matrix_postfix(&A, nrhs, &b, &ldb, &xtrue, &ldx, fp, postfix, &grid);
+    dcreate_matrix(&A, nrhs, &b, &ldb, &xtrue, &ldx, fp, &grid);
 
     if ( !(berr = doubleMalloc_dist(nrhs)) )
 	ABORT("Malloc fails for berr[].");
@@ -196,20 +154,12 @@ int main(int argc, char *argv[])
     set_default_options_dist(&options);
 #if 0
     options.ColPerm = PARMETIS;
-    options.ParSymbFact = YES;							  
+    options.ParSymbFact = YES;
     options.RowPerm = NOROWPERM;
     options.IterRefine = NOREFINE;
     options.Equil = NO; 
 #endif
 
-// //	options.ParSymbFact       = YES;
-// //	options.ColPerm           = PARMETIS;
-// //	options.RowPerm = NOROWPERM;
-	// options.IterRefine       = 0;
-// //	options.DiagInv       = YES;
-	// options.ReplaceTinyPivot = NO;	
-	// options.SymPattern = YES;						  
-	
     if (!iam) {
 	print_sp_ienv_dist(&options);
 	print_options_dist(&options);
diff --git a/EXAMPLE/pddrive1.c b/EXAMPLE/pddrive1.c
index cf43cd9..06c6329 100644
--- a/EXAMPLE/pddrive1.c
+++ b/EXAMPLE/pddrive1.c
@@ -22,7 +22,7 @@ at the top-level directory.
  */
 
 #include <math.h>
-#include <superlu_dist_config.h>								
+#include <superlu_dist_config.h>
 #include "superlu_ddefs.h"
 
 /*! \brief
@@ -56,10 +56,10 @@ int main(int argc, char *argv[])
     gridinfo_t grid;
     double   *berr;
     double   *b, *xtrue, *b1;
-    int    i, j, m, n, ii;
+    int    i, j, m, n;
     int    nprow, npcol;
     int    iam, info, ldb, ldx, nrhs;
-    char     **cpp, c, *postfix;
+    char     **cpp, c;
     FILE *fp, *fopen();
     int cpp_defs();
 
@@ -123,19 +123,11 @@ int main(int argc, char *argv[])
     CHECK_MALLOC(iam, "Enter main()");
 #endif
 
-	for(ii = 0;ii<strlen(*cpp);ii++){
-		if((*cpp)[ii]=='.'){
-			postfix = &((*cpp)[ii+1]);
-		}
-	}	
-	// printf("%s\n", postfix);
-	 
     /* ------------------------------------------------------------
        GET THE MATRIX FROM FILE AND SETUP THE RIGHT HAND SIDE. 
        ------------------------------------------------------------*/
-    dcreate_matrix_postfix(&A, nrhs, &b, &ldb, &xtrue, &ldx, fp, postfix, &grid);
-	
-	if ( !(b1 = doubleMalloc_dist(ldb * nrhs)) )
+    dcreate_matrix(&A, nrhs, &b, &ldb, &xtrue, &ldx, fp, &grid);
+    if ( !(b1 = doubleMalloc_dist(ldb * nrhs)) )
         ABORT("Malloc fails for b1[]");
     for (j = 0; j < nrhs; ++j)
         for (i = 0; i < ldb; ++i) b1[i+j*ldb] = b[i+j*ldb];
@@ -160,7 +152,7 @@ int main(int argc, char *argv[])
         options.PrintStat = YES;
      */
     set_default_options_dist(&options);
-    printf("options.ColPerm = %d\n", options.ColPerm);													  
+    printf("options.ColPerm = %d\n", options.ColPerm);
 
     if (!iam) {
 	print_sp_ienv_dist(&options);
diff --git a/EXAMPLE/pddrive2.c b/EXAMPLE/pddrive2.c
index 7e45dcf..c79a8a6 100644
--- a/EXAMPLE/pddrive2.c
+++ b/EXAMPLE/pddrive2.c
@@ -23,7 +23,7 @@ at the top-level directory.
  */
 
 #include <math.h>
-#include <superlu_dist_config.h>								
+#include <superlu_dist_config.h>
 #include "superlu_ddefs.h"
 
 /*! \brief
@@ -60,20 +60,17 @@ int main(int argc, char *argv[])
     double   *berr;
     double   *b, *b1, *xtrue, *xtrue1;
     int_t    *colind, *colind1, *rowptr, *rowptr1;
-    int_t    i, j, ii, m, n, nnz_loc, m_loc;
+    int_t    i, j, m, n, nnz_loc, m_loc;
     int      nprow, npcol;
     int      iam, info, ldb, ldx, nrhs;
-    char     **cpp, c, *postfix;
+    char     **cpp, c;
     FILE *fp, *fopen();
     int cpp_defs();
 
     /* prototypes */
     extern int dcreate_matrix_perturbed
         (SuperMatrix *, int, double **, int *, double **, int *,
-         FILE *, gridinfo_t *);    
-	extern int dcreate_matrix_perturbed_postfix
-        (SuperMatrix *, int, double **, int *, double **, int *,
-         FILE *, char *, gridinfo_t *);
+         FILE *, gridinfo_t *);
 
     nprow = 1;  /* Default process rows.      */
     npcol = 1;  /* Default process columns.   */
@@ -131,17 +128,10 @@ int main(int argc, char *argv[])
     CHECK_MALLOC(iam, "Enter main()");
 #endif
 
-	for(ii = 0;ii<strlen(*cpp);ii++){
-		if((*cpp)[ii]=='.'){
-			postfix = &((*cpp)[ii+1]);
-		}
-	}	
-	// printf("%s\n", postfix);
-	 
     /* ------------------------------------------------------------
-       GET THE MATRIX FROM FILE AND SETUP THE RIGHT HAND SIDE. 
+       GET THE MATRIX FROM FILE AND SETUP THE RIGHT-HAND SIDE. 
        ------------------------------------------------------------*/
-    dcreate_matrix_postfix(&A, nrhs, &b, &ldb, &xtrue, &ldx, fp, postfix, &grid);
+    dcreate_matrix(&A, nrhs, &b, &ldb, &xtrue, &ldx, fp, &grid);
 
     if ( !(berr = doubleMalloc_dist(nrhs)) )
 	ABORT("Malloc fails for berr[].");
@@ -213,7 +203,7 @@ int main(int argc, char *argv[])
     /* Get the matrix from file, perturbed some diagonal entries to force
        a different perm_r[]. Set up the right-hand side.   */
     if ( !(fp = fopen(*cpp, "r")) ) ABORT("File does not exist");
-    dcreate_matrix_perturbed_postfix(&A, nrhs, &b1, &ldb, &xtrue1, &ldx, fp, postfix, &grid);
+    dcreate_matrix_perturbed(&A, nrhs, &b1, &ldb, &xtrue1, &ldx, fp, &grid);
 
     PStatInit(&stat); /* Initialize the statistics variables. */
 
diff --git a/EXAMPLE/pddrive3.c b/EXAMPLE/pddrive3.c
index f2e40aa..11c80fe 100644
--- a/EXAMPLE/pddrive3.c
+++ b/EXAMPLE/pddrive3.c
@@ -22,7 +22,7 @@ at the top-level directory.
  */
 
 #include <math.h>
-#include <superlu_dist_config.h>								
+#include <superlu_dist_config.h>
 #include "superlu_ddefs.h"
 
 /*! \brief
@@ -65,10 +65,10 @@ int main(int argc, char *argv[])
     double   *berr;
     double   *b, *b1, *xtrue, *nzval, *nzval1;
     int_t    *colind, *colind1, *rowptr, *rowptr1;
-    int_t    i, j, ii, m, n, nnz_loc, m_loc, fst_row;
+    int_t    i, j, m, n, nnz_loc, m_loc, fst_row;
     int      nprow, npcol;
     int      iam, info, ldb, ldx, nrhs;
-    char     **cpp, c, *postfix;;
+    char     **cpp, c;
     FILE *fp, *fopen();
     int cpp_defs();
 
@@ -128,17 +128,10 @@ int main(int argc, char *argv[])
     CHECK_MALLOC(iam, "Enter main()");
 #endif
 
-	for(ii = 0;ii<strlen(*cpp);ii++){
-		if((*cpp)[ii]=='.'){
-			postfix = &((*cpp)[ii+1]);
-		}
-	}	
-	// printf("%s\n", postfix);
-	 
     /* ------------------------------------------------------------
        GET THE MATRIX FROM FILE AND SETUP THE RIGHT HAND SIDE. 
        ------------------------------------------------------------*/
-    dcreate_matrix_postfix(&A, nrhs, &b, &ldb, &xtrue, &ldx, fp, postfix, &grid);
+    dcreate_matrix(&A, nrhs, &b, &ldb, &xtrue, &ldx, fp, &grid);
 
     if ( !(b1 = doubleMalloc_dist(ldb * nrhs)) )
         ABORT("Malloc fails for b1[]");
diff --git a/EXAMPLE/pddrive4.c b/EXAMPLE/pddrive4.c
index 82a0b14..1a03add 100644
--- a/EXAMPLE/pddrive4.c
+++ b/EXAMPLE/pddrive4.c
@@ -59,12 +59,12 @@ int main(int argc, char *argv[])
     double   *berr;
     double   *a, *b, *xtrue;
     int_t    *asub, *xa;
-    int_t    i, j, ii, m, n;
+    int_t    i, j, m, n;
     int      nprow, npcol, ldumap, p;
     int_t    usermap[6];
     int      iam, info, ldb, ldx, nprocs;
     int      nrhs = 1;   /* Number of right-hand side. */
-    char     **cpp, c, *postfix;;
+    char     **cpp, c;
     FILE *fp, *fopen();
     int cpp_defs();
 
@@ -134,24 +134,14 @@ int main(int argc, char *argv[])
     CHECK_MALLOC(iam, "Enter main()");
 #endif
 
-
-	for(ii = 0;ii<strlen(*cpp);ii++){
-		if((*cpp)[ii]=='.'){
-			postfix = &((*cpp)[ii+1]);
-		}
-	}
-	// printf("%s\n", postfix);
-	
-
     if ( iam >= 0 && iam < 6 ) { /* I am in grid 1. */
 	iam = grid1.iam;  /* Get the logical number in the new grid. */
 
         /* ------------------------------------------------------------
            GET THE MATRIX FROM FILE AND SETUP THE RIGHT HAND SIDE. 
            ------------------------------------------------------------*/
-		dcreate_matrix_postfix(&A, nrhs, &b, &ldb, &xtrue, &ldx, fp, postfix, &grid1);
-
-		
+        dcreate_matrix(&A, nrhs, &b, &ldb, &xtrue, &ldx, fp, &grid1);
+	
 	if ( !(berr = doubleMalloc_dist(nrhs)) )
 	    ABORT("Malloc fails for berr[].");
 
@@ -220,7 +210,7 @@ int main(int argc, char *argv[])
         /* ------------------------------------------------------------
            GET THE MATRIX FROM FILE AND SETUP THE RIGHT HAND SIDE. 
            ------------------------------------------------------------*/
-		dcreate_matrix_postfix(&A, nrhs, &b, &ldb, &xtrue, &ldx, fp, postfix, &grid2);
+        dcreate_matrix(&A, nrhs, &b, &ldb, &xtrue, &ldx, fp, &grid2);
 
 	if ( !(berr = doubleMalloc_dist(nrhs)) )
 	    ABORT("Malloc fails for berr[].");
diff --git a/EXAMPLE/sp_ienv.c b/EXAMPLE/sp_ienv.c
index 4173ee7..08d1e8f 100644
--- a/EXAMPLE/sp_ienv.c
+++ b/EXAMPLE/sp_ienv.c
@@ -35,7 +35,7 @@ at the top-level directory.
 
     Arguments   
     =========   
- 
+
     ISPEC   (input) int
             Specifies the parameter to be returned as the value of SP_IENV_DIST.   
             = 1: the panel size w; a panel consists of w consecutive
@@ -62,9 +62,11 @@ at the top-level directory.
 </pre>
 */
 
+
 #include <stdlib.h>
 #include <stdio.h>
 
+
 int_t
 sp_ienv_dist(int_t ispec)
 {
@@ -98,7 +100,7 @@ sp_ienv_dist(int_t ispec)
                 return(atoi(ttemp));
             }
             else
-            return 128;	
+            return 128;
 
 #endif
         case 6: 
diff --git a/SRC/CMakeLists.txt b/SRC/CMakeLists.txt
index 400921d..88c0d75 100644
--- a/SRC/CMakeLists.txt
+++ b/SRC/CMakeLists.txt
@@ -9,11 +9,6 @@ set(headers
     supermatrix.h
     util_dist.h
     colamd.h
-    environment.hpp
-    TreeBcast_v2.hpp
-	TreeReduce_v2.hpp	
-    TreeBcast_v2_impl.hpp
-    TreeReduce_v2_impl.hpp	
     ${CMAKE_CURRENT_BINARY_DIR}/superlu_dist_config.h
 )
 if (MSVC)
@@ -22,8 +17,6 @@ endif ()
 
 # first: precision-independent files
 set(sources
-  global.cpp
-  TreeInterface.cpp
   sp_ienv.c
   etree.c 
   sp_colorder.c
@@ -45,7 +38,7 @@ set(sources
   smach_dist.c
   dmach_dist.c
   colamd.c
-  superlu_dist_version.c						
+  superlu_dist_version.c
 )
 if (MSVC)
   list(APPEND sources wingetopt.c)
@@ -69,7 +62,6 @@ if(enable_double)
     dreadhb.c
     dreadrb.c
     dreadtriple.c
-	dbinary_io.c	
     dreadMM.c
     pdgsequ.c
     pdlaqgs.c
@@ -175,4 +167,4 @@ install(TARGETS ${targets}
 install(FILES ${headers}
 # DESTINATION ${CMAKE_INSTALL_PREFIX}/include)
   DESTINATION ${CMAKE_INSTALL_INCLUDEDIR}
-)
\ No newline at end of file
+)
diff --git a/SRC/Cnames.h b/SRC/Cnames.h
index 792f514..b446c46 100644
--- a/SRC/Cnames.h
+++ b/SRC/Cnames.h
@@ -148,7 +148,6 @@ at the top-level directory.
 #define dtrsv_    DTRSV
 #define dgemm_    DGEMM
 #define dtrsm_    DTRSM
-#define dtrtri_   DTRTRI
 
 #define scasum_   SCASUM
 #define icamax_   ICAMAX
@@ -276,7 +275,6 @@ at the top-level directory.
 #define dtrsv_    dtrsv
 #define dgemm_    dgemm
 #define dtrsm_    dtrsm
-#define dtrtri_   dtrtri
 
 #define scasum_   scasum
 #define icamax_   icamax
diff --git a/SRC/Makefile b/SRC/Makefile
index eb0c002..532274e 100644
--- a/SRC/Makefile
+++ b/SRC/Makefile
@@ -29,7 +29,7 @@ include ../make.inc
 #
 # Precision independent routines
 #
-ALLAUX 	= global.o TreeInterface.o sp_ienv.o etree.o sp_colorder.o get_perm_c.o \
+ALLAUX 	= sp_ienv.o etree.o sp_colorder.o get_perm_c.o \
 	  colamd.o mmd.o comm.o memory.o util.o superlu_grid.o \
 	  pxerr_dist.o superlu_timer.o symbfact.o \
 	  psymbfact.o psymbfact_util.o get_perm_c_parmetis.o mc64ad_dist.o \
@@ -52,7 +52,7 @@ ZSLUSRC	= dcomplex_dist.o zlangs_dist.o zgsequ_dist.o zlaqgs_dist.o \
 #
 # Routines for double precision parallel SuperLU
 DPLUSRC = pdgssvx.o pdgssvx_ABglobal.o \
-	  dreadhb.o dreadrb.o dreadtriple.o dreadMM.o dbinary_io.o \
+	  dreadhb.o dreadrb.o dreadtriple.o dreadMM.o \
 	  pdgsequ.o pdlaqgs.o dldperm_dist.o pdlangs.o pdutil.o \
 	  pdsymbfact_distdata.o ddistribute.o pddistribute.o \
 	  pdgstrf.o pdgstrf2.o pdGetDiagU.o \
@@ -98,9 +98,6 @@ pzgstrf.o: zscatter.c zlook_ahead_update.c zSchCompUdt-2Ddynamic.c pzgstrf.c
 .c.o:
 	$(CC) $(CFLAGS) $(CDEFS) $(BLASDEF) -c $< $(VERBOSE)
 
-.cpp.o:
-	$(CPP) $(CPPFLAGS) $(CDEFS) $(BLASDEF) -c $< $(VERBOSE)
-
 .f.o:
 	$(FORTRAN) $(FFLAGS) -c $< $(VERBOSE)
 
diff --git a/SRC/TreeBcast_v2.hpp b/SRC/TreeBcast_v2.hpp
deleted file mode 100644
index 68f95ea..0000000
--- a/SRC/TreeBcast_v2.hpp
+++ /dev/null
@@ -1,149 +0,0 @@
-#ifndef _PEXSI_TREE_V2_HPP_
-#define _PEXSI_TREE_V2_HPP_
-
-#include "environment.hpp"
-// #include "blas.hpp"
-// #include "timer.h"
-#include "superlu_defs.h"
-
-
-#include <vector>
-#include <list>
-#include <map>
-#include <algorithm>
-#include <string>
-#include <memory>
-//#include <random>
-
-// options to switch from a flat bcast/reduce tree to a binary tree
-#ifndef FTREE_LIMIT
-#define FTREE_LIMIT 8
-#endif
-
-
-
-namespace ASYNCOMM{
-
-
-  extern std::map< MPI_Comm , std::vector<int> > commGlobRanks;
-
-
-
-  template< typename T>
-    class TreeBcast_v2{
-      protected:
-        std::vector<MPI_Request> recvRequests_;
-        std::vector<MPI_Status> recvStatuses_;
-        std::vector<int> recvDoneIdx_;
-        std::vector<T *> recvDataPtrs_;
-        std::vector<T> recvTempBuffer_;
-        Int recvPostedCount_;
-        Int recvCount_;
-
-        std::vector<MPI_Request> sendRequests_;
-        std::vector<MPI_Status> sendStatuses_;
-        std::vector<int> sendDoneIdx_;
-        std::vector<T *> sendDataPtrs_;
-        std::vector<T> sendTempBuffer_;
-        Int sendPostedCount_;
-        Int sendCount_;
-
-        bool done_;
-        bool fwded_;
-        bool isReady_;
-
-        MPI_Comm comm_;
-        Int myRoot_;
-        //not sure about this one
-        Int mainRoot_;
-        std::vector<Int> myDests_;
-
-        Int myRank_;
-        Int msgSize_;
-        Int tag_;
-
-        MPI_Datatype type_;
-
-
-      protected:
-        virtual void buildTree(Int * ranks, Int rank_cnt)=0;
-
-
-      public:
-        static TreeBcast_v2<T> * Create(const MPI_Comm & pComm, Int * ranks, Int rank_cnt, Int msgSize,double rseed);
-        TreeBcast_v2();
-        TreeBcast_v2(const MPI_Comm & pComm, Int * ranks, Int rank_cnt,Int msgSize);
-        TreeBcast_v2(const TreeBcast_v2 & Tree);
-        virtual ~TreeBcast_v2();
-        virtual TreeBcast_v2 * clone() const = 0; 
-        virtual void Copy(const TreeBcast_v2 & Tree);
-        virtual void Reset();
-
-
-        virtual inline Int GetNumMsgToRecv();
-        virtual inline Int GetNumRecvMsg();
-        virtual inline Int GetNumMsgToSend();
-        virtual inline Int GetNumSendMsg();
-        inline void SetDataReady(bool rdy);
-        inline void SetTag(Int tag);
-        inline Int GetTag();
-        Int * GetDests();
-        Int GetDest(Int i);
-        Int GetDestCount();
-        Int GetRoot();
-        bool IsRoot();
-        void SetMsgSize(Int msgSize){ this->msgSize_ = msgSize;}
-        Int GetMsgSize();
-        bool IsReady(){ return this->isReady_;}
-
-        //async wait and forward
-		virtual void AllocateBuffer();															
-
-
-        virtual void cleanupBuffers();
-
-		virtual void allocateRequest();
-		virtual void forwardMessageSimple(T * locBuffer);	
-		virtual void waitSendRequest();	
-
-    };
-
-
-  template< typename T>
-    class FTreeBcast2: public TreeBcast_v2<T>{
-      protected:
-        virtual void buildTree(Int * ranks, Int rank_cnt);
-
-      public:
-        FTreeBcast2(const MPI_Comm & pComm, Int * ranks, Int rank_cnt, Int msgSize);
-        virtual FTreeBcast2<T> * clone() const;
-    };
-
-  template< typename T>
-    class BTreeBcast2: public TreeBcast_v2<T>{
-      protected:
-        virtual void buildTree(Int * ranks, Int rank_cnt);
-
-      public:
-        BTreeBcast2(const MPI_Comm & pComm, Int * ranks, Int rank_cnt, Int msgSize);
-        virtual BTreeBcast2<T> * clone() const;
-    };
-
-  template< typename T>
-    class ModBTreeBcast2: public TreeBcast_v2<T>{
-      protected:
-        double rseed_;
-        virtual void buildTree(Int * ranks, Int rank_cnt);
-
-      public:
-        ModBTreeBcast2(const MPI_Comm & pComm, Int * ranks, Int rank_cnt, Int msgSize, double rseed);
-        virtual ModBTreeBcast2<T> * clone() const;
-    };
-
-
-
-
-}//namespace ASYNCOMM
-
-#include "TreeBcast_v2_impl.hpp"
-#endif
diff --git a/SRC/TreeBcast_v2_impl.hpp b/SRC/TreeBcast_v2_impl.hpp
deleted file mode 100644
index 6ca7f63..0000000
--- a/SRC/TreeBcast_v2_impl.hpp
+++ /dev/null
@@ -1,526 +0,0 @@
-#ifndef _PEXSI_TREE_IMPL_V2_HPP_
-#define _PEXSI_TREE_IMPL_V2_HPP_
-
-// #define CHECK_MPI_ERROR
-
-// #include "TreeBcast_v2.hpp"
-
-
-namespace ASYNCOMM{
-
-
-  template< typename T> 
-    TreeBcast_v2<T>::TreeBcast_v2(){
-      comm_ = MPI_COMM_NULL;
-      myRank_=-1;
-      myRoot_ = -1; 
-      msgSize_ = -1;
-      recvCount_ = -1;
-      sendCount_ = -1;
-      recvPostedCount_ = -1;
-      sendPostedCount_ = -1;
-      tag_=-1;
-      mainRoot_=-1;
-      isReady_ = false;
-      recvDataPtrs_.assign(1,NULL);
-      recvRequests_.assign(1,MPI_REQUEST_NULL);
-      fwded_=false;
-      done_ = false;
-
-
-      MPI_Type_contiguous( sizeof(T), MPI_BYTE, &type_ );
-      MPI_Type_commit( &type_ );
-
-    }
-
-
-  template< typename T> 
-    TreeBcast_v2<T>::TreeBcast_v2(const MPI_Comm & pComm, Int * ranks, Int rank_cnt,Int msgSize):TreeBcast_v2(){
-      comm_ = pComm;
-      MPI_Comm_rank(comm_,&myRank_);
-      msgSize_ = msgSize;
-      recvCount_ = 0;
-      sendCount_ = 0;
-      recvPostedCount_ = 0;
-      sendPostedCount_ = 0;
-      mainRoot_=ranks[0];
-#ifdef CHECK_MPI_ERROR
-          MPI_Errhandler_set(this->comm_, MPI_ERRORS_RETURN);
-          MPI_Errhandler_set(MPI_COMM_WORLD, MPI_ERRORS_RETURN);
-#endif
-    }
-
-
-  template< typename T> 
-    TreeBcast_v2<T>::TreeBcast_v2(const TreeBcast_v2 & Tree){
-      this->Copy(Tree);
-    }
-
-  template< typename T> 
-    inline void TreeBcast_v2<T>::Copy(const TreeBcast_v2 & Tree){
-      this->comm_ = Tree.comm_;
-      this->myRank_ = Tree.myRank_;
-      this->myRoot_ = Tree.myRoot_; 
-      this->msgSize_ = Tree.msgSize_;
-
-      this->recvCount_ = Tree.recvCount_;
-      this->sendCount_ = Tree.sendCount_;
-      this->recvPostedCount_ = Tree.recvPostedCount_;
-      this->sendPostedCount_ = Tree.sendPostedCount_;
-      this->tag_= Tree.tag_;
-      this->mainRoot_= Tree.mainRoot_;
-      this->isReady_ = Tree.isReady_;
-      this->myDests_ = Tree.myDests_;
-
-      this->recvRequests_ = Tree.recvRequests_;
-      this->recvTempBuffer_ = Tree.recvTempBuffer_;
-      this->sendRequests_ = Tree.sendRequests_;
-      this->recvDataPtrs_ = Tree.recvDataPtrs_;
-      if(Tree.recvDataPtrs_[0]==(T*)Tree.recvTempBuffer_.data()){
-        this->recvDataPtrs_[0]=(T*)this->recvTempBuffer_.data();
-      }
-
-      this->fwded_= Tree.fwded_;
-      this->done_= Tree.done_;
-    }
-
-  template< typename T> 
-    inline void TreeBcast_v2<T>::Reset(){
-      assert(done_);
-      cleanupBuffers();
-      done_=false;
-      fwded_=false;
-      recvDataPtrs_.assign(GetNumMsgToRecv(),NULL);
-      recvRequests_.assign(GetNumMsgToRecv(),MPI_REQUEST_NULL);
-      sendDataPtrs_.assign(GetNumMsgToSend(),NULL);
-      sendRequests_.assign(GetNumMsgToSend(),MPI_REQUEST_NULL);
-      // isAllocated_=false;
-      isReady_=false;
-      recvCount_ = 0;
-      sendCount_ = 0;
-      recvPostedCount_ = 0;
-      sendPostedCount_ = 0;
-    }
-
-
-  template< typename T> 
-    TreeBcast_v2<T>::~TreeBcast_v2(){
-      cleanupBuffers();
-      MPI_Type_free( &type_ );
-    }
-
-  template< typename T> 
-    inline Int TreeBcast_v2<T>::GetNumRecvMsg(){
-      return recvCount_;
-    }
-  template< typename T> 
-    inline Int TreeBcast_v2<T>::GetNumMsgToSend(){
-      return this->GetDestCount();
-    }
-
-  template< typename T> 
-    inline Int TreeBcast_v2<T>::GetNumMsgToRecv(){
-      return 1;//always one even for root//myRank_==myRoot_?0:1;
-    }
-
-  template< typename T> 
-    inline Int TreeBcast_v2<T>::GetNumSendMsg(){
-      return sendCount_;
-    }
-  template< typename T> 
-    inline void TreeBcast_v2<T>::SetDataReady(bool rdy){ 
-      isReady_=rdy;
-    }
-  template< typename T> 
-    inline void TreeBcast_v2<T>::SetTag(Int tag){
-      tag_ = tag;
-    }
-  template< typename T> 
-    inline Int TreeBcast_v2<T>::GetTag(){
-      return tag_;
-    }
-
-
-  template< typename T> 
-    inline Int * TreeBcast_v2<T>::GetDests(){
-      return &myDests_[0];
-    }
-  template< typename T> 
-    inline Int TreeBcast_v2<T>::GetDest(Int i){
-      return myDests_[i];
-    }
-  template< typename T> 
-    inline Int TreeBcast_v2<T>::GetDestCount(){
-      return this->myDests_.size();
-    }
-  template< typename T> 
-    inline Int TreeBcast_v2<T>::GetRoot(){
-      return this->myRoot_;
-    }
-
-  template< typename T> 
-    inline bool TreeBcast_v2<T>::IsRoot(){
-      return this->myRoot_==this->myRank_;
-    }
-	
-
-  template< typename T> 
-    inline Int TreeBcast_v2<T>::GetMsgSize(){
-      return this->msgSize_;
-    }
-
-	
-	
-  template< typename T> 
-    inline void TreeBcast_v2<T>::forwardMessageSimple(T * locBuffer){
-        MPI_Status status;
-		for( Int idxRecv = 0; idxRecv < this->myDests_.size(); ++idxRecv ){
-          Int iProc = this->myDests_[idxRecv];
-          // Use Isend to send to multiple targets
-          int error_code = MPI_Isend( locBuffer, this->msgSize_, this->type_, 
-              iProc, this->tag_,this->comm_, &this->sendRequests_[idxRecv] );
-			  // MPI_Wait(&this->sendRequests_[idxRecv],&status) ; 
-			  // std::cout<<this->myRank_<<" FWD to "<<iProc<<" on tag "<<this->tag_<<std::endl;
-        } // for (iProc)
-    }	  
-
-  template< typename T> 
-    inline void TreeBcast_v2<T>::waitSendRequest(){
-        MPI_Status status;
-		for( Int idxRecv = 0; idxRecv < this->myDests_.size(); ++idxRecv ){
-			  MPI_Wait(&this->sendRequests_[idxRecv],&status) ; 
-        } // for (iProc)
-    }	
-	
-	
-
- 
-
-  template< typename T> 
-    inline void TreeBcast_v2<T>::allocateRequest(){
-        if(this->sendRequests_.size()!=this->GetDestCount()){
-          this->sendRequests_.assign(this->GetDestCount(),MPI_REQUEST_NULL);
-        }
-    }
-	
-	
-
-	
-	
-	
-  template< typename T> 
-    inline void TreeBcast_v2<T>::cleanupBuffers(){
-      this->recvRequests_.clear();
-      this->recvStatuses_.clear();
-      this->recvDoneIdx_.clear();
-      this->recvDataPtrs_.clear();
-      this->recvTempBuffer_.clear();
-      this->sendRequests_.clear();
-      this->sendStatuses_.clear();
-      this->sendDoneIdx_.clear();
-      this->sendDataPtrs_.clear();
-      this->sendTempBuffer_.clear();
-
-      this->recvRequests_.shrink_to_fit();
-      this->recvStatuses_.shrink_to_fit();
-      this->recvDoneIdx_.shrink_to_fit();
-      this->recvDataPtrs_.shrink_to_fit();
-      this->recvTempBuffer_.shrink_to_fit();
-      this->sendRequests_.shrink_to_fit();
-      this->sendStatuses_.shrink_to_fit();
-      this->sendDoneIdx_.shrink_to_fit();
-      this->sendDataPtrs_.shrink_to_fit();
-      this->sendTempBuffer_.shrink_to_fit();
-	  
-	  this->myDests_.clear();
-	  
-    }
-
-
-  template< typename T> 
-    inline void TreeBcast_v2<T>::AllocateBuffer()
-    {
-
-      if(!this->IsRoot()){
-
-        if(this->recvDataPtrs_[0]==NULL){
-          this->recvTempBuffer_.resize(this->msgSize_);
-          this->recvDataPtrs_[0] = (T*)this->recvTempBuffer_.data();
-        }
-      }
-    }	
-	
-  template< typename T>
-    inline TreeBcast_v2<T> * TreeBcast_v2<T>::Create(const MPI_Comm & pComm, Int * ranks, Int rank_cnt, Int msgSize, double rseed){
-      //get communicator size
-      Int nprocs = 0;
-      MPI_Comm_size(pComm, &nprocs);
-
-#if defined(FTREE)
-      return new FTreeBcast2<T>(pComm,ranks,rank_cnt,msgSize);
-#elif defined(MODBTREE)
-      return new ModBTreeBcast2<T>(pComm,ranks,rank_cnt,msgSize,rseed);
-#elif defined(BTREE)
-      return new BTreeBcast2<T>(pComm,ranks,rank_cnt,msgSize);
-#endif
-
-
-      if(nprocs<=FTREE_LIMIT){
-#if ( _DEBUGlevel_ >= 1 ) || defined(REDUCE_VERBOSE)
-        statusOFS<<"FLAT TREE USED"<<std::endl;
-#endif
-
-        return new FTreeBcast2<T>(pComm,ranks,rank_cnt,msgSize);
-
-      }
-      else{
-#if ( _DEBUGlevel_ >= 1 ) || defined(REDUCE_VERBOSE)
-        statusOFS<<"BINARY TREE USED"<<std::endl;
-#endif
-        // return new ModBTreeBcast2<T>(pComm,ranks,rank_cnt,msgSize, rseed);
-		return new BTreeBcast2<T>(pComm,ranks,rank_cnt,msgSize);
-      }
-    }
-
-
-
-  template< typename T>
-    inline void FTreeBcast2<T>::buildTree(Int * ranks, Int rank_cnt){
-      Int idxStart = 0;
-      Int idxEnd = rank_cnt;
-      this->myRoot_ = ranks[0];
-      if(this->IsRoot() ){
-        this->myDests_.insert(this->myDests_.end(),&ranks[1],&ranks[0]+rank_cnt);
-      }
-#if (defined(BCAST_VERBOSE)) 
-      statusOFS<<"My root is "<<this->myRoot_<<std::endl;
-      statusOFS<<"My dests are ";
-      for(int i =0;i<this->myDests_.size();++i){statusOFS<<this->myDests_[i]<<" ";}
-      statusOFS<<std::endl;
-#endif
-    }
-
-
-
-  template< typename T>
-    FTreeBcast2<T>::FTreeBcast2(const MPI_Comm & pComm, Int * ranks, Int rank_cnt, Int msgSize):TreeBcast_v2<T>(pComm,ranks,rank_cnt,msgSize){
-      //build the binary tree;
-      buildTree(ranks,rank_cnt);
-    }
-
-
-  template< typename T>
-    inline FTreeBcast2<T> * FTreeBcast2<T>::clone() const{
-      FTreeBcast2 * out = new FTreeBcast2(*this);
-      return out;
-    } 
-
-
-
-
-
-
-
-
-  template< typename T>
-    inline BTreeBcast2<T>::BTreeBcast2(const MPI_Comm & pComm, Int * ranks, Int rank_cnt, Int msgSize):TreeBcast_v2<T>(pComm,ranks,rank_cnt,msgSize){
-      //build the binary tree;
-      buildTree(ranks,rank_cnt);
-    }
-
-  template< typename T>
-    inline BTreeBcast2<T> * BTreeBcast2<T>::clone() const{
-      BTreeBcast2<T> * out = new BTreeBcast2<T>(*this);
-      return out;
-    }
-
-
-
-  // template< typename T>
-    // inline void BTreeBcast2<T>::buildTree(Int * ranks, Int rank_cnt){
-
-      // Int idxStart = 0;
-      // Int idxEnd = rank_cnt;
-
-
-
-      // Int prevRoot = ranks[0];
-      // while(idxStart<idxEnd){
-        // Int curRoot = ranks[idxStart];
-        // Int listSize = idxEnd - idxStart;
-
-        // if(listSize == 1){
-          // if(curRoot == this->myRank_){
-            // this->myRoot_ = prevRoot;
-            // break;
-          // }
-        // }
-        // else{
-          // Int halfList = floor(ceil(double(listSize) / 2.0));
-          // Int idxStartL = idxStart+1;
-          // Int idxStartH = idxStart+halfList;
-
-          // if(curRoot == this->myRank_){
-            // if ((idxEnd - idxStartH) > 0 && (idxStartH - idxStartL)>0){
-              // Int childL = ranks[idxStartL];
-              // Int childR = ranks[idxStartH];
-
-              // this->myDests_.push_back(childL);
-              // this->myDests_.push_back(childR);
-            // }
-            // else if ((idxEnd - idxStartH) > 0){
-              // Int childR = ranks[idxStartH];
-              // this->myDests_.push_back(childR);
-            // }
-            // else{
-              // Int childL = ranks[idxStartL];
-              // this->myDests_.push_back(childL);
-            // }
-            // this->myRoot_ = prevRoot;
-            // break;
-          // } 
-
-          // if( this->myRank_ < ranks[idxStartH]){
-            // idxStart = idxStartL;
-            // idxEnd = idxStartH;
-          // }
-          // else{
-            // idxStart = idxStartH;
-          // }
-          // prevRoot = curRoot;
-        // }
-
-      // }
-
-// #if (defined(BCAST_VERBOSE))
-      // statusOFS<<"My root is "<<this->myRoot_<<std::endl;
-      // statusOFS<<"My dests are ";
-      // for(int i =0;i<this->myDests_.size();++i){statusOFS<<this->myDests_[i]<<" ";}
-      // statusOFS<<std::endl;
-// #endif
-    // }
-
-  template< typename T>
-    inline void BTreeBcast2<T>::buildTree(Int * ranks, Int rank_cnt){
-
-      Int myIdx = 0;
-      Int ii=0; 
-	  Int child,root;
-	  for (ii=0;ii<rank_cnt;ii++)
-		  if(this->myRank_ == ranks[ii]){
-			  myIdx = ii;
-			  break;
-		  }
-
-	  if(myIdx*2+1<rank_cnt){
-		   child = ranks[myIdx*2+1];
-           this->myDests_.push_back(child);
-	  }
-	  if(myIdx*2+2<rank_cnt){
-		   child = ranks[myIdx*2+2];
-           this->myDests_.push_back(child);
-	  }	 
-
-	  if(myIdx!=0){
-		  this->myRoot_ = ranks[(Int)floor((double)(myIdx-1.0)/2.0)];
-	  }else{
-		  this->myRoot_ = this->myRank_;
-	  } 
-	  
-    }
-
-
-
-  template< typename T>
-    ModBTreeBcast2<T>::ModBTreeBcast2(const MPI_Comm & pComm, Int * ranks, Int rank_cnt, Int msgSize, double rseed):TreeBcast_v2<T>(pComm,ranks,rank_cnt,msgSize){
-      //build the binary tree;
-      MPI_Comm_rank(this->comm_,&this->myRank_);
-      this->rseed_ = rseed;
-      buildTree(ranks,rank_cnt);
-    }
-
-  template< typename T>
-    inline ModBTreeBcast2<T> * ModBTreeBcast2<T>::clone() const{
-      ModBTreeBcast2 * out = new ModBTreeBcast2(*this);
-      return out;
-    }
-
-  template< typename T>
-    inline void ModBTreeBcast2<T>::buildTree(Int * ranks, Int rank_cnt){
-
-      Int idxStart = 0;
-      Int idxEnd = rank_cnt;
-
-      //sort the ranks with the modulo like operation
-      if(rank_cnt>1){
-        Int new_idx = (Int)this->rseed_ % (rank_cnt - 1) + 1; 
-        Int * new_start = &ranks[new_idx];
-        std::rotate(&ranks[1], new_start, &ranks[0]+rank_cnt);
-      }
-
-      Int prevRoot = ranks[0];
-      while(idxStart<idxEnd){
-        Int curRoot = ranks[idxStart];
-        Int listSize = idxEnd - idxStart;
-
-        if(listSize == 1){
-          if(curRoot == this->myRank_){
-            this->myRoot_ = prevRoot;
-            break;
-          }
-        }
-        else{
-          Int halfList = floor(ceil(double(listSize) / 2.0));
-          Int idxStartL = idxStart+1;
-          Int idxStartH = idxStart+halfList;
-
-          if(curRoot == this->myRank_){
-            if ((idxEnd - idxStartH) > 0 && (idxStartH - idxStartL)>0){
-              Int childL = ranks[idxStartL];
-              Int childR = ranks[idxStartH];
-
-              this->myDests_.push_back(childL);
-              this->myDests_.push_back(childR);
-            }
-            else if ((idxEnd - idxStartH) > 0){
-              Int childR = ranks[idxStartH];
-              this->myDests_.push_back(childR);
-            }
-            else{
-              Int childL = ranks[idxStartL];
-              this->myDests_.push_back(childL);
-            }
-            this->myRoot_ = prevRoot;
-            break;
-          } 
-
-          //not true anymore ?
-          //first half to 
-          TIMER_START(FIND_RANK);
-          Int * pos = std::find(&ranks[idxStartL], &ranks[idxStartH], this->myRank_);
-          TIMER_STOP(FIND_RANK);
-          if( pos != &ranks[idxStartH]){
-            idxStart = idxStartL;
-            idxEnd = idxStartH;
-          }
-          else{
-            idxStart = idxStartH;
-          }
-          prevRoot = curRoot;
-        }
-
-      }
-
-#if (defined(REDUCE_VERBOSE))
-      statusOFS<<"My root is "<<this->myRoot_<<std::endl;
-      statusOFS<<"My dests are ";
-      for(int i =0;i<this->myDests_.size();++i){statusOFS<<this->myDests_[i]<<" ";}
-      statusOFS<<std::endl;
-#endif
-    }
-
-
-} //namespace ASYNCOMM
-
-
-#endif
diff --git a/SRC/TreeInterface.cpp b/SRC/TreeInterface.cpp
deleted file mode 100644
index ecff034..0000000
--- a/SRC/TreeInterface.cpp
+++ /dev/null
@@ -1,159 +0,0 @@
-#include "TreeReduce_v2.hpp"
-
-
-namespace ASYNCOMM{
-	
-	
-	
-#ifdef __cplusplus
-	extern "C" {
-#endif
-
-	BcTree BcTree_Create(MPI_Comm comm, Int* ranks, Int rank_cnt, Int msgSize, double rseed){
-		assert(msgSize>0);
-		TreeBcast_v2<double>* BcastTree = TreeBcast_v2<double>::Create(comm,ranks,rank_cnt,msgSize,rseed);
-		return (BcTree) BcastTree;
-	}
-
-	void BcTree_Destroy(BcTree Tree){
-		TreeBcast_v2<double>* BcastTree = (TreeBcast_v2<double>*) Tree;
-		delete BcastTree; 
-	}		
-	
-	void BcTree_SetTag(BcTree Tree, Int tag){
-		TreeBcast_v2<double>* BcastTree = (TreeBcast_v2<double>*) Tree;
-		BcastTree->SetTag(tag); 
-	}
-
-
-	yes_no_t BcTree_IsRoot(BcTree Tree){
-		TreeBcast_v2<double>* BcastTree = (TreeBcast_v2<double>*) Tree;
-		return BcastTree->IsRoot()?YES:NO;
-	}
-
-	
-	void BcTree_forwardMessageSimple(BcTree Tree, void* localBuffer){
-		TreeBcast_v2<double>* BcastTree = (TreeBcast_v2<double>*) Tree;
-		BcastTree->forwardMessageSimple((double*)localBuffer);		
-	}
-
-	void BcTree_waitSendRequest(BcTree Tree){
-		TreeBcast_v2<double>* BcastTree = (TreeBcast_v2<double>*) Tree;
-		BcastTree->waitSendRequest();		
-	}
-	
-	
-	
-	void BcTree_allocateRequest(BcTree Tree){
-		TreeBcast_v2<double>* BcastTree = (TreeBcast_v2<double>*) Tree;
-		BcastTree->allocateRequest();			
-	}	
-	
-	int BcTree_getDestCount(BcTree Tree){
-		TreeBcast_v2<double>* BcastTree = (TreeBcast_v2<double>*) Tree;
-		return BcastTree->GetDestCount();			
-	}	
-
-
-	StdList StdList_Init(){
-		std::list<int_t>* lst = new std::list<int_t>();
-		return (StdList) lst;
-	}
-	void StdList_Pushback(StdList lst, int_t dat){
-		std::list<int_t>* list = (std::list<int_t>*) lst;
-		list->push_back(dat);
-	}
-	
-	void StdList_Pushfront(StdList lst, int_t dat){
-		std::list<int_t>* list = (std::list<int_t>*) lst;
-		list->push_front(dat);
-	}
-	
-	int_t StdList_Popfront(StdList lst){
-		std::list<int_t>* list = (std::list<int_t>*) lst;
-		int_t dat = -1;
-		if((*list).begin()!=(*list).end()){
-			dat = (*list).front();
-			list->pop_front();
-		}
-		return dat;		
-	}	
-	
-	yes_no_t StdList_Find(StdList lst, int_t dat){
-		std::list<int_t>* list = (std::list<int_t>*) lst;
-		for (std::list<int_t>::iterator itr = (*list).begin(); itr != (*list).end(); /*nothing*/){
-			if(*itr==dat)return YES;
-			++itr;
-		}
-		return NO;
-	}
-
-	int_t StdList_Size(StdList lst){
-		std::list<int_t>* list = (std::list<int_t>*) lst;
-		return list->size();
-	}	
-
-
-	yes_no_t StdList_Empty(StdList lst){
-		std::list<int_t>* list = (std::list<int_t>*) lst;
-		return (*list).begin()==(*list).end()?YES:NO;
-	}		
-	
-
-	RdTree RdTree_Create(MPI_Comm comm, Int* ranks, Int rank_cnt, Int msgSize, double rseed){
-		assert(msgSize>0);
-		TreeReduce_v2<double>* ReduceTree = TreeReduce_v2<double>::Create(comm,ranks,rank_cnt,msgSize,rseed);
-		return (RdTree) ReduceTree;
-	}
-	
-	void RdTree_Destroy(RdTree Tree){
-		TreeReduce_v2<double>* ReduceTree = (TreeReduce_v2<double>*) Tree;
-		delete ReduceTree; 
-	}	
-	
-
-	void RdTree_SetTag(RdTree Tree, Int tag){
-		TreeReduce_v2<double>* ReduceTree = (TreeReduce_v2<double>*) Tree;
-		ReduceTree->SetTag(tag); 
-	}
-
-	int  RdTree_GetDestCount(RdTree Tree){
-		TreeReduce_v2<double>* ReduceTree = (TreeReduce_v2<double>*) Tree;
-		return ReduceTree->GetDestCount();
-	}	
-	
-
-	yes_no_t RdTree_IsRoot(RdTree Tree){
-		TreeReduce_v2<double>* ReduceTree = (TreeReduce_v2<double>*) Tree;
-		return ReduceTree->IsRoot()?YES:NO;
-	}
-
-
-	void RdTree_forwardMessageSimple(RdTree Tree, void* localBuffer){
-		TreeReduce_v2<double>* ReduceTree = (TreeReduce_v2<double>*) Tree;
-		ReduceTree->forwardMessageSimple((double*)localBuffer);		
-	}
-	void RdTree_allocateRequest(RdTree Tree){
-		TreeReduce_v2<double>* ReduceTree = (TreeReduce_v2<double>*) Tree;
-		ReduceTree->allocateRequest();			
-	}
-
-	void RdTree_waitSendRequest(RdTree Tree){
-		TreeReduce_v2<double>* ReduceTree = (TreeReduce_v2<double>*) Tree;
-		ReduceTree->waitSendRequest();		
-	}
-	
-	
-
-
-#ifdef __cplusplus
-	}
-#endif
-	
-	
-	
-
-
-
-} //namespace ASYNCOMM
-
diff --git a/SRC/TreeReduce_v2.hpp b/SRC/TreeReduce_v2.hpp
deleted file mode 100644
index 63404cb..0000000
--- a/SRC/TreeReduce_v2.hpp
+++ /dev/null
@@ -1,114 +0,0 @@
-#ifndef _PEXSI_REDUCE_TREE_V2_HPP_
-#define _PEXSI_REDUCE_TREE_V2_HPP_
-
-#include "environment.hpp"
-// #include "timer.h"
-#include "TreeBcast_v2.hpp"
-
-#include <vector>
-#include <map>
-#include <algorithm>
-#include <string>
-//#include <random>
-
-
-
-namespace ASYNCOMM{
-
-
-
-  template< typename T>
-    class TreeReduce_v2: public TreeBcast_v2<T>{
-      protected:
-        bool isAllocated_;
-        bool isBufferSet_;
-
-      public:
-        static TreeReduce_v2<T> * Create(const MPI_Comm & pComm, Int * ranks, Int rank_cnt, Int msgSize,double rseed);
-
-        TreeReduce_v2();
-        TreeReduce_v2(const MPI_Comm & pComm, Int * ranks, Int rank_cnt, Int msgSize);
-        TreeReduce_v2(const TreeReduce_v2 & Tree);
-
-        virtual ~TreeReduce_v2();
-        virtual TreeReduce_v2 * clone() const = 0; 
-        virtual void Copy(const TreeReduce_v2 & Tree);
-        virtual void Reset();
-
-
-        bool IsAllocated(){return this->isAllocated_;}
-        virtual inline Int GetNumMsgToSend(){return this->myRank_==this->myRoot_?0:1;}
-        virtual inline Int GetNumMsgToRecv(){return this->GetDestCount();}
-
-        virtual T * GetLocalBuffer();
-
-
-		
-		virtual void forwardMessageSimple(T * locBuffer);
-		virtual void allocateRequest();	
-		virtual void waitSendRequest();
-    };
-
-
-template< typename T>
-class FTreeReduce_v2: public TreeReduce_v2<T>{
-protected:
-  virtual void buildTree(Int * ranks, Int rank_cnt);
-public:
-  FTreeReduce_v2(const MPI_Comm & pComm, Int * ranks, Int rank_cnt, Int msgSize);
-  virtual FTreeReduce_v2<T> * clone() const;
-};
-
-
-
-template< typename T>
-class BTreeReduce_v2: public TreeReduce_v2<T>{
-protected:
-  virtual void buildTree(Int * ranks, Int rank_cnt);
-
-public:
-  BTreeReduce_v2(const MPI_Comm & pComm, Int * ranks, Int rank_cnt, Int msgSize);
-
-
-  virtual BTreeReduce_v2<T> * clone() const;
-
-
-
-};
-
-
-template< typename T>
-class ModBTreeReduce_v2: public TreeReduce_v2<T>{
-protected:
-  double rseed_;
-  virtual void buildTree(Int * ranks, Int rank_cnt);
-  
-public:
-  ModBTreeReduce_v2(const MPI_Comm & pComm, Int * ranks, Int rank_cnt, Int msgSize, double rseed);
-  virtual void Copy(const ModBTreeReduce_v2<T> & Tree);
-  virtual ModBTreeReduce_v2<T> * clone() const;
-
-};
-
-
-template< typename T>
-class PalmTreeReduce_v2: public TreeReduce_v2<T>{
-protected:
-
-  virtual void buildTree(Int * ranks, Int rank_cnt);
-
-
-public:
-  PalmTreeReduce_v2(const MPI_Comm & pComm, Int * ranks, Int rank_cnt, Int msgSize);
-  virtual PalmTreeReduce_v2<T> * clone() const;
-
-
-
-
-};
-
-
-}//namespace ASYNCOMM
-
-#include "TreeReduce_v2_impl.hpp"
-#endif
diff --git a/SRC/TreeReduce_v2_impl.hpp b/SRC/TreeReduce_v2_impl.hpp
deleted file mode 100644
index b6dd4f1..0000000
--- a/SRC/TreeReduce_v2_impl.hpp
+++ /dev/null
@@ -1,431 +0,0 @@
-#ifndef _PEXSI_REDUCE_TREE_IMPL_V2_HPP_
-#define _PEXSI_REDUCE_TREE_IMPL_V2_HPP_
-
-#define _SELINV_TAG_COUNT_ 17
-
-// #include "TreeReduce_v2.hpp"
-
-
-namespace ASYNCOMM{
-	
-  template<typename T>
-    TreeReduce_v2<T>::TreeReduce_v2(const MPI_Comm & pComm, Int * ranks, Int rank_cnt, Int msgSize):TreeBcast_v2<T>(pComm,ranks,rank_cnt,msgSize){
-      this->sendDataPtrs_.assign(1,NULL);
-      this->sendRequests_.assign(1,MPI_REQUEST_NULL);
-      this->isAllocated_=false;
-      this->isBufferSet_=false;
-    }
-
-
-
-
-  template<typename T>
-    TreeReduce_v2<T>::TreeReduce_v2(const TreeReduce_v2<T> & Tree){
-      this->Copy(Tree);
-    }
-
-  template<typename T>
-    TreeReduce_v2<T>::TreeReduce_v2():TreeBcast_v2<T>(){
-    }
-
-  template<typename T>
-    TreeReduce_v2<T>::~TreeReduce_v2(){
-      this->cleanupBuffers();
-    }
-
-  template<typename T>
-    inline void TreeReduce_v2<T>::Copy(const TreeReduce_v2<T> & Tree){
-      ((TreeBcast_v2<T>*)this)->Copy(*(const TreeBcast_v2<T>*)&Tree);
-
-      this->sendDataPtrs_.assign(1,NULL);
-      this->sendRequests_.assign(1,MPI_REQUEST_NULL);
-      this->isAllocated_= Tree.isAllocated_;
-      this->isBufferSet_= Tree.isBufferSet_;
-
-      this->cleanupBuffers();
-    }
-	
-  template< typename T> 
-    inline void TreeReduce_v2<T>::forwardMessageSimple(T * locBuffer){
-        MPI_Status status;
-		if(this->myRank_!=this->myRoot_){
-			// if(this->recvCount_== this->GetDestCount()){		
-			  //forward to my root if I have reseived everything
-			  Int iProc = this->myRoot_;
-			  // Use Isend to send to multiple targets
-
-			  int error_code = MPI_Isend(locBuffer, this->msgSize_, this->type_, 
-				  iProc, this->tag_,this->comm_, &this->sendRequests_[0] );
-				  
-				  // std::cout<<this->myRank_<<" FWD to "<<iProc<<" on tag "<<this->tag_<<std::endl;
-				  
-				 // MPI_Wait(&this->sendRequests_[0],&status) ; 
-				  
-			// }
-		}
-      }
-	
- 
-
-  template< typename T> 
-    inline void TreeReduce_v2<T>::allocateRequest(){
-        if(this->sendRequests_.size()==0){
-          this->sendRequests_.assign(1,MPI_REQUEST_NULL);
-        }
-    }
-		
-	
-  template< typename T> 
-    inline void TreeReduce_v2<T>::waitSendRequest(){
-        MPI_Status status;		
-        if(this->sendRequests_.size()>0){
-		  MPI_Wait(&this->sendRequests_[0],&status) ; 
-        }	
-	}	
-
-
-
-  template< typename T> 
-    inline T * TreeReduce_v2<T>::GetLocalBuffer(){ 
-      return this->sendDataPtrs_[0];
-    }
-
-
-  template< typename T> 
-    inline void TreeReduce_v2<T>::Reset(){
-      TreeBcast_v2<T>::Reset();
-      this->isAllocated_=false;
-      this->isBufferSet_=false;
-    }
-
-
-  template< typename T>
-    inline TreeReduce_v2<T> * TreeReduce_v2<T>::Create(const MPI_Comm & pComm, Int * ranks, Int rank_cnt, Int msgSize, double rseed){
-      //get communicator size
-      Int nprocs = 0;
-      MPI_Comm_size(pComm, &nprocs);
-
-#if defined(FTREE)
-      return new FTreeReduce_v2<T>(pComm,ranks,rank_cnt,msgSize);
-#elif defined(MODBTREE)
-      return new ModBTreeReduce_v2<T>(pComm,ranks,rank_cnt,msgSize, rseed);
-#elif defined(BTREE)
-      return new BTreeReduce<T>(pComm,ranks,rank_cnt,msgSize);
-#elif defined(PALMTREE)
-      return new PalmTreeReduce_v2<T>(pComm,ranks,rank_cnt,msgSize);
-#endif
-
-
-      if(nprocs<=FTREE_LIMIT){
-#if ( _DEBUGlevel_ >= 1 ) || defined(REDUCE_VERBOSE)
-        statusOFS<<"FLAT TREE USED"<<std::endl;
-#endif
-        return new FTreeReduce_v2<T>(pComm,ranks,rank_cnt,msgSize);
-      }
-      else{
-#if ( _DEBUGlevel_ >= 1 ) || defined(REDUCE_VERBOSE)
-        statusOFS<<"BINARY TREE USED"<<std::endl;
-#endif
-        // return new ModBTreeReduce_v2<T>(pComm,ranks,rank_cnt,msgSize, rseed);
-		return new BTreeReduce_v2<T>(pComm,ranks,rank_cnt,msgSize);
-      }
-    }
-
-  template< typename T>
-    FTreeReduce_v2<T>::FTreeReduce_v2(const MPI_Comm & pComm, Int * ranks, Int rank_cnt, Int msgSize):TreeReduce_v2<T>(pComm, ranks, rank_cnt, msgSize){
-      buildTree(ranks,rank_cnt);
-    }
-
-  template< typename T>
-    inline FTreeReduce_v2<T> * FTreeReduce_v2<T>::clone() const{
-      FTreeReduce_v2<T> * out = new FTreeReduce_v2<T>(*this);
-      return out;
-    }
-
-
-
-  template< typename T>
-    inline void FTreeReduce_v2<T>::buildTree(Int * ranks, Int rank_cnt){
-
-      Int idxStart = 0;
-      Int idxEnd = rank_cnt;
-
-      this->myRoot_ = ranks[0];
-
-      if(this->myRank_==this->myRoot_){
-        this->myDests_.insert(this->myDests_.end(),&ranks[1],&ranks[0]+rank_cnt);
-      }
-
-#if (defined(REDUCE_VERBOSE))
-      statusOFS<<"My root is "<<this->myRoot_<<std::endl;
-      statusOFS<<"My dests are ";
-      for(int i =0;i<this->myDests_.size();++i){statusOFS<<this->myDests_[i]<<" ";}
-      statusOFS<<std::endl;
-#endif
-    }
-
-  template< typename T>
-    BTreeReduce_v2<T>::BTreeReduce_v2(const MPI_Comm & pComm, Int * ranks, Int rank_cnt, Int msgSize):TreeReduce_v2<T>(pComm, ranks, rank_cnt, msgSize){
-      buildTree(ranks,rank_cnt);
-    }
-
-  template< typename T>
-    inline BTreeReduce_v2<T> * BTreeReduce_v2<T>::clone() const{
-      BTreeReduce_v2<T> * out = new BTreeReduce_v2<T>(*this);
-      return out;
-    }
-
-  // template< typename T>
-    // inline void BTreeReduce_v2<T>::buildTree(Int * ranks, Int rank_cnt){
-      // Int idxStart = 0;
-      // Int idxEnd = rank_cnt;
-
-
-
-      // Int prevRoot = ranks[0];
-      // while(idxStart<idxEnd){
-        // Int curRoot = ranks[idxStart];
-        // Int listSize = idxEnd - idxStart;
-
-        // if(listSize == 1){
-          // if(curRoot == this->myRank_){
-            // this->myRoot_ = prevRoot;
-            // break;
-          // }
-        // }
-        // else{
-          // Int halfList = floor(ceil(double(listSize) / 2.0));
-          // Int idxStartL = idxStart+1;
-          // Int idxStartH = idxStart+halfList;
-
-          // if(curRoot == this->myRank_){
-            // if ((idxEnd - idxStartH) > 0 && (idxStartH - idxStartL)>0){
-              // Int childL = ranks[idxStartL];
-              // Int childR = ranks[idxStartH];
-
-              // this->myDests_.push_back(childL);
-              // this->myDests_.push_back(childR);
-            // }
-            // else if ((idxEnd - idxStartH) > 0){
-              // Int childR = ranks[idxStartH];
-              // this->myDests_.push_back(childR);
-            // }
-            // else{
-              // Int childL = ranks[idxStartL];
-              // this->myDests_.push_back(childL);
-            // }
-            // this->myRoot_ = prevRoot;
-            // break;
-          // } 
-
-          // if( this->myRank_ < ranks[idxStartH]){
-            // idxStart = idxStartL;
-            // idxEnd = idxStartH;
-          // }
-          // else{
-            // idxStart = idxStartH;
-          // }
-          // prevRoot = curRoot;
-        // }
-
-      // }
-
-	  
-// #if (defined(REDUCE_VERBOSE))
-      // statusOFS<<"My root is "<<this->myRoot_<<std::endl;
-      // statusOFS<<"My dests are ";
-      // for(int i =0;i<this->myDests_.size();++i){statusOFS<<this->myDests_[i]<<" ";}
-      // statusOFS<<std::endl;
-// #endif
-    // }  
-	
-	template< typename T>
-    inline void BTreeReduce_v2<T>::buildTree(Int * ranks, Int rank_cnt){
-      Int myIdx = 0;
-      Int ii=0; 
-	  Int child,root;
-	  for (ii=0;ii<rank_cnt;ii++)
-		  if(this->myRank_ == ranks[ii]){
-			  myIdx = ii;
-			  break;
-		  }
-
-	  if(myIdx*2+1<rank_cnt){
-		   child = ranks[myIdx*2+1];
-           this->myDests_.push_back(child);
-	  }
-	  if(myIdx*2+2<rank_cnt){
-		   child = ranks[myIdx*2+2];
-           this->myDests_.push_back(child);
-	  }	 
-
-	  if(myIdx!=0){
-		  this->myRoot_ = ranks[(Int)floor((double)(myIdx-1.0)/2.0)];
-	  }else{
-		  this->myRoot_ = this->myRank_;
-	  } 
-	  
-	  // for(int i =0;i<this->myDests_.size();++i){std::cout<<this->myRank_<<" "<<this->myDests_[i]<<" "<<std::endl;}
-
-	  // {std::cout<<this->myRank_<<" "<<this->myRoot_<<" "<<std::endl;}	  
-	  
-    }
-
-
-  template< typename T>
-    ModBTreeReduce_v2<T>::ModBTreeReduce_v2(const MPI_Comm & pComm, Int * ranks, Int rank_cnt, Int msgSize, double rseed):TreeReduce_v2<T>(pComm, ranks, rank_cnt, msgSize){
-      this->rseed_ = rseed;
-      buildTree(ranks,rank_cnt);
-    }
-
-  template< typename T>
-    inline void ModBTreeReduce_v2<T>::Copy(const ModBTreeReduce_v2<T> & Tree){
-      ((TreeReduce_v2<T>*)this)->Copy(*((const TreeReduce_v2<T>*)&Tree));
-      this->rseed_ = Tree.rseed_;
-    }
-
-  template< typename T>
-    inline ModBTreeReduce_v2<T> * ModBTreeReduce_v2<T>::clone() const{
-      ModBTreeReduce_v2<T> * out = new ModBTreeReduce_v2<T>(*this);
-      return out;
-    }
-
-  template< typename T>
-    inline void ModBTreeReduce_v2<T>::buildTree(Int * ranks, Int rank_cnt){
-
-      Int idxStart = 0;
-      Int idxEnd = rank_cnt;
-
-      //sort the ranks with the modulo like operation
-      if(rank_cnt>1){
-        //generate a random position in [1 .. rand_cnt]
-        //Int new_idx = (int)((rand()+1.0) * (double)rank_cnt / ((double)RAND_MAX+1.0));
-        //srand(ranks[0]+rank_cnt);
-        //Int new_idx = rseed_%(rank_cnt-1)+1;
-
-        //Int new_idx = (int)((rank_cnt - 0) * ( (double)this->rseed_ / (double)RAND_MAX ) + 0);// (this->rseed_)%(rank_cnt-1)+1;
-        //Int new_idx = (Int)rseed_ % (rank_cnt - 1) + 1; 
-        //      Int new_idx = (int)((rank_cnt - 0) * ( (double)this->rseed_ / (double)RAND_MAX ) + 0);// (this->rseed_)%(rank_cnt-1)+1;
-        Int new_idx = (int)(this->rseed_)%(rank_cnt-1)+1;
-
-        Int * new_start = &ranks[new_idx];
-        //        for(int i =0;i<rank_cnt;++i){statusOFS<<ranks[i]<<" ";} statusOFS<<std::endl;
-
-        //        Int * new_start = std::lower_bound(&ranks[1],&ranks[0]+rank_cnt,ranks[0]);
-        //just swap the two chunks   r[0] | r[1] --- r[new_start-1] | r[new_start] --- r[end]
-        // becomes                   r[0] | r[new_start] --- r[end] | r[1] --- r[new_start-1] 
-        std::rotate(&ranks[1], new_start, &ranks[0]+rank_cnt);
-        //        for(int i =0;i<rank_cnt;++i){statusOFS<<ranks[i]<<" ";} statusOFS<<std::endl;
-      }
-
-      Int prevRoot = ranks[0];
-      while(idxStart<idxEnd){
-        Int curRoot = ranks[idxStart];
-        Int listSize = idxEnd - idxStart;
-
-        if(listSize == 1){
-          if(curRoot == this->myRank_){
-            this->myRoot_ = prevRoot;
-            break;
-          }
-        }
-        else{
-          Int halfList = floor(ceil(double(listSize) / 2.0));
-          Int idxStartL = idxStart+1;
-          Int idxStartH = idxStart+halfList;
-
-          if(curRoot == this->myRank_){
-            if ((idxEnd - idxStartH) > 0 && (idxStartH - idxStartL)>0){
-              Int childL = ranks[idxStartL];
-              Int childR = ranks[idxStartH];
-
-              this->myDests_.push_back(childL);
-              this->myDests_.push_back(childR);
-            }
-            else if ((idxEnd - idxStartH) > 0){
-              Int childR = ranks[idxStartH];
-              this->myDests_.push_back(childR);
-            }
-            else{
-              Int childL = ranks[idxStartL];
-              this->myDests_.push_back(childL);
-            }
-            this->myRoot_ = prevRoot;
-            break;
-          } 
-
-          //not true anymore ?
-          //first half to 
-          TIMER_START(FIND_RANK);
-          Int * pos = std::find(&ranks[idxStartL], &ranks[idxStartH], this->myRank_);
-          TIMER_STOP(FIND_RANK);
-          if( pos != &ranks[idxStartH]){
-            idxStart = idxStartL;
-            idxEnd = idxStartH;
-          }
-          else{
-            idxStart = idxStartH;
-          }
-          prevRoot = curRoot;
-        }
-
-      }
-
-#if (defined(REDUCE_VERBOSE)) 
-      statusOFS<<"My root is "<<this->myRoot_<<std::endl;
-      statusOFS<<"My dests are ";
-      for(int i =0;i<this->myDests_.size();++i){statusOFS<<this->myDests_[i]<<" ";}
-      statusOFS<<std::endl;
-#endif
-    }
-
-  template< typename T>
-    PalmTreeReduce_v2<T>::PalmTreeReduce_v2(const MPI_Comm & pComm, Int * ranks, Int rank_cnt, Int msgSize):TreeReduce_v2<T>(pComm,ranks,rank_cnt,msgSize){
-      //build the binary tree;
-      buildTree(ranks,rank_cnt);
-    }
-
-  template< typename T>
-    inline PalmTreeReduce_v2<T> * PalmTreeReduce_v2<T>::clone() const{
-      PalmTreeReduce_v2<T> * out = new PalmTreeReduce_v2<T>(*this);
-      return out;
-    }
-
-
-  template< typename T>
-    inline void PalmTreeReduce_v2<T>::buildTree(Int * ranks, Int rank_cnt){
-      Int numLevel = floor(log2(rank_cnt));
-      Int numRoots = 0;
-      for(Int level=0;level<numLevel;++level){
-        numRoots = std::min( rank_cnt, numRoots + (Int)pow(2,level));
-        Int numNextRoots = std::min(rank_cnt,numRoots + (Int)pow(2,(level+1)));
-        Int numReceivers = numNextRoots - numRoots;
-        for(Int ip = 0; ip<numRoots;++ip){
-          Int p = ranks[ip];
-          for(Int ir = ip; ir<numReceivers;ir+=numRoots){
-            Int r = ranks[numRoots+ir];
-            if(r==this->myRank_){
-              this->myRoot_ = p;
-            }
-
-            if(p==this->myRank_){
-              this->myDests_.push_back(r);
-            }
-          }
-        }
-      }
-
-#if (defined(BCAST_VERBOSE))
-      statusOFS<<"My root is "<<this->myRoot_<<std::endl;
-      statusOFS<<"My dests are ";
-      for(int i =0;i<this->myDests_.size();++i){statusOFS<<this->myDests_[i]<<" ";}
-      statusOFS<<std::endl;
-#endif
-    }
-
-
-
-
-
-} //namespace ASYNCOMM
-#endif
diff --git a/SRC/environment.hpp b/SRC/environment.hpp
deleted file mode 100644
index fd58724..0000000
--- a/SRC/environment.hpp
+++ /dev/null
@@ -1,227 +0,0 @@
-/// @file environment.hpp
-/// @brief Environmental variables.
-/// @date 2012-08-10
-#ifndef _PEXSI_ENVIRONMENT_HPP_
-#define _PEXSI_ENVIRONMENT_HPP_
-
-// STL libraries
-#include <iostream> 
-#include <iomanip> 
-#include <fstream>
-#include <sstream>
-#include <unistd.h>
-
-#include <cfloat>
-#include <complex>
-#include <string>
-
-#include <set>
-#include <map>
-#include <stack>
-#include <vector>
-
-#include <algorithm>
-#include <cmath>
-
-#include <cassert>
-#include <stdexcept>
-#include <execinfo.h>
-//#include <signal.h>
-#include <exception>
-
-// For 32-bit and 64-bit integers
-#include <stdint.h>
-
-// MPI
-#include <mpi.h>
-
-
-
-// *********************************************************************
-// Redefine the global macros
-// *********************************************************************
-
-
-// FIXME Always use complex data for pexsi and ppexsi.
-#define _USE_COMPLEX_
-
-// The verbose level of debugging information
-#ifdef  DEBUG
-#define _DEBUGlevel_ DEBUG
-#endif
-
-// Release mode. For speed up the calculation and reduce verbose level.
-// Note that RELEASE overwrites DEBUG level.
-#ifdef RELEASE
-#define _RELEASE_
-#define _DEBUGlevel -1
-#endif
-
-/***********************************************************************
- *  Data types and constants
- **********************************************************************/
-
-/// @namespace ASYNCOMM
-/// @brief The main namespace.
-
-namespace ASYNCOMM{
-
-// Basic data types
-
-#ifndef Add_
-#define FORTRAN(name) name
-#define BLAS(name) name
-#define LAPACK(name) name
-#else
-#define FORTRAN(name) name##_
-#define BLAS(name) name##_
-#define LAPACK(name) name##_
-#endif
-typedef    int                   Int;
-typedef    int64_t               LongInt;
-typedef    double                Real;
-typedef    std::complex<double>  Complex; // Must use elemental form of complex
-#ifdef _USE_COMPLEX_
-typedef    std::complex<double>  Scalar;  // Must use elemental form of complex
-#else
-typedef    double                Scalar;
-#endif
-
-// IO
-extern  std::ofstream  statusOFS;
-
-// *********************************************************************
-// Define constants
-// *********************************************************************
-// Commonly used
-const Int I_ZERO = 0;
-const Int I_ONE  = 1;
-const Int I_MINUS_ONE  = -1;
-const Real D_ZERO = 0.0;
-const Real D_ONE  = 1.0;
-const Real D_MINUS_ONE  = -1.0;
-const Complex Z_ZERO = Complex(0.0, 0.0);
-const Complex Z_ONE  = Complex(1.0, 0.0);
-const Complex Z_MINUS_ONE  = Complex(-1.0, 0.0);
-const Complex Z_I    = Complex(0.0, 1.0);
-const Complex Z_MINUS_I    = Complex(0.0, -1.0);
-const Scalar SCALAR_ZERO    = static_cast<Scalar>(0.0);
-const Scalar SCALAR_ONE     = static_cast<Scalar>(1.0);
-const Scalar SCALAR_MINUS_ONE = static_cast<Scalar>(-1.0);
-
-template<typename T>
-const T ZERO(){ return static_cast<T>(0.0);};
-template<typename T>
-const T ONE(){ return static_cast<T>(1.0);};
-template<typename T>
-const T MINUS_ONE(){ return static_cast<T>(-1.0);};
-
-const char UPPER = 'U';
-const char LOWER = 'L';
-
-// Physical constants
-
-const Real au2K = 315774.67;
-const Real PI = 3.141592653589793;
-
-} // namespace ASYNCOMM
-
-/***********************************************************************
- *  Error handling
- **********************************************************************/
-
-namespace ASYNCOMM{
-
-
-
-
-
-
-  inline void gdb_lock(){
-    volatile int lock = 1;
-    statusOFS<<"LOCKED"<<std::endl;
-    while (lock == 1){ }
-  }
-
-
-
-
-
-
-
-#ifndef _RELEASE_
-  void PushCallStack( std::string s );
-  void PopCallStack();
-  void DumpCallStack();
-#endif // ifndef _RELEASE_
-
-  // We define an output stream that does nothing. This is done so that the 
-  // root process can be used to print data to a file's ostream while all other 
-  // processes use a null ostream. 
-  struct NullStream : std::ostream
-  {            
-    struct NullStreamBuffer : std::streambuf
-    {
-      Int overflow( Int c ) { return traits_type::not_eof(c); }
-    } nullStreamBuffer_;
-
-    NullStream() 
-      : std::ios(&nullStreamBuffer_), std::ostream(&nullStreamBuffer_)
-      { }
-  };  
-
-  /////////////////////////////////////////////
-
-  class ExceptionTracer
-  {
-  public:
-    ExceptionTracer()
-    {
-      void * array[25];
-      int nSize = backtrace(array, 25);
-      char ** symbols = backtrace_symbols(array, nSize);
-
-      for (int i = 0; i < nSize; i++)
-      {
-	std::cout << symbols[i] << std::endl;
-      }
-
-      free(symbols);
-    }
-  };
-
-  // *********************************************************************
-  // Global utility functions 
-  // These utility functions do not depend on local definitions
-  // *********************************************************************
-  // Return the closest integer to a real number
-	inline Int iround(Real a){ 
-		Int b = 0;
-		if(a>0) b = (a-Int(a)<0.5)?Int(a):(Int(a)+1);
-		else b = (Int(a)-a<0.5)?Int(a):(Int(a)-1);
-		return b; 
-	}
-
-  // Read the options from command line
-	inline void OptionsCreate(Int argc, char** argv, std::map<std::string,std::string>& options){
-		options.clear();
-		for(Int k=1; k<argc; k=k+2) {
-			options[ std::string(argv[k]) ] = std::string(argv[k+1]);
-		}
-	}
-
-	// Size information.
-	// Like sstm.str().length() but without making the copy
-	inline Int Size( std::stringstream& sstm ){
-		Int length;
-		sstm.seekg (0, std::ios::end);
-		length = sstm.tellg();
-		sstm.seekg (0, std::ios::beg);
-		return length;
-	}
-
-
-} // namespace ASYNCOMM
-
-
-#endif // _PEXSI_ENVIRONMENT_HPP_
diff --git a/SRC/global.cpp b/SRC/global.cpp
deleted file mode 100644
index 6d2f451..0000000
--- a/SRC/global.cpp
+++ /dev/null
@@ -1,38 +0,0 @@
-#include "environment.hpp"
-  #include <deque>
-
-namespace ASYNCOMM{
-
-// *********************************************************************
-// IO
-// *********************************************************************
-  std::ofstream  statusOFS;
-
-
-// *********************************************************************
-// Error handling
-// *********************************************************************
-	// If we are not in RELEASE mode, then implement wrappers for a
-	// CallStack
-#ifndef _RELEASE_
-	std::stack<std::string> callStack;	
-
-	void PushCallStack( std::string s )
-	{ callStack.push(s); }
-
-	void PopCallStack()
-	{ callStack.pop(); }
-
-	void DumpCallStack()
-	{
-		std::ostringstream msg;
-		while( ! callStack.empty() )
-		{
-			msg << "Stack[" << callStack.size() << "]: " << callStack.top() << "\n";
-			callStack.pop();
-		}
-		std::cerr << msg.str() << std::endl;
-	}
-
-#endif // ifndef _RELEASE_
-} // namespace ASYNCOMM
diff --git a/SRC/pddistribute.c b/SRC/pddistribute.c
index 746bc1b..bcb0d49 100644
--- a/SRC/pddistribute.c
+++ b/SRC/pddistribute.c
@@ -7,7 +7,7 @@ All rights reserved.
 
 The source code is distributed under BSD license, see the file License.txt
 at the top-level directory.
- */
+*/
 
 
 /*! @file 
@@ -21,10 +21,6 @@ at the top-level directory.
 
 #include "superlu_ddefs.h"
 
-#ifndef CACHELINE
-#define CACHELINE 64  /* bytes, Xeon Phi KNL, Cori haswell, Edision */
-#endif
-
 
 /*! \brief
  *
@@ -61,1926 +57,1015 @@ at the top-level directory.
  * ============
  * </pre>
  */
-	int_t
+int_t
 dReDistribute_A(SuperMatrix *A, ScalePermstruct_t *ScalePermstruct,
-		Glu_freeable_t *Glu_freeable, int_t *xsup, int_t *supno,
-		gridinfo_t *grid, int_t *colptr[], int_t *rowind[],
-		double *a[])
+                Glu_freeable_t *Glu_freeable, int_t *xsup, int_t *supno,
+                gridinfo_t *grid, int_t *colptr[], int_t *rowind[],
+                double *a[])
 {
-	NRformat_loc *Astore;
-	int_t  *perm_r; /* row permutation vector */
-	int_t  *perm_c; /* column permutation vector */
-	int_t  i, irow, fst_row, j, jcol, k, gbi, gbj, n, m_loc, jsize;
-	int_t  nnz_loc;    /* number of local nonzeros */
-	int_t  SendCnt; /* number of remote nonzeros to be sent */
-	int_t  RecvCnt; /* number of remote nonzeros to be sent */
-	int_t  *nnzToSend, *nnzToRecv, maxnnzToRecv;
-	int_t  *ia, *ja, **ia_send, *index, *itemp;
-	int_t  *ptr_to_send;
-	double *aij, **aij_send, *nzval, *dtemp;
-	double *nzval_a;
-	int    iam, it, p, procs;
-	MPI_Request *send_req;
-	MPI_Status  status;
-
-	/* ------------------------------------------------------------
-	   INITIALIZATION.
-	   ------------------------------------------------------------*/
-	iam = grid->iam;
+    NRformat_loc *Astore;
+    int_t  *perm_r; /* row permutation vector */
+    int_t  *perm_c; /* column permutation vector */
+    int_t  i, irow, fst_row, j, jcol, k, gbi, gbj, n, m_loc, jsize;
+    int_t  nnz_loc;    /* number of local nonzeros */
+    int_t  SendCnt; /* number of remote nonzeros to be sent */
+    int_t  RecvCnt; /* number of remote nonzeros to be sent */
+    int_t  *nnzToSend, *nnzToRecv, maxnnzToRecv;
+    int_t  *ia, *ja, **ia_send, *index, *itemp;
+    int_t  *ptr_to_send;
+    double *aij, **aij_send, *nzval, *dtemp;
+    double *nzval_a;
+    int    iam, it, p, procs;
+    MPI_Request *send_req;
+    MPI_Status  status;
+    
+
+    /* ------------------------------------------------------------
+       INITIALIZATION.
+       ------------------------------------------------------------*/
+    iam = grid->iam;
 #if ( DEBUGlevel>=1 )
-	CHECK_MALLOC(iam, "Enter dReDistribute_A()");
+    CHECK_MALLOC(iam, "Enter dReDistribute_A()");
 #endif
-	perm_r = ScalePermstruct->perm_r;
-	perm_c = ScalePermstruct->perm_c;
-	procs = grid->nprow * grid->npcol;
-	Astore = (NRformat_loc *) A->Store;
-	n = A->ncol;
-	m_loc = Astore->m_loc;
-	fst_row = Astore->fst_row;
-	nnzToRecv = intCalloc_dist(2*procs);
-	nnzToSend = nnzToRecv + procs;
-
-
-	/* ------------------------------------------------------------
-	   COUNT THE NUMBER OF NONZEROS TO BE SENT TO EACH PROCESS,
-	   THEN ALLOCATE SPACE.
-	   THIS ACCOUNTS FOR THE FIRST PASS OF A.
-	   ------------------------------------------------------------*/
-	for (i = 0; i < m_loc; ++i) {
-		for (j = Astore->rowptr[i]; j < Astore->rowptr[i+1]; ++j) {
-			irow = perm_c[perm_r[i+fst_row]];  /* Row number in Pc*Pr*A */
-			jcol = Astore->colind[j];
-			gbi = BlockNum( irow );
-			gbj = BlockNum( jcol );
-			p = PNUM( PROW(gbi,grid), PCOL(gbj,grid), grid );
-			++nnzToSend[p]; 
-		}
-	}
-
-	/* All-to-all communication */
-	MPI_Alltoall( nnzToSend, 1, mpi_int_t, nnzToRecv, 1, mpi_int_t,
-			grid->comm);
-
-	maxnnzToRecv = 0;
-	nnz_loc = SendCnt = RecvCnt = 0;
-
-	for (p = 0; p < procs; ++p) {
-		if ( p != iam ) {
-			SendCnt += nnzToSend[p];
-			RecvCnt += nnzToRecv[p];
-			maxnnzToRecv = SUPERLU_MAX( nnzToRecv[p], maxnnzToRecv );
-		} else {
-			nnz_loc += nnzToRecv[p];
-			/*assert(nnzToSend[p] == nnzToRecv[p]);*/
-		}
+    perm_r = ScalePermstruct->perm_r;
+    perm_c = ScalePermstruct->perm_c;
+    procs = grid->nprow * grid->npcol;
+    Astore = (NRformat_loc *) A->Store;
+    n = A->ncol;
+    m_loc = Astore->m_loc;
+    fst_row = Astore->fst_row;
+    nnzToRecv = intCalloc_dist(2*procs);
+    nnzToSend = nnzToRecv + procs;
+
+
+    /* ------------------------------------------------------------
+       COUNT THE NUMBER OF NONZEROS TO BE SENT TO EACH PROCESS,
+       THEN ALLOCATE SPACE.
+       THIS ACCOUNTS FOR THE FIRST PASS OF A.
+       ------------------------------------------------------------*/
+    for (i = 0; i < m_loc; ++i) {
+        for (j = Astore->rowptr[i]; j < Astore->rowptr[i+1]; ++j) {
+  	    irow = perm_c[perm_r[i+fst_row]];  /* Row number in Pc*Pr*A */
+	    jcol = Astore->colind[j];
+	    gbi = BlockNum( irow );
+	    gbj = BlockNum( jcol );
+	    p = PNUM( PROW(gbi,grid), PCOL(gbj,grid), grid );
+	    ++nnzToSend[p]; 
 	}
-	k = nnz_loc + RecvCnt; /* Total nonzeros ended up in my process. */
-
-	/* Allocate space for storing the triplets after redistribution. */
-	if ( k ) { /* count can be zero. */
-		if ( !(ia = intMalloc_dist(2*k)) )
-			ABORT("Malloc fails for ia[].");
-		if ( !(aij = doubleMalloc_dist(k)) )
-			ABORT("Malloc fails for aij[].");
-	}
-	ja = ia + k;
-
-	/* Allocate temporary storage for sending/receiving the A triplets. */
-	if ( procs > 1 ) {
-		if ( !(send_req = (MPI_Request *)
-					SUPERLU_MALLOC(2*procs *sizeof(MPI_Request))) )
-			ABORT("Malloc fails for send_req[].");
-		if ( !(ia_send = (int_t **) SUPERLU_MALLOC(procs*sizeof(int_t*))) )
-			ABORT("Malloc fails for ia_send[].");
-		if ( !(aij_send = (double **)SUPERLU_MALLOC(procs*sizeof(double*))) )
-			ABORT("Malloc fails for aij_send[].");
-		if ( SendCnt ) { /* count can be zero */
-			if ( !(index = intMalloc_dist(2*SendCnt)) )
-				ABORT("Malloc fails for index[].");
-			if ( !(nzval = doubleMalloc_dist(SendCnt)) )
-				ABORT("Malloc fails for nzval[].");
-		}
-		if ( !(ptr_to_send = intCalloc_dist(procs)) )
-			ABORT("Malloc fails for ptr_to_send[].");
-		if ( maxnnzToRecv ) { /* count can be zero */
-			if ( !(itemp = intMalloc_dist(2*maxnnzToRecv)) )
-				ABORT("Malloc fails for itemp[].");
-			if ( !(dtemp = doubleMalloc_dist(maxnnzToRecv)) )
-				ABORT("Malloc fails for dtemp[].");
-		}
-
-		for (i = 0, j = 0, p = 0; p < procs; ++p) {
-			if ( p != iam ) {
-				ia_send[p] = &index[i];
-				i += 2 * nnzToSend[p]; /* ia/ja indices alternate */
-				aij_send[p] = &nzval[j];
-				j += nnzToSend[p];
-			}
-		}
-	} /* if procs > 1 */
-
-	if ( !(*colptr = intCalloc_dist(n+1)) )
-		ABORT("Malloc fails for *colptr[].");
+    }
 
-	/* ------------------------------------------------------------
-	   LOAD THE ENTRIES OF A INTO THE (IA,JA,AIJ) STRUCTURES TO SEND.
-	   THIS ACCOUNTS FOR THE SECOND PASS OF A.
-	   ------------------------------------------------------------*/
-	nnz_loc = 0; /* Reset the local nonzero count. */
-	nzval_a = Astore->nzval;
-	for (i = 0; i < m_loc; ++i) {
-		for (j = Astore->rowptr[i]; j < Astore->rowptr[i+1]; ++j) {
-			irow = perm_c[perm_r[i+fst_row]];  /* Row number in Pc*Pr*A */
-			jcol = Astore->colind[j];
-			gbi = BlockNum( irow );
-			gbj = BlockNum( jcol );
-			p = PNUM( PROW(gbi,grid), PCOL(gbj,grid), grid );
-
-			if ( p != iam ) { /* remote */
-				k = ptr_to_send[p];
-				ia_send[p][k] = irow;
-				ia_send[p][k + nnzToSend[p]] = jcol;
-				aij_send[p][k] = nzval_a[j];
-				++ptr_to_send[p]; 
-			} else {          /* local */
-				ia[nnz_loc] = irow;
-				ja[nnz_loc] = jcol;
-				aij[nnz_loc] = nzval_a[j];
-				++nnz_loc;
-				++(*colptr)[jcol]; /* Count nonzeros in each column */
-			}
-		}
-	}
+    /* All-to-all communication */
+    MPI_Alltoall( nnzToSend, 1, mpi_int_t, nnzToRecv, 1, mpi_int_t,
+		  grid->comm);
 
-	/* ------------------------------------------------------------
-	   PERFORM REDISTRIBUTION. THIS INVOLVES ALL-TO-ALL COMMUNICATION.
-NOTE: Can possibly use MPI_Alltoallv.
-------------------------------------------------------------*/
-	for (p = 0; p < procs; ++p) {
-		if ( p != iam ) {
-			it = 2*nnzToSend[p];
-			MPI_Isend( ia_send[p], it, mpi_int_t,
-					p, iam, grid->comm, &send_req[p] );
-			it = nnzToSend[p];
-			MPI_Isend( aij_send[p], it, MPI_DOUBLE,
-					p, iam+procs, grid->comm, &send_req[procs+p] ); 
-		}
-	}
+    maxnnzToRecv = 0;
+    nnz_loc = SendCnt = RecvCnt = 0;
 
-	for (p = 0; p < procs; ++p) {
-		if ( p != iam ) {
-			it = 2*nnzToRecv[p];
-			MPI_Recv( itemp, it, mpi_int_t, p, p, grid->comm, &status ); 
-			it = nnzToRecv[p];
-			MPI_Recv( dtemp, it, MPI_DOUBLE, p, p+procs,
-					grid->comm, &status );
-			for (i = 0; i < nnzToRecv[p]; ++i) {
-				ia[nnz_loc] = itemp[i];
-				jcol = itemp[i + nnzToRecv[p]];
-				/*assert(jcol<n);*/
-				ja[nnz_loc] = jcol;
-				aij[nnz_loc] = dtemp[i];
-				++nnz_loc;
-				++(*colptr)[jcol]; /* Count nonzeros in each column */ 
-			}
-		}
-	}
-
-	for (p = 0; p < procs; ++p) {
-		if ( p != iam ) {
-			MPI_Wait( &send_req[p], &status);
-			MPI_Wait( &send_req[procs+p], &status);
-		}
+    for (p = 0; p < procs; ++p) {
+	if ( p != iam ) {
+	    SendCnt += nnzToSend[p];
+	    RecvCnt += nnzToRecv[p];
+	    maxnnzToRecv = SUPERLU_MAX( nnzToRecv[p], maxnnzToRecv );
+	} else {
+	    nnz_loc += nnzToRecv[p];
+	    /*assert(nnzToSend[p] == nnzToRecv[p]);*/
 	}
-
-	/* ------------------------------------------------------------
-	   DEALLOCATE TEMPORARY STORAGE
-	   ------------------------------------------------------------*/
-
-	SUPERLU_FREE(nnzToRecv);
-
-	if ( procs > 1 ) {
-		SUPERLU_FREE(send_req);
-		SUPERLU_FREE(ia_send);
-		SUPERLU_FREE(aij_send);
-		if ( SendCnt ) {
-			SUPERLU_FREE(index);
-			SUPERLU_FREE(nzval);
-		}
-		SUPERLU_FREE(ptr_to_send);
-		if ( maxnnzToRecv ) {
-			SUPERLU_FREE(itemp);
-			SUPERLU_FREE(dtemp);
-		}
+    }
+    k = nnz_loc + RecvCnt; /* Total nonzeros ended up in my process. */
+
+    /* Allocate space for storing the triplets after redistribution. */
+    if ( k ) { /* count can be zero. */
+        if ( !(ia = intMalloc_dist(2*k)) )
+            ABORT("Malloc fails for ia[].");
+        if ( !(aij = doubleMalloc_dist(k)) )
+            ABORT("Malloc fails for aij[].");
+    }
+    ja = ia + k;
+
+    /* Allocate temporary storage for sending/receiving the A triplets. */
+    if ( procs > 1 ) {
+      if ( !(send_req = (MPI_Request *)
+	     SUPERLU_MALLOC(2*procs *sizeof(MPI_Request))) )
+	ABORT("Malloc fails for send_req[].");
+      if ( !(ia_send = (int_t **) SUPERLU_MALLOC(procs*sizeof(int_t*))) )
+        ABORT("Malloc fails for ia_send[].");
+      if ( !(aij_send = (double **)SUPERLU_MALLOC(procs*sizeof(double*))) )
+        ABORT("Malloc fails for aij_send[].");
+      if ( SendCnt ) { /* count can be zero */
+          if ( !(index = intMalloc_dist(2*SendCnt)) )
+              ABORT("Malloc fails for index[].");
+          if ( !(nzval = doubleMalloc_dist(SendCnt)) )
+              ABORT("Malloc fails for nzval[].");
+      }
+      if ( !(ptr_to_send = intCalloc_dist(procs)) )
+        ABORT("Malloc fails for ptr_to_send[].");
+      if ( maxnnzToRecv ) { /* count can be zero */
+          if ( !(itemp = intMalloc_dist(2*maxnnzToRecv)) )
+              ABORT("Malloc fails for itemp[].");
+          if ( !(dtemp = doubleMalloc_dist(maxnnzToRecv)) )
+              ABORT("Malloc fails for dtemp[].");
+      }
+
+      for (i = 0, j = 0, p = 0; p < procs; ++p) {
+          if ( p != iam ) {
+	      ia_send[p] = &index[i];
+	      i += 2 * nnzToSend[p]; /* ia/ja indices alternate */
+	      aij_send[p] = &nzval[j];
+	      j += nnzToSend[p];
+	  }
+      }
+    } /* if procs > 1 */
+      
+    if ( !(*colptr = intCalloc_dist(n+1)) )
+        ABORT("Malloc fails for *colptr[].");
+
+    /* ------------------------------------------------------------
+       LOAD THE ENTRIES OF A INTO THE (IA,JA,AIJ) STRUCTURES TO SEND.
+       THIS ACCOUNTS FOR THE SECOND PASS OF A.
+       ------------------------------------------------------------*/
+    nnz_loc = 0; /* Reset the local nonzero count. */
+    nzval_a = Astore->nzval;
+    for (i = 0; i < m_loc; ++i) {
+        for (j = Astore->rowptr[i]; j < Astore->rowptr[i+1]; ++j) {
+  	    irow = perm_c[perm_r[i+fst_row]];  /* Row number in Pc*Pr*A */
+	    jcol = Astore->colind[j];
+	    gbi = BlockNum( irow );
+	    gbj = BlockNum( jcol );
+	    p = PNUM( PROW(gbi,grid), PCOL(gbj,grid), grid );
+
+	    if ( p != iam ) { /* remote */
+	        k = ptr_to_send[p];
+	        ia_send[p][k] = irow;
+	        ia_send[p][k + nnzToSend[p]] = jcol;
+		aij_send[p][k] = nzval_a[j];
+		++ptr_to_send[p]; 
+	    } else {          /* local */
+	        ia[nnz_loc] = irow;
+	        ja[nnz_loc] = jcol;
+		aij[nnz_loc] = nzval_a[j];
+		++nnz_loc;
+		++(*colptr)[jcol]; /* Count nonzeros in each column */
+	    }
 	}
-
-	/* ------------------------------------------------------------
-	   CONVERT THE TRIPLET FORMAT INTO THE CCS FORMAT.
-	   ------------------------------------------------------------*/
-	if ( nnz_loc ) { /* nnz_loc can be zero */
-		if ( !(*rowind = intMalloc_dist(nnz_loc)) )
-			ABORT("Malloc fails for *rowind[].");
-		if ( !(*a = doubleMalloc_dist(nnz_loc)) )
-			ABORT("Malloc fails for *a[].");
+    }
+
+    /* ------------------------------------------------------------
+       PERFORM REDISTRIBUTION. THIS INVOLVES ALL-TO-ALL COMMUNICATION.
+       NOTE: Can possibly use MPI_Alltoallv.
+       ------------------------------------------------------------*/
+    for (p = 0; p < procs; ++p) {
+        if ( p != iam ) {
+	    it = 2*nnzToSend[p];
+	    MPI_Isend( ia_send[p], it, mpi_int_t,
+		       p, iam, grid->comm, &send_req[p] );
+	    it = nnzToSend[p];
+	    MPI_Isend( aij_send[p], it, MPI_DOUBLE,
+	               p, iam+procs, grid->comm, &send_req[procs+p] ); 
 	}
-
-	/* Initialize the array of column pointers */
-	k = 0;
-	jsize = (*colptr)[0];
-	(*colptr)[0] = 0;
-	for (j = 1; j < n; ++j) {
-		k += jsize;
-		jsize = (*colptr)[j];
-		(*colptr)[j] = k;
+    }
+
+    for (p = 0; p < procs; ++p) {
+        if ( p != iam ) {
+	    it = 2*nnzToRecv[p];
+	    MPI_Recv( itemp, it, mpi_int_t, p, p, grid->comm, &status ); 
+	    it = nnzToRecv[p];
+            MPI_Recv( dtemp, it, MPI_DOUBLE, p, p+procs,
+		      grid->comm, &status );
+	    for (i = 0; i < nnzToRecv[p]; ++i) {
+	        ia[nnz_loc] = itemp[i];
+		jcol = itemp[i + nnzToRecv[p]];
+		/*assert(jcol<n);*/
+	        ja[nnz_loc] = jcol;
+		aij[nnz_loc] = dtemp[i];
+		++nnz_loc;
+		++(*colptr)[jcol]; /* Count nonzeros in each column */ 
+	    }
 	}
+    }
 
-	/* Copy the triplets into the column oriented storage */
-	for (i = 0; i < nnz_loc; ++i) {
-		j = ja[i];
-		k = (*colptr)[j];
-		(*rowind)[k] = ia[i];
-		(*a)[k] = aij[i];
-		++(*colptr)[j];
-	}
-
-	/* Reset the column pointers to the beginning of each column */
-	for (j = n; j > 0; --j) (*colptr)[j] = (*colptr)[j-1];
-	(*colptr)[0] = 0;
-
-	if ( nnz_loc ) {
-		SUPERLU_FREE(ia);
-		SUPERLU_FREE(aij);
+    for (p = 0; p < procs; ++p) {
+        if ( p != iam ) {
+	    MPI_Wait( &send_req[p], &status);
+	    MPI_Wait( &send_req[procs+p], &status);
 	}
+    }
+
+    /* ------------------------------------------------------------
+       DEALLOCATE TEMPORARY STORAGE
+       ------------------------------------------------------------*/
+
+    SUPERLU_FREE(nnzToRecv);
+
+    if ( procs > 1 ) {
+	SUPERLU_FREE(send_req);
+	SUPERLU_FREE(ia_send);
+	SUPERLU_FREE(aij_send);
+	if ( SendCnt ) {
+            SUPERLU_FREE(index);
+            SUPERLU_FREE(nzval);
+        }
+	SUPERLU_FREE(ptr_to_send);
+        if ( maxnnzToRecv ) {
+            SUPERLU_FREE(itemp);
+            SUPERLU_FREE(dtemp);
+        }
+    }
+
+    /* ------------------------------------------------------------
+       CONVERT THE TRIPLET FORMAT INTO THE CCS FORMAT.
+       ------------------------------------------------------------*/
+    if ( nnz_loc ) { /* nnz_loc can be zero */
+        if ( !(*rowind = intMalloc_dist(nnz_loc)) )
+            ABORT("Malloc fails for *rowind[].");
+        if ( !(*a = doubleMalloc_dist(nnz_loc)) )
+            ABORT("Malloc fails for *a[].");
+    }
+
+    /* Initialize the array of column pointers */
+    k = 0;
+    jsize = (*colptr)[0];
+    (*colptr)[0] = 0;
+    for (j = 1; j < n; ++j) {
+	k += jsize;
+	jsize = (*colptr)[j];
+	(*colptr)[j] = k;
+    }
+    
+    /* Copy the triplets into the column oriented storage */
+    for (i = 0; i < nnz_loc; ++i) {
+	j = ja[i];
+	k = (*colptr)[j];
+	(*rowind)[k] = ia[i];
+	(*a)[k] = aij[i];
+	++(*colptr)[j];
+    }
+
+    /* Reset the column pointers to the beginning of each column */
+    for (j = n; j > 0; --j) (*colptr)[j] = (*colptr)[j-1];
+    (*colptr)[0] = 0;
+
+    if ( nnz_loc ) {
+        SUPERLU_FREE(ia);
+        SUPERLU_FREE(aij);
+    }
 
 #if ( DEBUGlevel>=1 )
-	CHECK_MALLOC(iam, "Exit dReDistribute_A()");
+    CHECK_MALLOC(iam, "Exit dReDistribute_A()");
 #endif
-
-	return 0;
+ 
+    return 0;
 } /* dReDistribute_A */
 
-	float
+float
 pddistribute(fact_t fact, int_t n, SuperMatrix *A,
-		ScalePermstruct_t *ScalePermstruct,
-		Glu_freeable_t *Glu_freeable, LUstruct_t *LUstruct,
-		gridinfo_t *grid, int_t nrhs)
-	/*
-	 * -- Distributed SuperLU routine (version 2.0) --
-	 * Lawrence Berkeley National Lab, Univ. of California Berkeley.
-	 * March 15, 2003
-	 *
-	 *
-	 * Purpose
-	 * =======
-	 *   Distribute the matrix onto the 2D process mesh.
-	 * 
-	 * Arguments
-	 * =========
-	 * 
-	 * fact (input) fact_t
-	 *        Specifies whether or not the L and U structures will be re-used.
-	 *        = SamePattern_SameRowPerm: L and U structures are input, and
-	 *                                   unchanged on exit.
-	 *        = DOFACT or SamePattern: L and U structures are computed and output.
-	 *
-	 * n      (input) int
-	 *        Dimension of the matrix.
-	 *
-	 * A      (input) SuperMatrix*
-	 *	  The distributed input matrix A of dimension (A->nrow, A->ncol).
-	 *        A may be overwritten by diag(R)*A*diag(C)*Pc^T. The type of A can be:
-	 *        Stype = SLU_NR_loc; Dtype = SLU_D; Mtype = SLU_GE.
-	 *
-	 * ScalePermstruct (input) ScalePermstruct_t*
-	 *        The data structure to store the scaling and permutation vectors
-	 *        describing the transformations performed to the original matrix A.
-	 *
-	 * Glu_freeable (input) *Glu_freeable_t
-	 *        The global structure describing the graph of L and U.
-	 * 
-	 * LUstruct (input) LUstruct_t*
-	 *        Data structures for L and U factors.
-	 *
-	 * grid   (input) gridinfo_t*
-	 *        The 2D process mesh.
-	 *
-	 * Return value
-	 * ============
-	 *   > 0, working storage required (in bytes).
-	 *
-	 */
+	     ScalePermstruct_t *ScalePermstruct,
+	     Glu_freeable_t *Glu_freeable, LUstruct_t *LUstruct,
+	     gridinfo_t *grid)
+/*
+ * -- Distributed SuperLU routine (version 2.0) --
+ * Lawrence Berkeley National Lab, Univ. of California Berkeley.
+ * March 15, 2003
+ *
+ *
+ * Purpose
+ * =======
+ *   Distribute the matrix onto the 2D process mesh.
+ * 
+ * Arguments
+ * =========
+ * 
+ * fact (input) fact_t
+ *        Specifies whether or not the L and U structures will be re-used.
+ *        = SamePattern_SameRowPerm: L and U structures are input, and
+ *                                   unchanged on exit.
+ *        = DOFACT or SamePattern: L and U structures are computed and output.
+ *
+ * n      (input) int
+ *        Dimension of the matrix.
+ *
+ * A      (input) SuperMatrix*
+ *	  The distributed input matrix A of dimension (A->nrow, A->ncol).
+ *        A may be overwritten by diag(R)*A*diag(C)*Pc^T. The type of A can be:
+ *        Stype = SLU_NR_loc; Dtype = SLU_D; Mtype = SLU_GE.
+ *
+ * ScalePermstruct (input) ScalePermstruct_t*
+ *        The data structure to store the scaling and permutation vectors
+ *        describing the transformations performed to the original matrix A.
+ *
+ * Glu_freeable (input) *Glu_freeable_t
+ *        The global structure describing the graph of L and U.
+ * 
+ * LUstruct (input) LUstruct_t*
+ *        Data structures for L and U factors.
+ *
+ * grid   (input) gridinfo_t*
+ *        The 2D process mesh.
+ *
+ * Return value
+ * ============
+ *   > 0, working storage required (in bytes).
+ *
+ */
 {
-	Glu_persist_t *Glu_persist = LUstruct->Glu_persist;
-	LocalLU_t *Llu = LUstruct->Llu;
-	int_t bnnz, fsupc, fsupc1, i, ii, irow, istart, j, ib, jb, jj, k, k1, 
-	      len, len1, nsupc;
-	int_t lib;  /* local block row number */
-	int_t nlb;  /* local block rows*/
-	int_t ljb;  /* local block column number */
-	int_t nrbl; /* number of L blocks in current block column */
-	int_t nrbu; /* number of U blocks in current block column */
-	int_t gb;   /* global block number; 0 < gb <= nsuper */
-	int_t lb;   /* local block number; 0 < lb <= ceil(NSUPERS/Pr) */
-	int iam, jbrow, kcol,krow, mycol, myrow, pc, pr;
-	int_t mybufmax[NBUFFERS];
-	NRformat_loc *Astore;
-	double *a;
-	int_t *asub, *xa;
-	int_t *xsup = Glu_persist->xsup;    /* supernode and column mapping */
-	int_t *supno = Glu_persist->supno;   
-	int_t *lsub, *xlsub, *usub, *usub1, *xusub;
-	int_t nsupers;
-	int_t next_lind;      /* next available position in index[*] */
-	int_t next_lval;      /* next available position in nzval[*] */
-	int_t *index;         /* indices consist of headers and row subscripts */
-	int_t *index_srt;         /* indices consist of headers and row subscripts */	
-	int   *index1;        /* temporary pointer to array of int */
-	double *lusup, *lusup_srt, *uval; /* nonzero values in L and U */
-	double **Lnzval_bc_ptr;  /* size ceil(NSUPERS/Pc) */
-	int_t  **Lrowind_bc_ptr; /* size ceil(NSUPERS/Pc) */
-	int_t   **Lindval_loc_bc_ptr; /* size ceil(NSUPERS/Pc)                 */		
-	double **Unzval_br_ptr;  /* size ceil(NSUPERS/Pr) */
-	int_t  **Ufstnz_br_ptr;  /* size ceil(NSUPERS/Pr) */
-	BcTree  *LBtree_ptr;       /* size ceil(NSUPERS/Pc)                */
-	RdTree  *LRtree_ptr;		  /* size ceil(NSUPERS/Pr)                */
-	BcTree  *UBtree_ptr;       /* size ceil(NSUPERS/Pc)                */
-	RdTree  *URtree_ptr;		  /* size ceil(NSUPERS/Pr)                */	
-	int msgsize;
-
-    int_t  *Urbs,*Urbs1; /* Number of row blocks in each block column of U. */
-    Ucb_indptr_t **Ucb_indptr;/* Vertical linked list pointing to Uindex[] */
-    int_t  **Ucb_valptr;      /* Vertical linked list pointing to Unzval[] */  	
-	
-	/*-- Counts to be used in factorization. --*/
-	int  *ToRecv, *ToSendD, **ToSendR;
-
-	/*-- Counts to be used in lower triangular solve. --*/
-	int_t  *fmod;          /* Modification count for L-solve.        */
-	int_t  **fsendx_plist; /* Column process list to send down Xk.   */
-	int_t  nfrecvx = 0;    /* Number of Xk I will receive.           */
-	int_t  nfsendx = 0;    /* Number of Xk I will send               */
-	int_t  kseen;
-
-	/*-- Counts to be used in upper triangular solve. --*/
-	int_t  *bmod;          /* Modification count for U-solve.        */
-	int_t  **bsendx_plist; /* Column process list to send down Xk.   */
-	int_t  nbrecvx = 0;    /* Number of Xk I will receive.           */
-	int_t  nbsendx = 0;    /* Number of Xk I will send               */
-	int_t  *ilsum;         /* starting position of each supernode in 
-				  the full array (local)                 */
-
-	/*-- Auxiliary arrays; freed on return --*/
-	int_t *rb_marker;  /* block hit marker; size ceil(NSUPERS/Pr)           */
-	int_t *Urb_length; /* U block length; size ceil(NSUPERS/Pr)             */
-	int_t *Urb_indptr; /* pointers to U index[]; size ceil(NSUPERS/Pr)      */
-	int_t *Urb_fstnz;  /* # of fstnz in a block row; size ceil(NSUPERS/Pr)  */
-	int_t *Ucbs;       /* number of column blocks in a block row            */
-	int_t *Lrb_length; /* L block length; size ceil(NSUPERS/Pr)             */
-	int_t *Lrb_number; /* global block number; size ceil(NSUPERS/Pr)        */
-	int_t *Lrb_indptr; /* pointers to L index[]; size ceil(NSUPERS/Pr)      */
-	int_t *Lrb_valptr; /* pointers to L nzval[]; size ceil(NSUPERS/Pr)      */
-	int_t *ActiveFlag;
-	int_t *ActiveFlagAll;
-	int_t Iactive;
-	int *ranks;
-	int_t *idxs;
-	int_t **nzrows;
-	double rseed;
-	int rank_cnt,rank_cnt_ref,Root;
-	double *dense, *dense_col; /* SPA */
-	double zero = 0.0;
-	int_t ldaspa;     /* LDA of SPA */
-	int_t iword, dword;
-	float mem_use = 0.0;
-	int_t *mod_bit;
-	int_t *frecv, *brecv, *lloc;
-	double **Linv_bc_ptr;  /* size ceil(NSUPERS/Pc) */
-	double **Uinv_bc_ptr;  /* size ceil(NSUPERS/Pc) */
-	double *SeedSTD_BC,*SeedSTD_RD;				 
-	int_t idx_indx,idx_lusup;
-	int_t nbrow;
-	int_t  ik, il, lk, rel, knsupc, idx_r;
-	int_t  lptr1_tmp, idx_i, idx_v,m, uu, aln_i;
-	int_t nub;
-	int tag;
-	
+    Glu_persist_t *Glu_persist = LUstruct->Glu_persist;
+    LocalLU_t *Llu = LUstruct->Llu;
+    int_t bnnz, fsupc, fsupc1, i, ii, irow, istart, j, jb, jj, k, 
+          len, len1, nsupc;
+    int_t ljb;  /* local block column number */
+    int_t nrbl; /* number of L blocks in current block column */
+    int_t nrbu; /* number of U blocks in current block column */
+    int_t gb;   /* global block number; 0 < gb <= nsuper */
+    int_t lb;   /* local block number; 0 < lb <= ceil(NSUPERS/Pr) */
+    int iam, jbrow, kcol, mycol, myrow, pc, pr;
+    int_t mybufmax[NBUFFERS];
+    NRformat_loc *Astore;
+    double *a;
+    int_t *asub, *xa;
+    int_t *xsup = Glu_persist->xsup;    /* supernode and column mapping */
+    int_t *supno = Glu_persist->supno;   
+    int_t *lsub, *xlsub, *usub, *xusub;
+    int_t nsupers;
+    int_t next_lind;      /* next available position in index[*] */
+    int_t next_lval;      /* next available position in nzval[*] */
+    int_t *index;         /* indices consist of headers and row subscripts */
+    int   *index1;        /* temporary pointer to array of int */
+    double *lusup, *uval; /* nonzero values in L and U */
+    double **Lnzval_bc_ptr;  /* size ceil(NSUPERS/Pc) */
+    int_t  **Lrowind_bc_ptr; /* size ceil(NSUPERS/Pc) */
+    double **Unzval_br_ptr;  /* size ceil(NSUPERS/Pr) */
+    int_t  **Ufstnz_br_ptr;  /* size ceil(NSUPERS/Pr) */
+
+    /*-- Counts to be used in factorization. --*/
+    int  *ToRecv, *ToSendD, **ToSendR;
+
+    /*-- Counts to be used in lower triangular solve. --*/
+    int_t  *fmod;          /* Modification count for L-solve.        */
+    int_t  **fsendx_plist; /* Column process list to send down Xk.   */
+    int_t  nfrecvx = 0;    /* Number of Xk I will receive.           */
+    int_t  nfsendx = 0;    /* Number of Xk I will send               */
+    int_t  kseen;
+
+    /*-- Counts to be used in upper triangular solve. --*/
+    int_t  *bmod;          /* Modification count for U-solve.        */
+    int_t  **bsendx_plist; /* Column process list to send down Xk.   */
+    int_t  nbrecvx = 0;    /* Number of Xk I will receive.           */
+    int_t  nbsendx = 0;    /* Number of Xk I will send               */
+    int_t  *ilsum;         /* starting position of each supernode in 
+			      the full array (local)                 */
+
+    /*-- Auxiliary arrays; freed on return --*/
+    int_t *rb_marker;  /* block hit marker; size ceil(NSUPERS/Pr)           */
+    int_t *Urb_length; /* U block length; size ceil(NSUPERS/Pr)             */
+    int_t *Urb_indptr; /* pointers to U index[]; size ceil(NSUPERS/Pr)      */
+    int_t *Urb_fstnz;  /* # of fstnz in a block row; size ceil(NSUPERS/Pr)  */
+    int_t *Ucbs;       /* number of column blocks in a block row            */
+    int_t *Lrb_length; /* L block length; size ceil(NSUPERS/Pr)             */
+    int_t *Lrb_number; /* global block number; size ceil(NSUPERS/Pr)        */
+    int_t *Lrb_indptr; /* pointers to L index[]; size ceil(NSUPERS/Pr)      */
+    int_t *Lrb_valptr; /* pointers to L nzval[]; size ceil(NSUPERS/Pr)      */
+    double *dense, *dense_col; /* SPA */
+    double zero = 0.0;
+    int_t ldaspa;     /* LDA of SPA */
+    int_t iword, dword;
+    float mem_use = 0.0;
+
 #if ( PRNTlevel>=1 )
-	int_t nLblocks = 0, nUblocks = 0;
+    int_t nLblocks = 0, nUblocks = 0;
 #endif
 #if ( PROFlevel>=1 ) 
-	double t, t_u, t_l;
-	int_t u_blks;
+    double t, t_u, t_l;
+    int_t u_blks;
 #endif
 
-	/* Initialization. */
-	iam = grid->iam;
-	myrow = MYROW( iam, grid );
-	mycol = MYCOL( iam, grid );
-	for (i = 0; i < NBUFFERS; ++i) mybufmax[i] = 0;
-	nsupers  = supno[n-1] + 1;
-	Astore   = (NRformat_loc *) A->Store;
-
-	// #if ( PRNTlevel>=1 )
-	iword = sizeof(int_t);
-	dword = sizeof(double);
+    /* Initialization. */
+    iam = grid->iam;
+    myrow = MYROW( iam, grid );
+    mycol = MYCOL( iam, grid );
+    for (i = 0; i < NBUFFERS; ++i) mybufmax[i] = 0;
+    nsupers  = supno[n-1] + 1;
+    Astore   = (NRformat_loc *) A->Store;
 
-	aln_i = ceil(CACHELINE/(double)iword);
-
-	// #endif
+#if ( PRNTlevel>=1 )
+    iword = sizeof(int_t);
+    dword = sizeof(double);
+#endif
 
 #if ( DEBUGlevel>=1 )
-	CHECK_MALLOC(iam, "Enter pddistribute()");
+    CHECK_MALLOC(iam, "Enter pddistribute()");
 #endif
 #if ( PROFlevel>=1 )
-	t = SuperLU_timer_();
+    t = SuperLU_timer_();
 #endif
 
-	dReDistribute_A(A, ScalePermstruct, Glu_freeable, xsup, supno,
-			grid, &xa, &asub, &a);
+    dReDistribute_A(A, ScalePermstruct, Glu_freeable, xsup, supno,
+		      grid, &xa, &asub, &a);
 
 #if ( PROFlevel>=1 )
-	t = SuperLU_timer_() - t;
-	if ( !iam ) printf("--------\n"
-			".. Phase 1 - ReDistribute_A time: %.2f\t\n", t);
+    t = SuperLU_timer_() - t;
+    if ( !iam ) printf("--------\n"
+		       ".. Phase 1 - ReDistribute_A time: %.2f\t\n", t);
 #endif
 
-	if ( fact == SamePattern_SameRowPerm ) {
+    if ( fact == SamePattern_SameRowPerm ) {
 
 #if ( PROFlevel>=1 )
-		t_l = t_u = 0; u_blks = 0;
+	t_l = t_u = 0; u_blks = 0;
 #endif
-		/* We can propagate the new values of A into the existing
-		   L and U data structures.            */
-		ilsum = Llu->ilsum;
-		ldaspa = Llu->ldalsum;
-		if ( !(dense = doubleCalloc_dist(ldaspa * sp_ienv_dist(3))) )
-			ABORT("Calloc fails for SPA dense[].");
-		nrbu = CEILING( nsupers, grid->nprow ); /* No. of local block rows */
-		if ( !(Urb_length = intCalloc_dist(nrbu)) )
-			ABORT("Calloc fails for Urb_length[].");
-		if ( !(Urb_indptr = intMalloc_dist(nrbu)) )
-			ABORT("Malloc fails for Urb_indptr[].");
-		Lrowind_bc_ptr = Llu->Lrowind_bc_ptr;
-		Lindval_loc_bc_ptr = Llu->Lindval_loc_bc_ptr;
-		Lnzval_bc_ptr = Llu->Lnzval_bc_ptr;
-		Ufstnz_br_ptr = Llu->Ufstnz_br_ptr;
-		Unzval_br_ptr = Llu->Unzval_br_ptr;
+	/* We can propagate the new values of A into the existing
+	   L and U data structures.            */
+	ilsum = Llu->ilsum;
+	ldaspa = Llu->ldalsum;
+	if ( !(dense = doubleCalloc_dist(ldaspa * sp_ienv_dist(3))) )
+	    ABORT("Calloc fails for SPA dense[].");
+	nrbu = CEILING( nsupers, grid->nprow ); /* No. of local block rows */
+	if ( !(Urb_length = intCalloc_dist(nrbu)) )
+	    ABORT("Calloc fails for Urb_length[].");
+	if ( !(Urb_indptr = intMalloc_dist(nrbu)) )
+	    ABORT("Malloc fails for Urb_indptr[].");
+	Lrowind_bc_ptr = Llu->Lrowind_bc_ptr;
+	Lnzval_bc_ptr = Llu->Lnzval_bc_ptr;
+	Ufstnz_br_ptr = Llu->Ufstnz_br_ptr;
+	Unzval_br_ptr = Llu->Unzval_br_ptr;
 #if ( PRNTlevel>=1 )
-		mem_use += 2.0*nrbu*iword + ldaspa*sp_ienv_dist(3)*dword;
+	mem_use += 2.0*nrbu*iword + ldaspa*sp_ienv_dist(3)*dword;
 #endif
 #if ( PROFlevel>=1 )
-		t = SuperLU_timer_();
+	t = SuperLU_timer_();
 #endif
 
-		/* Initialize Uval to zero. */
-		for (lb = 0; lb < nrbu; ++lb) {
-			Urb_indptr[lb] = BR_HEADER; /* Skip header in U index[]. */
-			index = Ufstnz_br_ptr[lb];
-			if ( index ) {
-				uval = Unzval_br_ptr[lb];
-				len = index[1];
-				for (i = 0; i < len; ++i) uval[i] = zero;
-			} /* if index != NULL */
-		} /* for lb ... */
-
-		for (jb = 0; jb < nsupers; ++jb) { /* Loop through each block column */
-			pc = PCOL( jb, grid );
-			if ( mycol == pc ) { /* Block column jb in my process column */
-				fsupc = FstBlockC( jb );
-				nsupc = SuperSize( jb );
-
-				/* Scatter A into SPA (for L), or into U directly. */
-				for (j = fsupc, dense_col = dense; j < FstBlockC(jb+1); ++j) {
-					for (i = xa[j]; i < xa[j+1]; ++i) {
-						irow = asub[i];
-						gb = BlockNum( irow );
-						if ( myrow == PROW( gb, grid ) ) {
-							lb = LBi( gb, grid );
-							if ( gb < jb ) { /* in U */
-								index = Ufstnz_br_ptr[lb];
-								uval = Unzval_br_ptr[lb];
-								while (  (k = index[Urb_indptr[lb]]) < jb ) {
-									/* Skip nonzero values in this block */
-									Urb_length[lb] += index[Urb_indptr[lb]+1];
-									/* Move pointer to the next block */
-									Urb_indptr[lb] += UB_DESCRIPTOR
-										+ SuperSize( k );
-								}
-								/*assert(k == jb);*/
-								/* start fstnz */
-								istart = Urb_indptr[lb] + UB_DESCRIPTOR;
-								len = Urb_length[lb];
-								fsupc1 = FstBlockC( gb+1 );
-								k = j - fsupc;
-								/* Sum the lengths of the leading columns */
-								for (jj = 0; jj < k; ++jj)
-									len += fsupc1 - index[istart++];
-								/*assert(irow>=index[istart]);*/
-								uval[len + irow - index[istart]] = a[i];
-							} else { /* in L; put in SPA first */
-								irow = ilsum[lb] + irow - FstBlockC( gb );
-								dense_col[irow] = a[i];
-							}
-						}
-					} /* for i ... */
-					dense_col += ldaspa;
-				} /* for j ... */
+	/* Initialize Uval to zero. */
+	for (lb = 0; lb < nrbu; ++lb) {
+	    Urb_indptr[lb] = BR_HEADER; /* Skip header in U index[]. */
+	    index = Ufstnz_br_ptr[lb];
+	    if ( index ) {
+		uval = Unzval_br_ptr[lb];
+		len = index[1];
+		for (i = 0; i < len; ++i) uval[i] = zero;
+	    } /* if index != NULL */
+	} /* for lb ... */
+
+	for (jb = 0; jb < nsupers; ++jb) { /* Loop through each block column */
+	    pc = PCOL( jb, grid );
+	    if ( mycol == pc ) { /* Block column jb in my process column */
+		fsupc = FstBlockC( jb );
+		nsupc = SuperSize( jb );
+
+ 		/* Scatter A into SPA (for L), or into U directly. */
+		for (j = fsupc, dense_col = dense; j < FstBlockC(jb+1); ++j) {
+		    for (i = xa[j]; i < xa[j+1]; ++i) {
+			irow = asub[i];
+			gb = BlockNum( irow );
+			if ( myrow == PROW( gb, grid ) ) {
+			    lb = LBi( gb, grid );
+ 			    if ( gb < jb ) { /* in U */
+ 				index = Ufstnz_br_ptr[lb];
+ 				uval = Unzval_br_ptr[lb];
+ 				while (  (k = index[Urb_indptr[lb]]) < jb ) {
+ 				    /* Skip nonzero values in this block */
+ 				    Urb_length[lb] += index[Urb_indptr[lb]+1];
+ 				    /* Move pointer to the next block */
+ 				    Urb_indptr[lb] += UB_DESCRIPTOR
+ 					+ SuperSize( k );
+ 				}
+ 				/*assert(k == jb);*/
+ 				/* start fstnz */
+ 				istart = Urb_indptr[lb] + UB_DESCRIPTOR;
+ 				len = Urb_length[lb];
+ 				fsupc1 = FstBlockC( gb+1 );
+ 				k = j - fsupc;
+ 				/* Sum the lengths of the leading columns */
+ 				for (jj = 0; jj < k; ++jj)
+				    len += fsupc1 - index[istart++];
+				/*assert(irow>=index[istart]);*/
+				uval[len + irow - index[istart]] = a[i];
+			    } else { /* in L; put in SPA first */
+  				irow = ilsum[lb] + irow - FstBlockC( gb );
+  				dense_col[irow] = a[i];
+  			    }
+  			}
+		    } /* for i ... */
+  		    dense_col += ldaspa;
+		} /* for j ... */
 
 #if ( PROFlevel>=1 )
-				t_u += SuperLU_timer_() - t;
-				t = SuperLU_timer_();
+		t_u += SuperLU_timer_() - t;
+		t = SuperLU_timer_();
 #endif
 
-				/* Gather the values of A from SPA into Lnzval[]. */
-				ljb = LBj( jb, grid ); /* Local block number */
-				index = Lrowind_bc_ptr[ljb];
-				if ( index ) {
-					nrbl = index[0];   /* Number of row blocks. */
-					len = index[1];    /* LDA of lusup[]. */
-					lusup = Lnzval_bc_ptr[ljb];
-					next_lind = BC_HEADER;
-					next_lval = 0;
-					for (jj = 0; jj < nrbl; ++jj) {
-						gb = index[next_lind++];
-						len1 = index[next_lind++]; /* Rows in the block. */
-						lb = LBi( gb, grid );
-						for (bnnz = 0; bnnz < len1; ++bnnz) {
-							irow = index[next_lind++]; /* Global index. */
-							irow = ilsum[lb] + irow - FstBlockC( gb );
-							k = next_lval++;
-							for (j = 0, dense_col = dense; j < nsupc; ++j) {
-								lusup[k] = dense_col[irow];
-								dense_col[irow] = zero;
-								k += len;
-								dense_col += ldaspa;
-							}
-						} /* for bnnz ... */
-					} /* for jj ... */
-				} /* if index ... */
+		/* Gather the values of A from SPA into Lnzval[]. */
+		ljb = LBj( jb, grid ); /* Local block number */
+		index = Lrowind_bc_ptr[ljb];
+		if ( index ) {
+		    nrbl = index[0];   /* Number of row blocks. */
+		    len = index[1];    /* LDA of lusup[]. */
+		    lusup = Lnzval_bc_ptr[ljb];
+		    next_lind = BC_HEADER;
+		    next_lval = 0;
+		    for (jj = 0; jj < nrbl; ++jj) {
+			gb = index[next_lind++];
+			len1 = index[next_lind++]; /* Rows in the block. */
+			lb = LBi( gb, grid );
+			for (bnnz = 0; bnnz < len1; ++bnnz) {
+			    irow = index[next_lind++]; /* Global index. */
+			    irow = ilsum[lb] + irow - FstBlockC( gb );
+			    k = next_lval++;
+			    for (j = 0, dense_col = dense; j < nsupc; ++j) {
+				lusup[k] = dense_col[irow];
+				dense_col[irow] = zero;
+				k += len;
+				dense_col += ldaspa;
+			    }
+			} /* for bnnz ... */
+		    } /* for jj ... */
+		} /* if index ... */
 #if ( PROFlevel>=1 )
-				t_l += SuperLU_timer_() - t;
+		t_l += SuperLU_timer_() - t;
 #endif
-			} /* if mycol == pc */
-		} /* for jb ... */
+	    } /* if mycol == pc */
+	} /* for jb ... */
 
-		SUPERLU_FREE(dense);
-		SUPERLU_FREE(Urb_length);
-		SUPERLU_FREE(Urb_indptr);
+	SUPERLU_FREE(dense);
+	SUPERLU_FREE(Urb_length);
+	SUPERLU_FREE(Urb_indptr);
 #if ( PROFlevel>=1 )
-		if ( !iam ) printf(".. 2nd distribute time: L %.2f\tU %.2f\tu_blks %d\tnrbu %d\n",
-				t_l, t_u, u_blks, nrbu);
+	if ( !iam ) printf(".. 2nd distribute time: L %.2f\tU %.2f\tu_blks %d\tnrbu %d\n",
+			   t_l, t_u, u_blks, nrbu);
 #endif
 
-	} else {
-		/* ------------------------------------------------------------
-		   FIRST TIME CREATING THE L AND U DATA STRUCTURES.
-		   ------------------------------------------------------------*/
+    } else {
+        /* ------------------------------------------------------------
+	   FIRST TIME CREATING THE L AND U DATA STRUCTURES.
+	   ------------------------------------------------------------*/
 
 #if ( PROFlevel>=1 )
-		t_l = t_u = 0; u_blks = 0;
+	t_l = t_u = 0; u_blks = 0;
 #endif
-		/* We first need to set up the L and U data structures and then
-		 * propagate the values of A into them.
-		 */
-		lsub = Glu_freeable->lsub;    /* compressed L subscripts */
-		xlsub = Glu_freeable->xlsub;
-		usub = Glu_freeable->usub;    /* compressed U subscripts */
-		xusub = Glu_freeable->xusub;
-
-		if ( !(ToRecv = (int *) SUPERLU_MALLOC(nsupers * sizeof(int))) )
-			ABORT("Malloc fails for ToRecv[].");
-		for (i = 0; i < nsupers; ++i) ToRecv[i] = 0;
-
-		k = CEILING( nsupers, grid->npcol );/* Number of local column blocks */
-		if ( !(ToSendR = (int **) SUPERLU_MALLOC(k*sizeof(int*))) )
-			ABORT("Malloc fails for ToSendR[].");
-		j = k * grid->npcol;
-		if ( !(index1 = SUPERLU_MALLOC(j * sizeof(int))) )
-			ABORT("Malloc fails for index[].");
+	/* We first need to set up the L and U data structures and then
+	 * propagate the values of A into them.
+	 */
+	lsub = Glu_freeable->lsub;    /* compressed L subscripts */
+	xlsub = Glu_freeable->xlsub;
+	usub = Glu_freeable->usub;    /* compressed U subscripts */
+	xusub = Glu_freeable->xusub;
+    
+	if ( !(ToRecv = (int *) SUPERLU_MALLOC(nsupers * sizeof(int))) )
+	    ABORT("Malloc fails for ToRecv[].");
+	for (i = 0; i < nsupers; ++i) ToRecv[i] = 0;
+
+	k = CEILING( nsupers, grid->npcol );/* Number of local column blocks */
+	if ( !(ToSendR = (int **) SUPERLU_MALLOC(k*sizeof(int*))) )
+	    ABORT("Malloc fails for ToSendR[].");
+	j = k * grid->npcol;
+	if ( !(index1 = SUPERLU_MALLOC(j * sizeof(int))) )
+	    ABORT("Malloc fails for index[].");
 #if ( PRNTlevel>=1 )
-		mem_use += (float) k*sizeof(int_t*) + (j + nsupers)*iword;
+	mem_use += (float) k*sizeof(int_t*) + (j + nsupers)*iword;
 #endif
-		for (i = 0; i < j; ++i) index1[i] = EMPTY;
-		for (i = 0,j = 0; i < k; ++i, j += grid->npcol) ToSendR[i] = &index1[j];
-		k = CEILING( nsupers, grid->nprow ); /* Number of local block rows */
-
-		/* Pointers to the beginning of each block row of U. */
-		if ( !(Unzval_br_ptr = 
-					(double**)SUPERLU_MALLOC(k * sizeof(double*))) )
-			ABORT("Malloc fails for Unzval_br_ptr[].");
-		if ( !(Ufstnz_br_ptr = (int_t**)SUPERLU_MALLOC(k * sizeof(int_t*))) )
-			ABORT("Malloc fails for Ufstnz_br_ptr[].");
-
-		if ( !(ToSendD = SUPERLU_MALLOC(k * sizeof(int))) )
-			ABORT("Malloc fails for ToSendD[].");
-		for (i = 0; i < k; ++i) ToSendD[i] = NO;
-		if ( !(ilsum = intMalloc_dist(k+1)) )
-			ABORT("Malloc fails for ilsum[].");
-
-		/* Auxiliary arrays used to set up U block data structures.
-		   They are freed on return. */
-		if ( !(rb_marker = intCalloc_dist(k)) )
-			ABORT("Calloc fails for rb_marker[].");
-		if ( !(Urb_length = intCalloc_dist(k)) )
-			ABORT("Calloc fails for Urb_length[].");
-		if ( !(Urb_indptr = intMalloc_dist(k)) )
-			ABORT("Malloc fails for Urb_indptr[].");
-		if ( !(Urb_fstnz = intCalloc_dist(k)) )
-			ABORT("Calloc fails for Urb_fstnz[].");
-		if ( !(Ucbs = intCalloc_dist(k)) )
-			ABORT("Calloc fails for Ucbs[].");
+	for (i = 0; i < j; ++i) index1[i] = EMPTY;
+	for (i = 0,j = 0; i < k; ++i, j += grid->npcol) ToSendR[i] = &index1[j];
+	k = CEILING( nsupers, grid->nprow ); /* Number of local block rows */
+
+	/* Pointers to the beginning of each block row of U. */
+	if ( !(Unzval_br_ptr = 
+              (double**)SUPERLU_MALLOC(k * sizeof(double*))) )
+	    ABORT("Malloc fails for Unzval_br_ptr[].");
+	if ( !(Ufstnz_br_ptr = (int_t**)SUPERLU_MALLOC(k * sizeof(int_t*))) )
+	    ABORT("Malloc fails for Ufstnz_br_ptr[].");
+	
+	if ( !(ToSendD = SUPERLU_MALLOC(k * sizeof(int))) )
+	    ABORT("Malloc fails for ToSendD[].");
+	for (i = 0; i < k; ++i) ToSendD[i] = NO;
+	if ( !(ilsum = intMalloc_dist(k+1)) )
+	    ABORT("Malloc fails for ilsum[].");
+
+	/* Auxiliary arrays used to set up U block data structures.
+	   They are freed on return. */
+	if ( !(rb_marker = intCalloc_dist(k)) )
+	    ABORT("Calloc fails for rb_marker[].");
+	if ( !(Urb_length = intCalloc_dist(k)) )
+	    ABORT("Calloc fails for Urb_length[].");
+	if ( !(Urb_indptr = intMalloc_dist(k)) )
+	    ABORT("Malloc fails for Urb_indptr[].");
+	if ( !(Urb_fstnz = intCalloc_dist(k)) )
+	    ABORT("Calloc fails for Urb_fstnz[].");
+	if ( !(Ucbs = intCalloc_dist(k)) )
+	    ABORT("Calloc fails for Ucbs[].");
 #if ( PRNTlevel>=1 )	
-		mem_use += 2.0*k*sizeof(int_t*) + (7*k+1)*iword;
+	mem_use += 2.0*k*sizeof(int_t*) + (7*k+1)*iword;
 #endif
-		/* Compute ldaspa and ilsum[]. */
-		ldaspa = 0;
-		ilsum[0] = 0;
-		for (gb = 0; gb < nsupers; ++gb) {
-			if ( myrow == PROW( gb, grid ) ) {
-				i = SuperSize( gb );
-				ldaspa += i;
-				lb = LBi( gb, grid );
-				ilsum[lb + 1] = ilsum[lb] + i;
-			}
-		}
-
+	/* Compute ldaspa and ilsum[]. */
+	ldaspa = 0;
+	ilsum[0] = 0;
+	for (gb = 0; gb < nsupers; ++gb) {
+	    if ( myrow == PROW( gb, grid ) ) {
+		i = SuperSize( gb );
+		ldaspa += i;
+		lb = LBi( gb, grid );
+		ilsum[lb + 1] = ilsum[lb] + i;
+	    }
+	}
+	
 #if ( PROFlevel>=1 )
-		t = SuperLU_timer_();
+	t = SuperLU_timer_();
 #endif
-		/* ------------------------------------------------------------
-		   COUNT NUMBER OF ROW BLOCKS AND THE LENGTH OF EACH BLOCK IN U.
-		   THIS ACCOUNTS FOR ONE-PASS PROCESSING OF G(U).
-		   ------------------------------------------------------------*/
-
-		/* Loop through each supernode column. */
-		for (jb = 0; jb < nsupers; ++jb) {
-			pc = PCOL( jb, grid );
-			fsupc = FstBlockC( jb );
-			nsupc = SuperSize( jb );
-			/* Loop through each column in the block. */
-			for (j = fsupc; j < fsupc + nsupc; ++j) {
-				/* usub[*] contains only "first nonzero" in each segment. */
-				for (i = xusub[j]; i < xusub[j+1]; ++i) {
-					irow = usub[i]; /* First nonzero of the segment. */
-					gb = BlockNum( irow );
-					kcol = PCOL( gb, grid );
-					ljb = LBj( gb, grid );
-					if ( mycol == kcol && mycol != pc ) ToSendR[ljb][pc] = YES;
-					pr = PROW( gb, grid );
-					lb = LBi( gb, grid );
-					if ( mycol == pc ) {
-						if  ( myrow == pr ) {
-							ToSendD[lb] = YES;
-							/* Count nonzeros in entire block row. */
-							Urb_length[lb] += FstBlockC( gb+1 ) - irow;
-							if (rb_marker[lb] <= jb) {/* First see the block */
-								rb_marker[lb] = jb + 1;
-								Urb_fstnz[lb] += nsupc;
-								++Ucbs[lb]; /* Number of column blocks
-									       in block row lb. */
+	/* ------------------------------------------------------------
+	   COUNT NUMBER OF ROW BLOCKS AND THE LENGTH OF EACH BLOCK IN U.
+	   THIS ACCOUNTS FOR ONE-PASS PROCESSING OF G(U).
+	   ------------------------------------------------------------*/
+	
+	/* Loop through each supernode column. */
+	for (jb = 0; jb < nsupers; ++jb) {
+	    pc = PCOL( jb, grid );
+	    fsupc = FstBlockC( jb );
+	    nsupc = SuperSize( jb );
+	    /* Loop through each column in the block. */
+	    for (j = fsupc; j < fsupc + nsupc; ++j) {
+		/* usub[*] contains only "first nonzero" in each segment. */
+		for (i = xusub[j]; i < xusub[j+1]; ++i) {
+		    irow = usub[i]; /* First nonzero of the segment. */
+		    gb = BlockNum( irow );
+		    kcol = PCOL( gb, grid );
+		    ljb = LBj( gb, grid );
+		    if ( mycol == kcol && mycol != pc ) ToSendR[ljb][pc] = YES;
+		    pr = PROW( gb, grid );
+		    lb = LBi( gb, grid );
+		    if ( mycol == pc ) {
+			if  ( myrow == pr ) {
+			    ToSendD[lb] = YES;
+			    /* Count nonzeros in entire block row. */
+			    Urb_length[lb] += FstBlockC( gb+1 ) - irow;
+			    if (rb_marker[lb] <= jb) {/* First see the block */
+				rb_marker[lb] = jb + 1;
+				Urb_fstnz[lb] += nsupc;
+				++Ucbs[lb]; /* Number of column blocks
+					       in block row lb. */
 #if ( PRNTlevel>=1 )
-								++nUblocks;
+				++nUblocks;
 #endif
-							}
-							ToRecv[gb] = 1;
-						} else ToRecv[gb] = 2; /* Do I need 0, 1, 2 ? */
-					}
-				} /* for i ... */
-			} /* for j ... */
-		} /* for jb ... */
-
-		/* Set up the initial pointers for each block row in U. */
-		nrbu = CEILING( nsupers, grid->nprow );/* Number of local block rows */
-		for (lb = 0; lb < nrbu; ++lb) {
-			len = Urb_length[lb];
-			rb_marker[lb] = 0; /* Reset block marker. */
-			if ( len ) {
-				/* Add room for descriptors */
-				len1 = Urb_fstnz[lb] + BR_HEADER + Ucbs[lb] * UB_DESCRIPTOR;
-				if ( !(index = intMalloc_dist(len1+1)) )
-					ABORT("Malloc fails for Uindex[].");
-				Ufstnz_br_ptr[lb] = index;
-				if ( !(Unzval_br_ptr[lb] = doubleMalloc_dist(len)) )
-					ABORT("Malloc fails for Unzval_br_ptr[*][].");
-				mybufmax[2] = SUPERLU_MAX( mybufmax[2], len1 );
-				mybufmax[3] = SUPERLU_MAX( mybufmax[3], len );
-				index[0] = Ucbs[lb]; /* Number of column blocks */
-				index[1] = len;      /* Total length of nzval[] */
-				index[2] = len1;     /* Total length of index[] */
-				index[len1] = -1;    /* End marker */
-			} else {
-				Ufstnz_br_ptr[lb] = NULL;
-				Unzval_br_ptr[lb] = NULL;
-			}
-			Urb_length[lb] = 0; /* Reset block length. */
-			Urb_indptr[lb] = BR_HEADER; /* Skip header in U index[]. */
-			Urb_fstnz[lb] = BR_HEADER;
-		} /* for lb ... */
-
-		SUPERLU_FREE(Ucbs);
+			    }
+			    ToRecv[gb] = 1;
+			} else ToRecv[gb] = 2; /* Do I need 0, 1, 2 ? */
+		    }
+		} /* for i ... */
+	    } /* for j ... */
+	} /* for jb ... */
+	
+	/* Set up the initial pointers for each block row in U. */
+	nrbu = CEILING( nsupers, grid->nprow );/* Number of local block rows */
+	for (lb = 0; lb < nrbu; ++lb) {
+	    len = Urb_length[lb];
+	    rb_marker[lb] = 0; /* Reset block marker. */
+	    if ( len ) {
+		/* Add room for descriptors */
+		len1 = Urb_fstnz[lb] + BR_HEADER + Ucbs[lb] * UB_DESCRIPTOR;
+		if ( !(index = intMalloc_dist(len1+1)) )
+		    ABORT("Malloc fails for Uindex[].");
+		Ufstnz_br_ptr[lb] = index;
+		if ( !(Unzval_br_ptr[lb] = doubleMalloc_dist(len)) )
+		    ABORT("Malloc fails for Unzval_br_ptr[*][].");
+		mybufmax[2] = SUPERLU_MAX( mybufmax[2], len1 );
+		mybufmax[3] = SUPERLU_MAX( mybufmax[3], len );
+		index[0] = Ucbs[lb]; /* Number of column blocks */
+		index[1] = len;      /* Total length of nzval[] */
+		index[2] = len1;     /* Total length of index[] */
+		index[len1] = -1;    /* End marker */
+	    } else {
+		Ufstnz_br_ptr[lb] = NULL;
+		Unzval_br_ptr[lb] = NULL;
+	    }
+	    Urb_length[lb] = 0; /* Reset block length. */
+	    Urb_indptr[lb] = BR_HEADER; /* Skip header in U index[]. */
+ 	    Urb_fstnz[lb] = BR_HEADER;
+	} /* for lb ... */
+
+	SUPERLU_FREE(Ucbs);
 
 #if ( PROFlevel>=1 )
-		t = SuperLU_timer_() - t;
-		if ( !iam) printf(".. Phase 2 - setup U strut time: %.2f\t\n", t);
+	t = SuperLU_timer_() - t;
+	if ( !iam) printf(".. Phase 2 - setup U strut time: %.2f\t\n", t);
 #endif
 #if ( PRNTlevel>=1 )
-		mem_use -= 2.0*k * iword;
+        mem_use -= 2.0*k * iword;
 #endif
-		/* Auxiliary arrays used to set up L block data structures.
-		   They are freed on return.
-		   k is the number of local row blocks.   */
-		if ( !(Lrb_length = intCalloc_dist(k)) )
-			ABORT("Calloc fails for Lrb_length[].");
-		if ( !(Lrb_number = intMalloc_dist(k)) )
-			ABORT("Malloc fails for Lrb_number[].");
-		if ( !(Lrb_indptr = intMalloc_dist(k)) )
-			ABORT("Malloc fails for Lrb_indptr[].");
-		if ( !(Lrb_valptr = intMalloc_dist(k)) )
-			ABORT("Malloc fails for Lrb_valptr[].");
-		if ( !(dense = doubleCalloc_dist(ldaspa * sp_ienv_dist(3))) )
-			ABORT("Calloc fails for SPA dense[].");
-
-		/* These counts will be used for triangular solves. */
-		if ( !(fmod = intCalloc_dist(k)) )
-			ABORT("Calloc fails for fmod[].");
-		if ( !(bmod = intCalloc_dist(k)) )
-			ABORT("Calloc fails for bmod[].");
-
-		/* ------------------------------------------------ */
+	/* Auxiliary arrays used to set up L block data structures.
+	   They are freed on return.
+	   k is the number of local row blocks.   */
+	if ( !(Lrb_length = intCalloc_dist(k)) )
+	    ABORT("Calloc fails for Lrb_length[].");
+	if ( !(Lrb_number = intMalloc_dist(k)) )
+	    ABORT("Malloc fails for Lrb_number[].");
+	if ( !(Lrb_indptr = intMalloc_dist(k)) )
+	    ABORT("Malloc fails for Lrb_indptr[].");
+	if ( !(Lrb_valptr = intMalloc_dist(k)) )
+	    ABORT("Malloc fails for Lrb_valptr[].");
+	if ( !(dense = doubleCalloc_dist(ldaspa * sp_ienv_dist(3))) )
+	    ABORT("Calloc fails for SPA dense[].");
+
+	/* These counts will be used for triangular solves. */
+	if ( !(fmod = intCalloc_dist(k)) )
+	    ABORT("Calloc fails for fmod[].");
+	if ( !(bmod = intCalloc_dist(k)) )
+	    ABORT("Calloc fails for bmod[].");
+
+	/* ------------------------------------------------ */
 #if ( PRNTlevel>=1 )	
-		mem_use += 6.0*k*iword + ldaspa*sp_ienv_dist(3)*dword;
-#endif
-		k = CEILING( nsupers, grid->npcol );/* Number of local block columns */
-
-		/* Pointers to the beginning of each block column of L. */
-		if ( !(Lnzval_bc_ptr = 
-					(double**)SUPERLU_MALLOC(k * sizeof(double*))) )
-			ABORT("Malloc fails for Lnzval_bc_ptr[].");
-		if ( !(Lrowind_bc_ptr = (int_t**)SUPERLU_MALLOC(k * sizeof(int_t*))) )
-			ABORT("Malloc fails for Lrowind_bc_ptr[].");
-		Lrowind_bc_ptr[k-1] = NULL;
-		if ( !(Lindval_loc_bc_ptr = 
-					(int_t**)SUPERLU_MALLOC(k * sizeof(int_t*))) )
-			ABORT("Malloc fails for Lindval_loc_bc_ptr[].");
-		Lindval_loc_bc_ptr[k-1] = NULL;
-
-		if ( !(Linv_bc_ptr = 
-					(double**)SUPERLU_MALLOC(k * sizeof(double*))) ) {
-			fprintf(stderr, "Malloc fails for Linv_bc_ptr[].");
-		}  
-		if ( !(Uinv_bc_ptr = 
-					(double**)SUPERLU_MALLOC(k * sizeof(double*))) ) {
-			fprintf(stderr, "Malloc fails for Uinv_bc_ptr[].");
-		}  
-		Linv_bc_ptr[k-1] = NULL;
-		Uinv_bc_ptr[k-1] = NULL;
-
-		/* These lists of processes will be used for triangular solves. */
-		if ( !(fsendx_plist = (int_t **) SUPERLU_MALLOC(k*sizeof(int_t*))) )
-			ABORT("Malloc fails for fsendx_plist[].");
-		len = k * grid->nprow;
-		if ( !(index = intMalloc_dist(len)) )
-			ABORT("Malloc fails for fsendx_plist[0]");
-		for (i = 0; i < len; ++i) index[i] = EMPTY;
-		for (i = 0, j = 0; i < k; ++i, j += grid->nprow)
-			fsendx_plist[i] = &index[j];
-		if ( !(bsendx_plist = (int_t **) SUPERLU_MALLOC(k*sizeof(int_t*))) )
-			ABORT("Malloc fails for bsendx_plist[].");
-		if ( !(index = intMalloc_dist(len)) )
-			ABORT("Malloc fails for bsendx_plist[0]");
-		for (i = 0; i < len; ++i) index[i] = EMPTY;
-		for (i = 0, j = 0; i < k; ++i, j += grid->nprow)
-			bsendx_plist[i] = &index[j];
-		/* -------------------------------------------------------------- */
-#if ( PRNTlevel>=1 )
-		mem_use += 4.0*k*sizeof(int_t*) + 2.0*len*iword;
-#endif
-
-		/*------------------------------------------------------------
-		  PROPAGATE ROW SUBSCRIPTS AND VALUES OF A INTO L AND U BLOCKS.
-		  THIS ACCOUNTS FOR ONE-PASS PROCESSING OF A, L AND U.
-		  ------------------------------------------------------------*/
-
-		for (jb = 0; jb < nsupers; ++jb) { /* for each block column ... */
-			pc = PCOL( jb, grid );
-			if ( mycol == pc ) { /* Block column jb in my process column */
-				fsupc = FstBlockC( jb );
-				nsupc = SuperSize( jb );
-				ljb = LBj( jb, grid ); /* Local block number */
-
-				/* Scatter A into SPA. */
-				for (j = fsupc, dense_col = dense; j < FstBlockC(jb+1); ++j) {
-					for (i = xa[j]; i < xa[j+1]; ++i) {
-						irow = asub[i];
-						gb = BlockNum( irow );
-						if ( myrow == PROW( gb, grid ) ) {
-							lb = LBi( gb, grid );
-							irow = ilsum[lb] + irow - FstBlockC( gb );
-							dense_col[irow] = a[i];
-						}
-					}
-					dense_col += ldaspa;
-				} /* for j ... */
-
-				jbrow = PROW( jb, grid );
-
-				/*------------------------------------------------
-				 * SET UP U BLOCKS.
-				 *------------------------------------------------*/
-#if ( PROFlevel>=1 )
-				t = SuperLU_timer_();
+	mem_use += 6.0*k*iword + ldaspa*sp_ienv_dist(3)*dword;
 #endif
-				kseen = 0;
-				dense_col = dense;
-				/* Loop through each column in the block column. */
-				for (j = fsupc; j < FstBlockC( jb+1 ); ++j) {
-					istart = xusub[j];
-					/* NOTE: Only the first nonzero index of the segment
-					   is stored in usub[]. */
-					for (i = istart; i < xusub[j+1]; ++i) {
-						irow = usub[i]; /* First nonzero in the segment. */
-						gb = BlockNum( irow );
-						pr = PROW( gb, grid );
-						if ( pr != jbrow &&
-								myrow == jbrow &&  /* diag. proc. owning jb */
-								bsendx_plist[ljb][pr] == EMPTY ) {
-							bsendx_plist[ljb][pr] = YES;
-							// if(ljb==0){
-								// printf("no here??\n");
-								// fflush(stdout);
-							// }
-							++nbsendx;
-						}
-						if ( myrow == pr ) {
-							lb = LBi( gb, grid ); /* Local block number */
-							index = Ufstnz_br_ptr[lb];
-							uval = Unzval_br_ptr[lb];
-							fsupc1 = FstBlockC( gb+1 );
-							if (rb_marker[lb] <= jb) { /* First time see 
-										      the block       */
-								rb_marker[lb] = jb + 1;
-								Urb_indptr[lb] = Urb_fstnz[lb];;
-								index[Urb_indptr[lb]] = jb; /* Descriptor */
-								Urb_indptr[lb] += UB_DESCRIPTOR;
-								/* Record the first location in index[] of the
-								   next block */
-								Urb_fstnz[lb] = Urb_indptr[lb] + nsupc;
-								len = Urb_indptr[lb];/* Start fstnz in index */
-								index[len-1] = 0;
-								for (k = 0; k < nsupc; ++k)
-									index[len+k] = fsupc1;
-								if ( gb != jb )/* Exclude diagonal block. */
-									++bmod[lb];/* Mod. count for back solve */
-								if ( kseen == 0 && myrow != jbrow ) {
-									++nbrecvx;
-									kseen = 1;
-								}
-							} else { /* Already saw the block */
-								len = Urb_indptr[lb];/* Start fstnz in index */
-							}
-							jj = j - fsupc;
-							index[len+jj] = irow;
-							/* Load the numerical values */
-							k = fsupc1 - irow; /* No. of nonzeros in segment */
-							index[len-1] += k; /* Increment block length in
-									      Descriptor */
-							irow = ilsum[lb] + irow - FstBlockC( gb );
-							for (ii = 0; ii < k; ++ii) {
-								uval[Urb_length[lb]++] = dense_col[irow + ii];
-								dense_col[irow + ii] = zero;
-							}
-						} /* if myrow == pr ... */
-					} /* for i ... */
-					dense_col += ldaspa;
-				} /* for j ... */
-
-#if ( PROFlevel>=1 )
-				t_u += SuperLU_timer_() - t;
-				t = SuperLU_timer_();
-#endif		
-				/*------------------------------------------------
-				 * SET UP L BLOCKS.
-				 *------------------------------------------------*/
-
-				/* Count number of blocks and length of each block. */
-				nrbl = 0;
-				len = 0; /* Number of row subscripts I own. */
-				kseen = 0;
-				istart = xlsub[fsupc];
-				for (i = istart; i < xlsub[fsupc+1]; ++i) {
-					irow = lsub[i];
-					gb = BlockNum( irow ); /* Global block number */
-					pr = PROW( gb, grid ); /* Process row owning this block */
-					if ( pr != jbrow &&
-							myrow == jbrow &&  /* diag. proc. owning jb */
-							fsendx_plist[ljb][pr] == EMPTY /* first time */ ) {
-						fsendx_plist[ljb][pr] = YES;
-						++nfsendx;
-					}
-					if ( myrow == pr ) {
-						lb = LBi( gb, grid );  /* Local block number */
-						if (rb_marker[lb] <= jb) { /* First see this block */
-							rb_marker[lb] = jb + 1;
-							Lrb_length[lb] = 1;
-							Lrb_number[nrbl++] = gb;
-							// if(gb==747)printf("worita %5d%5d",iam,jb); 
-							if ( gb != jb ) /* Exclude diagonal block. */
-								++fmod[lb]; /* Mod. count for forward solve */
-							if ( kseen == 0 && myrow != jbrow ) {
-								++nfrecvx;
-								kseen = 1;
-							}
+	k = CEILING( nsupers, grid->npcol );/* Number of local block columns */
+
+	/* Pointers to the beginning of each block column of L. */
+	if ( !(Lnzval_bc_ptr = 
+              (double**)SUPERLU_MALLOC(k * sizeof(double*))) )
+	    ABORT("Malloc fails for Lnzval_bc_ptr[].");
+	if ( !(Lrowind_bc_ptr = (int_t**)SUPERLU_MALLOC(k * sizeof(int_t*))) )
+	    ABORT("Malloc fails for Lrowind_bc_ptr[].");
+	Lrowind_bc_ptr[k-1] = NULL;
+
+	/* These lists of processes will be used for triangular solves. */
+	if ( !(fsendx_plist = (int_t **) SUPERLU_MALLOC(k*sizeof(int_t*))) )
+	    ABORT("Malloc fails for fsendx_plist[].");
+	len = k * grid->nprow;
+	if ( !(index = intMalloc_dist(len)) )
+	    ABORT("Malloc fails for fsendx_plist[0]");
+	for (i = 0; i < len; ++i) index[i] = EMPTY;
+	for (i = 0, j = 0; i < k; ++i, j += grid->nprow)
+	    fsendx_plist[i] = &index[j];
+	if ( !(bsendx_plist = (int_t **) SUPERLU_MALLOC(k*sizeof(int_t*))) )
+	    ABORT("Malloc fails for bsendx_plist[].");
+	if ( !(index = intMalloc_dist(len)) )
+	    ABORT("Malloc fails for bsendx_plist[0]");
+	for (i = 0; i < len; ++i) index[i] = EMPTY;
+	for (i = 0, j = 0; i < k; ++i, j += grid->nprow)
+	    bsendx_plist[i] = &index[j];
+	/* -------------------------------------------------------------- */
 #if ( PRNTlevel>=1 )
-							++nLblocks;
-#endif
-						} else {
-							++Lrb_length[lb];
-						}
-						++len;
-					}
-				} /* for i ... */
-
-				if ( nrbl ) { /* Do not ensure the blocks are sorted! */
-					/* Set up the initial pointers for each block in 
-					   index[] and nzval[]. */
-					/* Add room for descriptors */
-					len1 = len + BC_HEADER + nrbl * LB_DESCRIPTOR;
-					if ( !(index = intMalloc_dist(len1)) ) 
-						ABORT("Malloc fails for index[]");												 
-					if (!(lusup = 
-								doubleMalloc_dist(len*nsupc))) {
-						fprintf(stderr, "col block " IFMT " ", jb);
-						ABORT("Malloc fails for lusup[]");
-					}
-					// if ( !(Lindval_loc_bc_ptr[ljb] = intCalloc_dist(nrbl*3)) ) 
-					if ( !(Lindval_loc_bc_ptr[ljb] = intCalloc_dist(((nrbl*3 + (aln_i - 1)) / aln_i) * aln_i)) ) 
-						ABORT("Malloc fails for Lindval_loc_bc_ptr[ljb][]");
-
-
-
-
-					if (!(Linv_bc_ptr[ljb] = 
-								doubleCalloc_dist(nsupc*nsupc))) {
-						fprintf(stderr, "Malloc fails for Linv_bc_ptr[*][] col block " IFMT, jb);
-					}
-					if (!(Uinv_bc_ptr[ljb] = 
-								doubleCalloc_dist(nsupc*nsupc))) {
-						fprintf(stderr, "Malloc fails for Uinv_bc_ptr[*][] col block " IFMT, jb);
-					}
-
-					mybufmax[0] = SUPERLU_MAX( mybufmax[0], len1 );
-					mybufmax[1] = SUPERLU_MAX( mybufmax[1], len*nsupc );
-					mybufmax[4] = SUPERLU_MAX( mybufmax[4], len );
-					index[0] = nrbl;  /* Number of row blocks */
-					index[1] = len;   /* LDA of the nzval[] */
-					next_lind = BC_HEADER;
-					next_lval = 0;
-					for (k = 0; k < nrbl; ++k) {
-						gb = Lrb_number[k];
-						lb = LBi( gb, grid );
-						len = Lrb_length[lb];
-
-
-						Lindval_loc_bc_ptr[ljb][k] = lb;
-						Lindval_loc_bc_ptr[ljb][k+nrbl] = next_lind;
-						Lindval_loc_bc_ptr[ljb][k+nrbl*2] = next_lval;			
-
-						// if(ljb==0){ 
-						// printf("lb %5d, ind %5d, val %5d\n",lb,next_lind,next_lval);
-						// fflush(stdout);
-						// }
-
-						Lrb_length[lb] = 0;  /* Reset vector of block length */
-						index[next_lind++] = gb; /* Descriptor */
-						index[next_lind++] = len; 
-						Lrb_indptr[lb] = next_lind;
-						Lrb_valptr[lb] = next_lval;
-						next_lind += len;
-						next_lval += len;
-					}
-
-
-					/* Propagate the compressed row subscripts to Lindex[],
-					   and the initial values of A from SPA into Lnzval[]. */
-					len = index[1];  /* LDA of lusup[] */
-					for (i = istart; i < xlsub[fsupc+1]; ++i) {
-						irow = lsub[i];
-						gb = BlockNum( irow );
-						if ( myrow == PROW( gb, grid ) ) {
-							lb = LBi( gb, grid );
-							k = Lrb_indptr[lb]++; /* Random access a block */
-							index[k] = irow;
-							k = Lrb_valptr[lb]++;
-							irow = ilsum[lb] + irow - FstBlockC( gb );
-							for (j = 0, dense_col = dense; j < nsupc; ++j) {
-								lusup[k] = dense_col[irow];
-								dense_col[irow] = zero;
-								k += len;
-								dense_col += ldaspa;
-							}
-						}
-					} /* for i ... */
-
-					Lrowind_bc_ptr[ljb] = index;
-					Lnzval_bc_ptr[ljb] = lusup; 
-
-
-					/* sort Lindval_loc_bc_ptr[ljb], Lrowind_bc_ptr[ljb] and Lnzval_bc_ptr[ljb] here*/
-					if(nrbl>1){
-						krow = PROW( jb, grid );
-						if(myrow==krow){ /* skip the diagonal block */
-							uu=nrbl-2;
-							lloc = &Lindval_loc_bc_ptr[ljb][1];
-						}else{
-							uu=nrbl-1;	
-							lloc = Lindval_loc_bc_ptr[ljb];
-						}	
-						quickSortM(lloc,0,uu,nrbl,0,3);	
-					}
-
-
-					if ( !(index_srt = intMalloc_dist(len1)) ) 
-						ABORT("Malloc fails for index_srt[]");				
-					if (!(lusup_srt = doubleMalloc_dist(len*nsupc))) 
-						ABORT("Malloc fails for lusup_srt[]");
-
-					idx_indx = BC_HEADER;
-					idx_lusup = 0;
-					for (jj=0;jj<BC_HEADER;jj++)
-						index_srt[jj] = index[jj];
-
-					for(i=0;i<nrbl;i++){
-						nbrow = index[Lindval_loc_bc_ptr[ljb][i+nrbl]+1];
-						for (jj=0;jj<LB_DESCRIPTOR+nbrow;jj++){
-							index_srt[idx_indx++] = index[Lindval_loc_bc_ptr[ljb][i+nrbl]+jj];
-						}
-
-						Lindval_loc_bc_ptr[ljb][i+nrbl] = idx_indx - LB_DESCRIPTOR - nbrow; 
-
-						for (jj=0;jj<nbrow;jj++){
-							k=idx_lusup;
-							k1=Lindval_loc_bc_ptr[ljb][i+nrbl*2]+jj;
-							for (j = 0; j < nsupc; ++j) {				
-								lusup_srt[k] = lusup[k1];
-								k += len;
-								k1 += len;
-							}	
-							idx_lusup++;
-						}				
-						Lindval_loc_bc_ptr[ljb][i+nrbl*2] = idx_lusup - nbrow;	
-					}
-
-					SUPERLU_FREE(lusup);
-					SUPERLU_FREE(index);
-
-					Lrowind_bc_ptr[ljb] = index_srt;
-					Lnzval_bc_ptr[ljb] = lusup_srt; 			
-
-
-
-					// if(ljb==0)
-					// for (jj=0;jj<nrbl*3;jj++){
-					// printf("iam %5d Lindval %5d\n",iam, Lindval_loc_bc_ptr[ljb][jj]);
-					// fflush(stdout);
-
-					// for (jj=0;jj<nrbl;jj++){
-					// printf("iam %5d Lindval %5d\n",iam, index[Lindval_loc_bc_ptr[ljb][jj+nrbl]]);
-					// fflush(stdout);			
-
-					// }	
-
-				} else {
-					Lrowind_bc_ptr[ljb] = NULL;
-					Lnzval_bc_ptr[ljb] = NULL;
-					Linv_bc_ptr[ljb] = NULL;
-					Uinv_bc_ptr[ljb] = NULL;
-					Lindval_loc_bc_ptr[ljb] = NULL;
-				} /* if nrbl ... */
-#if ( PROFlevel>=1 )
-				t_l += SuperLU_timer_() - t;
+	mem_use += 4.0*k*sizeof(int_t*) + 2.0*len*iword;
 #endif
-				} /* if mycol == pc */
-
-			} /* for jb ... */
-
-			// for (j=0;j<19*3;j++){
-			// printf("Lindval %5d\n",Lindval_loc_bc_ptr[0][j]);
-			// fflush(stdout);
-			// }
-
-			
-			/////////////////////////////////////////////////////////////////
-			
-			/* Set up additional pointers for the index and value arrays of U.
-			   nub is the number of local block columns. */
-			nub = CEILING( nsupers, grid->npcol); /* Number of local block columns. */
-			if ( !(Urbs = (int_t *) intCalloc_dist(2*nub)) )
-				ABORT("Malloc fails for Urbs[]"); /* Record number of nonzero
-									 blocks in a block column. */
-			Urbs1 = Urbs + nub;
-			if ( !(Ucb_indptr = SUPERLU_MALLOC(nub * sizeof(Ucb_indptr_t *))) )
-				ABORT("Malloc fails for Ucb_indptr[]");
-			if ( !(Ucb_valptr = SUPERLU_MALLOC(nub * sizeof(int_t *))) )
-				ABORT("Malloc fails for Ucb_valptr[]");
-			nlb = CEILING( nsupers, grid->nprow ); /* Number of local block rows. */
-
-			/* Count number of row blocks in a block column. 
-			   One pass of the skeleton graph of U. */
-			for (lk = 0; lk < nlb; ++lk) {
-				usub1 = Ufstnz_br_ptr[lk];
-				if ( usub1 ) { /* Not an empty block row. */
-					/* usub1[0] -- number of column blocks in this block row. */
-					i = BR_HEADER; /* Pointer in index array. */
-					for (lb = 0; lb < usub1[0]; ++lb) { /* For all column blocks. */
-						k = usub1[i];            /* Global block number */
-						++Urbs[LBj(k,grid)];
-						i += UB_DESCRIPTOR + SuperSize( k );
-					}
-				}
-			}
 
-			/* Set up the vertical linked lists for the row blocks.
-			   One pass of the skeleton graph of U. */
-			for (lb = 0; lb < nub; ++lb) {
-				if ( Urbs[lb] ) { /* Not an empty block column. */
-					if ( !(Ucb_indptr[lb]
-								= SUPERLU_MALLOC(Urbs[lb] * sizeof(Ucb_indptr_t))) )
-						ABORT("Malloc fails for Ucb_indptr[lb][]");
-					if ( !(Ucb_valptr[lb] = (int_t *) intMalloc_dist(Urbs[lb])) )
-						ABORT("Malloc fails for Ucb_valptr[lb][]");
-				}
+	/*------------------------------------------------------------
+	  PROPAGATE ROW SUBSCRIPTS AND VALUES OF A INTO L AND U BLOCKS.
+	  THIS ACCOUNTS FOR ONE-PASS PROCESSING OF A, L AND U.
+	  ------------------------------------------------------------*/
+
+	for (jb = 0; jb < nsupers; ++jb) { /* for each block column ... */
+	    pc = PCOL( jb, grid );
+	    if ( mycol == pc ) { /* Block column jb in my process column */
+		fsupc = FstBlockC( jb );
+		nsupc = SuperSize( jb );
+		ljb = LBj( jb, grid ); /* Local block number */
+		
+		/* Scatter A into SPA. */
+		for (j = fsupc, dense_col = dense; j < FstBlockC(jb+1); ++j) {
+		    for (i = xa[j]; i < xa[j+1]; ++i) {
+			irow = asub[i];
+			gb = BlockNum( irow );
+			if ( myrow == PROW( gb, grid ) ) {
+			    lb = LBi( gb, grid );
+			    irow = ilsum[lb] + irow - FstBlockC( gb );
+			    dense_col[irow] = a[i];
 			}
-			for (lk = 0; lk < nlb; ++lk) { /* For each block row. */
-				usub1 = Ufstnz_br_ptr[lk];
-				if ( usub1 ) { /* Not an empty block row. */
-					i = BR_HEADER; /* Pointer in index array. */
-					j = 0;         /* Pointer in nzval array. */
-
-					for (lb = 0; lb < usub1[0]; ++lb) { /* For all column blocks. */
-						k = usub1[i];          /* Global block number, column-wise. */
-						ljb = LBj( k, grid ); /* Local block number, column-wise. */
-						Ucb_indptr[ljb][Urbs1[ljb]].lbnum = lk;
-
-						Ucb_indptr[ljb][Urbs1[ljb]].indpos = i;
-						Ucb_valptr[ljb][Urbs1[ljb]] = j;
-						
-						++Urbs1[ljb];
-						j += usub1[i+1];
-						i += UB_DESCRIPTOR + SuperSize( k );
-					}
-				}
-			}				
-			
-			/////////////////////////////////////////////////////////////////
+		    }
+		    dense_col += ldaspa;
+		} /* for j ... */
 
-			// if(LSUM<nsupers)ABORT("Need increase LSUM."); /* temporary*/
+		jbrow = PROW( jb, grid );
 
+		/*------------------------------------------------
+		 * SET UP U BLOCKS.
+		 *------------------------------------------------*/
 #if ( PROFlevel>=1 )
-				t = SuperLU_timer_();
-#endif				
-			/* construct the Bcast tree for L ... */
-
-			k = CEILING( nsupers, grid->npcol );/* Number of local block columns */
-			if ( !(LBtree_ptr = (BcTree*)SUPERLU_MALLOC(k * sizeof(BcTree))) )
-				ABORT("Malloc fails for LBtree_ptr[].");
-			if ( !(ActiveFlag = intCalloc_dist(grid->nprow*2)) )
-				ABORT("Calloc fails for ActiveFlag[].");	
-			if ( !(ranks = (int*)SUPERLU_MALLOC(grid->nprow * sizeof(int))) )
-				ABORT("Malloc fails for ranks[].");	
-			if ( !(SeedSTD_BC = (double*)SUPERLU_MALLOC(k * sizeof(double))) )
-				ABORT("Malloc fails for SeedSTD_BC[].");	
-
-				
-			for (i=0;i<k;i++){
-				SeedSTD_BC[i]=rand();		
-			}
-
-			MPI_Allreduce(MPI_IN_PLACE,&SeedSTD_BC[0],k,MPI_DOUBLE,MPI_MAX,grid->cscp.comm);					  
-
-			for (ljb = 0; ljb <k ; ++ljb) {
-				LBtree_ptr[ljb]=NULL;
-			}			
-			
-
-			if ( !(ActiveFlagAll = intMalloc_dist(grid->nprow*k)) )
-				ABORT("Calloc fails for ActiveFlag[].");				
-			for (j=0;j<grid->nprow*k;++j)ActiveFlagAll[j]=3*nsupers;	
-			for (ljb = 0; ljb < k; ++ljb) { /* for each local block column ... */
-				jb = mycol+ljb*grid->npcol;  /* not sure */
-				if(jb<nsupers){
-				pc = PCOL( jb, grid );
-				fsupc = FstBlockC( jb );
-				nsupc = SuperSize( jb );
-
-				istart = xlsub[fsupc];
-				for (i = istart; i < xlsub[fsupc+1]; ++i) {
-					irow = lsub[i];
-					gb = BlockNum( irow );
-					pr = PROW( gb, grid );
-					ActiveFlagAll[pr+ljb*grid->nprow]=MIN(ActiveFlagAll[pr+ljb*grid->nprow],gb);
-				} /* for j ... */
-				}
-			}			
-			
-			for (ljb = 0; ljb < k; ++ljb) { /* for each local block column ... */
-				
-				jb = mycol+ljb*grid->npcol;  /* not sure */
-				if(jb<nsupers){
-				pc = PCOL( jb, grid );
-
-				for (j=0;j<grid->nprow;++j)ActiveFlag[j]=ActiveFlagAll[j+ljb*grid->nprow];
-				for (j=0;j<grid->nprow;++j)ActiveFlag[j+grid->nprow]=j;
-				for (j=0;j<grid->nprow;++j)ranks[j]=-1;
-
-				Root=-1; 
-				Iactive = 0;				
-				for (j=0;j<grid->nprow;++j){
-					if(ActiveFlag[j]!=3*nsupers){
-					gb = ActiveFlag[j];
-					pr = PROW( gb, grid );
-					if(gb==jb)Root=pr;
-					if(myrow==pr)Iactive=1;		
-					}					
-				}
-				
-
-				quickSortM(ActiveFlag,0,grid->nprow-1,grid->nprow,0,2);	
-
-				if(Iactive==1){
-					// printf("jb %5d damn\n",jb);
-					// fflush(stdout);
-					assert( Root>-1 );
-					rank_cnt = 1;
-					ranks[0]=Root;
-					for (j = 0; j < grid->nprow; ++j){
-						if(ActiveFlag[j]!=3*nsupers && ActiveFlag[j+grid->nprow]!=Root){
-							ranks[rank_cnt]=ActiveFlag[j+grid->nprow];
-							++rank_cnt;
-						}
-					}		
-
-					if(rank_cnt>1){
-
-						for (ii=0;ii<rank_cnt;ii++)   // use global ranks rather than local ranks
-							ranks[ii] = PNUM( ranks[ii], pc, grid );
-
-						// rseed=rand();
-						// rseed=1.0;
-						msgsize = SuperSize( jb )*nrhs+XK_H;
-						LBtree_ptr[ljb] = BcTree_Create(grid->comm, ranks, rank_cnt, msgsize,SeedSTD_BC[ljb]);  	
-						BcTree_SetTag(LBtree_ptr[ljb],BC_L);
-
-						// printf("iam %5d btree rank_cnt %5d \n",iam,rank_cnt);
-						// fflush(stdout);
-
-						// if(iam==15 || iam==3){
-						// printf("iam %5d btree lk %5d tag %5d root %5d\n",iam, ljb,jb,BcTree_IsRoot(LBtree_ptr[ljb]));
-						// fflush(stdout);
-						// }
-
-						// #if ( PRNTlevel>=1 )		
-						if(Root==myrow){
-							rank_cnt_ref=1;
-							for (j = 0; j < grid->nprow; ++j) {
-								if ( fsendx_plist[ljb][j] != EMPTY ) {	
-									++rank_cnt_ref;		
-								}
-							}
-							assert(rank_cnt==rank_cnt_ref);		
-
-							// printf("Partial Bcast Procs: col%7d np%4d\n",jb,rank_cnt);
-
-							// // printf("Partial Bcast Procs: %4d %4d: ",iam, rank_cnt);
-							// // for(j=0;j<rank_cnt;++j)printf("%4d",ranks[j]);
-							// // printf("\n");
-						}
-						// #endif
-					}	
-				}
+		t = SuperLU_timer_();
+#endif
+		kseen = 0;
+		dense_col = dense;
+		/* Loop through each column in the block column. */
+		for (j = fsupc; j < FstBlockC( jb+1 ); ++j) {
+		    istart = xusub[j];
+		    /* NOTE: Only the first nonzero index of the segment
+		       is stored in usub[]. */
+		    for (i = istart; i < xusub[j+1]; ++i) {
+			irow = usub[i]; /* First nonzero in the segment. */
+			gb = BlockNum( irow );
+			pr = PROW( gb, grid );
+			if ( pr != jbrow &&
+			     myrow == jbrow &&  /* diag. proc. owning jb */
+			     bsendx_plist[ljb][pr] == EMPTY ) {
+			    bsendx_plist[ljb][pr] = YES;
+			    ++nbsendx;
+                        }
+			if ( myrow == pr ) {
+			    lb = LBi( gb, grid ); /* Local block number */
+			    index = Ufstnz_br_ptr[lb];
+			    uval = Unzval_br_ptr[lb];
+			    fsupc1 = FstBlockC( gb+1 );
+			    if (rb_marker[lb] <= jb) { /* First time see 
+							  the block       */
+				rb_marker[lb] = jb + 1;
+				Urb_indptr[lb] = Urb_fstnz[lb];;
+				index[Urb_indptr[lb]] = jb; /* Descriptor */
+				Urb_indptr[lb] += UB_DESCRIPTOR;
+				/* Record the first location in index[] of the
+				   next block */
+				Urb_fstnz[lb] = Urb_indptr[lb] + nsupc;
+				len = Urb_indptr[lb];/* Start fstnz in index */
+				index[len-1] = 0;
+				for (k = 0; k < nsupc; ++k)
+				    index[len+k] = fsupc1;
+				if ( gb != jb )/* Exclude diagonal block. */
+				    ++bmod[lb];/* Mod. count for back solve */
+				if ( kseen == 0 && myrow != jbrow ) {
+				    ++nbrecvx;
+				    kseen = 1;
 				}
-			}
-
-			
-			SUPERLU_FREE(ActiveFlag);
-			SUPERLU_FREE(ActiveFlagAll);
-			SUPERLU_FREE(ranks);
-			SUPERLU_FREE(SeedSTD_BC);
-			
-			
-#if ( PROFlevel>=1 )
-		t = SuperLU_timer_() - t;
-		if ( !iam) printf(".. Construct Bcast tree for L: %.2f\t\n", t);
-#endif			
-	
+			    } else { /* Already saw the block */
+				len = Urb_indptr[lb];/* Start fstnz in index */
+			    }
+			    jj = j - fsupc;
+			    index[len+jj] = irow;
+			    /* Load the numerical values */
+			    k = fsupc1 - irow; /* No. of nonzeros in segment */
+			    index[len-1] += k; /* Increment block length in
+						  Descriptor */
+			    irow = ilsum[lb] + irow - FstBlockC( gb );
+			    for (ii = 0; ii < k; ++ii) {
+				uval[Urb_length[lb]++] = dense_col[irow + ii];
+				dense_col[irow + ii] = zero;
+			    }
+			} /* if myrow == pr ... */
+		    } /* for i ... */
+                    dense_col += ldaspa;
+		} /* for j ... */
 
 #if ( PROFlevel>=1 )
-				t = SuperLU_timer_();
-#endif			
-			/* construct the Reduce tree for L ... */
-			/* the following is used as reference */
-			nlb = CEILING( nsupers, grid->nprow );/* Number of local block rows */
-			if ( !(mod_bit = intMalloc_dist(nlb)) )
-				ABORT("Malloc fails for mod_bit[].");
-			if ( !(frecv = intMalloc_dist(nlb)) )
-				ABORT("Malloc fails for frecv[].");
-
-			for (k = 0; k < nlb; ++k) mod_bit[k] = 0;
-			for (k = 0; k < nsupers; ++k) {
-				pr = PROW( k, grid );
-				if ( myrow == pr ) {
-					lib = LBi( k, grid );    /* local block number */
-					kcol = PCOL( k, grid );
-					if (mycol == kcol || fmod[lib] )
-						mod_bit[lib] = 1;  /* contribution from off-diagonal and diagonal*/
-				}
-			}
-			/* Every process receives the count, but it is only useful on the
-			   diagonal processes.  */
-			MPI_Allreduce( mod_bit, frecv, nlb, mpi_int_t, MPI_SUM, grid->rscp.comm);
-
-
-
-			k = CEILING( nsupers, grid->nprow );/* Number of local block rows */
-			if ( !(LRtree_ptr = (RdTree*)SUPERLU_MALLOC(k * sizeof(RdTree))) )
-				ABORT("Malloc fails for LRtree_ptr[].");
-			if ( !(ActiveFlag = intCalloc_dist(grid->npcol*2)) )
-				ABORT("Calloc fails for ActiveFlag[].");	
-			if ( !(ranks = (int*)SUPERLU_MALLOC(grid->npcol * sizeof(int))) )
-				ABORT("Malloc fails for ranks[].");	
-
-			// if ( !(idxs = intCalloc_dist(nsupers)) )
-				// ABORT("Calloc fails for idxs[].");	
-
-			// if ( !(nzrows = (int_t**)SUPERLU_MALLOC(nsupers * sizeof(int_t*))) )
-				// ABORT("Malloc fails for nzrows[].");
-
-			if ( !(SeedSTD_RD = (double*)SUPERLU_MALLOC(k * sizeof(double))) )
-				ABORT("Malloc fails for SeedSTD_RD[].");	
-
-			for (i=0;i<k;i++){
-				SeedSTD_RD[i]=rand();		
-			}
-
-			MPI_Allreduce(MPI_IN_PLACE,&SeedSTD_RD[0],k,MPI_DOUBLE,MPI_MAX,grid->rscp.comm);					  
-
-
-			// for (jb = 0; jb < nsupers; ++jb) { /* for each block column ... */
-				// fsupc = FstBlockC( jb );
-				// len=xlsub[fsupc+1]-xlsub[fsupc];
-				// idxs[jb] = len-1;
-				// if(len>0){
-					// if ( !(nzrows[jb] = intMalloc_dist(len)) )
-						// ABORT("Malloc fails for nzrows[jb]");
-					// for(i=xlsub[fsupc];i<xlsub[fsupc+1];++i){
-						// irow = lsub[i];
-						// nzrows[jb][i-xlsub[fsupc]]=irow;
-					// }
-					// quickSort(nzrows[jb],0,len-1,0);
-				// }
-				// else{
-					// nzrows[jb] = NULL;
-				// }
-			// }
-
-
-			for (lib = 0; lib <k ; ++lib) {
-				LRtree_ptr[lib]=NULL;
-			}
-
-			
-			if ( !(ActiveFlagAll = intMalloc_dist(grid->npcol*k)) )
-				ABORT("Calloc fails for ActiveFlagAll[].");				
-			for (j=0;j<grid->npcol*k;++j)ActiveFlagAll[j]=-3*nsupers;	
-						
-			for (jb = 0; jb < nsupers; ++jb) { /* for each block column ... */
-				fsupc = FstBlockC( jb );
-				pc = PCOL( jb, grid );
-				for(i=xlsub[fsupc];i<xlsub[fsupc+1];++i){
-					irow = lsub[i];
-					ib = BlockNum( irow );
-					pr = PROW( ib, grid );
-					if ( myrow == pr ) { /* Block row ib in my process row */
-						lib = LBi( ib, grid ); /* Local block number */
-						ActiveFlagAll[pc+lib*grid->npcol]=MAX(ActiveFlagAll[pc+lib*grid->npcol],jb);
-					}
-				}
-			}
-
-			
-			for (lib=0;lib<k;++lib){
-				ib = myrow+lib*grid->nprow;  /* not sure */
-				if(ib<nsupers){
-					pr = PROW( ib, grid );
-					for (j=0;j<grid->npcol;++j)ActiveFlag[j]=ActiveFlagAll[j+lib*grid->npcol];;
-					for (j=0;j<grid->npcol;++j)ActiveFlag[j+grid->npcol]=j;
-					for (j=0;j<grid->npcol;++j)ranks[j]=-1;
-					Root=-1; 
-					Iactive = 0;				
-
-					for (j=0;j<grid->npcol;++j){
-						if(ActiveFlag[j]!=-3*nsupers){
-						jb = ActiveFlag[j];
-						pc = PCOL( jb, grid );
-						if(jb==ib)Root=pc;
-						if(mycol==pc)Iactive=1;		
-						}					
-					}
-				
-				
-					quickSortM(ActiveFlag,0,grid->npcol-1,grid->npcol,1,2);
-
-					if(Iactive==1){
-						assert( Root>-1 );
-						rank_cnt = 1;
-						ranks[0]=Root;
-						for (j = 0; j < grid->npcol; ++j){
-							if(ActiveFlag[j]!=-3*nsupers && ActiveFlag[j+grid->npcol]!=Root){
-								ranks[rank_cnt]=ActiveFlag[j+grid->npcol];
-								++rank_cnt;
-							}
-						}
-						if(rank_cnt>1){
-
-							for (ii=0;ii<rank_cnt;ii++)   // use global ranks rather than local ranks
-								ranks[ii] = PNUM( pr, ranks[ii], grid );		
-
-							// rseed=rand();
-							// rseed=1.0;
-							msgsize = SuperSize( ib )*nrhs+LSUM_H;
-
-							// if(ib==0){
-
-							LRtree_ptr[lib] = RdTree_Create(grid->comm, ranks, rank_cnt, msgsize,SeedSTD_RD[lib]);  	
-							RdTree_SetTag(LRtree_ptr[lib], RD_L);
-							// }
-
-							// printf("iam %5d rtree rank_cnt %5d \n",iam,rank_cnt);
-							// fflush(stdout);
-
-							// if(ib==15  || ib ==16){
-
-							// if(iam==15 || iam==3){
-							// printf("iam %5d rtree lk %5d tag %5d root %5d\n",iam,lib,ib,RdTree_IsRoot(LRtree_ptr[lib]));
-							// fflush(stdout);
-							// }		
-
-
-							// #if ( PRNTlevel>=1 )
-							// if(Root==mycol){
-							// assert(rank_cnt==frecv[lib]);
-							// printf("Partial Reduce Procs: row%7d np%4d\n",ib,rank_cnt);
-							// // printf("Partial Reduce Procs: %4d %4d: ",iam, rank_cnt);
-							// // // for(j=0;j<rank_cnt;++j)printf("%4d",ranks[j]);
-							// // printf("\n");
-							// }
-							// #endif		
-						}
-					}				
-				}	
+		t_u += SuperLU_timer_() - t;
+		t = SuperLU_timer_();
+#endif		
+		/*------------------------------------------------
+		 * SET UP L BLOCKS.
+		 *------------------------------------------------*/
+
+		/* Count number of blocks and length of each block. */
+		nrbl = 0;
+		len = 0; /* Number of row subscripts I own. */
+		kseen = 0;
+		istart = xlsub[fsupc];
+		for (i = istart; i < xlsub[fsupc+1]; ++i) {
+		    irow = lsub[i];
+		    gb = BlockNum( irow ); /* Global block number */
+		    pr = PROW( gb, grid ); /* Process row owning this block */
+		    if ( pr != jbrow &&
+			 myrow == jbrow &&  /* diag. proc. owning jb */
+			 fsendx_plist[ljb][pr] == EMPTY /* first time */ ) {
+			fsendx_plist[ljb][pr] = YES;
+			++nfsendx;
+                    }
+		    if ( myrow == pr ) {
+			lb = LBi( gb, grid );  /* Local block number */
+			if (rb_marker[lb] <= jb) { /* First see this block */
+			    rb_marker[lb] = jb + 1;
+			    Lrb_length[lb] = 1;
+			    Lrb_number[nrbl++] = gb;
+			    if ( gb != jb ) /* Exclude diagonal block. */
+				++fmod[lb]; /* Mod. count for forward solve */
+			    if ( kseen == 0 && myrow != jbrow ) {
+				++nfrecvx;
+				kseen = 1;
+			    }
+#if ( PRNTlevel>=1 )
+			    ++nLblocks;
+#endif
+			} else {
+			    ++Lrb_length[lb];
 			}
-
-			SUPERLU_FREE(mod_bit);
-			SUPERLU_FREE(frecv);
-
-
-			SUPERLU_FREE(ActiveFlag);
-			SUPERLU_FREE(ActiveFlagAll);
-			SUPERLU_FREE(ranks);	
-			// SUPERLU_FREE(idxs);	 
-			SUPERLU_FREE(SeedSTD_RD);	
-			// for(i=0;i<nsupers;++i){
-				// if(nzrows[i])SUPERLU_FREE(nzrows[i]);
-			// }
-			// SUPERLU_FREE(nzrows);
-
-				////////////////////////////////////////////////////////
-
-#if ( PROFlevel>=1 )
-		t = SuperLU_timer_() - t;
-		if ( !iam) printf(".. Construct Reduce tree for L: %.2f\t\n", t);
-#endif					
-
-#if ( PROFlevel>=1 )
-			t = SuperLU_timer_();
-#endif	
-
-			/* construct the Bcast tree for U ... */
-
-			k = CEILING( nsupers, grid->npcol );/* Number of local block columns */
-			if ( !(UBtree_ptr = (BcTree*)SUPERLU_MALLOC(k * sizeof(BcTree))) )
-				ABORT("Malloc fails for UBtree_ptr[].");
-			if ( !(ActiveFlag = intCalloc_dist(grid->nprow*2)) )
-				ABORT("Calloc fails for ActiveFlag[].");	
-			if ( !(ranks = (int*)SUPERLU_MALLOC(grid->nprow * sizeof(int))) )
-				ABORT("Malloc fails for ranks[].");	
-			if ( !(SeedSTD_BC = (double*)SUPERLU_MALLOC(k * sizeof(double))) )
-				ABORT("Malloc fails for SeedSTD_BC[].");	
-
-			for (i=0;i<k;i++){
-				SeedSTD_BC[i]=rand();		
+			++len;
+		    }
+		} /* for i ... */
+
+		if ( nrbl ) { /* Do not ensure the blocks are sorted! */
+		    /* Set up the initial pointers for each block in 
+		       index[] and nzval[]. */
+		    /* Add room for descriptors */
+		    len1 = len + BC_HEADER + nrbl * LB_DESCRIPTOR;
+		    if ( !(index = intMalloc_dist(len1)) ) 
+			ABORT("Malloc fails for index[]");
+		    Lrowind_bc_ptr[ljb] = index;
+		    if (!(Lnzval_bc_ptr[ljb] = 
+                         doubleMalloc_dist(len*nsupc))) {
+			fprintf(stderr, "col block " IFMT " ", jb);
+			ABORT("Malloc fails for Lnzval_bc_ptr[*][]");
+		    }
+		    mybufmax[0] = SUPERLU_MAX( mybufmax[0], len1 );
+		    mybufmax[1] = SUPERLU_MAX( mybufmax[1], len*nsupc );
+		    mybufmax[4] = SUPERLU_MAX( mybufmax[4], len );
+		    index[0] = nrbl;  /* Number of row blocks */
+		    index[1] = len;   /* LDA of the nzval[] */
+		    next_lind = BC_HEADER;
+		    next_lval = 0;
+		    for (k = 0; k < nrbl; ++k) {
+			gb = Lrb_number[k];
+			lb = LBi( gb, grid );
+			len = Lrb_length[lb];
+			Lrb_length[lb] = 0;  /* Reset vector of block length */
+			index[next_lind++] = gb; /* Descriptor */
+			index[next_lind++] = len; 
+			Lrb_indptr[lb] = next_lind;
+			Lrb_valptr[lb] = next_lval;
+			next_lind += len;
+			next_lval += len;
+		    }
+		    /* Propagate the compressed row subscripts to Lindex[],
+                       and the initial values of A from SPA into Lnzval[]. */
+		    lusup = Lnzval_bc_ptr[ljb];
+		    len = index[1];  /* LDA of lusup[] */
+		    for (i = istart; i < xlsub[fsupc+1]; ++i) {
+			irow = lsub[i];
+			gb = BlockNum( irow );
+			if ( myrow == PROW( gb, grid ) ) {
+			    lb = LBi( gb, grid );
+			    k = Lrb_indptr[lb]++; /* Random access a block */
+			    index[k] = irow;
+			    k = Lrb_valptr[lb]++;
+			    irow = ilsum[lb] + irow - FstBlockC( gb );
+			    for (j = 0, dense_col = dense; j < nsupc; ++j) {
+				lusup[k] = dense_col[irow];
+				dense_col[irow] = zero;
+				k += len;
+				dense_col += ldaspa;
+			    }
 			}
-
-			MPI_Allreduce(MPI_IN_PLACE,&SeedSTD_BC[0],k,MPI_DOUBLE,MPI_MAX,grid->cscp.comm);					  
-
-
-			for (ljb = 0; ljb <k ; ++ljb) {
-				UBtree_ptr[ljb]=NULL;
-			}	
-
-			if ( !(ActiveFlagAll = intMalloc_dist(grid->nprow*k)) )
-				ABORT("Calloc fails for ActiveFlagAll[].");				
-			for (j=0;j<grid->nprow*k;++j)ActiveFlagAll[j]=-3*nsupers;	
-			
-			for (ljb = 0; ljb < k; ++ljb) { /* for each local block column ... */
-				jb = mycol+ljb*grid->npcol;  /* not sure */
-				if(jb<nsupers){
-				pc = PCOL( jb, grid );
-
-				fsupc = FstBlockC( jb );
-				for (j = fsupc; j < FstBlockC( jb+1 ); ++j) {
-					istart = xusub[j];
-					/* NOTE: Only the first nonzero index of the segment
-					   is stored in usub[]. */
-					for (i = istart; i < xusub[j+1]; ++i) {
-						irow = usub[i]; /* First nonzero in the segment. */
-						gb = BlockNum( irow );
-						pr = PROW( gb, grid );
-						ActiveFlagAll[pr+ljb*grid->nprow]=MAX(ActiveFlagAll[pr+ljb*grid->nprow],gb);
-					// printf("gb:%5d jb: %5d nsupers: %5d\n",gb,jb,nsupers);
-					// fflush(stdout);								
-						//if(gb==jb)Root=pr;
-					}
-					
-					
-				}
-				pr = PROW( jb, grid ); // take care of diagonal node stored as L
-				// printf("jb %5d current: %5d",jb,ActiveFlagAll[pr+ljb*grid->nprow]);
-				// fflush(stdout);
-				ActiveFlagAll[pr+ljb*grid->nprow]=MAX(ActiveFlagAll[pr+ljb*grid->nprow],jb);	
-				}
-			}	
-				
-				
-				
-			for (ljb = 0; ljb < k; ++ljb) { /* for each block column ... */
-				jb = mycol+ljb*grid->npcol;  /* not sure */
-				if(jb<nsupers){
-				pc = PCOL( jb, grid );
-				// if ( mycol == pc ) { /* Block column jb in my process column */
-
-				for (j=0;j<grid->nprow;++j)ActiveFlag[j]=ActiveFlagAll[j+ljb*grid->nprow];
-				for (j=0;j<grid->nprow;++j)ActiveFlag[j+grid->nprow]=j;
-				for (j=0;j<grid->nprow;++j)ranks[j]=-1;
-
-				Root=-1; 
-				Iactive = 0;				
-				for (j=0;j<grid->nprow;++j){
-					if(ActiveFlag[j]!=-3*nsupers){
-					gb = ActiveFlag[j];
-					pr = PROW( gb, grid );
-					if(gb==jb)Root=pr;
-					if(myrow==pr)Iactive=1;		
-					}
-				}						
-				
-				quickSortM(ActiveFlag,0,grid->nprow-1,grid->nprow,1,2);	
-			// printf("jb: %5d Iactive %5d\n",jb,Iactive);
-			// fflush(stdout);
-				if(Iactive==1){
-					// printf("root:%5d jb: %5d\n",Root,jb);
-					// fflush(stdout);
-					assert( Root>-1 );
-					rank_cnt = 1;
-					ranks[0]=Root;
-					for (j = 0; j < grid->nprow; ++j){
-						if(ActiveFlag[j]!=-3*nsupers && ActiveFlag[j+grid->nprow]!=Root){
-							ranks[rank_cnt]=ActiveFlag[j+grid->nprow];
-							++rank_cnt;
-						}
-					}		
-			// printf("jb: %5d rank_cnt %5d\n",jb,rank_cnt);
-			// fflush(stdout);
-					if(rank_cnt>1){
-						for (ii=0;ii<rank_cnt;ii++)   // use global ranks rather than local ranks
-							ranks[ii] = PNUM( ranks[ii], pc, grid );
-
-						// rseed=rand();
-						// rseed=1.0;
-						msgsize = SuperSize( jb )*nrhs+XK_H;
-						UBtree_ptr[ljb] = BcTree_Create(grid->comm, ranks, rank_cnt, msgsize,SeedSTD_BC[ljb]);  	
-						BcTree_SetTag(UBtree_ptr[ljb],BC_U);
-
-						// printf("iam %5d btree rank_cnt %5d \n",iam,rank_cnt);
-						// fflush(stdout);
-						
-						if(Root==myrow){
-						rank_cnt_ref=1;
-						for (j = 0; j < grid->nprow; ++j) {
-							// printf("ljb %5d j %5d nprow %5d\n",ljb,j,grid->nprow);
-							// fflush(stdout);
-							if ( bsendx_plist[ljb][j] != EMPTY ) {	
-								++rank_cnt_ref;		
-							}
-						}
-						// printf("ljb %5d rank_cnt %5d rank_cnt_ref %5d\n",ljb,rank_cnt,rank_cnt_ref);
-						// fflush(stdout);								
-						assert(rank_cnt==rank_cnt_ref);		
-						}						
-					}
-				}
-				}
-			}	
-			SUPERLU_FREE(ActiveFlag);
-			SUPERLU_FREE(ActiveFlagAll);
-			SUPERLU_FREE(ranks);				
-			SUPERLU_FREE(SeedSTD_BC);				
-				
-#if ( PROFlevel>=1 )
-		t = SuperLU_timer_() - t;
-		if ( !iam) printf(".. Construct Bcast tree for U: %.2f\t\n", t);
-#endif					
-
+		    } /* for i ... */
+		} else {
+		    Lrowind_bc_ptr[ljb] = NULL;
+		    Lnzval_bc_ptr[ljb] = NULL;
+		} /* if nrbl ... */
 #if ( PROFlevel>=1 )
-				t = SuperLU_timer_();
-#endif					
-			/* construct the Reduce tree for U ... */
-			/* the following is used as reference */
-			nlb = CEILING( nsupers, grid->nprow );/* Number of local block rows */
-			if ( !(mod_bit = intMalloc_dist(nlb)) )
-				ABORT("Malloc fails for mod_bit[].");
-			if ( !(brecv = intMalloc_dist(nlb)) )
-				ABORT("Malloc fails for brecv[].");
-
-			for (k = 0; k < nlb; ++k) mod_bit[k] = 0;
-			for (k = 0; k < nsupers; ++k) {
-				pr = PROW( k, grid );
-				if ( myrow == pr ) {
-					lib = LBi( k, grid );    /* local block number */
-					kcol = PCOL( k, grid );
-					if (mycol == kcol || bmod[lib] )
-						mod_bit[lib] = 1;  /* contribution from off-diagonal and diagonal*/
-				}
-			}
-			/* Every process receives the count, but it is only useful on the
-			   diagonal processes.  */
-			MPI_Allreduce( mod_bit, brecv, nlb, mpi_int_t, MPI_SUM, grid->rscp.comm);
-
-
-
-			k = CEILING( nsupers, grid->nprow );/* Number of local block rows */
-			if ( !(URtree_ptr = (RdTree*)SUPERLU_MALLOC(k * sizeof(RdTree))) )
-				ABORT("Malloc fails for URtree_ptr[].");
-			if ( !(ActiveFlag = intCalloc_dist(grid->npcol*2)) )
-				ABORT("Calloc fails for ActiveFlag[].");	
-			if ( !(ranks = (int*)SUPERLU_MALLOC(grid->npcol * sizeof(int))) )
-				ABORT("Malloc fails for ranks[].");	
-
-			// if ( !(idxs = intCalloc_dist(nsupers)) )
-				// ABORT("Calloc fails for idxs[].");	
-
-			// if ( !(nzrows = (int_t**)SUPERLU_MALLOC(nsupers * sizeof(int_t*))) )
-				// ABORT("Malloc fails for nzrows[].");
-
-			if ( !(SeedSTD_RD = (double*)SUPERLU_MALLOC(k * sizeof(double))) )
-				ABORT("Malloc fails for SeedSTD_RD[].");	
-
-			for (i=0;i<k;i++){
-				SeedSTD_RD[i]=rand();		
-			}
-
-			MPI_Allreduce(MPI_IN_PLACE,&SeedSTD_RD[0],k,MPI_DOUBLE,MPI_MAX,grid->rscp.comm);					  
-
-
-			// for (jb = 0; jb < nsupers; ++jb) { /* for each block column ... */
-				// fsupc = FstBlockC( jb );
-				// len=0;  
-				// for (j = fsupc; j < FstBlockC( jb+1 ); ++j) {
-					// istart = xusub[j];
-					// /* NOTE: Only the first nonzero index of the segment
-					   // is stored in usub[]. */
-					// len +=  xusub[j+1] - xusub[j];  
-				// }	
-						
-				// idxs[jb] = len-1;
-
-				// if(len>0){
-					// if ( !(nzrows[jb] = intMalloc_dist(len)) )
-						// ABORT("Malloc fails for nzrows[jb]");
-					
-					// fsupc = FstBlockC( jb );
-					
-					// len=0; 
-					
-					// for (j = fsupc; j < FstBlockC( jb+1 ); ++j) {
-						// istart = xusub[j];
-						// /* NOTE: Only the first nonzero index of the segment
-						   // is stored in usub[]. */
-						// for (i = istart; i < xusub[j+1]; ++i) {
-							// irow = usub[i]; /* First nonzero in the segment. */
-							// nzrows[jb][len]=irow;
-							// len++;
-						// }
-					// }	
-					// quickSort(nzrows[jb],0,len-1,0);
-				// }
-				// else{
-					// nzrows[jb] = NULL;
-				// }
-			// }
-			
-
-			for (lib = 0; lib <k ; ++lib) {
-				URtree_ptr[lib]=NULL;
-			}
-
-			
-			if ( !(ActiveFlagAll = intMalloc_dist(grid->npcol*k)) )
-				ABORT("Calloc fails for ActiveFlagAll[].");				
-			for (j=0;j<grid->npcol*k;++j)ActiveFlagAll[j]=3*nsupers;	
-						
-			for (jb = 0; jb < nsupers; ++jb) { /* for each block column ... */
-				fsupc = FstBlockC( jb );
-				pc = PCOL( jb, grid );
-				
-				fsupc = FstBlockC( jb );
-				for (j = fsupc; j < FstBlockC( jb+1 ); ++j) {
-					istart = xusub[j];
-					/* NOTE: Only the first nonzero index of the segment
-					   is stored in usub[]. */
-					for (i = istart; i < xusub[j+1]; ++i) {
-						irow = usub[i]; /* First nonzero in the segment. */
-						ib = BlockNum( irow );
-						pr = PROW( ib, grid );
-						if ( myrow == pr ) { /* Block row ib in my process row */
-							lib = LBi( ib, grid ); /* Local block number */
-							ActiveFlagAll[pc+lib*grid->npcol]=MIN(ActiveFlagAll[pc+lib*grid->npcol],jb);
-						}						
-					}
-				}
-				
-				pr = PROW( jb, grid );
-				if ( myrow == pr ) { /* Block row ib in my process row */
-					lib = LBi( jb, grid ); /* Local block number */
-					ActiveFlagAll[pc+lib*grid->npcol]=MIN(ActiveFlagAll[pc+lib*grid->npcol],jb);
-				}					
-			}
-				
-
-			for (lib=0;lib<k;++lib){
-				ib = myrow+lib*grid->nprow;  /* not sure */
-				if(ib<nsupers){
-					pr = PROW( ib, grid );
-					for (j=0;j<grid->npcol;++j)ActiveFlag[j]=ActiveFlagAll[j+lib*grid->npcol];;
-					for (j=0;j<grid->npcol;++j)ActiveFlag[j+grid->npcol]=j;
-					for (j=0;j<grid->npcol;++j)ranks[j]=-1;
-					Root=-1; 
-					Iactive = 0;				
-
-					for (j=0;j<grid->npcol;++j){
-						if(ActiveFlag[j]!=3*nsupers){
-						jb = ActiveFlag[j];
-						pc = PCOL( jb, grid );
-						if(jb==ib)Root=pc;
-						if(mycol==pc)Iactive=1;		
-						}					
-					}
-					
-					quickSortM(ActiveFlag,0,grid->npcol-1,grid->npcol,0,2);
-
-					if(Iactive==1){
-						assert( Root>-1 );
-						rank_cnt = 1;
-						ranks[0]=Root;
-						for (j = 0; j < grid->npcol; ++j){
-							if(ActiveFlag[j]!=3*nsupers && ActiveFlag[j+grid->npcol]!=Root){
-								ranks[rank_cnt]=ActiveFlag[j+grid->npcol];
-								++rank_cnt;
-							}
-						}
-						if(rank_cnt>1){
-
-							for (ii=0;ii<rank_cnt;ii++)   // use global ranks rather than local ranks
-								ranks[ii] = PNUM( pr, ranks[ii], grid );		
-
-							// rseed=rand();
-							// rseed=1.0;
-							msgsize = SuperSize( ib )*nrhs+LSUM_H;
-
-							// if(ib==0){
-
-							URtree_ptr[lib] = RdTree_Create(grid->comm, ranks, rank_cnt, msgsize,SeedSTD_RD[lib]);  	
-							RdTree_SetTag(URtree_ptr[lib], RD_U);
-							// }
+		t_l += SuperLU_timer_() - t;
+#endif
+	    } /* if mycol == pc */
+
+	} /* for jb ... */
+
+	Llu->Lrowind_bc_ptr = Lrowind_bc_ptr;
+	Llu->Lnzval_bc_ptr = Lnzval_bc_ptr;
+	Llu->Ufstnz_br_ptr = Ufstnz_br_ptr;
+	Llu->Unzval_br_ptr = Unzval_br_ptr;
+	Llu->ToRecv = ToRecv;
+	Llu->ToSendD = ToSendD;
+	Llu->ToSendR = ToSendR;
+	Llu->fmod = fmod;
+	Llu->fsendx_plist = fsendx_plist;
+	Llu->nfrecvx = nfrecvx;
+	Llu->nfsendx = nfsendx;
+	Llu->bmod = bmod;
+	Llu->bsendx_plist = bsendx_plist;
+	Llu->nbrecvx = nbrecvx;
+	Llu->nbsendx = nbsendx;
+	Llu->ilsum = ilsum;
+	Llu->ldalsum = ldaspa;
 	
-							// #if ( PRNTlevel>=1 )
-							if(Root==mycol){
-							// printf("Partial Reduce Procs: %4d %4d %5d \n",iam, rank_cnt,brecv[lib]);
-							// fflush(stdout);
-							assert(rank_cnt==brecv[lib]);
-							// printf("Partial Reduce Procs: row%7d np%4d\n",ib,rank_cnt);
-							// printf("Partial Reduce Procs: %4d %4d: ",iam, rank_cnt);
-							// // for(j=0;j<rank_cnt;++j)printf("%4d",ranks[j]);
-							// printf("\n");
-							}
-							// #endif		
-						}
-					}
-				}						
-			}
-			SUPERLU_FREE(mod_bit);
-			SUPERLU_FREE(brecv);
-
-
-			SUPERLU_FREE(ActiveFlag);
-			SUPERLU_FREE(ActiveFlagAll);
-			SUPERLU_FREE(ranks);	
-			// SUPERLU_FREE(idxs);	
-			SUPERLU_FREE(SeedSTD_RD);	
-			// for(i=0;i<nsupers;++i){
-				// if(nzrows[i])SUPERLU_FREE(nzrows[i]);
-			// }
-			// SUPERLU_FREE(nzrows);				
-				
-#if ( PROFlevel>=1 )
-		t = SuperLU_timer_() - t;
-		if ( !iam) printf(".. Construct Reduce tree for U: %.2f\t\n", t);
-#endif						
-				
-				////////////////////////////////////////////////////////
-
-				
-				Llu->Lrowind_bc_ptr = Lrowind_bc_ptr;
-				Llu->Lindval_loc_bc_ptr = Lindval_loc_bc_ptr;
-				Llu->Lnzval_bc_ptr = Lnzval_bc_ptr;
-				Llu->Ufstnz_br_ptr = Ufstnz_br_ptr;
-				Llu->Unzval_br_ptr = Unzval_br_ptr;
-				Llu->ToRecv = ToRecv;
-				Llu->ToSendD = ToSendD;
-				Llu->ToSendR = ToSendR;
-				Llu->fmod = fmod;
-				Llu->fsendx_plist = fsendx_plist;
-				Llu->nfrecvx = nfrecvx;
-				Llu->nfsendx = nfsendx;
-				Llu->bmod = bmod;
-				Llu->bsendx_plist = bsendx_plist;
-				Llu->nbrecvx = nbrecvx;
-				Llu->nbsendx = nbsendx;
-				Llu->ilsum = ilsum;
-				Llu->ldalsum = ldaspa;
-				
-				Llu->LRtree_ptr = LRtree_ptr;
-				Llu->LBtree_ptr = LBtree_ptr;
-				Llu->URtree_ptr = URtree_ptr;
-				Llu->UBtree_ptr = UBtree_ptr;
-				Llu->Linv_bc_ptr = Linv_bc_ptr;
-				Llu->Uinv_bc_ptr = Uinv_bc_ptr;	
-				Llu->Urbs = Urbs; 
-				Llu->Ucb_indptr = Ucb_indptr; 
-				Llu->Ucb_valptr = Ucb_valptr; 
-
 #if ( PRNTlevel>=1 )
-				if ( !iam ) printf(".. # L blocks " IFMT "\t# U blocks " IFMT "\n",
-						nLblocks, nUblocks);
+	if ( !iam ) printf(".. # L blocks " IFMT "\t# U blocks " IFMT "\n",
+			   nLblocks, nUblocks);
 #endif
 
-				SUPERLU_FREE(rb_marker);
-				SUPERLU_FREE(Urb_fstnz);
-				SUPERLU_FREE(Urb_length);
-				SUPERLU_FREE(Urb_indptr);
-				SUPERLU_FREE(Lrb_length);
-				SUPERLU_FREE(Lrb_number);
-				SUPERLU_FREE(Lrb_indptr);
-				SUPERLU_FREE(Lrb_valptr);
-				SUPERLU_FREE(dense);
+	SUPERLU_FREE(rb_marker);
+	SUPERLU_FREE(Urb_fstnz);
+	SUPERLU_FREE(Urb_length);
+	SUPERLU_FREE(Urb_indptr);
+	SUPERLU_FREE(Lrb_length);
+	SUPERLU_FREE(Lrb_number);
+	SUPERLU_FREE(Lrb_indptr);
+	SUPERLU_FREE(Lrb_valptr);
+	SUPERLU_FREE(dense);
 
-				/* Find the maximum buffer size. */
-				MPI_Allreduce(mybufmax, Llu->bufmax, NBUFFERS, mpi_int_t, 
-						MPI_MAX, grid->comm);
+	/* Find the maximum buffer size. */
+	MPI_Allreduce(mybufmax, Llu->bufmax, NBUFFERS, mpi_int_t, 
+		      MPI_MAX, grid->comm);
 
-				k = CEILING( nsupers, grid->nprow );/* Number of local block rows */
-				if ( !(Llu->mod_bit = intMalloc_dist(k)) )
-					ABORT("Malloc fails for mod_bit[].");
+	k = CEILING( nsupers, grid->nprow );/* Number of local block rows */
+	if ( !(Llu->mod_bit = intMalloc_dist(k)) )
+	    ABORT("Malloc fails for mod_bit[].");
 
 #if ( PROFlevel>=1 )
-				if ( !iam ) printf(".. 1st distribute time:\n "
-						"\tL\t%.2f\n\tU\t%.2f\n"
-						"\tu_blks %d\tnrbu %d\n--------\n",
-						t_l, t_u, u_blks, nrbu);
+	if ( !iam ) printf(".. 1st distribute time:\n "
+			   "\tL\t%.2f\n\tU\t%.2f\n"
+			   "\tu_blks %d\tnrbu %d\n--------\n",
+  			   t_l, t_u, u_blks, nrbu);
 #endif
 
-			} /* else fact != SamePattern_SameRowPerm */
+    } /* else fact != SamePattern_SameRowPerm */
 
-			if ( xa[A->ncol] > 0 ) { /* may not have any entries on this process. */
-				SUPERLU_FREE(asub);
-				SUPERLU_FREE(a);
-			}
-			SUPERLU_FREE(xa);
+    if ( xa[A->ncol] > 0 ) { /* may not have any entries on this process. */
+        SUPERLU_FREE(asub);
+        SUPERLU_FREE(a);
+    }
+    SUPERLU_FREE(xa);
 
 #if ( DEBUGlevel>=1 )
-			/* Memory allocated but not freed:
-			   ilsum, fmod, fsendx_plist, bmod, bsendx_plist  */
-			CHECK_MALLOC(iam, "Exit pddistribute()");
+    /* Memory allocated but not freed:
+       ilsum, fmod, fsendx_plist, bmod, bsendx_plist  */
+    CHECK_MALLOC(iam, "Exit pddistribute()");
 #endif
-
-			return (mem_use);
-		} /* PDDISTRIBUTE */
+    
+    return (mem_use);
+} /* PDDISTRIBUTE */
diff --git a/SRC/pdgssvx.c b/SRC/pdgssvx.c
index 7d42782..3e413f2 100644
--- a/SRC/pdgssvx.c
+++ b/SRC/pdgssvx.c
@@ -551,7 +551,6 @@ pdgssvx(superlu_dist_options_t *options, SuperMatrix *A,
     int   col, key; /* parameters for creating a new communicator */
     Pslu_freeable_t Pslu_freeable;
     float  flinfo;
-	int blas_flag;
 
     /* Initialization. */
     m       = A->nrow;
@@ -925,8 +924,6 @@ pdgssvx(superlu_dist_options_t *options, SuperMatrix *A,
 #endif
     }
 
-	
-
     /* ------------------------------------------------------------
        Perform the LU factorization: symbolic factorization, 
        redistribution, and numerical factorization.
@@ -978,15 +975,7 @@ pdgssvx(superlu_dist_options_t *options, SuperMatrix *A,
 	if ( permc_spec != MY_PERMC && Fact == DOFACT ) {
           /* Reuse perm_c if Fact == SamePattern, or SamePattern_SameRowPerm */
 	  if ( permc_spec == PARMETIS ) {
-	      
-		  
-	// #pragma omp parallel  
-    // {  	
-	// #pragma omp master
-	// {	
-	
-		  
-		  /* Get column permutation vector in perm_c.                    *
+	      /* Get column permutation vector in perm_c.                    *
 	       * This routine takes as input the distributed input matrix A  *
 	       * and does not modify it.  It also allocates memory for       *
 	       * sizes[] and fstVtxSep[] arrays, that contain information    *
@@ -994,9 +983,6 @@ pdgssvx(superlu_dist_options_t *options, SuperMatrix *A,
 	      flinfo = get_perm_c_parmetis(A, perm_r, perm_c, nprocs_num,
                                   	   noDomains, &sizes, &fstVtxSep,
                                            grid, &symb_comm);
-	// }
-	// }	
-										   
 	      if (flinfo > 0) {
 #if ( PRNTlevel>=1 )
 	          fprintf(stderr, "Insufficient memory for get_perm_c parmetis\n");
@@ -1119,7 +1105,7 @@ pdgssvx(superlu_dist_options_t *options, SuperMatrix *A,
   	       distribution routine. */
 	    t = SuperLU_timer_();
 	    dist_mem_use = pddistribute(Fact, n, A, ScalePermstruct,
-                                      Glu_freeable, LUstruct, grid, nrhs);
+                                      Glu_freeable, LUstruct, grid);
 	    stat->utime[DIST] = SuperLU_timer_() - t;
 
   	    /* Deallocate storage used in symbolic factorization. */
@@ -1136,7 +1122,7 @@ pdgssvx(superlu_dist_options_t *options, SuperMatrix *A,
 
     	    t = SuperLU_timer_();
 	    dist_mem_use = ddist_psymbtonum(Fact, n, A, ScalePermstruct,
-		  			   &Pslu_freeable, LUstruct, grid, nrhs);
+		  			   &Pslu_freeable, LUstruct, grid);
 	    if (dist_mem_use > 0)
 	        ABORT ("Not enough memory available for dist_psymbtonum\n");
             
@@ -1147,20 +1133,8 @@ pdgssvx(superlu_dist_options_t *options, SuperMatrix *A,
 
 	/* Perform numerical factorization in parallel. */
 	t = SuperLU_timer_();
-	
-	
-
-	
-    // #pragma omp parallel  
-    // {  	
-	// #pragma omp master
-	// {	
-		
 	pdgstrf(options, m, n, anorm, LUstruct, grid, stat, info);
-	stat->utime[FACT] = SuperLU_timer_() - t;	
-	// }
-	// }	 
-	
+	stat->utime[FACT] = SuperLU_timer_() - t;
 
 #if 0
 
@@ -1331,30 +1305,10 @@ pdgssvx(superlu_dist_options_t *options, SuperMatrix *A,
 	       For repeated call to pdgssvx(), no need to re-initialilze
 	       the Solve data & communication structures, unless a new
 	       factorization with Fact == DOFACT or SamePattern is asked for. */
-		if(options->DiagInv==YES){	
-
-	#ifdef _CRAY
-			  blas_flag=1;
-	#elif defined (USE_VENDOR_BLAS)
-			  blas_flag=2;
-	#else
-			  blas_flag=0;
-	#endif	
-			if(blas_flag==0)
-			ABORT("DiagInv doesn't works with internal blas\n");
-			pdCompute_Diag_Inv(n, LUstruct, grid, stat, info);
-		}	
 	} 
 
-    // #pragma omp parallel  
-    // {  	
-	// #pragma omp master
-	// {
 	pdgstrs(n, LUstruct, ScalePermstruct, grid, X, m_loc, 
 		fst_row, ldb, nrhs, SOLVEstruct, stat, info);
-	// }
-	// }
-
 
 	/* ------------------------------------------------------------
 	   Use iterative refinement to improve the computed solution and
diff --git a/SRC/pdgstrf.c b/SRC/pdgstrf.c
index 06d7aa3..d049b86 100644
--- a/SRC/pdgstrf.c
+++ b/SRC/pdgstrf.c
@@ -766,9 +766,6 @@ pdgstrf(superlu_dist_options_t * options, int m, int n, double anorm,
 	}
 
     }
-	
-
-
 
     /* Max row size is global reduction within a row */
     MPI_Allreduce (&local_max_row_size, &max_row_size, 1, MPI_INT, MPI_MAX,
@@ -809,7 +806,6 @@ pdgstrf(superlu_dist_options_t * options, int m, int n, double anorm,
 					  Glu_persist, grid, perm_u );
 #endif
 
-
     /* +16 to avoid cache line false sharing */
     int_t bigv_size = SUPERLU_MAX(max_row_size * (bigu_size / ldt),
 				  (ldt*ldt + CACHELINE / dword) * num_threads);
diff --git a/SRC/pdgstrs.c b/SRC/pdgstrs.c
index 3423ece..7c47e74 100644
--- a/SRC/pdgstrs.c
+++ b/SRC/pdgstrs.c
@@ -1,13 +1,13 @@
 /*! \file
-  Copyright (c) 2003, The Regents of the University of California, through
-  Lawrence Berkeley National Laboratory (subject to receipt of any required 
-  approvals from U.S. Dept. of Energy) 
+Copyright (c) 2003, The Regents of the University of California, through
+Lawrence Berkeley National Laboratory (subject to receipt of any required 
+approvals from U.S. Dept. of Energy) 
 
-  All rights reserved. 
+All rights reserved. 
 
-  The source code is distributed under BSD license, see the file License.txt
-  at the top-level directory.
- */
+The source code is distributed under BSD license, see the file License.txt
+at the top-level directory.
+*/
 
 
 /*! @file 
@@ -20,12 +20,8 @@
  * October 15, 2008
  * </pre>
  */
-#include <math.h>
-#include "superlu_ddefs.h"
 
-#ifndef CACHELINE
-#define CACHELINE 64  /* bytes, Xeon Phi KNL, Cori haswell, Edision */
-#endif
+#include "superlu_ddefs.h"
 
 /*
  * Sketch of the algorithm for L-solve:
@@ -83,7 +79,7 @@
  *         | | | | |                 |
  *	   --------- <---------------|
  */
-
+  
 /*#define ISEND_IRECV*/
 
 /*
@@ -91,7 +87,7 @@
  */
 #ifdef _CRAY
 fortran void STRSM(_fcd, _fcd, _fcd, _fcd, int*, int*, double*,
-		double*, int*, double*, int*);
+		   double*, int*, double*, int*);
 _fcd ftcs1;
 _fcd ftcs2;
 _fcd ftcs3;
@@ -150,200 +146,118 @@ _fcd ftcs3;
  * </pre>
  */
 
-	int_t
+int_t
 pdReDistribute_B_to_X(double *B, int_t m_loc, int nrhs, int_t ldb,
-		int_t fst_row, int_t *ilsum, double *x,
-		ScalePermstruct_t *ScalePermstruct,
-		Glu_persist_t *Glu_persist,
-		gridinfo_t *grid, SOLVEstruct_t *SOLVEstruct)
+                      int_t fst_row, int_t *ilsum, double *x,
+		      ScalePermstruct_t *ScalePermstruct,
+		      Glu_persist_t *Glu_persist,
+		      gridinfo_t *grid, SOLVEstruct_t *SOLVEstruct)
 {
-	int  *SendCnt, *SendCnt_nrhs, *RecvCnt, *RecvCnt_nrhs;
-	int  *sdispls, *sdispls_nrhs, *rdispls, *rdispls_nrhs;
-	int  *ptr_to_ibuf, *ptr_to_dbuf;
-	int_t  *perm_r, *perm_c; /* row and column permutation vectors */
-	int_t  *send_ibuf, *recv_ibuf;
-	double *send_dbuf, *recv_dbuf;
-	int_t  *xsup, *supno;
-	int_t  i, ii, irow, gbi, j, jj, k, knsupc, l, lk, nbrow;
-	int    p, procs;
-	pxgstrs_comm_t *gstrs_comm = SOLVEstruct->gstrs_comm;
-
-	MPI_Request req_i, req_d, *req_send, *req_recv;
-	MPI_Status status, *status_send, *status_recv;
-	int Nreq_recv, Nreq_send, pp;
+    int  *SendCnt, *SendCnt_nrhs, *RecvCnt, *RecvCnt_nrhs;
+    int  *sdispls, *sdispls_nrhs, *rdispls, *rdispls_nrhs;
+    int  *ptr_to_ibuf, *ptr_to_dbuf;
+    int_t  *perm_r, *perm_c; /* row and column permutation vectors */
+    int_t  *send_ibuf, *recv_ibuf;
+    double *send_dbuf, *recv_dbuf;
+    int_t  *xsup, *supno;
+    int_t  i, ii, irow, gbi, j, jj, k, knsupc, l, lk;
+    int    p, procs;
+    pxgstrs_comm_t *gstrs_comm = SOLVEstruct->gstrs_comm;
 
 #if ( DEBUGlevel>=1 )
-	CHECK_MALLOC(grid->iam, "Enter pdReDistribute_B_to_X()");
+    CHECK_MALLOC(grid->iam, "Enter pdReDistribute_B_to_X()");
 #endif
 
-	/* ------------------------------------------------------------
-	   INITIALIZATION.
-	   ------------------------------------------------------------*/
-	perm_r = ScalePermstruct->perm_r;
-	perm_c = ScalePermstruct->perm_c;
-	procs = grid->nprow * grid->npcol;
-	xsup = Glu_persist->xsup;
-	supno = Glu_persist->supno;
-	SendCnt      = gstrs_comm->B_to_X_SendCnt;
-	SendCnt_nrhs = gstrs_comm->B_to_X_SendCnt +   procs;
-	RecvCnt      = gstrs_comm->B_to_X_SendCnt + 2*procs;
-	RecvCnt_nrhs = gstrs_comm->B_to_X_SendCnt + 3*procs;
-	sdispls      = gstrs_comm->B_to_X_SendCnt + 4*procs;
-	sdispls_nrhs = gstrs_comm->B_to_X_SendCnt + 5*procs;
-	rdispls      = gstrs_comm->B_to_X_SendCnt + 6*procs;
-	rdispls_nrhs = gstrs_comm->B_to_X_SendCnt + 7*procs;
-	ptr_to_ibuf  = gstrs_comm->ptr_to_ibuf;
-	ptr_to_dbuf  = gstrs_comm->ptr_to_dbuf;
-
-	/* ------------------------------------------------------------
-	   NOW COMMUNICATE THE ACTUAL DATA.
-	   ------------------------------------------------------------*/
-	k = sdispls[procs-1] + SendCnt[procs-1]; /* Total number of sends */
-	l = rdispls[procs-1] + RecvCnt[procs-1]; /* Total number of receives */
-	if ( !(send_ibuf = intMalloc_dist(k + l)) )
-		ABORT("Malloc fails for send_ibuf[].");
-	recv_ibuf = send_ibuf + k;
-	if ( !(send_dbuf = doubleMalloc_dist((k + l)* (size_t)nrhs)) )
-		ABORT("Malloc fails for send_dbuf[].");
-	recv_dbuf = send_dbuf + k * nrhs;
-	if ( !(req_send = (MPI_Request*) SUPERLU_MALLOC(procs*sizeof(MPI_Request))) )
-		ABORT("Malloc fails for req_send[].");	
-	if ( !(req_recv = (MPI_Request*) SUPERLU_MALLOC(procs*sizeof(MPI_Request))) )
-		ABORT("Malloc fails for req_recv[].");
-	if ( !(status_send = (MPI_Status*) SUPERLU_MALLOC(procs*sizeof(MPI_Status))) )
-		ABORT("Malloc fails for status_send[].");
-	if ( !(status_recv = (MPI_Status*) SUPERLU_MALLOC(procs*sizeof(MPI_Status))) )
-		ABORT("Malloc fails for status_recv[].");
-
-	for (p = 0; p < procs; ++p) {
-		ptr_to_ibuf[p] = sdispls[p];
-		ptr_to_dbuf[p] = sdispls[p] * nrhs;
+    /* ------------------------------------------------------------
+       INITIALIZATION.
+       ------------------------------------------------------------*/
+    perm_r = ScalePermstruct->perm_r;
+    perm_c = ScalePermstruct->perm_c;
+    procs = grid->nprow * grid->npcol;
+    xsup = Glu_persist->xsup;
+    supno = Glu_persist->supno;
+    SendCnt      = gstrs_comm->B_to_X_SendCnt;
+    SendCnt_nrhs = gstrs_comm->B_to_X_SendCnt +   procs;
+    RecvCnt      = gstrs_comm->B_to_X_SendCnt + 2*procs;
+    RecvCnt_nrhs = gstrs_comm->B_to_X_SendCnt + 3*procs;
+    sdispls      = gstrs_comm->B_to_X_SendCnt + 4*procs;
+    sdispls_nrhs = gstrs_comm->B_to_X_SendCnt + 5*procs;
+    rdispls      = gstrs_comm->B_to_X_SendCnt + 6*procs;
+    rdispls_nrhs = gstrs_comm->B_to_X_SendCnt + 7*procs;
+    ptr_to_ibuf  = gstrs_comm->ptr_to_ibuf;
+    ptr_to_dbuf  = gstrs_comm->ptr_to_dbuf;
+
+    /* ------------------------------------------------------------
+       NOW COMMUNICATE THE ACTUAL DATA.
+       ------------------------------------------------------------*/
+    k = sdispls[procs-1] + SendCnt[procs-1]; /* Total number of sends */
+    l = rdispls[procs-1] + RecvCnt[procs-1]; /* Total number of receives */
+    if ( !(send_ibuf = intMalloc_dist(k + l)) )
+        ABORT("Malloc fails for send_ibuf[].");
+    recv_ibuf = send_ibuf + k;
+    if ( !(send_dbuf = doubleMalloc_dist((k + l)* (size_t)nrhs)) )
+        ABORT("Malloc fails for send_dbuf[].");
+    recv_dbuf = send_dbuf + k * nrhs;
+    
+    for (p = 0; p < procs; ++p) {
+        ptr_to_ibuf[p] = sdispls[p];
+        ptr_to_dbuf[p] = sdispls[p] * nrhs;
+    }
+
+    /* Copy the row indices and values to the send buffer. */
+    for (i = 0, l = fst_row; i < m_loc; ++i, ++l) {
+        irow = perm_c[perm_r[l]]; /* Row number in Pc*Pr*B */
+	gbi = BlockNum( irow );
+	p = PNUM( PROW(gbi,grid), PCOL(gbi,grid), grid ); /* Diagonal process */
+	k = ptr_to_ibuf[p];
+	send_ibuf[k] = irow;
+	k = ptr_to_dbuf[p];
+	RHS_ITERATE(j) { /* RHS is stored in row major in the buffer. */
+	    send_dbuf[k++] = B[i + j*ldb];
 	}
-
-	/* Copy the row indices and values to the send buffer. */
-	for (i = 0, l = fst_row; i < m_loc; ++i, ++l) {
-		irow = perm_c[perm_r[l]]; /* Row number in Pc*Pr*B */
-		gbi = BlockNum( irow );
-		p = PNUM( PROW(gbi,grid), PCOL(gbi,grid), grid ); /* Diagonal process */
-		k = ptr_to_ibuf[p];
-		send_ibuf[k] = irow;
-		k = ptr_to_dbuf[p];
-		RHS_ITERATE(j) { /* RHS is stored in row major in the buffer. */
-			send_dbuf[k++] = B[i + j*ldb];
-		}
-		++ptr_to_ibuf[p];
-		ptr_to_dbuf[p] += nrhs;
-	}
-
-
-#if 1	
-	/* Communicate the (permuted) row indices. */
-	MPI_Alltoallv(send_ibuf, SendCnt, sdispls, mpi_int_t,
-			recv_ibuf, RecvCnt, rdispls, mpi_int_t, grid->comm);
-
-	/* Communicate the numerical values. */
-	MPI_Alltoallv(send_dbuf, SendCnt_nrhs, sdispls_nrhs, MPI_DOUBLE,
-			recv_dbuf, RecvCnt_nrhs, rdispls_nrhs, MPI_DOUBLE,
-			grid->comm);
-
-#else	
-
-	/* Communicate the (permuted) row indices. */
-	MPI_Ialltoallv(send_ibuf, SendCnt, sdispls, mpi_int_t,
-			recv_ibuf, RecvCnt, rdispls, mpi_int_t, grid->comm, &req_i);
-
-	/* Communicate the numerical values. */
-	MPI_Ialltoallv(send_dbuf, SendCnt_nrhs, sdispls_nrhs, MPI_DOUBLE,
-			recv_dbuf, RecvCnt_nrhs, rdispls_nrhs, MPI_DOUBLE,
-			grid->comm, &req_d);	
-	MPI_Wait(&req_i,&status);
-	MPI_Wait(&req_d,&status);
-
-#endif	
-
-
-
-	// MPI_Barrier( grid->comm );
-
-
-	// Nreq_send=0;
-	// for (pp=0;pp<procs;pp++){
-	// if(SendCnt[pp]>0){
-	// MPI_Isend(&send_ibuf[sdispls[pp]], SendCnt[pp], mpi_int_t, pp, 0, grid->comm,
-	// &req_send[Nreq_send] );
-	// Nreq_send++;
-	// }
-	// }
-
-	// Nreq_recv=0;
-	// for (pp=0;pp<procs;pp++){
-	// if(RecvCnt[pp]>0){
-	// MPI_Irecv(&recv_ibuf[rdispls[pp]], RecvCnt[pp], mpi_int_t, pp, 0, grid->comm,
-	// &req_recv[Nreq_recv] );
-	// Nreq_recv++;
-	// }
-	// }
-
-	// if(Nreq_send>0)MPI_Waitall(Nreq_send,req_send,status_send);
-	// if(Nreq_recv>0)MPI_Waitall(Nreq_recv,req_recv,status_recv);
-
-
-	// Nreq_send=0;
-	// for (pp=0;pp<procs;pp++){
-	// if(SendCnt_nrhs[pp]>0){
-	// MPI_Isend(&send_dbuf[sdispls_nrhs[pp]], SendCnt_nrhs[pp], MPI_DOUBLE, pp, 1, grid->comm,
-	// &req_send[Nreq_send] );
-	// Nreq_send++;
-	// }
-	// }
-	// Nreq_recv=0;
-	// for (pp=0;pp<procs;pp++){
-	// if(RecvCnt_nrhs[pp]>0){
-	// MPI_Irecv(&recv_dbuf[rdispls_nrhs[pp]], RecvCnt_nrhs[pp], MPI_DOUBLE, pp, 1, grid->comm,
-	// &req_recv[Nreq_recv] );
-	// Nreq_recv++;
-	// }
-	// }
-
-	// if(Nreq_send>0)MPI_Waitall(Nreq_send,req_send,status_send);
-	// if(Nreq_recv>0)MPI_Waitall(Nreq_recv,req_recv,status_recv);
-
-
-
-	/* ------------------------------------------------------------
-	   Copy buffer into X on the diagonal processes.
-	   ------------------------------------------------------------*/
-	ii = 0;
-	for (p = 0; p < procs; ++p) {
-		jj = rdispls_nrhs[p];
-		for (i = 0; i < RecvCnt[p]; ++i) {
-			/* Only the diagonal processes do this; the off-diagonal processes
-			   have 0 RecvCnt. */
-			irow = recv_ibuf[ii]; /* The permuted row index. */
-			k = BlockNum( irow );
-			knsupc = SuperSize( k );
-			lk = LBi( k, grid );  /* Local block number. */
-			l = X_BLK( lk );
-			x[l - XK_H] = k;      /* Block number prepended in the header. */
-			irow = irow - FstBlockC(k); /* Relative row number in X-block */
-			RHS_ITERATE(j) {
-				x[l + irow + j*knsupc] = recv_dbuf[jj++];
-			}
-			++ii;
-		}
+	++ptr_to_ibuf[p];
+	ptr_to_dbuf[p] += nrhs;
+    }
+
+    /* Communicate the (permuted) row indices. */
+    MPI_Alltoallv(send_ibuf, SendCnt, sdispls, mpi_int_t,
+		  recv_ibuf, RecvCnt, rdispls, mpi_int_t, grid->comm);
+
+    /* Communicate the numerical values. */
+    MPI_Alltoallv(send_dbuf, SendCnt_nrhs, sdispls_nrhs, MPI_DOUBLE,
+		  recv_dbuf, RecvCnt_nrhs, rdispls_nrhs, MPI_DOUBLE,
+		  grid->comm);
+    
+    /* ------------------------------------------------------------
+       Copy buffer into X on the diagonal processes.
+       ------------------------------------------------------------*/
+    ii = 0;
+    for (p = 0; p < procs; ++p) {
+        jj = rdispls_nrhs[p];
+        for (i = 0; i < RecvCnt[p]; ++i) {
+	    /* Only the diagonal processes do this; the off-diagonal processes
+	       have 0 RecvCnt. */
+	    irow = recv_ibuf[ii]; /* The permuted row index. */
+	    k = BlockNum( irow );
+	    knsupc = SuperSize( k );
+	    lk = LBi( k, grid );  /* Local block number. */
+	    l = X_BLK( lk );
+	    x[l - XK_H] = k;      /* Block number prepended in the header. */
+	    irow = irow - FstBlockC(k); /* Relative row number in X-block */
+	    RHS_ITERATE(j) {
+	        x[l + irow + j*knsupc] = recv_dbuf[jj++];
+	    }
+	    ++ii;
 	}
+    }
 
-	SUPERLU_FREE(send_ibuf);
-	SUPERLU_FREE(send_dbuf);
-	SUPERLU_FREE(req_send);
-	SUPERLU_FREE(req_recv);
-	SUPERLU_FREE(status_send);
-	SUPERLU_FREE(status_recv);
-
+    SUPERLU_FREE(send_ibuf);
+    SUPERLU_FREE(send_dbuf);
+    
 #if ( DEBUGlevel>=1 )
-	CHECK_MALLOC(grid->iam, "Exit pdReDistribute_B_to_X()");
+    CHECK_MALLOC(grid->iam, "Exit pdReDistribute_B_to_X()");
 #endif
-	return 0;
+    return 0;
 } /* pdReDistribute_B_to_X */
 
 /*! \brief
@@ -361,320 +275,122 @@ pdReDistribute_B_to_X(double *B, int_t m_loc, int nrhs, int_t ldb,
  * </pre>
  */
 
-	int_t
+int_t
 pdReDistribute_X_to_B(int_t n, double *B, int_t m_loc, int_t ldb, int_t fst_row,
-		int_t nrhs, double *x, int_t *ilsum,
-		ScalePermstruct_t *ScalePermstruct,
-		Glu_persist_t *Glu_persist, gridinfo_t *grid,
-		SOLVEstruct_t *SOLVEstruct)
+		      int_t nrhs, double *x, int_t *ilsum,
+		      ScalePermstruct_t *ScalePermstruct,
+		      Glu_persist_t *Glu_persist, gridinfo_t *grid,
+		      SOLVEstruct_t *SOLVEstruct)
 {
-	int_t  i, ii, irow, j, jj, k, knsupc, nsupers, l, lk;
-	int_t  *xsup, *supno;
-	int  *SendCnt, *SendCnt_nrhs, *RecvCnt, *RecvCnt_nrhs;
-	int  *sdispls, *rdispls, *sdispls_nrhs, *rdispls_nrhs;
-	int  *ptr_to_ibuf, *ptr_to_dbuf;
-	int_t  *send_ibuf, *recv_ibuf;
-	double *send_dbuf, *recv_dbuf;
-	int_t  *row_to_proc = SOLVEstruct->row_to_proc; /* row-process mapping */
-	pxgstrs_comm_t *gstrs_comm = SOLVEstruct->gstrs_comm;
-	int  iam, p, q, pkk, procs;
-	int_t  num_diag_procs, *diag_procs;
-	MPI_Request req_i, req_d, *req_send, *req_recv;
-	MPI_Status status, *status_send, *status_recv;
-	int Nreq_recv, Nreq_send, pp;
+    int_t  i, ii, irow, j, jj, k, knsupc, nsupers, l, lk;
+    int_t  *xsup, *supno;
+    int  *SendCnt, *SendCnt_nrhs, *RecvCnt, *RecvCnt_nrhs;
+    int  *sdispls, *rdispls, *sdispls_nrhs, *rdispls_nrhs;
+    int  *ptr_to_ibuf, *ptr_to_dbuf;
+    int_t  *send_ibuf, *recv_ibuf;
+    double *send_dbuf, *recv_dbuf;
+    int_t  *row_to_proc = SOLVEstruct->row_to_proc; /* row-process mapping */
+    pxgstrs_comm_t *gstrs_comm = SOLVEstruct->gstrs_comm;
+    int  iam, p, q, pkk, procs;
+    int_t  num_diag_procs, *diag_procs;
 
 #if ( DEBUGlevel>=1 )
-	CHECK_MALLOC(grid->iam, "Enter pdReDistribute_X_to_B()");
+    CHECK_MALLOC(grid->iam, "Enter pdReDistribute_X_to_B()");
 #endif
 
-	/* ------------------------------------------------------------
-	   INITIALIZATION.
-	   ------------------------------------------------------------*/
-	xsup = Glu_persist->xsup;
-	supno = Glu_persist->supno;
-	nsupers = Glu_persist->supno[n-1] + 1;
-	iam = grid->iam;
-	procs = grid->nprow * grid->npcol;
-
-	SendCnt      = gstrs_comm->X_to_B_SendCnt;
-	SendCnt_nrhs = gstrs_comm->X_to_B_SendCnt +   procs;
-	RecvCnt      = gstrs_comm->X_to_B_SendCnt + 2*procs;
-	RecvCnt_nrhs = gstrs_comm->X_to_B_SendCnt + 3*procs;
-	sdispls      = gstrs_comm->X_to_B_SendCnt + 4*procs;
-	sdispls_nrhs = gstrs_comm->X_to_B_SendCnt + 5*procs;
-	rdispls      = gstrs_comm->X_to_B_SendCnt + 6*procs;
-	rdispls_nrhs = gstrs_comm->X_to_B_SendCnt + 7*procs;
-	ptr_to_ibuf  = gstrs_comm->ptr_to_ibuf;
-	ptr_to_dbuf  = gstrs_comm->ptr_to_dbuf;
-
-	k = sdispls[procs-1] + SendCnt[procs-1]; /* Total number of sends */
-	l = rdispls[procs-1] + RecvCnt[procs-1]; /* Total number of receives */
-	if ( !(send_ibuf = intMalloc_dist(k + l)) )
-		ABORT("Malloc fails for send_ibuf[].");
-	recv_ibuf = send_ibuf + k;
-	if ( !(send_dbuf = doubleMalloc_dist((k + l)*nrhs)) )
-		ABORT("Malloc fails for send_dbuf[].");
-	if ( !(req_send = (MPI_Request*) SUPERLU_MALLOC(procs*sizeof(MPI_Request))) )
-		ABORT("Malloc fails for req_send[].");	
-	if ( !(req_recv = (MPI_Request*) SUPERLU_MALLOC(procs*sizeof(MPI_Request))) )
-		ABORT("Malloc fails for req_recv[].");
-	if ( !(status_send = (MPI_Status*) SUPERLU_MALLOC(procs*sizeof(MPI_Status))) )
-		ABORT("Malloc fails for status_send[].");
-	if ( !(status_recv = (MPI_Status*) SUPERLU_MALLOC(procs*sizeof(MPI_Status))) )
-		ABORT("Malloc fails for status_recv[].");	
-
-
-	recv_dbuf = send_dbuf + k * nrhs;
-	for (p = 0; p < procs; ++p) {
-		ptr_to_ibuf[p] = sdispls[p];
-		ptr_to_dbuf[p] = sdispls_nrhs[p];
-	}
-	num_diag_procs = SOLVEstruct->num_diag_procs;
-	diag_procs = SOLVEstruct->diag_procs;
-
-	for (p = 0; p < num_diag_procs; ++p) {  /* For all diagonal processes. */
-		pkk = diag_procs[p];
-		if ( iam == pkk ) {
-			for (k = p; k < nsupers; k += num_diag_procs) {
-				knsupc = SuperSize( k );
-				lk = LBi( k, grid ); /* Local block number */
-				irow = FstBlockC( k );
-				l = X_BLK( lk );
-				for (i = 0; i < knsupc; ++i) {
+    /* ------------------------------------------------------------
+       INITIALIZATION.
+       ------------------------------------------------------------*/
+    xsup = Glu_persist->xsup;
+    supno = Glu_persist->supno;
+    nsupers = Glu_persist->supno[n-1] + 1;
+    iam = grid->iam;
+    procs = grid->nprow * grid->npcol;
+ 
+    SendCnt      = gstrs_comm->X_to_B_SendCnt;
+    SendCnt_nrhs = gstrs_comm->X_to_B_SendCnt +   procs;
+    RecvCnt      = gstrs_comm->X_to_B_SendCnt + 2*procs;
+    RecvCnt_nrhs = gstrs_comm->X_to_B_SendCnt + 3*procs;
+    sdispls      = gstrs_comm->X_to_B_SendCnt + 4*procs;
+    sdispls_nrhs = gstrs_comm->X_to_B_SendCnt + 5*procs;
+    rdispls      = gstrs_comm->X_to_B_SendCnt + 6*procs;
+    rdispls_nrhs = gstrs_comm->X_to_B_SendCnt + 7*procs;
+    ptr_to_ibuf  = gstrs_comm->ptr_to_ibuf;
+    ptr_to_dbuf  = gstrs_comm->ptr_to_dbuf;
+
+    k = sdispls[procs-1] + SendCnt[procs-1]; /* Total number of sends */
+    l = rdispls[procs-1] + RecvCnt[procs-1]; /* Total number of receives */
+    if ( !(send_ibuf = intMalloc_dist(k + l)) )
+        ABORT("Malloc fails for send_ibuf[].");
+    recv_ibuf = send_ibuf + k;
+    if ( !(send_dbuf = doubleMalloc_dist((k + l)*nrhs)) )
+        ABORT("Malloc fails for send_dbuf[].");
+    recv_dbuf = send_dbuf + k * nrhs;
+    for (p = 0; p < procs; ++p) {
+        ptr_to_ibuf[p] = sdispls[p];
+        ptr_to_dbuf[p] = sdispls_nrhs[p];
+    }
+    num_diag_procs = SOLVEstruct->num_diag_procs;
+    diag_procs = SOLVEstruct->diag_procs;
+
+    for (p = 0; p < num_diag_procs; ++p) {  /* For all diagonal processes. */
+	pkk = diag_procs[p];
+	if ( iam == pkk ) {
+	    for (k = p; k < nsupers; k += num_diag_procs) {
+		knsupc = SuperSize( k );
+		lk = LBi( k, grid ); /* Local block number */
+		irow = FstBlockC( k );
+		l = X_BLK( lk );
+		for (i = 0; i < knsupc; ++i) {
 #if 0
-					ii = inv_perm_c[irow]; /* Apply X <== Pc'*Y */
+		    ii = inv_perm_c[irow]; /* Apply X <== Pc'*Y */
 #else
-					ii = irow;
+		    ii = irow;
 #endif
-					q = row_to_proc[ii];
-					jj = ptr_to_ibuf[q];
-					send_ibuf[jj] = ii;
-					jj = ptr_to_dbuf[q];
-					RHS_ITERATE(j) { /* RHS stored in row major in buffer. */
-						send_dbuf[jj++] = x[l + i + j*knsupc];
-					}
-					++ptr_to_ibuf[q];
-					ptr_to_dbuf[q] += nrhs;
-					++irow;
-				}
-			}
+		    q = row_to_proc[ii];
+		    jj = ptr_to_ibuf[q];
+		    send_ibuf[jj] = ii;
+		    jj = ptr_to_dbuf[q];
+		    RHS_ITERATE(j) { /* RHS stored in row major in buffer. */
+		        send_dbuf[jj++] = x[l + i + j*knsupc];
+		    }
+		    ++ptr_to_ibuf[q];
+		    ptr_to_dbuf[q] += nrhs;
+		    ++irow;
 		}
+	    }
 	}
-
-	/* ------------------------------------------------------------
-	   COMMUNICATE THE (PERMUTED) ROW INDICES AND NUMERICAL VALUES.
-	   ------------------------------------------------------------*/
-
-#if 1
-
-	MPI_Alltoallv(send_ibuf, SendCnt, sdispls, mpi_int_t,
-			recv_ibuf, RecvCnt, rdispls, mpi_int_t, grid->comm);
-	MPI_Alltoallv(send_dbuf, SendCnt_nrhs, sdispls_nrhs, MPI_DOUBLE, 
-			recv_dbuf, RecvCnt_nrhs, rdispls_nrhs, MPI_DOUBLE,
-			grid->comm);
-
-#else
-	MPI_Ialltoallv(send_ibuf, SendCnt, sdispls, mpi_int_t,
-			recv_ibuf, RecvCnt, rdispls, mpi_int_t, grid->comm,&req_i);
-	MPI_Ialltoallv(send_dbuf, SendCnt_nrhs, sdispls_nrhs, MPI_DOUBLE, 
-			recv_dbuf, RecvCnt_nrhs, rdispls_nrhs, MPI_DOUBLE,
-			grid->comm,&req_d);
-
-	MPI_Wait(&req_i,&status);
-	MPI_Wait(&req_d,&status);		 
-#endif	
-
-	// MPI_Barrier( grid->comm );
-	// Nreq_send=0;
-	// for (pp=0;pp<procs;pp++){
-	// if(SendCnt[pp]>0){
-	// MPI_Isend(&send_ibuf[sdispls[pp]], SendCnt[pp], mpi_int_t, pp, 0, grid->comm,
-	// &req_send[Nreq_send] );
-	// Nreq_send++;
-	// }
-	// }
-
-	// Nreq_recv=0;
-	// for (pp=0;pp<procs;pp++){
-	// if(RecvCnt[pp]>0){
-	// MPI_Irecv(&recv_ibuf[rdispls[pp]], RecvCnt[pp], mpi_int_t, pp, 0, grid->comm,
-	// &req_recv[Nreq_recv] );
-	// Nreq_recv++;
-	// }
-	// }
-
-	// if(Nreq_send>0)MPI_Waitall(Nreq_send,req_send,status_send);
-	// if(Nreq_recv>0)MPI_Waitall(Nreq_recv,req_recv,status_recv);
-	// // MPI_Barrier( grid->comm );
-
-	// Nreq_send=0;
-	// for (pp=0;pp<procs;pp++){
-	// if(SendCnt_nrhs[pp]>0){
-	// MPI_Isend(&send_dbuf[sdispls_nrhs[pp]], SendCnt_nrhs[pp], MPI_DOUBLE, pp, 1, grid->comm,
-	// &req_send[Nreq_send] );
-	// Nreq_send++;
-	// }
-	// }
-	// Nreq_recv=0;
-	// for (pp=0;pp<procs;pp++){
-	// if(RecvCnt_nrhs[pp]>0){
-	// MPI_Irecv(&recv_dbuf[rdispls_nrhs[pp]], RecvCnt_nrhs[pp], MPI_DOUBLE, pp, 1, grid->comm,
-	// &req_recv[Nreq_recv] );
-	// Nreq_recv++;
-	// }
-	// }
-
-	// if(Nreq_send>0)MPI_Waitall(Nreq_send,req_send,status_send);
-	// if(Nreq_recv>0)MPI_Waitall(Nreq_recv,req_recv,status_recv);
-	// // MPI_Barrier( grid->comm );
-
-
-
-
-
-	/* ------------------------------------------------------------
-	   COPY THE BUFFER INTO B.
-	   ------------------------------------------------------------*/
-	for (i = 0, k = 0; i < m_loc; ++i) {
-		irow = recv_ibuf[i];
-		irow -= fst_row; /* Relative row number */
-		RHS_ITERATE(j) { /* RHS is stored in row major in the buffer. */
-			B[irow + j*ldb] = recv_dbuf[k++];
-		}
+    }
+    
+    /* ------------------------------------------------------------
+        COMMUNICATE THE (PERMUTED) ROW INDICES AND NUMERICAL VALUES.
+       ------------------------------------------------------------*/
+    MPI_Alltoallv(send_ibuf, SendCnt, sdispls, mpi_int_t,
+		  recv_ibuf, RecvCnt, rdispls, mpi_int_t, grid->comm);
+    MPI_Alltoallv(send_dbuf, SendCnt_nrhs, sdispls_nrhs, MPI_DOUBLE, 
+		  recv_dbuf, RecvCnt_nrhs, rdispls_nrhs, MPI_DOUBLE,
+		  grid->comm);
+
+    /* ------------------------------------------------------------
+       COPY THE BUFFER INTO B.
+       ------------------------------------------------------------*/
+    for (i = 0, k = 0; i < m_loc; ++i) {
+	irow = recv_ibuf[i];
+	irow -= fst_row; /* Relative row number */
+	RHS_ITERATE(j) { /* RHS is stored in row major in the buffer. */
+	    B[irow + j*ldb] = recv_dbuf[k++];
 	}
+    }
 
-	SUPERLU_FREE(send_ibuf);
-	SUPERLU_FREE(send_dbuf);
-	SUPERLU_FREE(req_send);
-	SUPERLU_FREE(req_recv);
-	SUPERLU_FREE(status_send);
-	SUPERLU_FREE(status_recv);
-
+    SUPERLU_FREE(send_ibuf);
+    SUPERLU_FREE(send_dbuf);
 #if ( DEBUGlevel>=1 )
-	CHECK_MALLOC(grid->iam, "Exit pdReDistribute_X_to_B()");
+    CHECK_MALLOC(grid->iam, "Exit pdReDistribute_X_to_B()");
 #endif
-	return 0;
+    return 0;
 
 } /* pdReDistribute_X_to_B */
 
-
-
-	void
-pdCompute_Diag_Inv(int_t n, LUstruct_t *LUstruct,gridinfo_t *grid, SuperLUStat_t *stat, int *info)
-{
-	Glu_persist_t *Glu_persist = LUstruct->Glu_persist;
-	LocalLU_t *Llu = LUstruct->Llu;
-
-	double *lusup;
-	double *recvbuf, *tempv;
-	double *Linv;/* Inverse of diagonal block */
-	double *Uinv;/* Inverse of diagonal block */
-
-	int_t  kcol, krow, mycol, myrow;
-	int_t  i, ii, il, j, jj, k, lb, ljb, lk, lptr, luptr;
-	int_t  nb, nlb,nlb_nodiag, nub, nsupers;
-	int_t  *xsup, *supno, *lsub, *usub;
-	int_t  *ilsum;    /* Starting position of each supernode in lsum (LOCAL)*/
-	int    Pc, Pr, iam;
-	int    knsupc, nsupr;
-	int    ldalsum;   /* Number of lsum entries locally owned. */
-	int    maxrecvsz, p, pi;
-	int_t  **Lrowind_bc_ptr;
-	double **Lnzval_bc_ptr;
-	double **Linv_bc_ptr;
-	double **Uinv_bc_ptr;
-	int INFO;
-	double t;
-
-#if ( PROFlevel>=1 )
-	t = SuperLU_timer_();
-#endif 
-
-	// printf("wocao \n");
-	// fflush(stdout);
-	if(grid->iam==0){
-		printf("computing inverse of diagonal blocks...\n");
-		fflush(stdout);
-	}
-	/*
-	 * Initialization.
-	 */
-	iam = grid->iam;
-	Pc = grid->npcol;
-	Pr = grid->nprow;
-	myrow = MYROW( iam, grid );
-	mycol = MYCOL( iam, grid );
-	xsup = Glu_persist->xsup;
-	supno = Glu_persist->supno;
-	nsupers = supno[n-1] + 1;
-	Lrowind_bc_ptr = Llu->Lrowind_bc_ptr;
-	Linv_bc_ptr = Llu->Linv_bc_ptr;
-	Uinv_bc_ptr = Llu->Uinv_bc_ptr;
-	Lnzval_bc_ptr = Llu->Lnzval_bc_ptr;
-	nlb = CEILING( nsupers, Pr ); /* Number of local block rows. */
-
-
-	Llu->inv = 1;
-
-	/*---------------------------------------------------
-	 * Compute inverse of L(lk,lk).
-	 *---------------------------------------------------*/
-
-	for (k = 0; k < nsupers; ++k) {
-		krow = PROW( k, grid );
-		if ( myrow == krow ) {
-			lk = LBi( k, grid );    /* local block number */
-			kcol = PCOL( k, grid );
-			if ( mycol == kcol ) { /* diagonal process */
-
-				lk = LBj( k, grid ); /* Local block number, column-wise. */
-				lsub = Lrowind_bc_ptr[lk];
-				lusup = Lnzval_bc_ptr[lk];
-				Linv = Linv_bc_ptr[lk];
-				Uinv = Uinv_bc_ptr[lk];
-				nsupr = lsub[1];	
-				knsupc = SuperSize( k );
-
-				for (j=0 ; j<knsupc; j++){
-					Linv[j*knsupc+j] = 1.0;
-
-					for (i=j+1 ; i<knsupc; i++){
-						Linv[j*knsupc+i] = lusup[j*nsupr+i];	
-					}
-
-					for (i=0 ; i<j+1; i++){
-						Uinv[j*knsupc+i] = lusup[j*nsupr+i];	
-					}			
-
-				}
-
-#ifdef _CRAY
-				ABORT("Cray blas dtrtri not implemented\n");
-#elif defined (USE_VENDOR_BLAS)
-				dtrtri_("L","U",&knsupc,Linv,&knsupc,&INFO);		  	
-				dtrtri_("U","N",&knsupc,Uinv,&knsupc,&INFO);	
-#else
-				ABORT("internal blas dtrtri not implemented\n");
-#endif			
-
-			}
-		}
-	}
-
-#if ( PROFlevel>=1 )
-	if(grid->iam==0){
-		t = SuperLU_timer_() - t;
-		printf(".. L-diag_inv time\t%10.5f\n", t);
-		fflush(stdout);
-	}
-#endif	
-
-	return;
-}
-
-
-
-
 /*! \brief
  *
  * <pre>
@@ -746,1654 +462,880 @@ pdCompute_Diag_Inv(int_t n, LUstruct_t *LUstruct,gridinfo_t *grid, SuperLUStat_t
  * </pre>       
  */
 
-	void
+void
 pdgstrs(int_t n, LUstruct_t *LUstruct, 
-		ScalePermstruct_t *ScalePermstruct,
-		gridinfo_t *grid, double *B,
-		int_t m_loc, int_t fst_row, int_t ldb, int nrhs,
-		SOLVEstruct_t *SOLVEstruct,
-		SuperLUStat_t *stat, int *info)
+	ScalePermstruct_t *ScalePermstruct,
+	gridinfo_t *grid, double *B,
+	int_t m_loc, int_t fst_row, int_t ldb, int nrhs,
+	SOLVEstruct_t *SOLVEstruct,
+	SuperLUStat_t *stat, int *info)
 {
-	Glu_persist_t *Glu_persist = LUstruct->Glu_persist;
-	LocalLU_t *Llu = LUstruct->Llu;
-	double alpha = 1.0;
-	double beta = 0.0;
-	double zero = 0.0;
-	double *lsum;  /* Local running sum of the updates to B-components */
-	double *x;     /* X component at step k. */
-	/* NOTE: x and lsum are of same size. */
-	double *lusup, *dest;
-	double *recvbuf,*recvbuf_on, *tempv, *recvbufall, *recvbuf_BC_fwd, *recvbuf0, *xin;
-	double *rtemp, *rtemp_loc; /* Result of full matrix-vector multiply. */
-	double *Linv; /* Inverse of diagonal block */
-	double *Uinv; /* Inverse of diagonal block */
-	int *ipiv; 
-	int_t *leaf_send;
-	int_t nleaf_send, nleaf_send_tmp;
-	int_t *root_send;
-	int_t nroot_send, nroot_send_tmp;
-	
-	int_t  **Ufstnz_br_ptr = Llu->Ufstnz_br_ptr;
-	BcTree  *LBtree_ptr = Llu->LBtree_ptr;
-	RdTree  *LRtree_ptr = Llu->LRtree_ptr;
-	BcTree  *UBtree_ptr = Llu->UBtree_ptr;
-	RdTree  *URtree_ptr = Llu->URtree_ptr;	
-	int_t  *Urbs1, *Urbs2; /* Number of row blocks in each block column of U. */
-	int_t  *Urbs = Llu->Urbs; /* Number of row blocks in each block column of U. */
-	Ucb_indptr_t **Ucb_indptr = Llu->Ucb_indptr;/* Vertical linked list pointing to Uindex[] */
-	int_t  **Ucb_valptr = Llu->Ucb_valptr;      /* Vertical linked list pointing to Unzval[] */
-	int_t  kcol, krow, mycol, myrow;
-	int_t  i, ii, il, j, jj, k, kk, lb, ljb, lk, lib, lptr, luptr, gb, nn;
-	int_t  nb, nlb,nlb_nodiag, nub, nsupers, nsupers_j, nsupers_i;
-	int_t  *xsup, *supno, *lsub, *usub;
-	int_t  *ilsum;    /* Starting position of each supernode in lsum (LOCAL)*/
-	int    Pc, Pr, iam;
-	int    knsupc, nsupr, nprobe;
-	int    nbtree, nrtree, outcount;
-	int    ldalsum;   /* Number of lsum entries locally owned. */
-	int    maxrecvsz, p, pi;
-	int_t  **Lrowind_bc_ptr;
-	double **Lnzval_bc_ptr;
-	double **Linv_bc_ptr;
-	double **Uinv_bc_ptr;
-	double sum;
-	MPI_Status status,status_on,statusx,statuslsum;
-	MPI_Request *send_req, recv_req, req;
-	pxgstrs_comm_t *gstrs_comm = SOLVEstruct->gstrs_comm;
-	SuperLUStat_t **stat_loc;
-
-	double tmax;
-
-	/*-- Counts used for L-solve --*/
-	int_t  *fmod;         /* Modification count for L-solve --
-				 Count the number of local block products to
-				 be summed into lsum[lk]. */
-	int_t fmod_tmp;
-	int_t  **fsendx_plist = Llu->fsendx_plist;
-	int_t  nfrecvx = Llu->nfrecvx; /* Number of X components to be recv'd. */
-	int_t  nfrecvx_buf=0;						 					 
-	int_t  *frecv;        /* Count of lsum[lk] contributions to be received
-				 from processes in this row. 
-				 It is only valid on the diagonal processes. */
-	int_t  frecv_tmp;
-	int_t  nfrecvmod = 0; /* Count of total modifications to be recv'd. */
-	int_t  nfrecv = 0; /* Count of total messages to be recv'd. */
-	int_t  nbrecv = 0; /* Count of total messages to be recv'd. */
-	int_t  nleaf = 0, nroot = 0;
-	int_t  nleaftmp = 0, nroottmp = 0;
-	int_t  msgsize;
-	/*-- Counts used for U-solve --*/
-	int_t  *bmod;         /* Modification count for U-solve. */
-	int_t bmod_tmp;
-	int_t  **bsendx_plist = Llu->bsendx_plist;
-	int_t  nbrecvx = Llu->nbrecvx; /* Number of X components to be recv'd. */
-	int_t  nbrecvx_buf=0;		
-	int_t  *brecv;        /* Count of modifications to be recv'd from
-				 processes in this row. */
-	int_t  nbrecvmod = 0; /* Count of total modifications to be recv'd. */
-	int_t flagx,flaglsum,flag;
-	int_t *LBTree_active, *LRTree_active, *LBTree_finish, *LRTree_finish, *leafsups, *rootsups; 
-	int_t TAG;
-	double t1_sol, t2_sol, t;
+    Glu_persist_t *Glu_persist = LUstruct->Glu_persist;
+    LocalLU_t *Llu = LUstruct->Llu;
+    double alpha = 1.0;
+    double zero = 0.0;
+    double *lsum;  /* Local running sum of the updates to B-components */
+    double *x;     /* X component at step k. */
+		    /* NOTE: x and lsum are of same size. */
+    double *lusup, *dest;
+    double *recvbuf, *tempv;
+    double *rtemp; /* Result of full matrix-vector multiply. */
+    int_t  **Ufstnz_br_ptr = Llu->Ufstnz_br_ptr;
+    int_t  *Urbs, *Urbs1; /* Number of row blocks in each block column of U. */
+    Ucb_indptr_t **Ucb_indptr;/* Vertical linked list pointing to Uindex[] */
+    int_t  **Ucb_valptr;      /* Vertical linked list pointing to Unzval[] */
+    int_t  kcol, krow, mycol, myrow;
+    int_t  i, ii, il, j, jj, k, lb, ljb, lk, lptr, luptr;
+    int_t  nb, nlb, nub, nsupers;
+    int_t  *xsup, *supno, *lsub, *usub;
+    int_t  *ilsum;    /* Starting position of each supernode in lsum (LOCAL)*/
+    int    Pc, Pr, iam;
+    int    knsupc, nsupr;
+    int    ldalsum;   /* Number of lsum entries locally owned. */
+    int    maxrecvsz, p, pi;
+    int_t  **Lrowind_bc_ptr;
+    double **Lnzval_bc_ptr;
+    MPI_Status status;
+    MPI_Request *send_req, recv_req;
+    pxgstrs_comm_t *gstrs_comm = SOLVEstruct->gstrs_comm;
+
+    /*-- Counts used for L-solve --*/
+    int_t  *fmod;         /* Modification count for L-solve --
+                             Count the number of local block products to
+                             be summed into lsum[lk]. */
+    int_t  **fsendx_plist = Llu->fsendx_plist;
+    int_t  nfrecvx = Llu->nfrecvx; /* Number of X components to be recv'd. */
+    int_t  *frecv;        /* Count of lsum[lk] contributions to be received
+                             from processes in this row. 
+                             It is only valid on the diagonal processes. */
+    int_t  nfrecvmod = 0; /* Count of total modifications to be recv'd. */
+    int_t  nleaf = 0, nroot = 0;
+
+    /*-- Counts used for U-solve --*/
+    int_t  *bmod;         /* Modification count for U-solve. */
+    int_t  **bsendx_plist = Llu->bsendx_plist;
+    int_t  nbrecvx = Llu->nbrecvx; /* Number of X components to be recv'd. */
+    int_t  *brecv;        /* Count of modifications to be recv'd from
+			     processes in this row. */
+    int_t  nbrecvmod = 0; /* Count of total modifications to be recv'd. */
+    double t;
 #if ( DEBUGlevel>=2 )
-	int_t Ublocks = 0;
+    int_t Ublocks = 0;
 #endif
 
-	int_t gik,iklrow,fnz;
-	
-	int_t *mod_bit = Llu->mod_bit; /* flag contribution from each row block */
-	int INFO, pad;
-	int_t tmpresult;
-
-	// #if ( PROFlevel>=1 )
-	double t1, t2;
-	float msg_vol = 0, msg_cnt = 0;
-	// #endif 
-
-	int_t *msgcnt=(int_t *) SUPERLU_MALLOC(4 * sizeof(int_t));   /* Count the size of the message xfer'd in each buffer:
-								      *     0 : transferred in Lsub_buf[]
-								      *     1 : transferred in Lval_buf[]
-								      *     2 : transferred in Usub_buf[]
-								      *     3 : transferred in Uval_buf[]
-								      */
-	int iword = sizeof (int_t);
-	int dword = sizeof (double);	
-	int Nwork;
-
-	yes_no_t done;
-	yes_no_t startforward;
-
-	int nbrow;
-	int_t  ik, rel, idx_r, jb, nrbl, irow, pc,iknsupc;
-	int_t  lptr1_tmp, idx_i, idx_v,m; 
-
-	int_t thread_id,ready;
-	yes_no_t empty;
-	int_t sizelsum,sizertemp,aln_d,aln_i;
-
-	aln_d = ceil(CACHELINE/(double)dword);
-	aln_i = ceil(CACHELINE/(double)iword);
-
-
-	int num_thread = 1;
-#ifdef _OPENMP
-#pragma omp parallel default(shared)
-	{
-		if (omp_get_thread_num () == 0) {
-			num_thread = omp_get_num_threads ();
-		}
-	}
-#endif
-	if(grid->iam==0){
-		printf("num_thread: %5d\n",num_thread);
-		fflush(stdout);
-	}
-
-	MPI_Barrier( grid->comm );
-	TIC(t1_sol);
-	t = SuperLU_timer_();
-
-	/* Test input parameters. */
-	*info = 0;
-	if ( n < 0 ) *info = -1;
-	else if ( nrhs < 0 ) *info = -9;
-	if ( *info ) {
-		pxerr_dist("PDGSTRS", grid, -*info);
-		return;
-	}
-
-	/*
-	 * Initialization.
-	 */
-	iam = grid->iam;
-	Pc = grid->npcol;
-	Pr = grid->nprow;
-	myrow = MYROW( iam, grid );
-	mycol = MYCOL( iam, grid );
-	xsup = Glu_persist->xsup;
-	supno = Glu_persist->supno;
-	nsupers = supno[n-1] + 1;
-	Lrowind_bc_ptr = Llu->Lrowind_bc_ptr;
-	Lnzval_bc_ptr = Llu->Lnzval_bc_ptr;
-	Linv_bc_ptr = Llu->Linv_bc_ptr;
-	Uinv_bc_ptr = Llu->Uinv_bc_ptr;
-	nlb = CEILING( nsupers, Pr ); /* Number of local block rows. */
-
-	stat->utime[SOL_COMM] = 0.0;
-	stat->utime[SOL_GEMM] = 0.0;
-	stat->utime[SOL_TRSM] = 0.0;
-	stat->utime[SOL_L] = 0.0;
+    int_t *mod_bit = Llu->mod_bit; /* flag contribution from each row block */
+ 
+    t = SuperLU_timer_();
 
+    /* Test input parameters. */
+    *info = 0;
+    if ( n < 0 ) *info = -1;
+    else if ( nrhs < 0 ) *info = -9;
+    if ( *info ) {
+	pxerr_dist("PDGSTRS", grid, -*info);
+	return;
+    }
+	
+    /*
+     * Initialization.
+     */
+    iam = grid->iam;
+    Pc = grid->npcol;
+    Pr = grid->nprow;
+    myrow = MYROW( iam, grid );
+    mycol = MYCOL( iam, grid );
+    xsup = Glu_persist->xsup;
+    supno = Glu_persist->supno;
+    nsupers = supno[n-1] + 1;
+    Lrowind_bc_ptr = Llu->Lrowind_bc_ptr;
+    Lnzval_bc_ptr = Llu->Lnzval_bc_ptr;
+    nlb = CEILING( nsupers, Pr ); /* Number of local block rows. */
 
 #if ( DEBUGlevel>=1 )
-	CHECK_MALLOC(iam, "Enter pdgstrs()");
+    CHECK_MALLOC(iam, "Enter pdgstrs()");
 #endif
 
-	stat->ops[SOLVE] = 0.0;
-	Llu->SolveMsgSent = 0;
+    stat->ops[SOLVE] = 0.0;
+    Llu->SolveMsgSent = 0;
 
-	/* Save the count to be altered so it can be used by
-	   subsequent call to PDGSTRS. */
+    /* Save the count to be altered so it can be used by
+       subsequent call to PDGSTRS. */
+    if ( !(fmod = intMalloc_dist(nlb)) )
+	ABORT("Calloc fails for fmod[].");
+    for (i = 0; i < nlb; ++i) fmod[i] = Llu->fmod[i];
+    if ( !(frecv = intMalloc_dist(nlb)) )
+	ABORT("Malloc fails for frecv[].");
+    Llu->frecv = frecv;
 
-	if ( !(fmod = intMalloc_dist(nlb)) )
-		ABORT("Calloc fails for fmod[].");
-	for (i = 0; i < nlb; ++i) fmod[i] = Llu->fmod[i];
-
-	if ( !(frecv = intCalloc_dist(nlb)) )
-		ABORT("Malloc fails for frecv[].");
-	Llu->frecv = frecv;
-
-	if ( !(leaf_send = intMalloc_dist(CEILING( nsupers, Pr )+CEILING( nsupers, Pc ))) )
-		ABORT("Malloc fails for leaf_send[].");
-	nleaf_send=0;
-
-	if ( !(root_send = intMalloc_dist(CEILING( nsupers, Pr )+CEILING( nsupers, Pc ))) )
-		ABORT("Malloc fails for root_send[].");
-	nroot_send=0;	
-	
+    k = SUPERLU_MAX( Llu->nfsendx, Llu->nbsendx ) + nlb;
+    if ( !(send_req = (MPI_Request*) SUPERLU_MALLOC(k*sizeof(MPI_Request))) )
+	ABORT("Malloc fails for send_req[].");
 
 #ifdef _CRAY
-	ftcs1 = _cptofcd("L", strlen("L"));
-	ftcs2 = _cptofcd("N", strlen("N"));
-	ftcs3 = _cptofcd("U", strlen("U"));
+    ftcs1 = _cptofcd("L", strlen("L"));
+    ftcs2 = _cptofcd("N", strlen("N"));
+    ftcs3 = _cptofcd("U", strlen("U"));
 #endif
 
-	/* Obtain ilsum[] and ldalsum for process column 0. */
-	ilsum = Llu->ilsum;
-	ldalsum = Llu->ldalsum;
-
-	/* Allocate working storage. */
-	knsupc = sp_ienv_dist(3);
-	maxrecvsz = knsupc * nrhs + SUPERLU_MAX( XK_H, LSUM_H );
-	sizelsum = (((size_t)ldalsum)*nrhs + nlb*LSUM_H);
-	sizelsum = ((sizelsum + (aln_d - 1)) / aln_d) * aln_d;
-
-#ifdef _OPENMP
-	if ( !(lsum = doubleMalloc_dist(sizelsum*num_thread)))
-		ABORT("Calloc fails for lsum[].");	
-#pragma omp parallel default(shared) private(thread_id,ii)
-	{
-		thread_id = omp_get_thread_num ();
-		for(ii=0;ii<sizelsum;ii++)
-			lsum[thread_id*sizelsum+ii]=0;
-	}
-#else	
-	if ( !(lsum = doubleCalloc_dist(sizelsum*num_thread)))
-		ABORT("Calloc fails for lsum[].");	
-#endif	
-
-	if ( !(x = doubleCalloc_dist(ldalsum * nrhs + nlb * XK_H)) )
-		ABORT("Calloc fails for x[].");
-	// if ( !(recvbuf = doubleMalloc_dist(maxrecvsz)) )
-		// ABORT("Malloc fails for recvbuf[].");   
-
-	sizertemp=ldalsum * nrhs;
-	sizertemp = ((sizertemp + (aln_d - 1)) / aln_d) * aln_d;
-#ifdef _OPENMP
-	if ( !(rtemp = doubleMalloc_dist(sizertemp* num_thread)) )
-		ABORT("Malloc fails for rtemp[].");
-#pragma omp parallel default(shared) private(thread_id,ii)
-	{
-		thread_id = omp_get_thread_num ();
-		for(ii=0;ii<sizertemp;ii++)
-			rtemp[thread_id*sizertemp+ii]=0;
-	}
-#else	
-	if ( !(rtemp = doubleCalloc_dist(sizertemp* num_thread)) )
-		ABORT("Malloc fails for rtemp[].");
-#endif	
-
-	if ( !(stat_loc = (SuperLUStat_t**) SUPERLU_MALLOC(num_thread*sizeof(SuperLUStat_t*))) )
-		ABORT("Malloc fails for stat_loc[].");
 
-	for(i=0;i<num_thread;i++){
-		stat_loc[i] = (SuperLUStat_t*)SUPERLU_MALLOC(sizeof(SuperLUStat_t));
-		PStatInit(stat_loc[i]);
+    /* Obtain ilsum[] and ldalsum for process column 0. */
+    ilsum = Llu->ilsum;
+    ldalsum = Llu->ldalsum;
+
+    /* Allocate working storage. */
+    knsupc = sp_ienv_dist(3);
+    maxrecvsz = knsupc * nrhs + SUPERLU_MAX( XK_H, LSUM_H );
+    if ( !(lsum = doubleCalloc_dist(((size_t)ldalsum)*nrhs + nlb*LSUM_H)) )
+	ABORT("Calloc fails for lsum[].");
+    if ( !(x = doubleCalloc_dist(ldalsum * nrhs + nlb * XK_H)) )
+	ABORT("Calloc fails for x[].");
+    if ( !(recvbuf = doubleMalloc_dist(maxrecvsz)) )
+	ABORT("Malloc fails for recvbuf[].");
+    if ( !(rtemp = doubleCalloc_dist(maxrecvsz)) )
+	ABORT("Malloc fails for rtemp[].");
+
+    
+    /*---------------------------------------------------
+     * Forward solve Ly = b.
+     *---------------------------------------------------*/
+    /* Redistribute B into X on the diagonal processes. */
+    pdReDistribute_B_to_X(B, m_loc, nrhs, ldb, fst_row, ilsum, x, 
+			  ScalePermstruct, Glu_persist, grid, SOLVEstruct);
+
+    /* Set up the headers in lsum[]. */
+    ii = 0;
+    for (k = 0; k < nsupers; ++k) {
+	knsupc = SuperSize( k );
+	krow = PROW( k, grid );
+	if ( myrow == krow ) {
+	    lk = LBi( k, grid );   /* Local block number. */
+	    il = LSUM_BLK( lk );
+	    lsum[il - LSUM_H] = k; /* Block number prepended in the header. */
 	}
+	ii += knsupc;
+    }
 
-	/*---------------------------------------------------
-	 * Forward solve Ly = b.
-	 *---------------------------------------------------*/
-	/* Redistribute B into X on the diagonal processes. */
-	pdReDistribute_B_to_X(B, m_loc, nrhs, ldb, fst_row, ilsum, x, 
-			ScalePermstruct, Glu_persist, grid, SOLVEstruct);
-
+    /*
+     * Compute frecv[] and nfrecvmod counts on the diagonal processes.
+     */
+    {
+	superlu_scope_t *scp = &grid->rscp;
 
-#if ( PRNTlevel>=1 )
-	t = SuperLU_timer_() - t;
-	if ( !iam) printf(".. B to X redistribute time\t%8.4f\n", t);
-	fflush(stdout);
-	t = SuperLU_timer_();
-#endif			  
-
-	/* Set up the headers in lsum[]. */
-	ii = 0;
+#if 1
+	for (k = 0; k < nlb; ++k) mod_bit[k] = 0;
 	for (k = 0; k < nsupers; ++k) {
-		knsupc = SuperSize( k );
-		krow = PROW( k, grid );
-		if ( myrow == krow ) {
-			lk = LBi( k, grid );   /* Local block number. */
-			il = LSUM_BLK( lk );
-			lsum[il - LSUM_H] = k; /* Block number prepended in the header. */
-		}
-		ii += knsupc;
-	}
-
-	/* ---------------------------------------------------------
-	   Precompute mapping from Lrowind_bc_ptr to lsum.
-	   --------------------------------------------------------- */		
-
-	   
-	   
-	// nsupers_j = CEILING( nsupers, grid->npcol ); /* Number of local block columns */
-	// if ( !(Llu->Lrowind_bc_2_lsum = 
-				// (int_t**)SUPERLU_MALLOC(nsupers_j * sizeof(int_t*))) )
-		// ABORT("Malloc fails for Lrowind_bc_2_lsum[].");
-
-
-	// for (ljb = 0; ljb < nsupers_j; ++ljb) {
-
-		// if(Lrowind_bc_ptr[ljb]!=NULL){
-
-			// jb = mycol+ljb*grid->npcol;
-
-			// knsupc = SuperSize( jb );
-			// krow = PROW( jb, grid );
-			// nrbl = Lrowind_bc_ptr[ljb][0];
-
-			// if(myrow==krow){ /* skip the diagonal block */
-				// nlb_nodiag=nrbl-1;
-				// idx_i = nlb_nodiag+2;
-				// m = Lrowind_bc_ptr[ljb][1]-knsupc;
-			// }else{
-				// nlb_nodiag=nrbl;
-				// idx_i = nlb_nodiag;
-				// m = Lrowind_bc_ptr[ljb][1];
-			// }	
-
-			// if(nlb_nodiag>0){		
-				// if ( !(Llu->Lrowind_bc_2_lsum[ljb] = intMalloc_dist(((m*nrhs + (aln_i - 1)) / aln_i) * aln_i)) )
-					// ABORT("Malloc fails for Lrowind_bc_2_lsum[ljb][].");	
-				// idx_r=0;
-				// RHS_ITERATE(j)
-					// for (lb = 0; lb < nlb_nodiag; ++lb) {
-						// lptr1_tmp = Llu->Lindval_loc_bc_ptr[ljb][lb+idx_i];	
-						// ik = Lrowind_bc_ptr[ljb][lptr1_tmp]; /* Global block number, row-wise. */
-						// iknsupc = SuperSize( ik );
-						// nbrow = Lrowind_bc_ptr[ljb][lptr1_tmp+1];
-						// lk = LBi( ik, grid ); /* Local block number, row-wise. */
-						// il = LSUM_BLK( lk );
-						// rel = xsup[ik]; /* Global row index of block ik. */
-						// for (ii = 0; ii < nbrow; ++ii) {	
-							// irow = Lrowind_bc_ptr[ljb][lptr1_tmp+LB_DESCRIPTOR+ii] - rel; /* Relative row. */		
-							// Llu->Lrowind_bc_2_lsum[ljb][idx_r++] = il+irow+ j*iknsupc;								
-						// }			
-					// }		
-			// }else{
-				// Llu->Lrowind_bc_2_lsum[ljb]=NULL;
-			// } 
-		// }else{
-			// Llu->Lrowind_bc_2_lsum[ljb]=NULL;		
-		// }		
-	// }	
-
-	/* ---------------------------------------------------------
-	   Initialize the async Bcast trees on all processes.
-	   --------------------------------------------------------- */		
-	nsupers_j = CEILING( nsupers, grid->npcol ); /* Number of local block columns */
-
-	nbtree = 0;
-	for (lk=0;lk<nsupers_j;++lk){
-		if(LBtree_ptr[lk]!=NULL){
-			// printf("LBtree_ptr lk %5d\n",lk); 
-			if(BcTree_IsRoot(LBtree_ptr[lk])==NO){			
-				nbtree++;
-				if(BcTree_getDestCount(LBtree_ptr[lk])>0)nfrecvx_buf++;				  
-			}
-			BcTree_allocateRequest(LBtree_ptr[lk]);
-		}
+	    krow = PROW( k, grid );
+	    if ( myrow == krow ) {
+		lk = LBi( k, grid );    /* local block number */
+		kcol = PCOL( k, grid );
+		if ( mycol != kcol && fmod[lk] )
+		    mod_bit[lk] = 1;  /* contribution from off-diagonal */
+	    }
 	}
-
-	nsupers_i = CEILING( nsupers, grid->nprow ); /* Number of local block rows */
-	if ( !(	leafsups = (int_t*)intCalloc_dist(nsupers_i)) )
-		ABORT("Calloc fails for leafsups.");
-
-	nrtree = 0;
-	nleaf=0;
-	for (lk=0;lk<nsupers_i;++lk){
-		if(LRtree_ptr[lk]!=NULL){
-			nrtree++;
-			RdTree_allocateRequest(LRtree_ptr[lk]);			
-			frecv[lk] = RdTree_GetDestCount(LRtree_ptr[lk]);
-			nfrecvmod += frecv[lk];
-		}else{
-			gb = myrow+lk*grid->nprow;  /* not sure */
-			if(gb<nsupers){
-				kcol = PCOL( gb, grid );
-				if(mycol==kcol) { /* Diagonal process */
-					if (fmod[lk]==0){
-						leafsups[nleaf]=gb;				
-						++nleaf;
-					}
-				}
-			}
-		}
-	}	
-
-
-	for (i = 0; i < nlb; ++i) fmod[i] += frecv[i];
-
-	if ( !(recvbuf_BC_fwd = doubleMalloc_dist(maxrecvsz*(nfrecvx+1))) )   // this needs to be optimized for 1D row mapping
-		ABORT("Malloc fails for recvbuf_BC_fwd[].");	
-	nfrecvx_buf=0;			
-
-#if ( DEBUGlevel>=2 )
-	printf("(%2d) nfrecvx %4d,  nfrecvmod %4d,  nleaf %4d\n,  nbtree %4d\n,  nrtree %4d\n",
-			iam, nfrecvx, nfrecvmod, nleaf, nbtree, nrtree);
-	fflush(stdout);
+	/*PrintInt10("mod_bit", nlb, mod_bit);*/
+	
+#if ( PROFlevel>=2 )
+	t_reduce_tmp = SuperLU_timer_();
 #endif
+	/* Every process receives the count, but it is only useful on the
+	   diagonal processes.  */
+	MPI_Allreduce( mod_bit, frecv, nlb, mpi_int_t, MPI_SUM, scp->comm );
 
-
-
-#if ( PRNTlevel>=1 )
-	t = SuperLU_timer_() - t;
-	if ( !iam) printf(".. Setup L-solve time\t%8.4f\n", t);
-	fflush(stdout);
-	MPI_Barrier( grid->comm );	
-	t = SuperLU_timer_();
+#if ( PROFlevel>=2 )
+	t_reduce += SuperLU_timer_() - t_reduce_tmp;
 #endif
 
+	for (k = 0; k < nsupers; ++k) {
+	    krow = PROW( k, grid );
+	    if ( myrow == krow ) {
+		lk = LBi( k, grid );    /* local block number */
+		kcol = PCOL( k, grid );
+		if ( mycol == kcol ) { /* diagonal process */
+		    nfrecvmod += frecv[lk];
+		    if ( !frecv[lk] && !fmod[lk] ) ++nleaf;
+		}
+	    }
+	}
 
-#if ( VAMPIR>=1 )
-	// VT_initialize(); 
-	VT_traceon();	
-#endif
-
+#else /* old */
 
-	/* ---------------------------------------------------------
-	   Solve the leaf nodes first by all the diagonal processes.
-	   --------------------------------------------------------- */
+	for (k = 0; k < nsupers; ++k) {
+	    krow = PROW( k, grid );
+	    if ( myrow == krow ) {
+		lk = LBi( k, grid );    /* Local block number. */
+		kcol = PCOL( k, grid ); /* Root process in this row scope. */
+		if ( mycol != kcol && fmod[lk] )
+		    i = 1;  /* Contribution from non-diagonal process. */
+		else i = 0;
+		MPI_Reduce( &i, &frecv[lk], 1, mpi_int_t,
+			   MPI_SUM, kcol, scp->comm );
+		if ( mycol == kcol ) { /* Diagonal process. */
+		    nfrecvmod += frecv[lk];
+		    if ( !frecv[lk] && !fmod[lk] ) ++nleaf;
 #if ( DEBUGlevel>=2 )
-	printf("(%2d) nleaf %4d\n", iam, nleaf);
-	fflush(stdout);
+		    printf("(%2d) frecv[%4d]  %2d\n", iam, k, frecv[lk]);
+		    assert( frecv[lk] < Pc );
 #endif
-
-
-#ifdef _OPENMP
-#pragma omp parallel default (shared) 
-#endif
-	{	
-#ifdef _OPENMP
-#pragma omp master
+		}
+	    }
+	}
 #endif
-		{
+    }
 
-#ifdef _OPENMP
-#pragma	omp	taskloop firstprivate (nrhs,beta,alpha,x,rtemp,ldalsum) private (ii,k,knsupc,lk,luptr,lsub,nsupr,lusup,thread_id,t1,t2,Linv,i,lib,rtemp_loc,nleaf_send_tmp) nogroup	
-#endif
-			for (jj=0;jj<nleaf;jj++){
-				k=leafsups[jj];
-
-				// #ifdef _OPENMP
-				// #pragma	omp	task firstprivate (k,nrhs,beta,alpha,x,rtemp,ldalsum) private (ii,knsupc,lk,luptr,lsub,nsupr,lusup,thread_id,t1,t2,Linv,i,lib,rtemp_loc)	 	
-				// #endif
-				{
-
-#if ( PROFlevel>=1 )
-					TIC(t1);
-#endif	 
-#ifdef _OPENMP
-					thread_id = omp_get_thread_num ();
-#else
-					thread_id = 0;
+    /* ---------------------------------------------------------
+       Solve the leaf nodes first by all the diagonal processes.
+       --------------------------------------------------------- */
+#if ( DEBUGlevel>=2 )
+    printf("(%2d) nleaf %4d\n", iam, nleaf);
 #endif
-					rtemp_loc = &rtemp[sizertemp* thread_id];
-
-
-					knsupc = SuperSize( k );
-					lk = LBi( k, grid );
-
-					// if ( frecv[lk]==0 && fmod[lk]==0 ) { 
-					// fmod[lk] = -1;  /* Do not solve X[k] in the future. */
-					ii = X_BLK( lk );
-					lk = LBj( k, grid ); /* Local block number, column-wise. */
-					lsub = Lrowind_bc_ptr[lk];
-					lusup = Lnzval_bc_ptr[lk];
-
-					nsupr = lsub[1];
-
-
-
-					if(Llu->inv == 1){
-						Linv = Linv_bc_ptr[lk];
-#ifdef _CRAY
-						SGEMM( ftcs2, ftcs2, &knsupc, &nrhs, &knsupc,
-								&alpha, Linv, &knsupc, &x[ii],
-								&knsupc, &beta, rtemp_loc, &knsupc );
-#elif defined (USE_VENDOR_BLAS)
-						dgemm_( "N", "N", &knsupc, &nrhs, &knsupc,
-								&alpha, Linv, &knsupc, &x[ii],
-								&knsupc, &beta, rtemp_loc, &knsupc, 1, 1 );
-#else
-						dgemm_( "N", "N", &knsupc, &nrhs, &knsupc,
-								&alpha, Linv, &knsupc, &x[ii],
-								&knsupc, &beta, rtemp_loc, &knsupc );
-#endif		   
-						for (i=0 ; i<knsupc*nrhs ; i++){
-							x[ii+i] = rtemp_loc[i];
-						}		
-					}else{
+    for (k = 0; k < nsupers && nleaf; ++k) {
+	krow = PROW( k, grid );
+	kcol = PCOL( k, grid );
+	if ( myrow == krow && mycol == kcol ) { /* Diagonal process */
+	    knsupc = SuperSize( k );
+	    lk = LBi( k, grid );
+	    if ( frecv[lk]==0 && fmod[lk]==0 ) {
+		fmod[lk] = -1;  /* Do not solve X[k] in the future. */
+		ii = X_BLK( lk );
+		lk = LBj( k, grid ); /* Local block number, column-wise. */
+		lsub = Lrowind_bc_ptr[lk];
+		lusup = Lnzval_bc_ptr[lk];
+		nsupr = lsub[1];
 #ifdef _CRAY
-						STRSM(ftcs1, ftcs1, ftcs2, ftcs3, &knsupc, &nrhs, &alpha,
-								lusup, &nsupr, &x[ii], &knsupc);
+		STRSM(ftcs1, ftcs1, ftcs2, ftcs3, &knsupc, &nrhs, &alpha,
+		      lusup, &nsupr, &x[ii], &knsupc);
 #elif defined (USE_VENDOR_BLAS)
-						dtrsm_("L", "L", "N", "U", &knsupc, &nrhs, &alpha, 
-								lusup, &nsupr, &x[ii], &knsupc, 1, 1, 1, 1);	
+		dtrsm_("L", "L", "N", "U", &knsupc, &nrhs, &alpha, 
+		       lusup, &nsupr, &x[ii], &knsupc, 1, 1, 1, 1);
 #else
-						dtrsm_("L", "L", "N", "U", &knsupc, &nrhs, &alpha, 
-								lusup, &nsupr, &x[ii], &knsupc);
+		dtrsm_("L", "L", "N", "U", &knsupc, &nrhs, &alpha, 
+		       lusup, &nsupr, &x[ii], &knsupc);
 #endif
-					}
-
-
-#if ( PROFlevel>=1 )
-					TOC(t2, t1);
-					stat_loc[thread_id]->utime[SOL_TRSM] += t2;
-
-#endif	
-
-					stat_loc[thread_id]->ops[SOLVE] += knsupc * (knsupc - 1) * nrhs;
-					// --nleaf;
+		stat->ops[SOLVE] += knsupc * (knsupc - 1) * nrhs;
+		--nleaf;
 #if ( DEBUGlevel>=2 )
-					printf("(%2d) Solve X[%2d]\n", iam, k);
-#endif
-
-					/*
-					 * Send Xk to process column Pc[k].
-					 */
-
-					if(LBtree_ptr[lk]!=NULL){ 
-						lib = LBi( k, grid ); /* Local block number, row-wise. */
-						ii = X_BLK( lib );	
-
-#ifdef _OPENMP
-#pragma omp atomic capture
-#endif
-						nleaf_send_tmp = ++nleaf_send;
-						leaf_send[nleaf_send_tmp-1] = lk;
-						// BcTree_forwardMessageSimple(LBtree_ptr[lk],&x[ii - XK_H]);
-					}
-				}		
-				}
-			}	
-		}
-
-
-#if ( VTUNE>=1 )
-		__itt_resume();
+		printf("(%2d) Solve X[%2d]\n", iam, k);
 #endif
+		
+		/*
+		 * Send Xk to process column Pc[k].
+		 */
+		for (p = 0; p < Pr; ++p) {
+		    if ( fsendx_plist[lk][p] != EMPTY ) {
+			pi = PNUM( p, kcol, grid );
 
-		jj=0;
-#ifdef _OPENMP
-#pragma omp parallel default (shared) private(thread_id)
-		{
-			thread_id = omp_get_thread_num ();
-#else
-			{
-				thread_id = 0;
+			MPI_Isend( &x[ii - XK_H], knsupc * nrhs + XK_H,
+				   MPI_DOUBLE, pi, Xk, grid->comm,
+                                   &send_req[Llu->SolveMsgSent++]);
+#if 0
+			MPI_Send( &x[ii - XK_H], knsupc * nrhs + XK_H,
+				 MPI_DOUBLE, pi, Xk, grid->comm );
 #endif
-
-#ifdef _OPENMP
-#pragma omp master
+#if ( DEBUGlevel>=2 )
+			printf("(%2d) Sent X[%2.0f] to P %2d\n",
+			       iam, x[ii-XK_H], pi);
 #endif
-				{
-
-#ifdef _OPENMP
-#pragma	omp	taskloop private (i,k,ii,knsupc,lk,nb,lptr,luptr,lsub,lusup,thread_id) untied num_tasks(num_thread*8) nogroup
+		    }
+		}
+		/*
+		 * Perform local block modifications: lsum[i] -= L_i,k * X[k]
+		 */
+		nb = lsub[0] - 1;
+		lptr = BC_HEADER + LB_DESCRIPTOR + knsupc;
+		luptr = knsupc; /* Skip diagonal block L(k,k). */
+		
+		dlsum_fmod(lsum, x, &x[ii], rtemp, nrhs, knsupc, k,
+			   fmod, nb, lptr, luptr, xsup, grid, Llu, 
+			   send_req, stat);
+	    }
+	} /* if diagonal process ... */
+    } /* for k ... */
+
+    /* -----------------------------------------------------------
+       Compute the internal nodes asynchronously by all processes.
+       ----------------------------------------------------------- */
+#if ( DEBUGlevel>=2 )
+    printf("(%2d) nfrecvx %4d,  nfrecvmod %4d,  nleaf %4d\n",
+	   iam, nfrecvx, nfrecvmod, nleaf);
 #endif
 
-					for (jj=0;jj<nleaf;jj++){
-						k=leafsups[jj];		
+    while ( nfrecvx || nfrecvmod ) { /* While not finished. */
 
-						// #ifdef _OPENMP
-						// #pragma	omp	task firstprivate (k) private (ii,knsupc,lk,nb,lptr,luptr,lsub,lusup,thread_id) untied	 	
-						// #endif
-						{
+	/* Receive a message. */
+	MPI_Recv( recvbuf, maxrecvsz, MPI_DOUBLE,
+                  MPI_ANY_SOURCE, MPI_ANY_TAG, grid->comm, &status );
 
-#ifdef _OPENMP
-							thread_id = omp_get_thread_num ();
-#else
-							thread_id = 0;
-#endif								
-
-							/* Diagonal process */
-							knsupc = SuperSize( k );
-							lk = LBi( k, grid );
-
-							// if ( frecv[lk]==0 && fmod[lk]==0 ) { 
-							// fmod[lk] = -1;  /* Do not solve X[k] in the future. */
-							ii = X_BLK( lk );
-							lk = LBj( k, grid ); /* Local block number, column-wise. */
-							lsub = Lrowind_bc_ptr[lk];
-							lusup = Lnzval_bc_ptr[lk];
-
-							/*
-							 * Perform local block modifications: lsum[i] -= L_i,k * X[k]
-							 */
-							nb = lsub[0] - 1;
-							dlsum_fmod_inv(lsum, x, &x[ii], rtemp, nrhs, knsupc, k,
-									fmod, nb, xsup, grid, Llu, 
-									stat_loc, leaf_send, &nleaf_send,sizelsum,sizertemp,0);	
-						}
-
-						// } /* if diagonal process ... */
-					} /* for k ... */
-				}
+	k = *recvbuf;
 
-			}
-
-			for (i=0;i<nleaf_send;i++){
-				lk = leaf_send[i];
-				if(lk>=0){ // this is a bcast forwarding
-					gb = mycol+lk*grid->npcol;  /* not sure */
-					lib = LBi( gb, grid ); /* Local block number, row-wise. */
-					ii = X_BLK( lib );			
-					BcTree_forwardMessageSimple(LBtree_ptr[lk],&x[ii - XK_H]);
-				}else{ // this is a reduce forwarding
-					lk = -lk - 1;
-					il = LSUM_BLK( lk );
-					RdTree_forwardMessageSimple(LRtree_ptr[lk],&lsum[il - LSUM_H ]);
-				}
-			}
-
-
-
-#if ( VTUNE>=1 )
-			__itt_pause();
-#endif
-
-			/* -----------------------------------------------------------
-			   Compute the internal nodes asynchronously by all processes.
-			   ----------------------------------------------------------- */
-
-#ifdef _OPENMP
-#pragma omp parallel default (shared) 
-#endif
-			{	
-#ifdef _OPENMP
-#pragma omp master 
-#endif
-				{									 
-					for ( nfrecv =0; nfrecv<nfrecvx+nfrecvmod;nfrecv++) { /* While not finished. */
-						thread_id = 0;
-#if ( PROFlevel>=1 )
-						TIC(t1);
-						// msgcnt[1] = maxrecvsz;
-#endif	
-
-						recvbuf0 = &recvbuf_BC_fwd[nfrecvx_buf*maxrecvsz];
-
-						/* Receive a message. */
-						MPI_Recv( recvbuf0, maxrecvsz, MPI_DOUBLE,
-								MPI_ANY_SOURCE, MPI_ANY_TAG, grid->comm, &status );	 	
-						// MPI_Irecv(recvbuf0,maxrecvsz,MPI_DOUBLE,MPI_ANY_SOURCE,MPI_ANY_TAG,grid->comm,&req);
-						// ready=0;
-						// while(ready==0){
-						// MPI_Test(&req,&ready,&status);
-						// #pragma omp taskyield
-						// }
-
-#if ( PROFlevel>=1 )		 
-						TOC(t2, t1);
-						stat_loc[thread_id]->utime[SOL_COMM] += t2;
-
-						msg_cnt += 1;
-						msg_vol += maxrecvsz * dword;			
-#endif					  
-
-						{  
-
-							k = *recvbuf0;
 #if ( DEBUGlevel>=2 )
-							printf("(%2d) Recv'd block %d, tag %2d\n", iam, k, status.MPI_TAG);
+	printf("(%2d) Recv'd block %d, tag %2d\n", iam, k, status.MPI_TAG);
 #endif
-
-							if(status.MPI_TAG==BC_L){
-								// --nfrecvx;
-								nfrecvx_buf++;
-								{
-									lk = LBj( k, grid );    /* local block number */
-
-									if(BcTree_getDestCount(LBtree_ptr[lk])>0){
-
-										BcTree_forwardMessageSimple(LBtree_ptr[lk],recvbuf0);	
-										// nfrecvx_buf++;
-									}
-
-									/*
-									 * Perform local block modifications: lsum[i] -= L_i,k * X[k]
-									 */	  
-
-									lk = LBj( k, grid ); /* Local block number, column-wise. */
-									lsub = Lrowind_bc_ptr[lk];
-									lusup = Lnzval_bc_ptr[lk];
-									if ( lsub ) {
-										krow = PROW( k, grid );
-										if(myrow==krow){
-											nb = lsub[0] - 1;
-											knsupc = SuperSize( k );
-											ii = X_BLK( LBi( k, grid ) );
-											xin = &x[ii];
-										}else{
-											nb   = lsub[0];
-											knsupc = SuperSize( k );
-											xin = &recvbuf0[XK_H] ;					
-										}
-
-										dlsum_fmod_inv_master(lsum, x, xin, rtemp, nrhs, knsupc, k,
-												fmod, nb, xsup, grid, Llu,
-												stat_loc,sizelsum,sizertemp,0);	
-
-									} /* if lsub */
-								}
-
-							}else if(status.MPI_TAG==RD_L){
-								// --nfrecvmod;		  
-								lk = LBi( k, grid ); /* Local block number, row-wise. */
-
-								knsupc = SuperSize( k );
-								tempv = &recvbuf0[LSUM_H];
-								il = LSUM_BLK( lk );		  
-								RHS_ITERATE(j) {
-									for (i = 0; i < knsupc; ++i)
-										lsum[i + il + j*knsupc + thread_id*sizelsum] += tempv[i + j*knsupc];
-								}			
-
-								// #ifdef _OPENMP
-								// #pragma omp atomic capture
-								// #endif
-								fmod_tmp=--fmod[lk];
-								{
-									thread_id = 0;
-									rtemp_loc = &rtemp[sizertemp* thread_id];
-									if ( fmod_tmp==0 ) {	  
-										if(RdTree_IsRoot(LRtree_ptr[lk])==YES){
-											// ii = X_BLK( lk );
-											knsupc = SuperSize( k );
-											for (ii=1;ii<num_thread;ii++)
-												for (jj=0;jj<knsupc*nrhs;jj++)
-													lsum[il+ jj ] += lsum[il + jj + ii*sizelsum];
-
-											ii = X_BLK( lk );
-											RHS_ITERATE(j)
-												for (i = 0; i < knsupc; ++i)	
-													x[i + ii + j*knsupc] += lsum[i + il + j*knsupc ];
-
-											// fmod[lk] = -1; /* Do not solve X[k] in the future. */
-											lk = LBj( k, grid ); /* Local block number, column-wise. */
-											lsub = Lrowind_bc_ptr[lk];
-											lusup = Lnzval_bc_ptr[lk];
-											nsupr = lsub[1];
-
-#if ( PROFlevel>=1 )
-											TIC(t1);
-#endif			  
-
-											if(Llu->inv == 1){
-												Linv = Linv_bc_ptr[lk];		  
-#ifdef _CRAY
-												SGEMM( ftcs2, ftcs2, &knsupc, &nrhs, &knsupc,
-														&alpha, Linv, &knsupc, &x[ii],
-														&knsupc, &beta, rtemp_loc, &knsupc );
-#elif defined (USE_VENDOR_BLAS)
-												dgemm_( "N", "N", &knsupc, &nrhs, &knsupc,
-														&alpha, Linv, &knsupc, &x[ii],
-														&knsupc, &beta, rtemp_loc, &knsupc, 1, 1 );
-#else
-												dgemm_( "N", "N", &knsupc, &nrhs, &knsupc,
-														&alpha, Linv, &knsupc, &x[ii],
-														&knsupc, &beta, rtemp_loc, &knsupc );
-#endif			   
-												for (i=0 ; i<knsupc*nrhs ; i++){
-													x[ii+i] = rtemp_loc[i];
-												}		
-											}
-											else{
+	
+	switch ( status.MPI_TAG ) {
+	  case Xk:
+	      --nfrecvx;
+	      lk = LBj( k, grid ); /* Local block number, column-wise. */
+	      lsub = Lrowind_bc_ptr[lk];
+	      lusup = Lnzval_bc_ptr[lk];
+	      if ( lsub ) {
+		  nb   = lsub[0];
+		  lptr = BC_HEADER;
+		  luptr = 0;
+		  knsupc = SuperSize( k );
+
+		  /*
+		   * Perform local block modifications: lsum[i] -= L_i,k * X[k]
+		   */
+		  dlsum_fmod(lsum, x, &recvbuf[XK_H], rtemp, nrhs, knsupc, k,
+			     fmod, nb, lptr, luptr, xsup, grid, Llu, 
+			     send_req, stat);
+	      } /* if lsub */
+
+	      break;
+
+	  case LSUM: /* Receiver must be a diagonal process */
+	      --nfrecvmod;
+	      lk = LBi( k, grid ); /* Local block number, row-wise. */
+	      ii = X_BLK( lk );
+	      knsupc = SuperSize( k );
+	      tempv = &recvbuf[LSUM_H];
+	      RHS_ITERATE(j) {
+		  for (i = 0; i < knsupc; ++i)
+		      x[i + ii + j*knsupc] += tempv[i + j*knsupc];
+	      }
+
+	      if ( (--frecv[lk])==0 && fmod[lk]==0 ) {
+		  fmod[lk] = -1; /* Do not solve X[k] in the future. */
+		  lk = LBj( k, grid ); /* Local block number, column-wise. */
+		  lsub = Lrowind_bc_ptr[lk];
+		  lusup = Lnzval_bc_ptr[lk];
+		  nsupr = lsub[1];
 #ifdef _CRAY
-												STRSM(ftcs1, ftcs1, ftcs2, ftcs3, &knsupc, &nrhs, &alpha,
-														lusup, &nsupr, &x[ii], &knsupc);
+		  STRSM(ftcs1, ftcs1, ftcs2, ftcs3, &knsupc, &nrhs, &alpha,
+			lusup, &nsupr, &x[ii], &knsupc);
 #elif defined (USE_VENDOR_BLAS)
-												dtrsm_("L", "L", "N", "U", &knsupc, &nrhs, &alpha, 
-														lusup, &nsupr, &x[ii], &knsupc, 1, 1, 1, 1);		
+		  dtrsm_("L", "L", "N", "U", &knsupc, &nrhs, &alpha, 
+			 lusup, &nsupr, &x[ii], &knsupc, 1, 1, 1, 1);
 #else
-												dtrsm_("L", "L", "N", "U", &knsupc, &nrhs, &alpha, 
-														lusup, &nsupr, &x[ii], &knsupc);
+		  dtrsm_("L", "L", "N", "U", &knsupc, &nrhs, &alpha, 
+			 lusup, &nsupr, &x[ii], &knsupc);
 #endif
-											}
-
-
-#if ( PROFlevel>=1 )
-											TOC(t2, t1);
-											stat_loc[thread_id]->utime[SOL_TRSM] += t2;
-
-#endif	
-
-
-
-											stat_loc[thread_id]->ops[SOLVE] += knsupc * (knsupc - 1) * nrhs;
+		  stat->ops[SOLVE] += knsupc * (knsupc - 1) * nrhs;
 #if ( DEBUGlevel>=2 )
-											printf("(%2d) Solve X[%2d]\n", iam, k);
+		  printf("(%2d) Solve X[%2d]\n", iam, k);
 #endif
+		
+		  /*
+		   * Send Xk to process column Pc[k].
+		   */
+		  kcol = PCOL( k, grid );
+		  for (p = 0; p < Pr; ++p) {
+		      if ( fsendx_plist[lk][p] != EMPTY ) {
+			  pi = PNUM( p, kcol, grid );
+
+			  MPI_Isend( &x[ii-XK_H], knsupc * nrhs + XK_H,
+                                     MPI_DOUBLE, pi, Xk, grid->comm,
+                                     &send_req[Llu->SolveMsgSent++]);
+#if 0
+			  MPI_Send( &x[ii - XK_H], knsupc * nrhs + XK_H,
+				    MPI_DOUBLE, pi, Xk, grid->comm );
+#endif
+#if ( DEBUGlevel>=2 )
+			  printf("(%2d) Sent X[%2.0f] to P %2d\n",
+				 iam, x[ii-XK_H], pi);
+#endif
+		      }
+                  }
+		  /*
+		   * Perform local block modifications.
+		   */
+		  nb = lsub[0] - 1;
+		  lptr = BC_HEADER + LB_DESCRIPTOR + knsupc;
+		  luptr = knsupc; /* Skip diagonal block L(k,k). */
+
+		  dlsum_fmod(lsum, x, &x[ii], rtemp, nrhs, knsupc, k,
+			     fmod, nb, lptr, luptr, xsup, grid, Llu,
+			     send_req, stat);
+	      } /* if */
+
+	      break;
 
-											/*
-											 * Send Xk to process column Pc[k].
-											 */						  
-											if(LBtree_ptr[lk]!=NULL){ 
-												BcTree_forwardMessageSimple(LBtree_ptr[lk],&x[ii - XK_H]);
-											}		  
-
-
-											/*
-											 * Perform local block modifications.
-											 */
-											lk = LBj( k, grid ); /* Local block number, column-wise. */
-											lsub = Lrowind_bc_ptr[lk];
-											lusup = Lnzval_bc_ptr[lk];
-											if ( lsub ) {
-												krow = PROW( k, grid );
-												nb = lsub[0] - 1;
-												knsupc = SuperSize( k );
-												ii = X_BLK( LBi( k, grid ) );
-												xin = &x[ii];		
-												dlsum_fmod_inv_master(lsum, x, xin, rtemp, nrhs, knsupc, k,
-														fmod, nb, xsup, grid, Llu,
-														stat_loc,sizelsum,sizertemp,0);	
-											} /* if lsub */
-											// }
-
-									}else{
-
-										il = LSUM_BLK( lk );		  
-										knsupc = SuperSize( k );
-
-										for (ii=1;ii<num_thread;ii++)
-											for (jj=0;jj<knsupc*nrhs;jj++)
-												lsum[il + jj] += lsum[il + jj + ii*sizelsum];
-
-										RdTree_forwardMessageSimple(LRtree_ptr[lk],&lsum[il-LSUM_H]); 
-									}  
-
-								}
-
-							}					
-						} /* check Tag */		  
-					}
-
-				} /* while not finished ... */
-
-			}
-		}
-
-#if ( PRNTlevel>=1 )
-		t = SuperLU_timer_() - t;
-		stat->utime[SOL_L] = t;
-		if ( !iam ) {
-			printf(".. L-solve time\t%8.4f\n", t);
-			fflush(stdout);
-		}
-
+#if ( DEBUGlevel>=2 )
+	    default:
+	      printf("(%2d) Recv'd wrong message tag %4d\n", iam, status.MPI_TAG);
+	      break;
+#endif
+	  } /* switch */
 
-		MPI_Reduce (&t, &tmax, 1, MPI_DOUBLE,
-				MPI_MAX, 0, grid->comm);
-		if ( !iam ) {
-			printf(".. L-solve time (MAX) \t%8.4f\n", tmax);	
-			fflush(stdout);
-		}	
+    } /* while not finished ... */
 
 
-		t = SuperLU_timer_();
+#if ( PRNTlevel>=2 )
+    t = SuperLU_timer_() - t;
+    if ( !iam ) printf(".. L-solve time\t%8.2f\n", t);
+    t = SuperLU_timer_();
 #endif
 
-
 #if ( DEBUGlevel==2 )
-		{
-			printf("(%d) .. After L-solve: y =\n", iam);
-			for (i = 0, k = 0; k < nsupers; ++k) {
-				krow = PROW( k, grid );
-				kcol = PCOL( k, grid );
-				if ( myrow == krow && mycol == kcol ) { /* Diagonal process */
-					knsupc = SuperSize( k );
-					lk = LBi( k, grid );
-					ii = X_BLK( lk );
-					for (j = 0; j < knsupc; ++j)
-						printf("\t(%d)\t%4d\t%.10f\n", iam, xsup[k]+j, x[ii+j]);
-					fflush(stdout);
-				}
-				MPI_Barrier( grid->comm );
-			}
-		}
+    {
+      printf("(%d) .. After L-solve: y =\n", iam);
+      for (i = 0, k = 0; k < nsupers; ++k) {
+	  krow = PROW( k, grid );
+	  kcol = PCOL( k, grid );
+	  if ( myrow == krow && mycol == kcol ) { /* Diagonal process */
+	      knsupc = SuperSize( k );
+	      lk = LBi( k, grid );
+	      ii = X_BLK( lk );
+	      for (j = 0; j < knsupc; ++j)
+		printf("\t(%d)\t%4d\t%.10f\n", iam, xsup[k]+j, x[ii+j]);
+	      fflush(stdout);
+	  }
+	  MPI_Barrier( grid->comm );
+      }
+    }
 #endif
 
-		SUPERLU_FREE(fmod);
-		SUPERLU_FREE(frecv);
-		SUPERLU_FREE(leaf_send);
-		SUPERLU_FREE(leafsups);
-		SUPERLU_FREE(recvbuf_BC_fwd);
+    SUPERLU_FREE(fmod);
+    SUPERLU_FREE(frecv);
+    SUPERLU_FREE(rtemp);
 
-		// for (ljb = 0; ljb < nsupers_j; ++ljb) 
-			// if(Llu->Lrowind_bc_2_lsum[ljb]!=NULL)
-				// SUPERLU_FREE(Llu->Lrowind_bc_2_lsum[ljb]);
-		// SUPERLU_FREE(Llu->Lrowind_bc_2_lsum);
-	
+    /*for (i = 0; i < Llu->SolveMsgSent; ++i) MPI_Request_free(&send_req[i]);*/
 
-		for (lk=0;lk<nsupers_j;++lk){
-			if(LBtree_ptr[lk]!=NULL){
-				// if(BcTree_IsRoot(LBtree_ptr[lk])==YES){			
-				BcTree_waitSendRequest(LBtree_ptr[lk]);		
-				// }
-				// deallocate requests here
-			}
-		}
+    for (i = 0; i < Llu->SolveMsgSent; ++i) MPI_Wait(&send_req[i], &status);
+    Llu->SolveMsgSent = 0;
 
-		for (lk=0;lk<nsupers_i;++lk){
-			if(LRtree_ptr[lk]!=NULL){		
-				RdTree_waitSendRequest(LRtree_ptr[lk]);		
-				// deallocate requests here
-			}
-		}		
-		MPI_Barrier( grid->comm );
+    MPI_Barrier( grid->comm );
 
 
-#if ( VAMPIR>=1 )	
-		VT_traceoff();	
-		VT_finalize(); 
-#endif
+    /*---------------------------------------------------
+     * Back solve Ux = y.
+     *
+     * The Y components from the forward solve is already
+     * on the diagonal processes.
+     *---------------------------------------------------*/
 
+    /* Save the count to be altered so it can be used by
+       subsequent call to PDGSTRS. */
+    if ( !(bmod = intMalloc_dist(nlb)) )
+	ABORT("Calloc fails for bmod[].");
+    for (i = 0; i < nlb; ++i) bmod[i] = Llu->bmod[i];
+    if ( !(brecv = intMalloc_dist(nlb)) )
+	ABORT("Malloc fails for brecv[].");
+    Llu->brecv = brecv;
 
-		/*---------------------------------------------------
-		 * Back solve Ux = y.
-		 *
-		 * The Y components from the forward solve is already
-		 * on the diagonal processes.
-		 *---------------------------------------------------*/
-		 
-		 
-		/* Save the count to be altered so it can be used by
-		   subsequent call to PDGSTRS. */
-		if ( !(bmod = intMalloc_dist(nlb)) )
-			ABORT("Calloc fails for bmod[].");
-		for (i = 0; i < nlb; ++i) bmod[i] = Llu->bmod[i];
-		if ( !(brecv = intCalloc_dist(nlb)) )
-			ABORT("Malloc fails for brecv[].");
-		Llu->brecv = brecv;
-
-		k = SUPERLU_MAX( Llu->nfsendx, Llu->nbsendx ) + nlb;
-		// if ( !(send_req = (MPI_Request*) SUPERLU_MALLOC(k*sizeof(MPI_Request))) )
-			// ABORT("Malloc fails for send_req[].");
-		
-		/* Re-initialize lsum to zero. Each block header is already in place. */
-		
-#ifdef _OPENMP
-
-	#pragma omp parallel default(shared) private(thread_id,k,krow,knsupc,lk,il,dest,j,i)
-	{
-		thread_id = omp_get_thread_num ();
-		for (k = 0; k < nsupers; ++k) {
-			krow = PROW( k, grid );
-			if ( myrow == krow ) {
-				knsupc = SuperSize( k );
-				lk = LBi( k, grid );
-				il = LSUM_BLK( lk );
-				dest = &lsum[il];
-					
-				RHS_ITERATE(j) {
-					for (i = 0; i < knsupc; ++i) dest[i + j*knsupc + thread_id*sizelsum] = zero;
-				}	
-			}
-		}	
-	}	
+    /*
+     * Compute brecv[] and nbrecvmod counts on the diagonal processes.
+     */
+    {
+	superlu_scope_t *scp = &grid->rscp;
 
-#else	
+#if 1
+	for (k = 0; k < nlb; ++k) mod_bit[k] = 0;
 	for (k = 0; k < nsupers; ++k) {
-		krow = PROW( k, grid );
-		if ( myrow == krow ) {
-			knsupc = SuperSize( k );
-			lk = LBi( k, grid );
-			il = LSUM_BLK( lk );
-			dest = &lsum[il];
-			
-			for (jj = 0; jj < num_thread; ++jj) {						
-				RHS_ITERATE(j) {
-					for (i = 0; i < knsupc; ++i) dest[i + j*knsupc + jj*sizelsum] = zero;
-				}	
-			}	
-		}
+	    krow = PROW( k, grid );
+	    if ( myrow == krow ) {
+		lk = LBi( k, grid );    /* local block number */
+		kcol = PCOL( k, grid ); /* root process in this row scope */
+		if ( mycol != kcol && bmod[lk] )
+		    mod_bit[lk] = 1;  /* Contribution from off-diagonal */
+	    }
 	}
-#endif		
-		
 
+	/* Every process receives the count, but it is only useful on the
+	   diagonal processes.  */
+	MPI_Allreduce( mod_bit, brecv, nlb, mpi_int_t, MPI_SUM, scp->comm );
 
-		// /* Set up additional pointers for the index and value arrays of U.
-		   // nub is the number of local block columns. */
-		// nub = CEILING( nsupers, Pc ); /* Number of local block columns. */
-		// if ( !(Urbs = (int_t *) intCalloc_dist(3*nub)) )
-			// ABORT("Malloc fails for Urbs[]"); /* Record number of nonzero
-							     // blocks in a block column. */
-		// Urbs1 = Urbs + nub;
-		// Urbs2 = Urbs + nub*2;
-		// if ( !(Ucb_indptr = SUPERLU_MALLOC(nub * sizeof(Ucb_indptr_t *))) )
-			// ABORT("Malloc fails for Ucb_indptr[]");
-		// if ( !(Ucb_valptr = SUPERLU_MALLOC(nub * sizeof(int_t *))) )
-			// ABORT("Malloc fails for Ucb_valptr[]");
-
-		// /* Count number of row blocks in a block column. 
-		   // One pass of the skeleton graph of U. */
-		// for (lk = 0; lk < nlb; ++lk) {
-			// usub = Ufstnz_br_ptr[lk];
-			// if ( usub ) { /* Not an empty block row. */
-				// /* usub[0] -- number of column blocks in this block row. */
-// #if ( DEBUGlevel>=2 )
-				// Ublocks += usub[0];
-// #endif
-				// i = BR_HEADER; /* Pointer in index array. */
-				// for (lb = 0; lb < usub[0]; ++lb) { /* For all column blocks. */
-					// k = usub[i];            /* Global block number */
-					// ++Urbs[LBj(k,grid)];
-					// i += UB_DESCRIPTOR + SuperSize( k );
-				// }
-			// }
-		// }
-
-		// /* Set up the vertical linked lists for the row blocks.
-		   // One pass of the skeleton graph of U. */
-		// for (lb = 0; lb < nub; ++lb) {
-			// if ( Urbs[lb] ) { /* Not an empty block column. */
-				// if ( !(Ucb_indptr[lb]
-							// = SUPERLU_MALLOC(Urbs[lb] * sizeof(Ucb_indptr_t))) )
-					// ABORT("Malloc fails for Ucb_indptr[lb][]");
-				// if ( !(Ucb_valptr[lb] = (int_t *) intMalloc_dist(Urbs[lb])) )
-					// ABORT("Malloc fails for Ucb_valptr[lb][]");
-			// }
-		// }
-		// for (lk = 0; lk < nlb; ++lk) { /* For each block row. */
-			// usub = Ufstnz_br_ptr[lk];
-			// if ( usub ) { /* Not an empty block row. */
-				// i = BR_HEADER; /* Pointer in index array. */
-				// j = 0;         /* Pointer in nzval array. */
-				
-				// // gik = lk * grid->nprow + myrow;/* Global block number, row-wise. */
-				// // iklrow = FstBlockC( gik+1 );
-				
-				// for (lb = 0; lb < usub[0]; ++lb) { /* For all column blocks. */
-					// k = usub[i];          /* Global block number, column-wise. */
-					// ljb = LBj( k, grid ); /* Local block number, column-wise. */
-					// Ucb_indptr[ljb][Urbs1[ljb]].lbnum = lk;
-					
-					
-					
-					
-					// Ucb_indptr[ljb][Urbs1[ljb]].indpos = i;
-					// Ucb_valptr[ljb][Urbs1[ljb]] = j;
-					
-					// // knsupc = SuperSize( k );
-					// // nbrow = 0;
-					// // for (jj = 0; jj < knsupc; ++jj) {
-						// // fnz = usub[i +UB_DESCRIPTOR+ jj];
-						// // if ( fnz < iklrow ) {
-						// // if(nbrow<iklrow-fnz)nbrow=iklrow-fnz;
-						// // }
-					// // }
-					
-					// // Urbs2[ljb] += nbrow;
-					// // // if(ljb==53253){
-						// // // printf("Urbs2[ljb] %5d nbrow %5d \n",Urbs2[ljb],nbrow);
-						// // // fflush(stdout);
-					// // // }
-					
-					// ++Urbs1[ljb];
-					// j += usub[i+1];
-					// i += UB_DESCRIPTOR + SuperSize( k );
-				// }
-			// }
-		// }
-
+	for (k = 0; k < nsupers; ++k) {
+	    krow = PROW( k, grid );
+	    if ( myrow == krow ) {
+		lk = LBi( k, grid );    /* local block number */
+		kcol = PCOL( k, grid ); /* root process in this row scope. */
+		if ( mycol == kcol ) { /* diagonal process */
+		    nbrecvmod += brecv[lk];
+		    if ( !brecv[lk] && !bmod[lk] ) ++nroot;
 #if ( DEBUGlevel>=2 )
-		for (p = 0; p < Pr*Pc; ++p) {
-			if (iam == p) {
-				printf("(%2d) .. Ublocks %d\n", iam, Ublocks);
-				for (lb = 0; lb < nub; ++lb) {
-					printf("(%2d) Local col %2d: # row blocks %2d\n",
-							iam, lb, Urbs[lb]);
-					if ( Urbs[lb] ) {
-						for (i = 0; i < Urbs[lb]; ++i)
-							printf("(%2d) .. row blk %2d:\
-									lbnum %d, indpos %d, valpos %d\n",
-									iam, i, 
-									Ucb_indptr[lb][i].lbnum,
-									Ucb_indptr[lb][i].indpos,
-									Ucb_valptr[lb][i]);
-					}
-				}
-			}
-			MPI_Barrier( grid->comm );
-		}
-		for (p = 0; p < Pr*Pc; ++p) {
-			if ( iam == p ) {
-				printf("\n(%d) bsendx_plist[][]", iam);
-				for (lb = 0; lb < nub; ++lb) {
-					printf("\n(%d) .. local col %2d: ", iam, lb);
-					for (i = 0; i < Pr; ++i)
-						printf("%4d", bsendx_plist[lb][i]);
-				}
-				printf("\n");
-			}
-			MPI_Barrier( grid->comm );
-		}
-#endif /* DEBUGlevel */
-
-
-
-
-	/* ---------------------------------------------------------
-	   Initialize the async Bcast trees on all processes.
-	   --------------------------------------------------------- */		
-	nsupers_j = CEILING( nsupers, grid->npcol ); /* Number of local block columns */
-
-	nbtree = 0;
-	for (lk=0;lk<nsupers_j;++lk){
-		if(UBtree_ptr[lk]!=NULL){
-			// printf("UBtree_ptr lk %5d\n",lk); 
-			if(BcTree_IsRoot(UBtree_ptr[lk])==NO){			
-				nbtree++;
-				if(BcTree_getDestCount(UBtree_ptr[lk])>0)nbrecvx_buf++;				  
-			}
-			BcTree_allocateRequest(UBtree_ptr[lk]);
+		    printf("(%2d) brecv[%4d]  %2d\n", iam, k, brecv[lk]);
+		    assert( brecv[lk] < Pc );
+#endif
 		}
+	    }
 	}
 
-	nsupers_i = CEILING( nsupers, grid->nprow ); /* Number of local block rows */
-	if ( !(	rootsups = (int_t*)intCalloc_dist(nsupers_i)) )
-		ABORT("Calloc fails for rootsups.");
-
-	nrtree = 0;
-	nroot=0;
-	for (lk=0;lk<nsupers_i;++lk){
-		if(URtree_ptr[lk]!=NULL){
-			// printf("here lk %5d myid %5d\n",lk,iam);
-			// fflush(stdout);
-			nrtree++;
-			RdTree_allocateRequest(URtree_ptr[lk]);			
-			brecv[lk] = RdTree_GetDestCount(URtree_ptr[lk]);
-			nbrecvmod += brecv[lk];
-		}else{
-			gb = myrow+lk*grid->nprow;  /* not sure */
-			if(gb<nsupers){
-				kcol = PCOL( gb, grid );
-				if(mycol==kcol) { /* Diagonal process */
-					if (bmod[lk]==0){
-						rootsups[nroot]=gb;				
-						++nroot;
-					}
-				}
-			}
-		}
-	}	
-
-
-	for (i = 0; i < nlb; ++i) bmod[i] += brecv[i];
-	// for (i = 0; i < nlb; ++i)printf("bmod[i]: %5d\n",bmod[i]);
-	
-	
-	if ( !(recvbuf_BC_fwd = doubleMalloc_dist(maxrecvsz*(nbrecvx+1))) )   // this needs to be optimized for 1D row mapping
-		ABORT("Malloc fails for recvbuf_BC_fwd[].");	
-	nbrecvx_buf=0;			
+#else /* old */
 
+	for (k = 0; k < nsupers; ++k) {
+	    krow = PROW( k, grid );
+	    if ( myrow == krow ) {
+		lk = LBi( k, grid );    /* Local block number. */
+		kcol = PCOL( k, grid ); /* Root process in this row scope. */
+		if ( mycol != kcol && bmod[lk] )
+		    i = 1;  /* Contribution from non-diagonal process. */
+		else i = 0;
+		MPI_Reduce( &i, &brecv[lk], 1, mpi_int_t,
+			   MPI_SUM, kcol, scp->comm );
+		if ( mycol == kcol ) { /* Diagonal process. */
+		    nbrecvmod += brecv[lk];
+		    if ( !brecv[lk] && !bmod[lk] ) ++nroot;
 #if ( DEBUGlevel>=2 )
-	printf("(%2d) nbrecvx %4d,  nbrecvmod %4d,  nroot %4d\n,  nbtree %4d\n,  nrtree %4d\n",
-			iam, nbrecvx, nbrecvmod, nroot, nbtree, nrtree);
-	fflush(stdout);
+		    printf("(%2d) brecv[%4d]  %2d\n", iam, k, brecv[lk]);
+		    assert( brecv[lk] < Pc );
 #endif
-
-
-#if ( PRNTlevel>=1 )
-	t = SuperLU_timer_() - t;
-	if ( !iam) printf(".. Setup U-solve time\t%8.4f\n", t);
-	fflush(stdout);
-	MPI_Barrier( grid->comm );	
-	t = SuperLU_timer_();
+		}
+	    }
+	}
 #endif
-
-		/*
-		 * Solve the roots first by all the diagonal processes.
-		 */
+    }
+
+    /* Re-initialize lsum to zero. Each block header is already in place. */
+    for (k = 0; k < nsupers; ++k) {
+	krow = PROW( k, grid );
+	if ( myrow == krow ) {
+	    knsupc = SuperSize( k );
+	    lk = LBi( k, grid );
+	    il = LSUM_BLK( lk );
+	    dest = &lsum[il];
+	    RHS_ITERATE(j) {
+		for (i = 0; i < knsupc; ++i) dest[i + j*knsupc] = zero;
+	    }
+	}
+    }
+
+    /* Set up additional pointers for the index and value arrays of U.
+       nub is the number of local block columns. */
+    nub = CEILING( nsupers, Pc ); /* Number of local block columns. */
+    if ( !(Urbs = (int_t *) intCalloc_dist(2*nub)) )
+	ABORT("Malloc fails for Urbs[]"); /* Record number of nonzero
+					     blocks in a block column. */
+    Urbs1 = Urbs + nub;
+    if ( !(Ucb_indptr = SUPERLU_MALLOC(nub * sizeof(Ucb_indptr_t *))) )
+        ABORT("Malloc fails for Ucb_indptr[]");
+    if ( !(Ucb_valptr = SUPERLU_MALLOC(nub * sizeof(int_t *))) )
+        ABORT("Malloc fails for Ucb_valptr[]");
+
+    /* Count number of row blocks in a block column. 
+       One pass of the skeleton graph of U. */
+    for (lk = 0; lk < nlb; ++lk) {
+	usub = Ufstnz_br_ptr[lk];
+	if ( usub ) { /* Not an empty block row. */
+	    /* usub[0] -- number of column blocks in this block row. */
 #if ( DEBUGlevel>=2 )
-		printf("(%2d) nroot %4d\n", iam, nroot);
-		fflush(stdout);				
-#endif
-		
-		
-
-#ifdef _OPENMP
-#pragma omp parallel default (shared) 
-#endif
-	{	
-#ifdef _OPENMP
-#pragma omp master
-#endif
-		{
-#ifdef _OPENMP
-#pragma	omp	taskloop firstprivate (nrhs,beta,alpha,x,rtemp,ldalsum) private (ii,jj,k,knsupc,lk,luptr,lsub,nsupr,lusup,thread_id,t1,t2,Uinv,i,lib,rtemp_loc,nroot_send_tmp) nogroup	
-#endif		
-		for (jj=0;jj<nroot;jj++){
-			k=rootsups[jj];	
-
-#if ( PROFlevel>=1 )
-			TIC(t1);
-#endif	
-#ifdef _OPENMP
-			thread_id = omp_get_thread_num ();
-#else
-			thread_id = 0;
+	    Ublocks += usub[0];
 #endif
-			rtemp_loc = &rtemp[sizertemp* thread_id];
-
-
-			
-			knsupc = SuperSize( k );
-			lk = LBi( k, grid ); /* Local block number, row-wise. */		
+	    i = BR_HEADER; /* Pointer in index array. */
+	    for (lb = 0; lb < usub[0]; ++lb) { /* For all column blocks. */
+		k = usub[i];            /* Global block number */
+		++Urbs[LBj(k,grid)];
+		i += UB_DESCRIPTOR + SuperSize( k );
+	    }
+	}
+    }
+
+    /* Set up the vertical linked lists for the row blocks.
+       One pass of the skeleton graph of U. */
+    for (lb = 0; lb < nub; ++lb) {
+	if ( Urbs[lb] ) { /* Not an empty block column. */
+	    if ( !(Ucb_indptr[lb]
+		   = SUPERLU_MALLOC(Urbs[lb] * sizeof(Ucb_indptr_t))) )
+		ABORT("Malloc fails for Ucb_indptr[lb][]");
+	    if ( !(Ucb_valptr[lb] = (int_t *) intMalloc_dist(Urbs[lb])) )
+		ABORT("Malloc fails for Ucb_valptr[lb][]");
+	}
+    }
+    for (lk = 0; lk < nlb; ++lk) { /* For each block row. */
+	usub = Ufstnz_br_ptr[lk];
+	if ( usub ) { /* Not an empty block row. */
+	    i = BR_HEADER; /* Pointer in index array. */
+	    j = 0;         /* Pointer in nzval array. */
+	    for (lb = 0; lb < usub[0]; ++lb) { /* For all column blocks. */
+		k = usub[i];          /* Global block number, column-wise. */
+		ljb = LBj( k, grid ); /* Local block number, column-wise. */
+		Ucb_indptr[ljb][Urbs1[ljb]].lbnum = lk;
+		Ucb_indptr[ljb][Urbs1[ljb]].indpos = i;
+		Ucb_valptr[ljb][Urbs1[ljb]] = j;
+		++Urbs1[ljb];
+		j += usub[i+1];
+		i += UB_DESCRIPTOR + SuperSize( k );
+	    }
+	}
+    }
 
-			// bmod[lk] = -1;       /* Do not solve X[k] in the future. */
-			ii = X_BLK( lk );
-			lk = LBj( k, grid ); /* Local block number, column-wise */
-			lsub = Lrowind_bc_ptr[lk];
-			lusup = Lnzval_bc_ptr[lk];
-			nsupr = lsub[1];
+#if ( DEBUGlevel>=2 )
+    for (p = 0; p < Pr*Pc; ++p) {
+	if (iam == p) {
+	    printf("(%2d) .. Ublocks %d\n", iam, Ublocks);
+	    for (lb = 0; lb < nub; ++lb) {
+		printf("(%2d) Local col %2d: # row blocks %2d\n",
+		       iam, lb, Urbs[lb]);
+		if ( Urbs[lb] ) {
+		    for (i = 0; i < Urbs[lb]; ++i)
+			printf("(%2d) .. row blk %2d:\
+                               lbnum %d, indpos %d, valpos %d\n",
+			       iam, i, 
+			       Ucb_indptr[lb][i].lbnum,
+			       Ucb_indptr[lb][i].indpos,
+			       Ucb_valptr[lb][i]);
+		}
+	    }
+	}
+	MPI_Barrier( grid->comm );
+    }
+    for (p = 0; p < Pr*Pc; ++p) {
+	if ( iam == p ) {
+	    printf("\n(%d) bsendx_plist[][]", iam);
+	    for (lb = 0; lb < nub; ++lb) {
+		printf("\n(%d) .. local col %2d: ", iam, lb);
+		for (i = 0; i < Pr; ++i)
+		    printf("%4d", bsendx_plist[lb][i]);
+	    }
+	    printf("\n");
+	}
+	MPI_Barrier( grid->comm );
+    }
+#endif /* DEBUGlevel */
 
 
-			if(Llu->inv == 1){
+#if ( PRNTlevel>=3 )
+    t = SuperLU_timer_() - t;
+    if ( !iam) printf(".. Setup U-solve time\t%8.2f\n", t);
+    t = SuperLU_timer_();
+#endif
 
-				Uinv = Uinv_bc_ptr[lk];
-#ifdef _CRAY
-				SGEMM( ftcs2, ftcs2, &knsupc, &nrhs, &knsupc,
-						&alpha, Uinv, &knsupc, &x[ii],
-						&knsupc, &beta, rtemp_loc, &knsupc );
-#elif defined (USE_VENDOR_BLAS)
-				dgemm_( "N", "N", &knsupc, &nrhs, &knsupc,
-						&alpha, Uinv, &knsupc, &x[ii],
-						&knsupc, &beta, rtemp_loc, &knsupc, 1, 1 );
-#else
-				dgemm_( "N", "N", &knsupc, &nrhs, &knsupc,
-						&alpha, Uinv, &knsupc, &x[ii],
-						&knsupc, &beta, rtemp_loc, &knsupc );
-#endif			   
-
-				for (i=0 ; i<knsupc*nrhs ; i++){
-					x[ii+i] = rtemp_loc[i];
-				}		
-			}else{
+    /*
+     * Solve the roots first by all the diagonal processes.
+     */
+#if ( DEBUGlevel>=2 )
+    printf("(%2d) nroot %4d\n", iam, nroot);
+#endif
+    for (k = nsupers-1; k >= 0 && nroot; --k) {
+	krow = PROW( k, grid );
+	kcol = PCOL( k, grid );
+	if ( myrow == krow && mycol == kcol ) { /* Diagonal process. */
+	    knsupc = SuperSize( k );
+	    lk = LBi( k, grid ); /* Local block number, row-wise. */
+	    if ( brecv[lk]==0 && bmod[lk]==0 ) {
+		bmod[lk] = -1;       /* Do not solve X[k] in the future. */
+		ii = X_BLK( lk );
+		lk = LBj( k, grid ); /* Local block number, column-wise */
+		lsub = Lrowind_bc_ptr[lk];
+		lusup = Lnzval_bc_ptr[lk];
+		nsupr = lsub[1];
 #ifdef _CRAY
-				STRSM(ftcs1, ftcs3, ftcs2, ftcs2, &knsupc, &nrhs, &alpha,
-						lusup, &nsupr, &x[ii], &knsupc);
+		STRSM(ftcs1, ftcs3, ftcs2, ftcs2, &knsupc, &nrhs, &alpha,
+		      lusup, &nsupr, &x[ii], &knsupc);
 #elif defined (USE_VENDOR_BLAS)
-				dtrsm_("L", "U", "N", "N", &knsupc, &nrhs, &alpha, 
-						lusup, &nsupr, &x[ii], &knsupc, 1, 1, 1, 1);	
+		dtrsm_("L", "U", "N", "N", &knsupc, &nrhs, &alpha, 
+		       lusup, &nsupr, &x[ii], &knsupc, 1, 1, 1, 1);
 #else
-				dtrsm_("L", "U", "N", "N", &knsupc, &nrhs, &alpha, 
-						lusup, &nsupr, &x[ii], &knsupc);
+		dtrsm_("L", "U", "N", "N", &knsupc, &nrhs, &alpha, 
+		       lusup, &nsupr, &x[ii], &knsupc);
 #endif
-			}		
-
-			// for (i=0 ; i<knsupc*nrhs ; i++){
-				// printf("x: %f\n",x[ii+i]);
-				// fflush(stdout);
-			// }
-
-#if ( PROFlevel>=1 )
-			TOC(t2, t1);
-			stat_loc[thread_id]->utime[SOL_TRSM] += t2;
-#endif	
-			stat_loc[thread_id]->ops[SOLVE] += knsupc * (knsupc + 1) * nrhs;
-
+		stat->ops[SOLVE] += knsupc * (knsupc + 1) * nrhs;
+		--nroot;
 #if ( DEBUGlevel>=2 )
-			printf("(%2d) Solve X[%2d]\n", iam, k);
+		printf("(%2d) Solve X[%2d]\n", iam, k);
 #endif
+		/*
+		 * Send Xk to process column Pc[k].
+		 */
+		for (p = 0; p < Pr; ++p) {
+		    if ( bsendx_plist[lk][p] != EMPTY ) {
+			pi = PNUM( p, kcol, grid );
 
-			/*
-			 * Send Xk to process column Pc[k].
-			 */
-
-			if(UBtree_ptr[lk]!=NULL){ 
-#ifdef _OPENMP
-#pragma omp atomic capture
+			MPI_Isend( &x[ii - XK_H], knsupc * nrhs + XK_H,
+                                   MPI_DOUBLE, pi, Xk, grid->comm,
+                                   &send_req[Llu->SolveMsgSent++]);
+#if 0
+			MPI_Send( &x[ii - XK_H], knsupc * nrhs + XK_H,
+                                  MPI_DOUBLE, pi, Xk,
+                                  grid->comm );
 #endif
-				nroot_send_tmp = ++nroot_send;
-				root_send[nroot_send_tmp-1] = lk;
-				
-				// lib = LBi( k, grid ); /* Local block number, row-wise. */
-				// ii = X_BLK( lib );				
-				// BcTree_forwardMessageSimple(UBtree_ptr[lk],&x[ii - XK_H]);
-			}
+#if ( DEBUGlevel>=2 )
+			printf("(%2d) Sent X[%2.0f] to P %2d\n",
+			       iam, x[ii-XK_H], pi);
+#endif
+		    }
+		}
+		/*
+		 * Perform local block modifications: lsum[i] -= U_i,k * X[k]
+		 */
+		if ( Urbs[lk] ) 
+		    dlsum_bmod(lsum, x, &x[ii], nrhs, k, bmod, Urbs,
+			       Ucb_indptr, Ucb_valptr, xsup, grid, Llu,
+			       send_req, stat);
+	    } /* if root ... */
+	} /* if diagonal process ... */
+    } /* for k ... */
 
-			/*
-			 * Perform local block modifications: lsum[i] -= U_i,k * X[k]
-			 */
-			if ( Urbs[lk] ) 
-				dlsum_bmod_inv(lsum, x, &x[ii], rtemp, nrhs, k, bmod, Urbs,Urbs2, 
-						Ucb_indptr, Ucb_valptr, xsup, grid, Llu,
-						send_req, stat_loc, root_send, &nroot_send, sizelsum,sizertemp);
-									
-		} /* for k ... */
-	}
-}
-
-
-for (i=0;i<nroot_send;i++){
-	lk = root_send[i];
-	if(lk>=0){ // this is a bcast forwarding
-		gb = mycol+lk*grid->npcol;  /* not sure */
-		lib = LBi( gb, grid ); /* Local block number, row-wise. */
-		ii = X_BLK( lib );			
-		BcTree_forwardMessageSimple(UBtree_ptr[lk],&x[ii - XK_H]);
-	}else{ // this is a reduce forwarding
-		lk = -lk - 1;
-		il = LSUM_BLK( lk );
-		RdTree_forwardMessageSimple(URtree_ptr[lk],&lsum[il - LSUM_H ]);
-	}
-}
 
+    /*
+     * Compute the internal nodes asynchronously by all processes.
+     */
+    while ( nbrecvx || nbrecvmod ) { /* While not finished. */
 
-		/*
-		 * Compute the internal nodes asychronously by all processes.
-		 */
+	/* Receive a message. */
+	MPI_Recv( recvbuf, maxrecvsz, MPI_DOUBLE,
+                  MPI_ANY_SOURCE, MPI_ANY_TAG, grid->comm, &status );
+	k = *recvbuf;
 
-#ifdef _OPENMP
-#pragma omp parallel default (shared) 
-#endif
-	{	
-#ifdef _OPENMP
-#pragma omp master 
-#endif		 
-		for ( nbrecv =0; nbrecv<nbrecvx+nbrecvmod;nbrecv++) { /* While not finished. */
-
-			// printf("iam %4d nbrecv %4d nbrecvx %4d nbrecvmod %4d\n", iam, nbrecv, nbrecvxnbrecvmod);
-			// fflush(stdout);			
-			
-			
-			
-			thread_id = 0;
-#if ( PROFlevel>=1 )
-			TIC(t1);
-#endif	
-
-			recvbuf0 = &recvbuf_BC_fwd[nbrecvx_buf*maxrecvsz];
-
-			/* Receive a message. */
-			MPI_Recv( recvbuf0, maxrecvsz, MPI_DOUBLE,
-					MPI_ANY_SOURCE, MPI_ANY_TAG, grid->comm, &status );	 	
-
-#if ( PROFlevel>=1 )		 
-			TOC(t2, t1);
-			stat_loc[thread_id]->utime[SOL_COMM] += t2;
-
-			msg_cnt += 1;
-			msg_vol += maxrecvsz * dword;			
-#endif	
-		 
-			k = *recvbuf0;
 #if ( DEBUGlevel>=2 )
-			printf("(%2d) Recv'd block %d, tag %2d\n", iam, k, status.MPI_TAG);
-			fflush(stdout);
+	printf("(%2d) Recv'd block %d, tag %2d\n", iam, k, status.MPI_TAG);
 #endif
 
-			if(status.MPI_TAG==BC_U){
-				// --nfrecvx;
-				nbrecvx_buf++;
-				
-				lk = LBj( k, grid );    /* local block number */
-
-				if(BcTree_getDestCount(UBtree_ptr[lk])>0){
-
-					BcTree_forwardMessageSimple(UBtree_ptr[lk],recvbuf0);	
-					// nfrecvx_buf++;
-				}
-
-				/*
-				 * Perform local block modifications: lsum[i] -= L_i,k * X[k]
-				 */	  
-
-				lk = LBj( k, grid ); /* Local block number, column-wise. */
-				dlsum_bmod_inv_master(lsum, x, &recvbuf0[XK_H], rtemp, nrhs, k, bmod, Urbs,Urbs2,
-						Ucb_indptr, Ucb_valptr, xsup, grid, Llu, 
-						send_req, stat_loc, sizelsum,sizertemp);
-			}else if(status.MPI_TAG==RD_U){
-
-				lk = LBi( k, grid ); /* Local block number, row-wise. */
-				
-				knsupc = SuperSize( k );
-				tempv = &recvbuf0[LSUM_H];
-				il = LSUM_BLK( lk );		  
-				RHS_ITERATE(j) {
-					for (i = 0; i < knsupc; ++i)
-						lsum[i + il + j*knsupc + thread_id*sizelsum] += tempv[i + j*knsupc];
-				}					
-			// #ifdef _OPENMP
-			// #pragma omp atomic capture
-			// #endif
-				bmod_tmp=--bmod[lk];
-				thread_id = 0;									
-				rtemp_loc = &rtemp[sizertemp* thread_id];
-				if ( bmod_tmp==0 ) {
-					if(RdTree_IsRoot(URtree_ptr[lk])==YES){							
-						
-						knsupc = SuperSize( k );
-						for (ii=1;ii<num_thread;ii++)
-							for (jj=0;jj<knsupc*nrhs;jj++)
-								lsum[il+ jj ] += lsum[il + jj + ii*sizelsum];						
-						
-						ii = X_BLK( lk );
-						RHS_ITERATE(j)
-							for (i = 0; i < knsupc; ++i)	
-								x[i + ii + j*knsupc] += lsum[i + il + j*knsupc ];						
-					
-						lk = LBj( k, grid ); /* Local block number, column-wise. */
-						lsub = Lrowind_bc_ptr[lk];
-						lusup = Lnzval_bc_ptr[lk];
-						nsupr = lsub[1];
-
-						if(Llu->inv == 1){
-
-							Uinv = Uinv_bc_ptr[lk];
+	switch ( status.MPI_TAG ) {
+	    case Xk:
+	        --nbrecvx;
+		lk = LBj( k, grid ); /* Local block number, column-wise. */
+		/*
+		 * Perform local block modifications:
+		 *         lsum[i] -= U_i,k * X[k]
+		 */
+		dlsum_bmod(lsum, x, &recvbuf[XK_H], nrhs, k, bmod, Urbs,
+			   Ucb_indptr, Ucb_valptr, xsup, grid, Llu, 
+			   send_req, stat);
+
+	        break;
 
-#ifdef _CRAY
-							SGEMM( ftcs2, ftcs2, &knsupc, &nrhs, &knsupc,
-									&alpha, Uinv, &knsupc, &x[ii],
-									&knsupc, &beta, rtemp_loc, &knsupc );
-#elif defined (USE_VENDOR_BLAS)
-							dgemm_( "N", "N", &knsupc, &nrhs, &knsupc,
-									&alpha, Uinv, &knsupc, &x[ii],
-									&knsupc, &beta, rtemp_loc, &knsupc, 1, 1 );
-#else
-							dgemm_( "N", "N", &knsupc, &nrhs, &knsupc,
-									&alpha, Uinv, &knsupc, &x[ii],
-									&knsupc, &beta, rtemp_loc, &knsupc );
-#endif		
+	    case LSUM: /* Receiver must be a diagonal process */
+		--nbrecvmod;
+		lk = LBi( k, grid ); /* Local block number, row-wise. */
+		ii = X_BLK( lk );
+		knsupc = SuperSize( k );
+		tempv = &recvbuf[LSUM_H];
+		RHS_ITERATE(j) {
+		    for (i = 0; i < knsupc; ++i)
+			x[i + ii + j*knsupc] += tempv[i + j*knsupc];
+		}
 
-							for (i=0 ; i<knsupc*nrhs ; i++){
-								x[ii+i] = rtemp_loc[i];
-							}		
-						}else{
+		if ( (--brecv[lk])==0 && bmod[lk]==0 ) {
+		    bmod[lk] = -1; /* Do not solve X[k] in the future. */
+		    lk = LBj( k, grid ); /* Local block number, column-wise. */
+		    lsub = Lrowind_bc_ptr[lk];
+		    lusup = Lnzval_bc_ptr[lk];
+		    nsupr = lsub[1];
 #ifdef _CRAY
-							STRSM(ftcs1, ftcs3, ftcs2, ftcs2, &knsupc, &nrhs, &alpha,
-									lusup, &nsupr, &x[ii], &knsupc);
+		    STRSM(ftcs1, ftcs3, ftcs2, ftcs2, &knsupc, &nrhs, &alpha,
+			  lusup, &nsupr, &x[ii], &knsupc);
 #elif defined (USE_VENDOR_BLAS)
-							dtrsm_("L", "U", "N", "N", &knsupc, &nrhs, &alpha, 
-									lusup, &nsupr, &x[ii], &knsupc, 1, 1, 1, 1);		
+		    dtrsm_("L", "U", "N", "N", &knsupc, &nrhs, &alpha, 
+			   lusup, &nsupr, &x[ii], &knsupc, 1, 1, 1, 1);
 #else
-							dtrsm_("L", "U", "N", "N", &knsupc, &nrhs, &alpha, 
-									lusup, &nsupr, &x[ii], &knsupc);
+		    dtrsm_("L", "U", "N", "N", &knsupc, &nrhs, &alpha, 
+			   lusup, &nsupr, &x[ii], &knsupc);
 #endif
-						}
-
-#if ( PROFlevel>=1 )
-							TOC(t2, t1);
-							stat_loc[thread_id]->utime[SOL_TRSM] += t2;
-#endif	
-							stat_loc[thread_id]->ops[SOLVE] += knsupc * (knsupc + 1) * nrhs;
+		    stat->ops[SOLVE] += knsupc * (knsupc + 1) * nrhs;
 #if ( DEBUGlevel>=2 )
-						printf("(%2d) Solve X[%2d]\n", iam, k);
+		    printf("(%2d) Solve X[%2d]\n", iam, k);
 #endif
-
-						/*
-						 * Send Xk to process column Pc[k].
-						 */						
-						if(UBtree_ptr[lk]!=NULL){ 
-							BcTree_forwardMessageSimple(UBtree_ptr[lk],&x[ii - XK_H]);
-						}							
-						
-
-						/*
-						 * Perform local block modifications: 
-						 *         lsum[i] -= U_i,k * X[k]
-						 */
-						if ( Urbs[lk] )
-							dlsum_bmod_inv_master(lsum, x, &x[ii], rtemp, nrhs, k, bmod, Urbs,Urbs2,
-									Ucb_indptr, Ucb_valptr, xsup, grid, Llu,
-									send_req, stat_loc, sizelsum,sizertemp);
-
-					}else{
-						il = LSUM_BLK( lk );		  
-						knsupc = SuperSize( k );
-
-						for (ii=1;ii<num_thread;ii++)
-							for (jj=0;jj<knsupc*nrhs;jj++)
-								lsum[il + jj] += lsum[il + jj + ii*sizelsum];
-												
-						RdTree_forwardMessageSimple(URtree_ptr[lk],&lsum[il-LSUM_H]); 
-					}						
-				
-				}
-			}
-		} /* while not finished ... */
-	}
-#if ( PRNTlevel>=1 )
-		t = SuperLU_timer_() - t;
-		if ( !iam ) printf(".. U-solve time\t%8.4f\n", t);
-		MPI_Reduce (&t, &tmax, 1, MPI_DOUBLE,
-				MPI_MAX, 0, grid->comm);
-		if ( !iam ) {
-			printf(".. U-solve time (MAX) \t%8.4f\n", tmax);	
-			fflush(stdout);
-		}			
-		t = SuperLU_timer_();			
+		    /*
+		     * Send Xk to process column Pc[k].
+		     */
+		    kcol = PCOL( k, grid );
+		    for (p = 0; p < Pr; ++p) {
+			if ( bsendx_plist[lk][p] != EMPTY ) {
+			    pi = PNUM( p, kcol, grid );
+
+			    MPI_Isend( &x[ii - XK_H], knsupc * nrhs + XK_H,
+                                       MPI_DOUBLE, pi, Xk, grid->comm,
+                                       &send_req[Llu->SolveMsgSent++] );
+#if 0
+			    MPI_Send( &x[ii - XK_H], knsupc * nrhs + XK_H,
+                                      MPI_DOUBLE, pi, Xk,
+                                      grid->comm );
 #endif
-
-
-
-
 #if ( DEBUGlevel>=2 )
-		{
-			double *x_col;
-			int diag;
-			printf("\n(%d) .. After U-solve: x (ON DIAG PROCS) = \n", iam);
-			ii = 0;
-			for (k = 0; k < nsupers; ++k) {
-				knsupc = SuperSize( k );
-				krow = PROW( k, grid );
-				kcol = PCOL( k, grid );
-				diag = PNUM( krow, kcol, grid);
-				if ( iam == diag ) { /* Diagonal process. */
-					lk = LBi( k, grid );
-					jj = X_BLK( lk );
-					x_col = &x[jj];
-					RHS_ITERATE(j) {
-						for (i = 0; i < knsupc; ++i) { /* X stored in blocks */
-							printf("\t(%d)\t%4d\t%.10f\n",
-									iam, xsup[k]+i, x_col[i]);
-						}
-						x_col += knsupc;
-					}
-				}
-				ii += knsupc;
-			} /* for k ... */
-		}
+			    printf("(%2d) Sent X[%2.0f] to P %2d\n",
+				   iam, x[ii - XK_H], pi);
 #endif
+			}
+		    }
+		    /*
+		     * Perform local block modifications: 
+		     *         lsum[i] -= U_i,k * X[k]
+		     */
+		    if ( Urbs[lk] )
+			dlsum_bmod(lsum, x, &x[ii], nrhs, k, bmod, Urbs,
+				   Ucb_indptr, Ucb_valptr, xsup, grid, Llu,
+				   send_req, stat);
+		} /* if becomes solvable */
+		
+		break;
 
-		pdReDistribute_X_to_B(n, B, m_loc, ldb, fst_row, nrhs, x, ilsum,
-				ScalePermstruct, Glu_persist, grid, SOLVEstruct);
-
-
-#if ( PRNTlevel>=1 )
-		t = SuperLU_timer_() - t;
-		if ( !iam) printf(".. X to B redistribute time\t%8.4f\n", t);
-		t = SuperLU_timer_();
-#endif	
-
-
-		double tmp1=0; 
-		double tmp2=0;
-		double tmp3=0;
-		double tmp4=0;
-		for(i=0;i<num_thread;i++){
-			tmp1 = MAX(tmp1,stat_loc[i]->utime[SOL_TRSM]);
-			tmp2 = MAX(tmp2,stat_loc[i]->utime[SOL_GEMM]);
-			tmp3 = MAX(tmp3,stat_loc[i]->utime[SOL_COMM]);
-			tmp4 += stat_loc[i]->ops[SOLVE];
-#if ( PRNTlevel>=2 )
-			if(iam==0)printf("thread %5d gemm %9.5f\n",i,stat_loc[i]->utime[SOL_GEMM]);
-#endif	
-		}
+#if ( DEBUGlevel>=2 )
+	      default:
+		printf("(%2d) Recv'd wrong message tag %4d\n", iam, status.MPI_TAG);
+		break;
+#endif		
 
+	} /* switch */
 
-		stat->utime[SOL_TRSM] += tmp1;
-		stat->utime[SOL_GEMM] += tmp2;
-		stat->utime[SOL_COMM] += tmp3;
-		stat->ops[SOLVE]+= tmp4;	  
+    } /* while not finished ... */
 
+#if ( PRNTlevel>=3 )
+    t = SuperLU_timer_() - t;
+    if ( !iam ) printf(".. U-solve time\t%8.2f\n", t);
+#endif
 
-		/* Deallocate storage. */
-		SUPERLU_FREE(stat_loc);
-		SUPERLU_FREE(rtemp);
-		SUPERLU_FREE(lsum);
-		SUPERLU_FREE(x);
-		// SUPERLU_FREE(recvbuf);
-		
-		
-		// for (i = 0; i < nub; ++i) {
-			// if ( Urbs[i] ) {
-				// SUPERLU_FREE(Ucb_indptr[i]);
-				// SUPERLU_FREE(Ucb_valptr[i]);
-			// }
-		// }
-		// SUPERLU_FREE(Ucb_indptr);
-		// SUPERLU_FREE(Ucb_valptr);
-		// SUPERLU_FREE(Urbs);
-		
-		
-		SUPERLU_FREE(bmod);
-		SUPERLU_FREE(brecv);
-		SUPERLU_FREE(root_send);
-		
-		SUPERLU_FREE(rootsups);
-		SUPERLU_FREE(recvbuf_BC_fwd);		
-		
-		for (lk=0;lk<nsupers_j;++lk){
-			if(UBtree_ptr[lk]!=NULL){
-				// if(BcTree_IsRoot(LBtree_ptr[lk])==YES){			
-				BcTree_waitSendRequest(UBtree_ptr[lk]);		
-				// }
-				// deallocate requests here
-			}
+#if ( DEBUGlevel>=2 )
+    {
+	double *x_col;
+	int diag;
+	printf("\n(%d) .. After U-solve: x (ON DIAG PROCS) = \n", iam);
+	ii = 0;
+	for (k = 0; k < nsupers; ++k) {
+	    knsupc = SuperSize( k );
+	    krow = PROW( k, grid );
+	    kcol = PCOL( k, grid );
+	    diag = PNUM( krow, kcol, grid);
+	    if ( iam == diag ) { /* Diagonal process. */
+		lk = LBi( k, grid );
+		jj = X_BLK( lk );
+		x_col = &x[jj];
+		RHS_ITERATE(j) {
+		    for (i = 0; i < knsupc; ++i) { /* X stored in blocks */
+			printf("\t(%d)\t%4d\t%.10f\n",
+			       iam, xsup[k]+i, x_col[i]);
+		    }
+		    x_col += knsupc;
 		}
+	    }
+	    ii += knsupc;
+	} /* for k ... */
+    }
+#endif
 
-		for (lk=0;lk<nsupers_i;++lk){
-			if(URtree_ptr[lk]!=NULL){		
-				RdTree_waitSendRequest(URtree_ptr[lk]);		
-				// deallocate requests here
-			}
-		}		
-		MPI_Barrier( grid->comm );
+    pdReDistribute_X_to_B(n, B, m_loc, ldb, fst_row, nrhs, x, ilsum,
+			  ScalePermstruct, Glu_persist, grid, SOLVEstruct);
 
-		
-		
-		
-		
-		/*for (i = 0; i < Llu->SolveMsgSent; ++i) MPI_Request_free(&send_req[i]);*/
 
-		// for (i = 0; i < Llu->SolveMsgSent; ++i) MPI_Wait(&send_req[i], &status);
-		// SUPERLU_FREE(send_req);
+    /* Deallocate storage. */
+    SUPERLU_FREE(lsum);
+    SUPERLU_FREE(x);
+    SUPERLU_FREE(recvbuf);
+    for (i = 0; i < nub; ++i) {
+	if ( Urbs[i] ) {
+	    SUPERLU_FREE(Ucb_indptr[i]);
+	    SUPERLU_FREE(Ucb_valptr[i]);
+	}
+    }
+    SUPERLU_FREE(Ucb_indptr);
+    SUPERLU_FREE(Ucb_valptr);
+    SUPERLU_FREE(Urbs);
+    SUPERLU_FREE(bmod);
+    SUPERLU_FREE(brecv);
 
-		// MPI_Barrier( grid->comm );
+    /*for (i = 0; i < Llu->SolveMsgSent; ++i) MPI_Request_free(&send_req[i]);*/
 
+    for (i = 0; i < Llu->SolveMsgSent; ++i) MPI_Wait(&send_req[i], &status);
+    SUPERLU_FREE(send_req);
 
-#if ( PROFlevel>=2 )
-		{
-			float msg_vol_max, msg_vol_sum, msg_cnt_max, msg_cnt_sum;
-
-			MPI_Reduce (&msg_cnt, &msg_cnt_sum,
-					1, MPI_FLOAT, MPI_SUM, 0, grid->comm);
-			MPI_Reduce (&msg_cnt, &msg_cnt_max,
-					1, MPI_FLOAT, MPI_MAX, 0, grid->comm);
-			MPI_Reduce (&msg_vol, &msg_vol_sum,
-					1, MPI_FLOAT, MPI_SUM, 0, grid->comm);
-			MPI_Reduce (&msg_vol, &msg_vol_max,
-					1, MPI_FLOAT, MPI_MAX, 0, grid->comm);
-			if (!iam) {
-				printf ("\tPDGSTRS comm stat:"
-						"\tAvg\tMax\t\tAvg\tMax\n"
-						"\t\t\tCount:\t%.0f\t%.0f\tVol(MB)\t%.2f\t%.2f\n",
-						msg_cnt_sum / Pr / Pc, msg_cnt_max,
-						msg_vol_sum / Pr / Pc * 1e-6, msg_vol_max * 1e-6);
-			}
-		}
-#endif	
+    MPI_Barrier( grid->comm );
 
-		TOC(t2_sol,t1_sol);
-		stat->utime[SOLVE] = t2_sol;
+    stat->utime[SOLVE] = SuperLU_timer_() - t;
 
 #if ( DEBUGlevel>=1 )
-		CHECK_MALLOC(iam, "Exit pdgstrs()");
+    CHECK_MALLOC(iam, "Exit pdgstrs()");
 #endif
 
-		return;
-	} /* PDGSTRS */
+    return;
+} /* PDGSTRS */
 
diff --git a/SRC/pdgstrs_lsum.c b/SRC/pdgstrs_lsum.c
index b9abffe..8d3da84 100644
--- a/SRC/pdgstrs_lsum.c
+++ b/SRC/pdgstrs_lsum.c
@@ -1,13 +1,13 @@
 /*! \file
-  Copyright (c) 2003, The Regents of the University of California, through
-  Lawrence Berkeley National Laboratory (subject to receipt of any required 
-  approvals from U.S. Dept. of Energy) 
+Copyright (c) 2003, The Regents of the University of California, through
+Lawrence Berkeley National Laboratory (subject to receipt of any required 
+approvals from U.S. Dept. of Energy) 
 
-  All rights reserved. 
+All rights reserved. 
 
-  The source code is distributed under BSD license, see the file License.txt
-  at the top-level directory.
- */
+The source code is distributed under BSD license, see the file License.txt
+at the top-level directory.
+*/
 
 
 /*! @file 
@@ -25,7 +25,6 @@
  */
 
 #include "superlu_ddefs.h"
-#include "superlu_defs.h"
 
 #define ISEND_IRECV
 
@@ -34,20 +33,14 @@
  */
 #ifdef _CRAY
 fortran void STRSM(_fcd, _fcd, _fcd, _fcd, int*, int*, double*,
-		double*, int*, double*, int*);
+		   double*, int*, double*, int*);
 fortran void SGEMM(_fcd, _fcd, int*, int*, int*, double*, double*, 
-		int*, double*, int*, double*, double*, int*);
+		   int*, double*, int*, double*, double*, int*);
 _fcd ftcs1;
 _fcd ftcs2;
 _fcd ftcs3;
 #endif
 
-
-// #ifndef CACHELINE
-// #define CACHELINE 64  /* bytes, Xeon Phi KNL, Cori haswell, Edision */
-// #endif
-
-
 /************************************************************************/
 /*! \brief
  *
@@ -76,181 +69,146 @@ void dlsum_fmod
  LocalLU_t *Llu,
  MPI_Request send_req[], /* input/output */
  SuperLUStat_t *stat
- )
+)
 {
-	double alpha = 1.0, beta = 0.0;
-	double *lusup, *lusup1;
-	double *dest;
-	int    iam, iknsupc, myrow, nbrow, nsupr, nsupr1, p, pi;
-	int_t  i, ii, ik, il, ikcol, irow, j, lb, lk, lib, rel;
-	int_t  *lsub, *lsub1, nlb1, lptr1, luptr1;
-	int_t  *ilsum = Llu->ilsum; /* Starting position of each supernode in lsum.   */
-	int_t  *frecv = Llu->frecv;
-	int_t  **fsendx_plist = Llu->fsendx_plist;
-	MPI_Status status;
-	int test_flag;
-
-#if ( PROFlevel>=1 )
-	double t1, t2;
-	float msg_vol = 0, msg_cnt = 0;
-#endif 
-
-
-#if ( PROFlevel>=1 )
-	TIC(t1);
-#endif	
-
-	iam = grid->iam;
-	myrow = MYROW( iam, grid );
-	lk = LBj( k, grid ); /* Local block number, column-wise. */
-	lsub = Llu->Lrowind_bc_ptr[lk];
-	lusup = Llu->Lnzval_bc_ptr[lk];
-	nsupr = lsub[1];
-
-	for (lb = 0; lb < nlb; ++lb) {
-		ik = lsub[lptr]; /* Global block number, row-wise. */
-		nbrow = lsub[lptr+1];
+    double alpha = 1.0, beta = 0.0;
+    double *lusup, *lusup1;
+    double *dest;
+    int    iam, iknsupc, myrow, nbrow, nsupr, nsupr1, p, pi;
+    int_t  i, ii, ik, il, ikcol, irow, j, lb, lk, rel;
+    int_t  *lsub, *lsub1, nlb1, lptr1, luptr1;
+    int_t  *ilsum = Llu->ilsum; /* Starting position of each supernode in lsum.   */
+    int_t  *frecv = Llu->frecv;
+    int_t  **fsendx_plist = Llu->fsendx_plist;
+    MPI_Status status;
+    int test_flag;
+
+    iam = grid->iam;
+    myrow = MYROW( iam, grid );
+    lk = LBj( k, grid ); /* Local block number, column-wise. */
+    lsub = Llu->Lrowind_bc_ptr[lk];
+    lusup = Llu->Lnzval_bc_ptr[lk];
+    nsupr = lsub[1];
+
+    for (lb = 0; lb < nlb; ++lb) {
+	ik = lsub[lptr]; /* Global block number, row-wise. */
+	nbrow = lsub[lptr+1];
 #ifdef _CRAY
-		SGEMM( ftcs2, ftcs2, &nbrow, &nrhs, &knsupc,
-				&alpha, &lusup[luptr], &nsupr, xk,
-				&knsupc, &beta, rtemp, &nbrow );
+	SGEMM( ftcs2, ftcs2, &nbrow, &nrhs, &knsupc,
+	      &alpha, &lusup[luptr], &nsupr, xk,
+	      &knsupc, &beta, rtemp, &nbrow );
 #elif defined (USE_VENDOR_BLAS)
-		dgemm_( "N", "N", &nbrow, &nrhs, &knsupc,
-				&alpha, &lusup[luptr], &nsupr, xk,
-				&knsupc, &beta, rtemp, &nbrow, 1, 1 );
-#else
-		dgemm_( "N", "N", &nbrow, &nrhs, &knsupc,
-				&alpha, &lusup[luptr], &nsupr, xk,
-				&knsupc, &beta, rtemp, &nbrow );
-#endif
-		stat->ops[SOLVE] += 2 * nbrow * nrhs * knsupc + nbrow * nrhs;
-
-		lk = LBi( ik, grid ); /* Local block number, row-wise. */
-		iknsupc = SuperSize( ik );
-		il = LSUM_BLK( lk );
-		dest = &lsum[il];
-		lptr += LB_DESCRIPTOR;
-		rel = xsup[ik]; /* Global row index of block ik. */
-		for (i = 0; i < nbrow; ++i) {
-			irow = lsub[lptr++] - rel; /* Relative row. */
-			RHS_ITERATE(j)
-				dest[irow + j*iknsupc] -= rtemp[i + j*nbrow];
-		}
-		luptr += nbrow;
-
-
-
-#if ( PROFlevel>=1 )
-		TOC(t2, t1);
-		stat->utime[SOL_GEMM] += t2;
-
-#endif	
-
-
-
-
-		if ( (--fmod[lk])==0 ) { /* Local accumulation done. */
-			ikcol = PCOL( ik, grid );
-			p = PNUM( myrow, ikcol, grid );
-			if ( iam != p ) {
+	dgemm_( "N", "N", &nbrow, &nrhs, &knsupc,
+	       &alpha, &lusup[luptr], &nsupr, xk,
+	       &knsupc, &beta, rtemp, &nbrow, 1, 1 );
+#else
+	dgemm_( "N", "N", &nbrow, &nrhs, &knsupc,
+	       &alpha, &lusup[luptr], &nsupr, xk,
+	       &knsupc, &beta, rtemp, &nbrow );
+#endif
+	stat->ops[SOLVE] += 2 * nbrow * nrhs * knsupc + nbrow * nrhs;
+   
+	lk = LBi( ik, grid ); /* Local block number, row-wise. */
+	iknsupc = SuperSize( ik );
+	il = LSUM_BLK( lk );
+	dest = &lsum[il];
+	lptr += LB_DESCRIPTOR;
+	rel = xsup[ik]; /* Global row index of block ik. */
+	for (i = 0; i < nbrow; ++i) {
+	    irow = lsub[lptr++] - rel; /* Relative row. */
+	    RHS_ITERATE(j)
+		dest[irow + j*iknsupc] -= rtemp[i + j*nbrow];
+	}
+	luptr += nbrow;
+		    
+	if ( (--fmod[lk])==0 ) { /* Local accumulation done. */
+	    ikcol = PCOL( ik, grid );
+	    p = PNUM( myrow, ikcol, grid );
+	    if ( iam != p ) {
 #ifdef ISEND_IRECV
-				MPI_Isend( &lsum[il - LSUM_H], iknsupc * nrhs + LSUM_H,
-						MPI_DOUBLE, p, LSUM, grid->comm,
-						&send_req[Llu->SolveMsgSent++] );
+		MPI_Isend( &lsum[il - LSUM_H], iknsupc * nrhs + LSUM_H,
+			   MPI_DOUBLE, p, LSUM, grid->comm,
+                           &send_req[Llu->SolveMsgSent++] );
 #else
 #ifdef BSEND
-				MPI_Bsend( &lsum[il - LSUM_H], iknsupc * nrhs + LSUM_H,
-						MPI_DOUBLE, p, LSUM, grid->comm );
+		MPI_Bsend( &lsum[il - LSUM_H], iknsupc * nrhs + LSUM_H,
+			   MPI_DOUBLE, p, LSUM, grid->comm );
 #else
-				MPI_Send( &lsum[il - LSUM_H], iknsupc * nrhs + LSUM_H,
-						MPI_DOUBLE, p, LSUM, grid->comm );
+		MPI_Send( &lsum[il - LSUM_H], iknsupc * nrhs + LSUM_H,
+			 MPI_DOUBLE, p, LSUM, grid->comm );
 #endif
 #endif
 #if ( DEBUGlevel>=2 )
-				printf("(%2d) Sent LSUM[%2.0f], size %2d, to P %2d\n",
-						iam, lsum[il-LSUM_H], iknsupc*nrhs+LSUM_H, p);
-#endif
-			} else { /* Diagonal process: X[i] += lsum[i]. */
-				ii = X_BLK( lk );
-				RHS_ITERATE(j)
-					for (i = 0; i < iknsupc; ++i)
-						x[i + ii + j*iknsupc] += lsum[i + il + j*iknsupc];
-				if ( frecv[lk]==0 ) { /* Becomes a leaf node. */
-					fmod[lk] = -1; /* Do not solve X[k] in the future. */
-					lk = LBj( ik, grid );/* Local block number, column-wise. */
-					lsub1 = Llu->Lrowind_bc_ptr[lk];
-					lusup1 = Llu->Lnzval_bc_ptr[lk];
-					nsupr1 = lsub1[1];
-
-
-#if ( PROFlevel>=1 )
-					TIC(t1);
-#endif				
-
+		printf("(%2d) Sent LSUM[%2.0f], size %2d, to P %2d\n",
+		       iam, lsum[il-LSUM_H], iknsupc*nrhs+LSUM_H, p);
+#endif
+	    } else { /* Diagonal process: X[i] += lsum[i]. */
+		ii = X_BLK( lk );
+		RHS_ITERATE(j)
+		    for (i = 0; i < iknsupc; ++i)
+			x[i + ii + j*iknsupc] += lsum[i + il + j*iknsupc];
+		if ( frecv[lk]==0 ) { /* Becomes a leaf node. */
+		    fmod[lk] = -1; /* Do not solve X[k] in the future. */
+		    lk = LBj( ik, grid );/* Local block number, column-wise. */
+		    lsub1 = Llu->Lrowind_bc_ptr[lk];
+		    lusup1 = Llu->Lnzval_bc_ptr[lk];
+		    nsupr1 = lsub1[1];
 #ifdef _CRAY
-					STRSM(ftcs1, ftcs1, ftcs2, ftcs3, &iknsupc, &nrhs, &alpha,
-							lusup1, &nsupr1, &x[ii], &iknsupc);
+		    STRSM(ftcs1, ftcs1, ftcs2, ftcs3, &iknsupc, &nrhs, &alpha,
+			  lusup1, &nsupr1, &x[ii], &iknsupc);
 #elif defined (USE_VENDOR_BLAS)
-					dtrsm_("L", "L", "N", "U", &iknsupc, &nrhs, &alpha, 
-							lusup1, &nsupr1, &x[ii], &iknsupc, 1, 1, 1, 1);		   
+		    dtrsm_("L", "L", "N", "U", &iknsupc, &nrhs, &alpha, 
+			   lusup1, &nsupr1, &x[ii], &iknsupc, 1, 1, 1, 1);
 #else
-					dtrsm_("L", "L", "N", "U", &iknsupc, &nrhs, &alpha, 
-							lusup1, &nsupr1, &x[ii], &iknsupc);
+		    dtrsm_("L", "L", "N", "U", &iknsupc, &nrhs, &alpha, 
+			   lusup1, &nsupr1, &x[ii], &iknsupc);
 #endif
-
-
-#if ( PROFlevel>=1 )
-					TOC(t2, t1);
-					stat->utime[SOL_TRSM] += t2;
-
-#endif	
-
-
-					stat->ops[SOLVE] += iknsupc * (iknsupc - 1) * nrhs;
+		    stat->ops[SOLVE] += iknsupc * (iknsupc - 1) * nrhs;
 #if ( DEBUGlevel>=2 )
-					printf("(%2d) Solve X[%2d]\n", iam, ik);
+		    printf("(%2d) Solve X[%2d]\n", iam, ik);
 #endif
-
-					/*
-					 * Send Xk to process column Pc[k].
-					 */			 
-					for (p = 0; p < grid->nprow; ++p) {
-						if ( fsendx_plist[lk][p] != EMPTY ) {
-							pi = PNUM( p, ikcol, grid );
+		
+		    /*
+		     * Send Xk to process column Pc[k].
+		     */
+		    for (p = 0; p < grid->nprow; ++p) {
+			if ( fsendx_plist[lk][p] != EMPTY ) {
+			    pi = PNUM( p, ikcol, grid );
 #ifdef ISEND_IRECV
-							MPI_Isend( &x[ii - XK_H], iknsupc * nrhs + XK_H,
-									MPI_DOUBLE, pi, Xk, grid->comm,
-									&send_req[Llu->SolveMsgSent++] );
+			    MPI_Isend( &x[ii - XK_H], iknsupc * nrhs + XK_H,
+				       MPI_DOUBLE, pi, Xk, grid->comm,
+				       &send_req[Llu->SolveMsgSent++] );
 #else
 #ifdef BSEND
-							MPI_Bsend( &x[ii - XK_H], iknsupc * nrhs + XK_H,
-									MPI_DOUBLE, pi, Xk, grid->comm );
+			    MPI_Bsend( &x[ii - XK_H], iknsupc * nrhs + XK_H,
+				       MPI_DOUBLE, pi, Xk, grid->comm );
 #else
-							MPI_Send( &x[ii - XK_H], iknsupc * nrhs + XK_H,
-									MPI_DOUBLE, pi, Xk, grid->comm );
+			    MPI_Send( &x[ii - XK_H], iknsupc * nrhs + XK_H,
+				     MPI_DOUBLE, pi, Xk, grid->comm );
 #endif
 #endif
 #if ( DEBUGlevel>=2 )
-							printf("(%2d) Sent X[%2.0f] to P %2d\n",
-									iam, x[ii-XK_H], pi);
+			    printf("(%2d) Sent X[%2.0f] to P %2d\n",
+				   iam, x[ii-XK_H], pi);
 #endif
-						}
-					}
-					/*
-					 * Perform local block modifications.
-					 */
-					nlb1 = lsub1[0] - 1;
-					lptr1 = BC_HEADER + LB_DESCRIPTOR + iknsupc;
-					luptr1 = iknsupc; /* Skip diagonal block L(I,I). */
-
-					dlsum_fmod(lsum, x, &x[ii], rtemp, nrhs, iknsupc, ik,
-							fmod, nlb1, lptr1, luptr1, xsup,
-							grid, Llu, send_req, stat);
-				} /* if frecv[lk] == 0 */
-			} /* if iam == p */
-		} /* if fmod[lk] == 0 */
+			}
+                    }
+		    /*
+		     * Perform local block modifications.
+		     */
+		    nlb1 = lsub1[0] - 1;
+		    lptr1 = BC_HEADER + LB_DESCRIPTOR + iknsupc;
+		    luptr1 = iknsupc; /* Skip diagonal block L(I,I). */
+
+		    dlsum_fmod(lsum, x, &x[ii], rtemp, nrhs, iknsupc, ik,
+			       fmod, nlb1, lptr1, luptr1, xsup,
+			       grid, Llu, send_req, stat);
+		} /* if frecv[lk] == 0 */
+	    } /* if iam == p */
+	} /* if fmod[lk] == 0 */
+
+    } /* for lb ... */
 
-	} /* for lb ... */
 } /* dLSUM_FMOD */
 
 
@@ -274,1871 +232,143 @@ void dlsum_bmod
  SuperLUStat_t *stat
  )
 {
-	/*
-	 * Purpose
-	 * =======
-	 *   Perform local block modifications: lsum[i] -= U_i,k * X[k].
-	 */
-	double alpha = 1.0, beta = 0.0;
-	int    iam, iknsupc, knsupc, myrow, nsupr, p, pi;
-	int_t  fnz, gik, gikcol, i, ii, ik, ikfrow, iklrow, il, irow,
-	       j, jj, lk, lk1, nub, ub, uptr;
-	int_t  *usub;
-	double *uval, *dest, *y;
-	int_t  *lsub;
-	double *lusup;
-	int_t  *ilsum = Llu->ilsum; /* Starting position of each supernode in lsum.   */
-	int_t  *brecv = Llu->brecv;
-	int_t  **bsendx_plist = Llu->bsendx_plist;
-	MPI_Status status;
-	int test_flag;
-
-	iam = grid->iam;
-	myrow = MYROW( iam, grid );
-	knsupc = SuperSize( k );
-	lk = LBj( k, grid ); /* Local block number, column-wise. */
-	nub = Urbs[lk];      /* Number of U blocks in block column lk */
-
-	for (ub = 0; ub < nub; ++ub) {
-		ik = Ucb_indptr[lk][ub].lbnum; /* Local block number, row-wise. */
-		usub = Llu->Ufstnz_br_ptr[ik];
-		uval = Llu->Unzval_br_ptr[ik];
-		i = Ucb_indptr[lk][ub].indpos; /* Start of the block in usub[]. */
-		i += UB_DESCRIPTOR;
-		il = LSUM_BLK( ik );
-		gik = ik * grid->nprow + myrow;/* Global block number, row-wise. */
-		iknsupc = SuperSize( gik );
-		ikfrow = FstBlockC( gik );
-		iklrow = FstBlockC( gik+1 );
-
-		RHS_ITERATE(j) {
-			dest = &lsum[il + j*iknsupc];
-			y = &xk[j*knsupc];
-			uptr = Ucb_valptr[lk][ub]; /* Start of the block in uval[]. */
-			for (jj = 0; jj < knsupc; ++jj) {
-				fnz = usub[i + jj];
-				if ( fnz < iklrow ) { /* Nonzero segment. */
-					/* AXPY */
-					for (irow = fnz; irow < iklrow; ++irow)
-						dest[irow - ikfrow] -= uval[uptr++] * y[jj];
-					stat->ops[SOLVE] += 2 * (iklrow - fnz);
-				}
-			} /* for jj ... */
+/*
+ * Purpose
+ * =======
+ *   Perform local block modifications: lsum[i] -= U_i,k * X[k].
+ */
+    double alpha = 1.0;
+    int    iam, iknsupc, knsupc, myrow, nsupr, p, pi;
+    int_t  fnz, gik, gikcol, i, ii, ik, ikfrow, iklrow, il, irow,
+           j, jj, lk, lk1, nub, ub, uptr;
+    int_t  *usub;
+    double *uval, *dest, *y;
+    int_t  *lsub;
+    double *lusup;
+    int_t  *ilsum = Llu->ilsum; /* Starting position of each supernode in lsum.   */
+    int_t  *brecv = Llu->brecv;
+    int_t  **bsendx_plist = Llu->bsendx_plist;
+    MPI_Status status;
+    int test_flag;
+
+    iam = grid->iam;
+    myrow = MYROW( iam, grid );
+    knsupc = SuperSize( k );
+    lk = LBj( k, grid ); /* Local block number, column-wise. */
+    nub = Urbs[lk];      /* Number of U blocks in block column lk */
+
+    for (ub = 0; ub < nub; ++ub) {
+	ik = Ucb_indptr[lk][ub].lbnum; /* Local block number, row-wise. */
+	usub = Llu->Ufstnz_br_ptr[ik];
+	uval = Llu->Unzval_br_ptr[ik];
+	i = Ucb_indptr[lk][ub].indpos; /* Start of the block in usub[]. */
+	i += UB_DESCRIPTOR;
+	il = LSUM_BLK( ik );
+	gik = ik * grid->nprow + myrow;/* Global block number, row-wise. */
+	iknsupc = SuperSize( gik );
+	ikfrow = FstBlockC( gik );
+	iklrow = FstBlockC( gik+1 );
+
+	RHS_ITERATE(j) {
+	    dest = &lsum[il + j*iknsupc];
+	    y = &xk[j*knsupc];
+	    uptr = Ucb_valptr[lk][ub]; /* Start of the block in uval[]. */
+	    for (jj = 0; jj < knsupc; ++jj) {
+		fnz = usub[i + jj];
+		if ( fnz < iklrow ) { /* Nonzero segment. */
+		    /* AXPY */
+		    for (irow = fnz; irow < iklrow; ++irow)
+			dest[irow - ikfrow] -= uval[uptr++] * y[jj];
+		    stat->ops[SOLVE] += 2 * (iklrow - fnz);
 		}
+	    } /* for jj ... */
+	}
 
-		if ( (--bmod[ik]) == 0 ) { /* Local accumulation done. */
-			gikcol = PCOL( gik, grid );
-			p = PNUM( myrow, gikcol, grid );
-			if ( iam != p ) {
+	if ( (--bmod[ik]) == 0 ) { /* Local accumulation done. */
+	    gikcol = PCOL( gik, grid );
+	    p = PNUM( myrow, gikcol, grid );
+	    if ( iam != p ) {
 #ifdef ISEND_IRECV
-				MPI_Isend( &lsum[il - LSUM_H], iknsupc * nrhs + LSUM_H,
-						MPI_DOUBLE, p, LSUM, grid->comm,
-						&send_req[Llu->SolveMsgSent++] );
+		MPI_Isend( &lsum[il - LSUM_H], iknsupc * nrhs + LSUM_H,
+			   MPI_DOUBLE, p, LSUM, grid->comm,
+                           &send_req[Llu->SolveMsgSent++] );
 #else
 #ifdef BSEND
-				MPI_Bsend( &lsum[il - LSUM_H], iknsupc * nrhs + LSUM_H,
-						MPI_DOUBLE, p, LSUM, grid->comm );
+		MPI_Bsend( &lsum[il - LSUM_H], iknsupc * nrhs + LSUM_H,
+			   MPI_DOUBLE, p, LSUM, grid->comm );
 #else
-				MPI_Send( &lsum[il - LSUM_H], iknsupc * nrhs + LSUM_H,
-						MPI_DOUBLE, p, LSUM, grid->comm );
+		MPI_Send( &lsum[il - LSUM_H], iknsupc * nrhs + LSUM_H,
+			  MPI_DOUBLE, p, LSUM, grid->comm );
 #endif
 #endif
 #if ( DEBUGlevel>=2 )
-				printf("(%2d) Sent LSUM[%2.0f], size %2d, to P %2d\n",
-						iam, lsum[il-LSUM_H], iknsupc*nrhs+LSUM_H, p);
-#endif
-			} else { /* Diagonal process: X[i] += lsum[i]. */
-				ii = X_BLK( ik );
-				dest = &x[ii];
-				RHS_ITERATE(j)
-					for (i = 0; i < iknsupc; ++i)
-						dest[i + j*iknsupc] += lsum[i + il + j*iknsupc];
-				if ( !brecv[ik] ) { /* Becomes a leaf node. */
-					bmod[ik] = -1; /* Do not solve X[k] in the future. */
-					lk1 = LBj( gik, grid ); /* Local block number. */
-					lsub = Llu->Lrowind_bc_ptr[lk1];
-					lusup = Llu->Lnzval_bc_ptr[lk1];
-					nsupr = lsub[1];
+		printf("(%2d) Sent LSUM[%2.0f], size %2d, to P %2d\n",
+		       iam, lsum[il-LSUM_H], iknsupc*nrhs+LSUM_H, p);
+#endif
+	    } else { /* Diagonal process: X[i] += lsum[i]. */
+		ii = X_BLK( ik );
+		dest = &x[ii];
+		RHS_ITERATE(j)
+		    for (i = 0; i < iknsupc; ++i)
+			dest[i + j*iknsupc] += lsum[i + il + j*iknsupc];
+		if ( !brecv[ik] ) { /* Becomes a leaf node. */
+		    bmod[ik] = -1; /* Do not solve X[k] in the future. */
+		    lk1 = LBj( gik, grid ); /* Local block number. */
+		    lsub = Llu->Lrowind_bc_ptr[lk1];
+		    lusup = Llu->Lnzval_bc_ptr[lk1];
+		    nsupr = lsub[1];
 #ifdef _CRAY
-					STRSM(ftcs1, ftcs3, ftcs2, ftcs2, &iknsupc, &nrhs, &alpha,
-							lusup, &nsupr, &x[ii], &iknsupc);
+		    STRSM(ftcs1, ftcs3, ftcs2, ftcs2, &iknsupc, &nrhs, &alpha,
+			  lusup, &nsupr, &x[ii], &iknsupc);
 #elif defined (USE_VENDOR_BLAS)
-					dtrsm_("L", "U", "N", "N", &iknsupc, &nrhs, &alpha, 
-							lusup, &nsupr, &x[ii], &iknsupc, 1, 1, 1, 1);	
+		    dtrsm_("L", "U", "N", "N", &iknsupc, &nrhs, &alpha, 
+			   lusup, &nsupr, &x[ii], &iknsupc, 1, 1, 1, 1);
 #else
-					dtrsm_("L", "U", "N", "N", &iknsupc, &nrhs, &alpha, 
-							lusup, &nsupr, &x[ii], &iknsupc);
+		    dtrsm_("L", "U", "N", "N", &iknsupc, &nrhs, &alpha, 
+			   lusup, &nsupr, &x[ii], &iknsupc);
 #endif
-					stat->ops[SOLVE] += iknsupc * (iknsupc + 1) * nrhs;
+		    stat->ops[SOLVE] += iknsupc * (iknsupc + 1) * nrhs;
 #if ( DEBUGlevel>=2 )
-					printf("(%2d) Solve X[%2d]\n", iam, gik);
+		    printf("(%2d) Solve X[%2d]\n", iam, gik);
 #endif
 
-					/*
-					 * Send Xk to process column Pc[k].
-					 */
-					for (p = 0; p < grid->nprow; ++p) {
-						if ( bsendx_plist[lk1][p] != EMPTY ) {
-							pi = PNUM( p, gikcol, grid );
+		    /*
+		     * Send Xk to process column Pc[k].
+		     */
+		    for (p = 0; p < grid->nprow; ++p) {
+			if ( bsendx_plist[lk1][p] != EMPTY ) {
+			    pi = PNUM( p, gikcol, grid );
 #ifdef ISEND_IRECV
-							MPI_Isend( &x[ii - XK_H], iknsupc * nrhs + XK_H,
-									MPI_DOUBLE, pi, Xk, grid->comm,
-									&send_req[Llu->SolveMsgSent++] );
+			    MPI_Isend( &x[ii - XK_H], iknsupc * nrhs + XK_H,
+				       MPI_DOUBLE, pi, Xk, grid->comm,
+				       &send_req[Llu->SolveMsgSent++] );
 #else
 #ifdef BSEND
-							MPI_Bsend( &x[ii - XK_H], iknsupc * nrhs + XK_H,
-									MPI_DOUBLE, pi, Xk, grid->comm );
+			    MPI_Bsend( &x[ii - XK_H], iknsupc * nrhs + XK_H,
+				       MPI_DOUBLE, pi, Xk, grid->comm );
 #else
-							MPI_Send( &x[ii - XK_H], iknsupc * nrhs + XK_H,
-									MPI_DOUBLE, pi, Xk, grid->comm );
+			    MPI_Send( &x[ii - XK_H], iknsupc * nrhs + XK_H,
+				     MPI_DOUBLE, pi, Xk, grid->comm );
 #endif
 #endif
 #if ( DEBUGlevel>=2 )
-							printf("(%2d) Sent X[%2.0f] to P %2d\n",
-									iam, x[ii-XK_H], pi);
+			    printf("(%2d) Sent X[%2.0f] to P %2d\n",
+				   iam, x[ii-XK_H], pi);
 #endif
-						}
-					}
-					/*
-					 * Perform local block modifications.
-					 */
-					if ( Urbs[lk1] )
-						dlsum_bmod(lsum, x, &x[ii], nrhs, gik, bmod, Urbs,
-								Ucb_indptr, Ucb_valptr, xsup, grid, Llu,
-								send_req, stat);
-				} /* if brecv[ik] == 0 */
 			}
-		} /* if bmod[ik] == 0 */
-
-	} /* for ub ... */
+                     }
+		    /*
+		     * Perform local block modifications.
+		     */
+		    if ( Urbs[lk1] )
+			dlsum_bmod(lsum, x, &x[ii], nrhs, gik, bmod, Urbs,
+				   Ucb_indptr, Ucb_valptr, xsup, grid, Llu,
+				   send_req, stat);
+		} /* if brecv[ik] == 0 */
+	    }
+	} /* if bmod[ik] == 0 */
+
+    } /* for ub ... */
 
 } /* dlSUM_BMOD */
 
-
-/************************************************************************/
-/*! \brief
- *
- * <pre>
- * Purpose
- * =======
- *   Perform local block modifications: lsum[i] -= L_i,k * X[k].
- * </pre>
- */
-void dlsum_fmod_inv
-/************************************************************************/
-(
- double *lsum,    /* Sum of local modifications.                        */
- double *x,       /* X array (local)                                    */
- double *xk,      /* X[k].                                              */
- double *rtemp,   /* Result of full matrix-vector multiply.             */
- int   nrhs,      /* Number of right-hand sides.                        */
- int   knsupc,    /* Size of supernode k.                               */
- int_t k,         /* The k-th component of X.                           */
- int_t *fmod,     /* Modification count for L-solve.                    */
- int_t nlb,       /* Number of L blocks.                                */
- int_t *xsup,
- gridinfo_t *grid,
- LocalLU_t *Llu,
- SuperLUStat_t **stat,
- int_t *leaf_send,
- int_t *nleaf_send,
- int_t sizelsum,
- int_t sizertemp,
- int_t recurlevel
- )
-{
-	double alpha = 1.0, beta = 0.0,malpha=-1.0;
-	double *lusup, *lusup1;
-	double *dest;
-	double *Linv;/* Inverse of diagonal block */    
-	int    iam, iknsupc, myrow, krow, nbrow, nbrow1, nbrow_ref, nsupr, nsupr1, p, pi, idx_r,m;
-	int_t  i, ii,jj, ik, il, ikcol, irow, j, lb, lk, rel, lib,lready;
-	int_t  *lsub, *lsub1, nlb1, lptr1, luptr1,*lloc;
-	int_t  *ilsum = Llu->ilsum; /* Starting position of each supernode in lsum.   */
-	int_t  *frecv = Llu->frecv;
-	int_t  **fsendx_plist = Llu->fsendx_plist;
-	int_t  luptr_tmp,luptr_tmp1,lptr1_tmp,maxrecvsz, idx_i, idx_v,idx_n,  idx_l, fmod_tmp, lbstart,lbend,nn,Nchunk,nlb_loc,remainder;
-	int thread_id,thread_id1,num_thread;
-	flops_t ops_loc=0.0;         
-	MPI_Status status;
-	int test_flag;
-	yes_no_t done;
-	BcTree  *LBtree_ptr = Llu->LBtree_ptr;
-	RdTree  *LRtree_ptr = Llu->LRtree_ptr;
-	int_t* idx_lsum,idx_lsum1;
-	double *rtemp_loc;
-	int_t ldalsum,maxsuper,aln_d;
-	int dword = sizeof (double);	
-	int_t nleaf_send_tmp;
-	int_t lptr;      /* Starting position in lsub[*].                      */
-	int_t luptr;     /* Starting position in lusup[*].                     */
-
-	maxsuper = sp_ienv_dist(3);
-
-#ifdef _OPENMP
-	thread_id = omp_get_thread_num ();
-	num_thread = omp_get_num_threads ();
-#else
-	thread_id = 0;
-	num_thread = 1;
-#endif
-	ldalsum=Llu->ldalsum;
-
-	rtemp_loc = &rtemp[sizertemp* thread_id];
-
-	// #if ( PROFlevel>=1 )
-	double t1, t2, t3, t4;
-	float msg_vol = 0, msg_cnt = 0;
-	// #endif 
-
-
-	if(nlb>0){
-		maxrecvsz = sp_ienv_dist(3) * nrhs + SUPERLU_MAX( XK_H, LSUM_H );
-
-		iam = grid->iam;
-		myrow = MYROW( iam, grid );
-		lk = LBj( k, grid ); /* Local block number, column-wise. */
-
-		// printf("ya1 %5d k %5d lk %5d\n",thread_id,k,lk);
-		// fflush(stdout);	
-
-		lsub = Llu->Lrowind_bc_ptr[lk];
-
-		// printf("ya2 %5d k %5d lk %5d\n",thread_id,k,lk);
-		// fflush(stdout);	
-
-		lusup = Llu->Lnzval_bc_ptr[lk];
-		lloc = Llu->Lindval_loc_bc_ptr[lk];
-		// idx_lsum = Llu->Lrowind_bc_2_lsum[lk];
-
-		nsupr = lsub[1];
-
-		// printf("nlb: %5d lk: %5d\n",nlb,lk);
-		// fflush(stdout);
-
-		krow = PROW( k, grid );
-		if(myrow==krow){
-			idx_n = 1;
-			idx_i = nlb+2;
-			idx_v = 2*nlb+3;
-			luptr_tmp = lloc[idx_v];
-			m = nsupr-knsupc;
-		}else{
-			idx_n = 0;
-			idx_i = nlb;
-			idx_v = 2*nlb;
-			luptr_tmp = lloc[idx_v];
-			m = nsupr;
-		}
-
-		assert(m>0);
-				
-		if(m>8*maxsuper){ 
-			// if(m<1){
-			// TIC(t1);
-			Nchunk=num_thread;
-			nlb_loc = floor(((double)nlb)/Nchunk);
-			remainder = nlb % Nchunk;
-
-#ifdef _OPENMP
-#pragma	omp	taskloop private (lptr1,luptr1,nlb1,thread_id1,lsub1,lusup1,nsupr1,Linv,nn,lbstart,lbend,luptr_tmp1,nbrow,lb,lptr1_tmp,rtemp_loc,nbrow_ref,lptr,nbrow1,ik,rel,lk,iknsupc,il,i,irow,fmod_tmp,ikcol,p,ii,jj,t1,t2,j,nleaf_send_tmp) untied nogroup	
-#endif	
-			for (nn=0;nn<Nchunk;++nn){
-
-#ifdef _OPENMP				 
-				thread_id1 = omp_get_thread_num ();
-#else
-				thread_id1 = 0;
-#endif		
-				rtemp_loc = &rtemp[sizertemp* thread_id1];
-
-				if(nn<remainder){
-					lbstart = nn*(nlb_loc+1);
-					lbend = (nn+1)*(nlb_loc+1);
-				}else{
-					lbstart = remainder+nn*nlb_loc;
-					lbend = remainder + (nn+1)*nlb_loc;
-				}
-
-				if(lbstart<lbend){
-
-#if ( PROFlevel>=1 )
-					TIC(t1);
-#endif				
-					luptr_tmp1 = lloc[lbstart+idx_v];
-					nbrow=0;
-					for (lb = lbstart; lb < lbend; ++lb){ 		
-						lptr1_tmp = lloc[lb+idx_i];		
-						nbrow += lsub[lptr1_tmp+1];
-					}
-
-#ifdef _CRAY
-					SGEMM( ftcs2, ftcs2, &nbrow, &nrhs, &knsupc,
-							&alpha, &lusup[luptr_tmp1], &nsupr, xk,
-							&knsupc, &beta, rtemp_loc, &nbrow );
-#elif defined (USE_VENDOR_BLAS)
-					dgemm_( "N", "N", &nbrow, &nrhs, &knsupc,
-							&alpha, &lusup[luptr_tmp1], &nsupr, xk,
-							&knsupc, &beta, rtemp_loc, &nbrow, 1, 1 );
-#else
-					dgemm_( "N", "N", &nbrow, &nrhs, &knsupc,
-							&alpha, &lusup[luptr_tmp1], &nsupr, xk,
-							&knsupc, &beta, rtemp_loc, &nbrow );
-#endif  			
-
-					nbrow_ref=0;
-					for (lb = lbstart; lb < lbend; ++lb){ 		
-						lptr1_tmp = lloc[lb+idx_i];	
-						lptr= lptr1_tmp+2;	
-						nbrow1 = lsub[lptr1_tmp+1];
-						ik = lsub[lptr1_tmp]; /* Global block number, row-wise. */
-						rel = xsup[ik]; /* Global row index of block ik. */
-
-
-						lk = LBi( ik, grid ); /* Local block number, row-wise. */	
-
-						iknsupc = SuperSize( ik );
-						il = LSUM_BLK( lk );
-
-						RHS_ITERATE(j)					
-							for (i = 0; i < nbrow1; ++i) {
-								irow = lsub[lptr+i] - rel; /* Relative row. */
-
-								lsum[il+irow + j*iknsupc+sizelsum*thread_id1] -= rtemp_loc[nbrow_ref+i + j*nbrow];
-							}
-						nbrow_ref+=nbrow1;
-					}
-
-#if ( PROFlevel>=1 )
-					TOC(t2, t1);
-					stat[thread_id1]->utime[SOL_GEMM] += t2;
-#endif	
-
-					for (lb=lbstart;lb<lbend;lb++){
-						lk = lloc[lb+idx_n];
-
-#ifdef _OPENMP
-#pragma omp atomic capture
-#endif
-						fmod_tmp=--fmod[lk];
-
-						if ( fmod_tmp==0 ) { /* Local accumulation done. */
-
-							lptr1_tmp = lloc[lb+idx_i];	
-
-							ik = lsub[lptr1_tmp]; /* Global block number, row-wise. */
-							lk = LBi( ik, grid ); /* Local block number, row-wise. */	
-
-							iknsupc = SuperSize( ik );
-							il = LSUM_BLK( lk );
-
-							ikcol = PCOL( ik, grid );
-							p = PNUM( myrow, ikcol, grid );
-							if ( iam != p ) {
-
-								for (ii=1;ii<num_thread;ii++)
-									for (jj=0;jj<iknsupc*nrhs;jj++)
-										lsum[il + jj ] += lsum[il + jj + ii*sizelsum];
-
-#ifdef _OPENMP
-#pragma omp atomic capture
-#endif
-								nleaf_send_tmp = ++nleaf_send[0];
-								leaf_send[nleaf_send_tmp-1] = -lk-1;		
-								// RdTree_forwardMessageSimple(LRtree_ptr[lk],&lsum[il - LSUM_H ]);
-
-							} else { /* Diagonal process: X[i] += lsum[i]. */
-
-#if ( PROFlevel>=1 )
-								TIC(t1);
-#endif		
-								for (ii=1;ii<num_thread;ii++)
-									// if(ii!=thread_id1)
-									for (jj=0;jj<iknsupc*nrhs;jj++)
-										lsum[il+ jj] += lsum[il + jj + ii*sizelsum];
-
-								ii = X_BLK( lk );
-								// for (jj=0;jj<num_thread;jj++)
-								RHS_ITERATE(j)
-									for (i = 0; i < iknsupc; ++i)	
-										x[i + ii + j*iknsupc] += lsum[i + il + j*iknsupc ] ;
-
-
-								// fmod[lk] = -1; /* Do not solve X[k] in the future. */
-								lk = LBj( ik, grid );/* Local block number, column-wise. */
-								lsub1 = Llu->Lrowind_bc_ptr[lk];
-								lusup1 = Llu->Lnzval_bc_ptr[lk];
-								nsupr1 = lsub1[1];
-
-								if(Llu->inv == 1){
-									Linv = Llu->Linv_bc_ptr[lk];
-#ifdef _CRAY
-									SGEMM( ftcs2, ftcs2, &iknsupc, &nrhs, &iknsupc,
-											&alpha, Linv, &iknsupc, &x[ii],
-											&iknsupc, &beta, rtemp_loc, &iknsupc );
-#elif defined (USE_VENDOR_BLAS)
-									dgemm_( "N", "N", &iknsupc, &nrhs, &iknsupc,
-											&alpha, Linv, &iknsupc, &x[ii],
-											&iknsupc, &beta, rtemp_loc, &iknsupc, 1, 1 );
-#else
-									dgemm_( "N", "N", &iknsupc, &nrhs, &iknsupc,
-											&alpha, Linv, &iknsupc, &x[ii],
-											&iknsupc, &beta, rtemp_loc, &iknsupc );
-#endif   
-									for (i=0 ; i<iknsupc*nrhs ; i++){
-										x[ii+i] = rtemp_loc[i];
-									}		
-								}else{
-#ifdef _CRAY
-									STRSM(ftcs1, ftcs1, ftcs2, ftcs3, &iknsupc, &nrhs, &alpha,
-											lusup1, &nsupr1, &x[ii], &iknsupc);
-#elif defined (USE_VENDOR_BLAS)
-									dtrsm_("L", "L", "N", "U", &iknsupc, &nrhs, &alpha, 
-											lusup1, &nsupr1, &x[ii], &iknsupc, 1, 1, 1, 1);		   
-#else
-									dtrsm_("L", "L", "N", "U", &iknsupc, &nrhs, &alpha, 
-											lusup1, &nsupr1, &x[ii], &iknsupc);
-#endif
-								}
-
-#if ( PROFlevel>=1 )
-								TOC(t2, t1);
-								stat[thread_id1]->utime[SOL_TRSM] += t2;
-
-#endif	
-								stat[thread_id1]->ops[SOLVE] += iknsupc * (iknsupc - 1) * nrhs;
-#if ( DEBUGlevel>=2 )
-								printf("(%2d) Solve X[%2d]\n", iam, ik);
-#endif
-
-								/*
-								 * Send Xk to process column Pc[k].
-								 */
-
-								if(LBtree_ptr[lk]!=NULL){
-#ifdef _OPENMP
-#pragma omp atomic capture
-#endif
-									nleaf_send_tmp = ++nleaf_send[0];
-									leaf_send[nleaf_send_tmp-1] = lk;
-									// BcTree_forwardMessageSimple(LBtree_ptr[lk],&x[ii - XK_H]);
-								}
-
-								/*
-								 * Perform local block modifications.
-								 */
-
-								// #ifdef _OPENMP
-								// #pragma	omp	task firstprivate (Llu,sizelsum,iknsupc,ii,ik,lsub1,x,rtemp,fmod,lsum,stat,nrhs,grid,xsup,recurlevel) private(lptr1,luptr1,nlb1,thread_id1) untied priority(1) 	
-								// #endif
-								{
-
-									nlb1 = lsub1[0] - 1;						
-									dlsum_fmod_inv(lsum, x, &x[ii], rtemp, nrhs, iknsupc, ik,
-											fmod, nlb1, xsup,
-											grid, Llu, stat, leaf_send, nleaf_send ,sizelsum,sizertemp,1+recurlevel);
-								}		   
-
-								// } /* if frecv[lk] == 0 */
-						} /* if iam == p */
-					} /* if fmod[lk] == 0 */				
-				}
-
-			}
-		}	
-
-		}else{ 
-
-#if ( PROFlevel>=1 )
-			TIC(t1);
-#endif	
-
-#ifdef _CRAY
-			SGEMM( ftcs2, ftcs2, &m, &nrhs, &knsupc,
-					&alpha, &lusup[luptr_tmp], &nsupr, xk,
-					&knsupc, &beta, rtemp_loc, &m );
-#elif defined (USE_VENDOR_BLAS)
-			dgemm_( "N", "N", &m, &nrhs, &knsupc,
-					&alpha, &lusup[luptr_tmp], &nsupr, xk,
-					&knsupc, &beta, rtemp_loc, &m, 1, 1 );
-#else
-			dgemm_( "N", "N", &m, &nrhs, &knsupc,
-					&alpha, &lusup[luptr_tmp], &nsupr, xk,
-					&knsupc, &beta, rtemp_loc, &m );
-#endif   	
-
-			// for (i = 0; i < m*nrhs; ++i) {
-				// lsum[idx_lsum[i]+sizelsum*thread_id] -=rtemp_loc[i];
-			// }
-			
-			nbrow=0;
-			for (lb = 0; lb < nlb; ++lb){ 		
-				lptr1_tmp = lloc[lb+idx_i];		
-				nbrow += lsub[lptr1_tmp+1];
-			}			
-			nbrow_ref=0;
-			for (lb = 0; lb < nlb; ++lb){ 		
-				lptr1_tmp = lloc[lb+idx_i];	
-				lptr= lptr1_tmp+2;	
-				nbrow1 = lsub[lptr1_tmp+1];
-				ik = lsub[lptr1_tmp]; /* Global block number, row-wise. */
-				rel = xsup[ik]; /* Global row index of block ik. */
-
-				lk = LBi( ik, grid ); /* Local block number, row-wise. */	
-
-				iknsupc = SuperSize( ik );
-				il = LSUM_BLK( lk );
-
-				RHS_ITERATE(j)					
-					for (i = 0; i < nbrow1; ++i) {
-						irow = lsub[lptr+i] - rel; /* Relative row. */
-
-						lsum[il+irow + j*iknsupc+sizelsum*thread_id] -= rtemp_loc[nbrow_ref+i + j*nbrow];
-					}
-				nbrow_ref+=nbrow1;
-			}			
-			
-
-
-			// TOC(t3, t1);
-
-#if ( PROFlevel>=1 )
-			TOC(t2, t1);
-			stat[thread_id]->utime[SOL_GEMM] += t2;
-
-#endif		
-
-			thread_id1 = omp_get_thread_num ();
-			rtemp_loc = &rtemp[sizertemp* thread_id1];
-			for (lb=0;lb<nlb;lb++){
-				lk = lloc[lb+idx_n];
-
-#ifdef _OPENMP
-#pragma omp atomic capture
-#endif
-				fmod_tmp=--fmod[lk];
-
-
-				if ( fmod_tmp==0 ) { /* Local accumulation done. */
-
-					lptr1_tmp = lloc[lb+idx_i];	
-
-					ik = lsub[lptr1_tmp]; /* Global block number, row-wise. */
-					lk = LBi( ik, grid ); /* Local block number, row-wise. */	
-
-					iknsupc = SuperSize( ik );
-					il = LSUM_BLK( lk );
-					ikcol = PCOL( ik, grid );
-					p = PNUM( myrow, ikcol, grid );
-					if ( iam != p ) {
-						for (ii=1;ii<num_thread;ii++)
-							// if(ii!=thread_id1)
-							for (jj=0;jj<iknsupc*nrhs;jj++)
-								lsum[il + jj] += lsum[il + jj + ii*sizelsum];
-
-#ifdef _OPENMP
-#pragma omp atomic capture
-#endif
-						nleaf_send_tmp = ++nleaf_send[0];
-						leaf_send[nleaf_send_tmp-1] = -lk-1;						
-
-						// RdTree_forwardMessageSimple(LRtree_ptr[lk],&lsum[il - LSUM_H ]);
-					} else { /* Diagonal process: X[i] += lsum[i]. */
-
-#if ( PROFlevel>=1 )
-						TIC(t1);
-#endif		
-						for (ii=1;ii<num_thread;ii++)
-							// if(ii!=thread_id1)
-							for (jj=0;jj<iknsupc*nrhs;jj++)
-								lsum[il+ jj] += lsum[il + jj + ii*sizelsum];
-
-						ii = X_BLK( lk );
-						// for (jj=0;jj<num_thread;jj++)
-						RHS_ITERATE(j)
-							for (i = 0; i < iknsupc; ++i)	
-								x[i + ii + j*iknsupc] += lsum[i + il + j*iknsupc] ;
-
-
-						// fmod[lk] = -1; /* Do not solve X[k] in the future. */
-						lk = LBj( ik, grid );/* Local block number, column-wise. */
-						lsub1 = Llu->Lrowind_bc_ptr[lk];
-						lusup1 = Llu->Lnzval_bc_ptr[lk];
-						nsupr1 = lsub1[1];
-
-						if(Llu->inv == 1){
-							Linv = Llu->Linv_bc_ptr[lk];
-#ifdef _CRAY
-							SGEMM( ftcs2, ftcs2, &iknsupc, &nrhs, &iknsupc,
-									&alpha, Linv, &iknsupc, &x[ii],
-									&iknsupc, &beta, rtemp_loc, &iknsupc );
-#elif defined (USE_VENDOR_BLAS)
-							dgemm_( "N", "N", &iknsupc, &nrhs, &iknsupc,
-									&alpha, Linv, &iknsupc, &x[ii],
-									&iknsupc, &beta, rtemp_loc, &iknsupc, 1, 1 );
-#else
-							dgemm_( "N", "N", &iknsupc, &nrhs, &iknsupc,
-									&alpha, Linv, &iknsupc, &x[ii],
-									&iknsupc, &beta, rtemp_loc, &iknsupc );
-#endif   
-							for (i=0 ; i<iknsupc*nrhs ; i++){
-								x[ii+i] = rtemp_loc[i];
-							}		
-						}else{
-#ifdef _CRAY
-							STRSM(ftcs1, ftcs1, ftcs2, ftcs3, &iknsupc, &nrhs, &alpha,
-									lusup1, &nsupr1, &x[ii], &iknsupc);
-#elif defined (USE_VENDOR_BLAS)
-							dtrsm_("L", "L", "N", "U", &iknsupc, &nrhs, &alpha, 
-									lusup1, &nsupr1, &x[ii], &iknsupc, 1, 1, 1, 1);		   
-#else
-							dtrsm_("L", "L", "N", "U", &iknsupc, &nrhs, &alpha, 
-									lusup1, &nsupr1, &x[ii], &iknsupc);
-#endif
-						}
-
-#if ( PROFlevel>=1 )
-						TOC(t2, t1);
-						stat[thread_id1]->utime[SOL_TRSM] += t2;
-
-#endif	
-
-
-						stat[thread_id1]->ops[SOLVE] += iknsupc * (iknsupc - 1) * nrhs;
-#if ( DEBUGlevel>=2 )
-						printf("(%2d) Solve X[%2d]\n", iam, ik);
-#endif
-
-						/*
-						 * Send Xk to process column Pc[k].
-						 */
-
-						if(LBtree_ptr[lk]!=NULL){
-
-#ifdef _OPENMP
-#pragma omp atomic capture
-#endif
-							nleaf_send_tmp = ++nleaf_send[0];
-							// printf("nleaf_send_tmp %5d lk %5d\n",nleaf_send_tmp);
-							leaf_send[nleaf_send_tmp-1] = lk;
-							// BcTree_forwardMessageSimple(LBtree_ptr[lk],&x[ii - XK_H]);
-						}
-
-						/*
-						 * Perform local block modifications.
-						 */
-
-						// #ifdef _OPENMP
-						// #pragma	omp	task firstprivate (Llu,sizelsum,iknsupc,ii,ik,lsub1,x,rtemp,fmod,lsum,send_req,stat,nrhs,grid,xsup,recurlevel) private(lptr1,luptr1,nlb1,thread_id1) untied priority(1) 	
-						// #endif
-
-						{
-							nlb1 = lsub1[0] - 1;
-							dlsum_fmod_inv(lsum, x, &x[ii], rtemp, nrhs, iknsupc, ik,
-									fmod, nlb1, xsup,
-									grid, Llu, stat, leaf_send, nleaf_send ,sizelsum,sizertemp,1+recurlevel);
-						}		   
-
-						// } /* if frecv[lk] == 0 */
-				} /* if iam == p */
-			} /* if fmod[lk] == 0 */				
-		}
-		// }
-
-}
-
-
-stat[thread_id]->ops[SOLVE] += 2 * m * nrhs * knsupc;	
-
-} /* if nlb>0*/
-} /* dLSUM_FMOD_inv */
-
-
-
-
-
-/************************************************************************/
-/*! \brief
- *
- * <pre>
- * Purpose
- * =======
- *   Perform local block modifications: lsum[i] -= L_i,k * X[k].
- * </pre>
- */
-void dlsum_fmod_inv_master
-/************************************************************************/
-(
- double *lsum,    /* Sum of local modifications.                        */
- double *x,       /* X array (local)                                    */
- double *xk,      /* X[k].                                              */
- double *rtemp,   /* Result of full matrix-vector multiply.             */
- int   nrhs,      /* Number of right-hand sides.                        */
- int   knsupc,    /* Size of supernode k.                               */
- int_t k,         /* The k-th component of X.                           */
- int_t *fmod,     /* Modification count for L-solve.                    */
- int_t nlb,       /* Number of L blocks.                                */
- int_t *xsup,
- gridinfo_t *grid,
- LocalLU_t *Llu,
- SuperLUStat_t **stat,
- int_t sizelsum,
- int_t sizertemp,
- int_t recurlevel
- )
-{
-	double alpha = 1.0, beta = 0.0,malpha=-1.0;
-	double *lusup, *lusup1;
-	double *dest;
-	double *Linv;/* Inverse of diagonal block */    
-	int    iam, iknsupc, myrow, krow, nbrow, nbrow1, nbrow_ref, nsupr, nsupr1, p, pi, idx_r;
-	int_t  i, ii,jj, ik, il, ikcol, irow, j, lb, lk, rel, lib,lready;
-	int_t  *lsub, *lsub1, nlb1, lptr1, luptr1,*lloc;
-	int_t  *ilsum = Llu->ilsum; /* Starting position of each supernode in lsum.   */
-	int_t  *frecv = Llu->frecv;
-	int_t  **fsendx_plist = Llu->fsendx_plist;
-	int_t  luptr_tmp,luptr_tmp1,lptr1_tmp,maxrecvsz, idx_i, idx_v,idx_n, idx_l, fmod_tmp, lbstart,lbend,nn,Nchunk,nlb_loc,remainder;
-	int thread_id,thread_id1,num_thread;
-	int m;
-	flops_t ops_loc=0.0;         
-	MPI_Status status;
-	int test_flag;
-	yes_no_t done;
-	BcTree  *LBtree_ptr = Llu->LBtree_ptr;
-	RdTree  *LRtree_ptr = Llu->LRtree_ptr;
-	int_t* idx_lsum,idx_lsum1;
-	double *rtemp_loc;
-	int_t ldalsum,maxsuper,aln_d;
-	int dword = sizeof (double);	
-	int_t lptr;      /* Starting position in lsub[*].                      */
-	int_t luptr;     /* Starting position in lusup[*].                     */
-
-	maxsuper = sp_ienv_dist(3);
-
-
-#ifdef _OPENMP
-	thread_id = omp_get_thread_num ();
-	num_thread = omp_get_num_threads ();
-#else
-	thread_id = 0;
-	num_thread = 1;
-#endif
-	ldalsum=Llu->ldalsum;
-
-	rtemp_loc = &rtemp[sizertemp* thread_id];
-
-
-	// #if ( PROFlevel>=1 )
-	double t1, t2, t3, t4;
-	float msg_vol = 0, msg_cnt = 0;
-	// #endif 
-
-
-	if(nlb>0){
-
-		maxrecvsz = sp_ienv_dist(3) * nrhs + SUPERLU_MAX( XK_H, LSUM_H );
-
-		iam = grid->iam;
-		myrow = MYROW( iam, grid );
-		lk = LBj( k, grid ); /* Local block number, column-wise. */
-
-		lsub = Llu->Lrowind_bc_ptr[lk];
-
-		lusup = Llu->Lnzval_bc_ptr[lk];
-		lloc = Llu->Lindval_loc_bc_ptr[lk];
-		// idx_lsum = Llu->Lrowind_bc_2_lsum[lk];
-
-		nsupr = lsub[1];
-
-
-		krow = PROW( k, grid );
-		if(myrow==krow){
-			idx_n = 1;
-			idx_i = nlb+2;
-			idx_v = 2*nlb+3;
-			luptr_tmp = lloc[idx_v];
-			m = nsupr-knsupc;
-		}else{
-			idx_n = 0;
-			idx_i = nlb;
-			idx_v = 2*nlb;
-			luptr_tmp = lloc[idx_v];
-			m = nsupr;
-		}
-
-		assert(m>0);
-		
-		if(m>4*maxsuper || nrhs>10){ 
-			// if(m<1){
-
-
-
-			// TIC(t1);
-			Nchunk=num_thread;
-			nlb_loc = floor(((double)nlb)/Nchunk);
-			remainder = nlb % Nchunk;
-
-
-
-#ifdef _OPENMP
-#pragma	omp	taskloop private (lptr1,luptr1,nlb1,thread_id1,lsub1,lusup1,nsupr1,Linv,nn,lbstart,lbend,luptr_tmp1,nbrow,lb,lptr1_tmp,rtemp_loc,nbrow_ref,lptr,nbrow1,ik,rel,lk,iknsupc,il,i,irow,fmod_tmp,ikcol,p,ii,jj,t1,t2,j) untied	
-#endif	
-			for (nn=0;nn<Nchunk;++nn){
-
-#ifdef _OPENMP				 
-				thread_id1 = omp_get_thread_num ();
-#else
-				thread_id1 = 0;
-#endif		
-
-
-
-				rtemp_loc = &rtemp[sizertemp* thread_id1];
-
-				if(nn<remainder){
-					lbstart = nn*(nlb_loc+1);
-					lbend = (nn+1)*(nlb_loc+1);
-				}else{
-					lbstart = remainder+nn*nlb_loc;
-					lbend = remainder + (nn+1)*nlb_loc;
-				}
-
-				if(lbstart<lbend){
-
-#if ( PROFlevel>=1 )
-					TIC(t1);
-#endif				
-
-					luptr_tmp1 = lloc[lbstart+idx_v];
-					nbrow=0;
-					for (lb = lbstart; lb < lbend; ++lb){ 		
-						lptr1_tmp = lloc[lb+idx_i];		
-						nbrow += lsub[lptr1_tmp+1];
-					}
-
-
-#ifdef _CRAY
-					SGEMM( ftcs2, ftcs2, &nbrow, &nrhs, &knsupc,
-							&alpha, &lusup[luptr_tmp1], &nsupr, xk,
-							&knsupc, &beta, rtemp_loc, &nbrow );
-#elif defined (USE_VENDOR_BLAS)
-					dgemm_( "N", "N", &nbrow, &nrhs, &knsupc,
-							&alpha, &lusup[luptr_tmp1], &nsupr, xk,
-							&knsupc, &beta, rtemp_loc, &nbrow, 1, 1 );
-#else
-					dgemm_( "N", "N", &nbrow, &nrhs, &knsupc,
-							&alpha, &lusup[luptr_tmp1], &nsupr, xk,
-							&knsupc, &beta, rtemp_loc, &nbrow );
-#endif  			
-
-
-
-					nbrow_ref=0;
-					for (lb = lbstart; lb < lbend; ++lb){ 		
-						lptr1_tmp = lloc[lb+idx_i];	
-						lptr= lptr1_tmp+2;	
-						nbrow1 = lsub[lptr1_tmp+1];
-						ik = lsub[lptr1_tmp]; /* Global block number, row-wise. */
-						rel = xsup[ik]; /* Global row index of block ik. */
-
-
-						lk = LBi( ik, grid ); /* Local block number, row-wise. */	
-
-						iknsupc = SuperSize( ik );
-						il = LSUM_BLK( lk );
-
-						RHS_ITERATE(j)					
-							for (i = 0; i < nbrow1; ++i) {
-								irow = lsub[lptr+i] - rel; /* Relative row. */
-
-								lsum[il+irow + j*iknsupc] -= rtemp_loc[nbrow_ref+i + j*nbrow];
-							}
-						nbrow_ref+=nbrow1;
-					}
-
-#if ( PROFlevel>=1 )
-					TOC(t2, t1);
-					stat[thread_id1]->utime[SOL_GEMM] += t2;
-#endif	
-				}
-			}	
-		}else{ 
-
-#if ( PROFlevel>=1 )
-			TIC(t1);
-#endif	
-
-#ifdef _CRAY
-			SGEMM( ftcs2, ftcs2, &m, &nrhs, &knsupc,
-					&alpha, &lusup[luptr_tmp], &nsupr, xk,
-					&knsupc, &beta, rtemp_loc, &m );
-#elif defined (USE_VENDOR_BLAS)
-			dgemm_( "N", "N", &m, &nrhs, &knsupc,
-					&alpha, &lusup[luptr_tmp], &nsupr, xk,
-					&knsupc, &beta, rtemp_loc, &m, 1, 1 );
-#else
-			dgemm_( "N", "N", &m, &nrhs, &knsupc,
-					&alpha, &lusup[luptr_tmp], &nsupr, xk,
-					&knsupc, &beta, rtemp_loc, &m );
-#endif   	
-
-			// for (i = 0; i < m*nrhs; ++i) {
-				// lsum[idx_lsum[i]] -=rtemp_loc[i];
-			// }
-			
-			nbrow=0;
-			for (lb = 0; lb < nlb; ++lb){ 		
-				lptr1_tmp = lloc[lb+idx_i];		
-				nbrow += lsub[lptr1_tmp+1];
-			}
-			nbrow_ref=0;
-			for (lb = 0; lb < nlb; ++lb){ 		
-				lptr1_tmp = lloc[lb+idx_i];	
-				lptr= lptr1_tmp+2;	
-				nbrow1 = lsub[lptr1_tmp+1];
-				ik = lsub[lptr1_tmp]; /* Global block number, row-wise. */
-				rel = xsup[ik]; /* Global row index of block ik. */
-
-				lk = LBi( ik, grid ); /* Local block number, row-wise. */	
-
-				iknsupc = SuperSize( ik );
-				il = LSUM_BLK( lk );
-
-				RHS_ITERATE(j)					
-					for (i = 0; i < nbrow1; ++i) {
-						irow = lsub[lptr+i] - rel; /* Relative row. */
-
-						lsum[il+irow + j*iknsupc+sizelsum*thread_id] -= rtemp_loc[nbrow_ref+i + j*nbrow];
-					}
-				nbrow_ref+=nbrow1;
-			}					
-			
-#if ( PROFlevel>=1 )
-			TOC(t2, t1);
-			stat[thread_id]->utime[SOL_GEMM] += t2;
-
-#endif	
-
-		}
-
-		// TOC(t3, t1);
-
-
-
-		thread_id1 = omp_get_thread_num ();
-
-
-
-
-		rtemp_loc = &rtemp[sizertemp* thread_id1];
-
-
-		for (lb=0;lb<nlb;lb++){
-			lk = lloc[lb+idx_n];
-
-			// #ifdef _OPENMP
-			// #pragma omp atomic capture
-			// #endif
-			fmod_tmp=--fmod[lk];
-
-
-			if ( fmod_tmp==0 ) { /* Local accumulation done. */
-				// --fmod[lk];
-
-
-				lptr1_tmp = lloc[lb+idx_i];	
-				// luptr_tmp = lloc[lb+idx_v];
-
-				ik = lsub[lptr1_tmp]; /* Global block number, row-wise. */
-				lk = LBi( ik, grid ); /* Local block number, row-wise. */	
-
-				iknsupc = SuperSize( ik );
-				il = LSUM_BLK( lk );
-
-				// nbrow = lsub[lptr1_tmp+1];
-
-				ikcol = PCOL( ik, grid );
-				p = PNUM( myrow, ikcol, grid );
-				if ( iam != p ) {
-					// if(frecv[lk]==0){
-					// fmod[lk] = -1;
-
-					for (ii=1;ii<num_thread;ii++)
-						// if(ii!=thread_id1)
-						for (jj=0;jj<iknsupc*nrhs;jj++)
-							lsum[il + jj] += lsum[il + jj + ii*sizelsum];
-
-
-					RdTree_forwardMessageSimple(LRtree_ptr[lk],&lsum[il - LSUM_H ]);
-					// }
-
-
-				} else { /* Diagonal process: X[i] += lsum[i]. */
-
-
-
-					// if ( frecv[lk]==0 ) { /* Becomes a leaf node. */
-
-#if ( PROFlevel>=1 )
-					TIC(t1);
-#endif		
-					for (ii=1;ii<num_thread;ii++)
-						// if(ii!=thread_id1)
-						for (jj=0;jj<iknsupc*nrhs;jj++)
-							lsum[il+ jj] += lsum[il + jj + ii*sizelsum];
-
-					ii = X_BLK( lk );
-					// for (jj=0;jj<num_thread;jj++)
-					RHS_ITERATE(j)
-						for (i = 0; i < iknsupc; ++i)	
-							x[i + ii + j*iknsupc] += lsum[i + il + j*iknsupc] ;
-
-
-					// fmod[lk] = -1; /* Do not solve X[k] in the future. */
-					lk = LBj( ik, grid );/* Local block number, column-wise. */
-					lsub1 = Llu->Lrowind_bc_ptr[lk];
-					lusup1 = Llu->Lnzval_bc_ptr[lk];
-					nsupr1 = lsub1[1];
-
-
-
-
-					if(Llu->inv == 1){
-						Linv = Llu->Linv_bc_ptr[lk];
-#ifdef _CRAY
-						SGEMM( ftcs2, ftcs2, &iknsupc, &nrhs, &iknsupc,
-								&alpha, Linv, &iknsupc, &x[ii],
-								&iknsupc, &beta, rtemp_loc, &iknsupc );
-#elif defined (USE_VENDOR_BLAS)
-						dgemm_( "N", "N", &iknsupc, &nrhs, &iknsupc,
-								&alpha, Linv, &iknsupc, &x[ii],
-								&iknsupc, &beta, rtemp_loc, &iknsupc, 1, 1 );
-#else
-						dgemm_( "N", "N", &iknsupc, &nrhs, &iknsupc,
-								&alpha, Linv, &iknsupc, &x[ii],
-								&iknsupc, &beta, rtemp_loc, &iknsupc );
-#endif   
-						for (i=0 ; i<iknsupc*nrhs ; i++){
-							x[ii+i] = rtemp_loc[i];
-						}		
-					}else{
-#ifdef _CRAY
-						STRSM(ftcs1, ftcs1, ftcs2, ftcs3, &iknsupc, &nrhs, &alpha,
-								lusup1, &nsupr1, &x[ii], &iknsupc);
-#elif defined (USE_VENDOR_BLAS)
-						dtrsm_("L", "L", "N", "U", &iknsupc, &nrhs, &alpha, 
-								lusup1, &nsupr1, &x[ii], &iknsupc, 1, 1, 1, 1);		   
-#else
-						dtrsm_("L", "L", "N", "U", &iknsupc, &nrhs, &alpha, 
-								lusup1, &nsupr1, &x[ii], &iknsupc);
-#endif
-					}
-
-#if ( PROFlevel>=1 )
-					TOC(t2, t1);
-					stat[thread_id1]->utime[SOL_TRSM] += t2;
-
-#endif	
-
-
-					stat[thread_id1]->ops[SOLVE] += iknsupc * (iknsupc - 1) * nrhs;
-#if ( DEBUGlevel>=2 )
-					printf("(%2d) Solve X[%2d]\n", iam, ik);
-#endif
-
-					/*
-					 * Send Xk to process column Pc[k].
-					 */
-
-					if(LBtree_ptr[lk]!=NULL)
-						BcTree_forwardMessageSimple(LBtree_ptr[lk],&x[ii - XK_H]);
-
-					/*
-					 * Perform local block modifications.
-					 */
-
-					// #ifdef _OPENMP
-					// #pragma	omp	task firstprivate (Llu,sizelsum,iknsupc,ii,ik,lsub1,x,rtemp,fmod,lsum,send_req,stat,nrhs,grid,xsup,recurlevel) private(lptr1,luptr1,nlb1,thread_id1) untied priority(1) 	
-					// #endif
-					{
-						nlb1 = lsub1[0] - 1;
-
-
-						dlsum_fmod_inv_master(lsum, x, &x[ii], rtemp, nrhs, iknsupc, ik,
-								fmod, nlb1, xsup,
-								grid, Llu, stat,sizelsum,sizertemp,1+recurlevel);
-					}		   
-
-					// } /* if frecv[lk] == 0 */
-				} /* if iam == p */
-			} /* if fmod[lk] == 0 */				
-		}
-		// }
-
-
-		stat[thread_id]->ops[SOLVE] += 2 * m * nrhs * knsupc;	
-
-	} /* if nlb>0*/
-} /* dlsum_fmod_inv_master */
-
-
-
-
-/************************************************************************/
-void dlsum_bmod_inv
-/************************************************************************/
-(
- double *lsum,        /* Sum of local modifications.                    */
- double *x,           /* X array (local).                               */
- double *xk,          /* X[k].                                          */
- double *rtemp,   /* Result of full matrix-vector multiply.             */
- int    nrhs,	      /* Number of right-hand sides.                    */
- int_t  k,            /* The k-th component of X.                       */
- int_t  *bmod,        /* Modification count for L-solve.                */
- int_t  *Urbs,        /* Number of row blocks in each block column of U.*/
- int_t  *Urbs2,
- Ucb_indptr_t **Ucb_indptr,/* Vertical linked list pointing to Uindex[].*/
- int_t  **Ucb_valptr, /* Vertical linked list pointing to Unzval[].     */
- int_t  *xsup,
- gridinfo_t *grid,
- LocalLU_t *Llu,
- MPI_Request send_req[], /* input/output */
- SuperLUStat_t **stat,
- int_t* root_send, 
- int_t* nroot_send, 
- int_t sizelsum,
- int_t sizertemp
- )
-{
-	/*
-	 * Purpose
-	 * =======
-	 *   Perform local block modifications: lsum[i] -= U_i,k * X[k].
-	 */
-	double alpha = 1.0, beta = 0.0;
-	int    iam, iknsupc, knsupc, myrow, nsupr, p, pi;
-	int_t  fnz, gik, gikcol, i, ii, ik, ikfrow, iklrow, il, irow,
-	       j, jj, lk, lk1, nub, ub, uptr;
-	int_t  *usub;
-	double *uval, *dest, *y;
-	int_t  *lsub;
-	double *lusup;
-	int_t  *ilsum = Llu->ilsum; /* Starting position of each supernode in lsum.   */
-	int_t  *brecv = Llu->brecv;
-	int_t  **bsendx_plist = Llu->bsendx_plist;
-	BcTree  *UBtree_ptr = Llu->UBtree_ptr;
-	RdTree  *URtree_ptr = Llu->URtree_ptr;	
-	MPI_Status status;
-	int test_flag;
-	int_t bmod_tmp;
-	int thread_id,thread_id1,num_thread;
-	double *rtemp_loc;
-	int_t nroot_send_tmp;	
-	double *Uinv;/* Inverse of diagonal block */    
-
-	double t1, t2;
-	float msg_vol = 0, msg_cnt = 0;
-	int_t Nchunk, nub_loc,remainder,nn,lbstart,lbend;  
-	
-#ifdef _OPENMP
-	thread_id = omp_get_thread_num ();
-	num_thread = omp_get_num_threads ();
-#else
-	thread_id = 0;
-	num_thread = 1;
-#endif	
-	rtemp_loc = &rtemp[sizertemp* thread_id];
-	
-	
-	iam = grid->iam;
-	myrow = MYROW( iam, grid );
-	knsupc = SuperSize( k );
-	lk = LBj( k, grid ); /* Local block number, column-wise. */
-	nub = Urbs[lk];      /* Number of U blocks in block column lk */	
-
-	
-	 
-	// printf("Urbs2[lk] %5d lk %5d nub %5d\n",Urbs2[lk],lk,nub);
-	// fflush(stdout);
-	
-	if(nub>num_thread){
-	// // // // if(Urbs2[lk]>num_thread){
-	// if(Urbs2[lk]>0){
-		Nchunk=num_thread;
-		nub_loc = floor(((double)nub)/Nchunk);
-		remainder = nub % Nchunk;
-
-#ifdef _OPENMP
-#pragma	omp	taskloop firstprivate (send_req,stat) private (thread_id1,Uinv,nn,lbstart,lbend,ub,rtemp_loc,ik,lk1,gik,gikcol,usub,uval,lsub,lusup,iknsupc,il,i,irow,bmod_tmp,p,ii,jj,t1,t2,j,ikfrow,iklrow,dest,y,uptr,fnz,nsupr) untied nogroup	
-#endif	
-		for (nn=0;nn<Nchunk;++nn){
-
-#ifdef _OPENMP				 
-			thread_id1 = omp_get_thread_num ();
-#else
-			thread_id1 = 0;
-#endif		
-			rtemp_loc = &rtemp[sizertemp* thread_id1];
-
-			if(nn<remainder){
-				lbstart = nn*(nub_loc+1);
-				lbend = (nn+1)*(nub_loc+1);
-			}else{
-				lbstart = remainder+nn*nub_loc;
-				lbend = remainder + (nn+1)*nub_loc;
-			}			
-			for (ub = lbstart; ub < lbend; ++ub){
-				ik = Ucb_indptr[lk][ub].lbnum; /* Local block number, row-wise. */
-				usub = Llu->Ufstnz_br_ptr[ik];
-				uval = Llu->Unzval_br_ptr[ik];
-				i = Ucb_indptr[lk][ub].indpos; /* Start of the block in usub[]. */
-				i += UB_DESCRIPTOR;
-				il = LSUM_BLK( ik );
-				gik = ik * grid->nprow + myrow;/* Global block number, row-wise. */
-				iknsupc = SuperSize( gik );
-				ikfrow = FstBlockC( gik );
-				iklrow = FstBlockC( gik+1 );
-
-#if ( PROFlevel>=1 )
-				TIC(t1);
-#endif					
-				
-				RHS_ITERATE(j) {
-					dest = &lsum[il + j*iknsupc+sizelsum*thread_id1];
-					y = &xk[j*knsupc];
-					uptr = Ucb_valptr[lk][ub]; /* Start of the block in uval[]. */
-					for (jj = 0; jj < knsupc; ++jj) {
-						fnz = usub[i + jj];
-						if ( fnz < iklrow ) { /* Nonzero segment. */
-							/* AXPY */
-							for (irow = fnz; irow < iklrow; ++irow)
-								dest[irow - ikfrow] -= uval[uptr++] * y[jj];
-							stat[thread_id1]->ops[SOLVE] += 2 * (iklrow - fnz);
-						}
-					} /* for jj ... */
-				}
-				
-#if ( PROFlevel>=1 )
-				TOC(t2, t1);
-				stat[thread_id1]->utime[SOL_GEMM] += t2;
-#endif					
-				
-
-		#ifdef _OPENMP
-		#pragma omp atomic capture
-		#endif		
-				bmod_tmp=--bmod[ik];
-				
-				if ( bmod_tmp == 0 ) { /* Local accumulation done. */
-					gikcol = PCOL( gik, grid );
-					p = PNUM( myrow, gikcol, grid );
-					if ( iam != p ) {
-						for (ii=1;ii<num_thread;ii++)
-							// if(ii!=thread_id1)
-							for (jj=0;jj<iknsupc*nrhs;jj++)
-								lsum[il + jj] += lsum[il + jj + ii*sizelsum];			
-						
-#ifdef _OPENMP
-#pragma omp atomic capture
-#endif
-						nroot_send_tmp = ++nroot_send[0];
-						root_send[nroot_send_tmp-1] = -ik-1;							
-						// RdTree_forwardMessageSimple(URtree_ptr[ik],&lsum[il - LSUM_H ]);
-
-		#if ( DEBUGlevel>=2 )
-						printf("(%2d) Sent LSUM[%2.0f], size %2d, to P %2d\n",
-								iam, lsum[il-LSUM_H], iknsupc*nrhs+LSUM_H, p);
-		#endif
-					} else { /* Diagonal process: X[i] += lsum[i]. */
-						
-#if ( PROFlevel>=1 )
-						TIC(t1);
-#endif								
-						
-						for (ii=1;ii<num_thread;ii++)
-							// if(ii!=thread_id1)
-							for (jj=0;jj<iknsupc*nrhs;jj++)
-								lsum[il + jj] += lsum[il + jj + ii*sizelsum];					
-
-						ii = X_BLK( ik );
-						dest = &x[ii];
-								
-						RHS_ITERATE(j)
-							for (i = 0; i < iknsupc; ++i)
-								dest[i + j*iknsupc] += lsum[i + il + j*iknsupc];
-						// if ( !brecv[ik] ) { /* Becomes a leaf node. */
-							// bmod[ik] = -1; /* Do not solve X[k] in the future. */
-							lk1 = LBj( gik, grid ); /* Local block number. */
-							lsub = Llu->Lrowind_bc_ptr[lk1];
-							lusup = Llu->Lnzval_bc_ptr[lk1];
-							nsupr = lsub[1];
-
-							if(Llu->inv == 1){
-								Uinv = Llu->Uinv_bc_ptr[lk1];  
-		#ifdef _CRAY
-								SGEMM( ftcs2, ftcs2, &iknsupc, &nrhs, &iknsupc,
-										&alpha, Uinv, &iknsupc, &x[ii],
-										&iknsupc, &beta, rtemp_loc, &iknsupc );
-		#elif defined (USE_VENDOR_BLAS)
-								dgemm_( "N", "N", &iknsupc, &nrhs, &iknsupc,
-										&alpha, Uinv, &iknsupc, &x[ii],
-										&iknsupc, &beta, rtemp_loc, &iknsupc, 1, 1 );
-		#else
-								dgemm_( "N", "N", &iknsupc, &nrhs, &iknsupc,
-										&alpha, Uinv, &iknsupc, &x[ii],
-										&iknsupc, &beta, rtemp_loc, &iknsupc );
-		#endif	   
-								for (i=0 ; i<iknsupc*nrhs ; i++){
-									x[ii+i] = rtemp_loc[i];
-								}		
-							}else{
-		#ifdef _CRAY
-								STRSM(ftcs1, ftcs3, ftcs2, ftcs2, &iknsupc, &nrhs, &alpha,
-										lusup, &nsupr, &x[ii], &iknsupc);
-		#elif defined (USE_VENDOR_BLAS)
-								dtrsm_("L", "U", "N", "N", &iknsupc, &nrhs, &alpha, 
-										lusup, &nsupr, &x[ii], &iknsupc, 1, 1, 1, 1);	
-		#else
-								dtrsm_("L", "U", "N", "N", &iknsupc, &nrhs, &alpha, 
-										lusup, &nsupr, &x[ii], &iknsupc);
-		#endif
-							}
-					
-		#if ( PROFlevel>=1 )
-							TOC(t2, t1);
-							stat[thread_id1]->utime[SOL_TRSM] += t2;
-		#endif	
-							stat[thread_id1]->ops[SOLVE] += iknsupc * (iknsupc + 1) * nrhs;					
-							
-		#if ( DEBUGlevel>=2 )
-							printf("(%2d) Solve X[%2d]\n", iam, gik);
-		#endif
-
-							/*
-							 * Send Xk to process column Pc[k].
-							 */
-
-							 // for (i=0 ; i<iknsupc*nrhs ; i++){
-								// printf("xre: %f\n",x[ii+i]);
-								// fflush(stdout);
-							// }
-							if(UBtree_ptr[lk1]!=NULL){							
-#ifdef _OPENMP
-#pragma omp atomic capture
-#endif
-							nroot_send_tmp = ++nroot_send[0];
-							root_send[nroot_send_tmp-1] = lk1;							
-							// BcTree_forwardMessageSimple(UBtree_ptr[lk1],&x[ii - XK_H]); 
-							} 
-
-							/*
-							 * Perform local block modifications.
-							 */
-							if ( Urbs[lk1] ){
-								// #ifdef _OPENMP
-								// #pragma	omp	task firstprivate (Ucb_indptr,Ucb_valptr,Llu,sizelsum,ii,gik,x,rtemp,bmod,Urbs,Urbs2,lsum,stat,nrhs,grid,xsup) untied 
-								// #endif
-								{
-								dlsum_bmod_inv(lsum, x, &x[ii], rtemp, nrhs, gik, bmod, Urbs,Urbs2,
-										Ucb_indptr, Ucb_valptr, xsup, grid, Llu,
-										send_req, stat, root_send, nroot_send, sizelsum,sizertemp);
-								}
-							}
-						// } /* if brecv[ik] == 0 */
-					}
-				} /* if bmod[ik] == 0 */				
-			}				
-		}
-
-	}else{ 
-	
-		thread_id1 = omp_get_thread_num ();
-		rtemp_loc = &rtemp[sizertemp* thread_id1];
-
-		for (ub = 0; ub < nub; ++ub) {
-			ik = Ucb_indptr[lk][ub].lbnum; /* Local block number, row-wise. */
-			usub = Llu->Ufstnz_br_ptr[ik];
-			uval = Llu->Unzval_br_ptr[ik];
-			i = Ucb_indptr[lk][ub].indpos; /* Start of the block in usub[]. */
-			i += UB_DESCRIPTOR;
-			il = LSUM_BLK( ik );
-			gik = ik * grid->nprow + myrow;/* Global block number, row-wise. */
-			iknsupc = SuperSize( gik );
-			ikfrow = FstBlockC( gik );
-			iklrow = FstBlockC( gik+1 );
-
-#if ( PROFlevel>=1 )
-		TIC(t1);
-#endif					
-			RHS_ITERATE(j) {
-				dest = &lsum[il + j*iknsupc+sizelsum*thread_id1];
-				y = &xk[j*knsupc];
-				uptr = Ucb_valptr[lk][ub]; /* Start of the block in uval[]. */
-				for (jj = 0; jj < knsupc; ++jj) {
-					fnz = usub[i + jj];
-					if ( fnz < iklrow ) { /* Nonzero segment. */
-						/* AXPY */
-						for (irow = fnz; irow < iklrow; ++irow)
-							dest[irow - ikfrow] -= uval[uptr++] * y[jj];
-						stat[thread_id1]->ops[SOLVE] += 2 * (iklrow - fnz);
-					}
-				} /* for jj ... */
-			}
-
-#if ( PROFlevel>=1 )
-		TOC(t2, t1);
-		stat[thread_id1]->utime[SOL_GEMM] += t2;
-#endif				
-			
-	#ifdef _OPENMP
-	#pragma omp atomic capture
-	#endif		
-			bmod_tmp=--bmod[ik];
-
-			if ( bmod_tmp == 0 ) { /* Local accumulation done. */
-				gikcol = PCOL( gik, grid );
-				p = PNUM( myrow, gikcol, grid );
-				if ( iam != p ) {
-					for (ii=1;ii<num_thread;ii++)
-						// if(ii!=thread_id1)
-						for (jj=0;jj<iknsupc*nrhs;jj++)
-							lsum[il + jj] += lsum[il + jj + ii*sizelsum];			
-					
-#ifdef _OPENMP
-#pragma omp atomic capture
-#endif
-					nroot_send_tmp = ++nroot_send[0];
-					root_send[nroot_send_tmp-1] = -ik-1;					
-					// RdTree_forwardMessageSimple(URtree_ptr[ik],&lsum[il - LSUM_H ]);
-
-	#if ( DEBUGlevel>=2 )
-					printf("(%2d) Sent LSUM[%2.0f], size %2d, to P %2d\n",
-							iam, lsum[il-LSUM_H], iknsupc*nrhs+LSUM_H, p);
-	#endif
-				} else { /* Diagonal process: X[i] += lsum[i]. */
-					
-#if ( PROFlevel>=1 )
-					TIC(t1);
-#endif							
-					
-					for (ii=1;ii<num_thread;ii++)
-						// if(ii!=thread_id1)
-						for (jj=0;jj<iknsupc*nrhs;jj++)
-							lsum[il + jj] += lsum[il + jj + ii*sizelsum];					
-
-					ii = X_BLK( ik );
-					dest = &x[ii];
-							
-					RHS_ITERATE(j)
-						for (i = 0; i < iknsupc; ++i)
-							dest[i + j*iknsupc] += lsum[i + il + j*iknsupc];
-					// if ( !brecv[ik] ) { /* Becomes a leaf node. */
-						// bmod[ik] = -1; /* Do not solve X[k] in the future. */
-						lk1 = LBj( gik, grid ); /* Local block number. */
-						lsub = Llu->Lrowind_bc_ptr[lk1];
-						lusup = Llu->Lnzval_bc_ptr[lk1];
-						nsupr = lsub[1];
-
-						if(Llu->inv == 1){
-							Uinv = Llu->Uinv_bc_ptr[lk1];  
-	#ifdef _CRAY
-							SGEMM( ftcs2, ftcs2, &iknsupc, &nrhs, &iknsupc,
-									&alpha, Uinv, &iknsupc, &x[ii],
-									&iknsupc, &beta, rtemp_loc, &iknsupc );
-	#elif defined (USE_VENDOR_BLAS)
-							dgemm_( "N", "N", &iknsupc, &nrhs, &iknsupc,
-									&alpha, Uinv, &iknsupc, &x[ii],
-									&iknsupc, &beta, rtemp_loc, &iknsupc, 1, 1 );
-	#else
-							dgemm_( "N", "N", &iknsupc, &nrhs, &iknsupc,
-									&alpha, Uinv, &iknsupc, &x[ii],
-									&iknsupc, &beta, rtemp_loc, &iknsupc );
-	#endif	   
-							for (i=0 ; i<iknsupc*nrhs ; i++){
-								x[ii+i] = rtemp_loc[i];
-							}		
-						}else{
-	#ifdef _CRAY
-							STRSM(ftcs1, ftcs3, ftcs2, ftcs2, &iknsupc, &nrhs, &alpha,
-									lusup, &nsupr, &x[ii], &iknsupc);
-	#elif defined (USE_VENDOR_BLAS)
-							dtrsm_("L", "U", "N", "N", &iknsupc, &nrhs, &alpha, 
-									lusup, &nsupr, &x[ii], &iknsupc, 1, 1, 1, 1);	
-	#else
-							dtrsm_("L", "U", "N", "N", &iknsupc, &nrhs, &alpha, 
-									lusup, &nsupr, &x[ii], &iknsupc);
-	#endif
-						}
-				
-	#if ( PROFlevel>=1 )
-						TOC(t2, t1);
-						stat[thread_id1]->utime[SOL_TRSM] += t2;
-	#endif	
-						stat[thread_id1]->ops[SOLVE] += iknsupc * (iknsupc + 1) * nrhs;					
-						
-	#if ( DEBUGlevel>=2 )
-						printf("(%2d) Solve X[%2d]\n", iam, gik);
-	#endif
-
-						/*
-						 * Send Xk to process column Pc[k].
-						 */
-
-						 // for (i=0 ; i<iknsupc*nrhs ; i++){
-							// printf("xre: %f\n",x[ii+i]);
-							// fflush(stdout);
-						// }
-						if(UBtree_ptr[lk1]!=NULL){
-#ifdef _OPENMP
-#pragma omp atomic capture
-#endif
-						nroot_send_tmp = ++nroot_send[0];
-						root_send[nroot_send_tmp-1] = lk1;							
-						// BcTree_forwardMessageSimple(UBtree_ptr[lk1],&x[ii - XK_H]); 
-						} 
-
-						/*
-						 * Perform local block modifications.
-						 */
-						if ( Urbs[lk1] )
-						
-							if(Urbs[lk1]>num_thread){
-							#ifdef _OPENMP
-							#pragma	omp	task firstprivate (Ucb_indptr,Ucb_valptr,Llu,sizelsum,ii,gik,x,rtemp,bmod,Urbs,Urbs2,lsum,stat,nrhs,grid,xsup) untied 
-							#endif						
-								dlsum_bmod_inv(lsum, x, &x[ii], rtemp, nrhs, gik, bmod, Urbs,Urbs2,
-										Ucb_indptr, Ucb_valptr, xsup, grid, Llu,
-										send_req, stat, root_send, nroot_send, sizelsum,sizertemp);
-							}else{
-								dlsum_bmod_inv(lsum, x, &x[ii], rtemp, nrhs, gik, bmod, Urbs,Urbs2,
-										Ucb_indptr, Ucb_valptr, xsup, grid, Llu,
-										send_req, stat, root_send, nroot_send, sizelsum,sizertemp);							
-							}		
-									
-					// } /* if brecv[ik] == 0 */
-				}
-			} /* if bmod[ik] == 0 */
-
-		} /* for ub ... */
-	}
-
-} /* dlSUM_BMOD_inv */
-
-
-
-
-
-
-
-/************************************************************************/
-void dlsum_bmod_inv_master
-/************************************************************************/
-(
- double *lsum,        /* Sum of local modifications.                    */
- double *x,           /* X array (local).                               */
- double *xk,          /* X[k].                                          */
- double *rtemp,   /* Result of full matrix-vector multiply.             */
- int    nrhs,	      /* Number of right-hand sides.                    */
- int_t  k,            /* The k-th component of X.                       */
- int_t  *bmod,        /* Modification count for L-solve.                */
- int_t  *Urbs,        /* Number of row blocks in each block column of U.*/
- int_t  *Urbs2,
- Ucb_indptr_t **Ucb_indptr,/* Vertical linked list pointing to Uindex[].*/
- int_t  **Ucb_valptr, /* Vertical linked list pointing to Unzval[].     */
- int_t  *xsup,
- gridinfo_t *grid,
- LocalLU_t *Llu,
- MPI_Request send_req[], /* input/output */
- SuperLUStat_t **stat,
- int_t sizelsum,
- int_t sizertemp
- )
-{
-	/*
-	 * Purpose
-	 * =======
-	 *   Perform local block modifications: lsum[i] -= U_i,k * X[k].
-	 */
-	double alpha = 1.0, beta = 0.0;
-	int    iam, iknsupc, knsupc, myrow, nsupr, p, pi;
-	int_t  fnz, gik, gikcol, i, ii, ik, ikfrow, iklrow, il, irow,
-	       j, jj, lk, lk1, nub, ub, uptr;
-	int_t  *usub;
-	double *uval, *dest, *y;
-	int_t  *lsub;
-	double *lusup;
-	int_t  *ilsum = Llu->ilsum; /* Starting position of each supernode in lsum.   */
-	int_t  *brecv = Llu->brecv;
-	int_t  **bsendx_plist = Llu->bsendx_plist;
-	BcTree  *UBtree_ptr = Llu->UBtree_ptr;
-	RdTree  *URtree_ptr = Llu->URtree_ptr;	
-	MPI_Status status;
-	int test_flag;
-	int_t bmod_tmp;
-	int thread_id,thread_id1,num_thread;
-	double *rtemp_loc;
-		
-	double *Uinv;/* Inverse of diagonal block */    
-
-	double t1, t2;
-	float msg_vol = 0, msg_cnt = 0;
-	int_t Nchunk, nub_loc,remainder,nn,lbstart,lbend;  
-	
-#ifdef _OPENMP
-	thread_id = omp_get_thread_num ();
-	num_thread = omp_get_num_threads ();
-#else
-	thread_id = 0;
-	num_thread = 1;
-#endif	
-	rtemp_loc = &rtemp[sizertemp* thread_id];
-	
-	
-	iam = grid->iam;
-	myrow = MYROW( iam, grid );
-	knsupc = SuperSize( k );
-	lk = LBj( k, grid ); /* Local block number, column-wise. */
-	nub = Urbs[lk];      /* Number of U blocks in block column lk */	
-
-	
-	 
-	// printf("Urbs2[lk] %5d lk %5d nub %5d\n",Urbs2[lk],lk,nub);
-	// fflush(stdout);
-	
-	if(nub>num_thread){
-	// if(nub>0){
-		Nchunk=num_thread;
-		nub_loc = floor(((double)nub)/Nchunk);
-		remainder = nub % Nchunk;
-
-#ifdef _OPENMP
-#pragma	omp	taskloop firstprivate (send_req,stat) private (thread_id1,nn,lbstart,lbend,ub,rtemp_loc,ik,gik,usub,uval,iknsupc,il,i,irow,jj,t1,t2,j,ikfrow,iklrow,dest,y,uptr,fnz) untied	
-#endif	
-		for (nn=0;nn<Nchunk;++nn){
-
-#ifdef _OPENMP				 
-			thread_id1 = omp_get_thread_num ();
-#else
-			thread_id1 = 0;
-#endif		
-			rtemp_loc = &rtemp[sizertemp* thread_id1];
-
-#if ( PROFlevel>=1 )
-			TIC(t1);
-#endif				
-			
-			if(nn<remainder){
-				lbstart = nn*(nub_loc+1);
-				lbend = (nn+1)*(nub_loc+1);
-			}else{
-				lbstart = remainder+nn*nub_loc;
-				lbend = remainder + (nn+1)*nub_loc;
-			}			
-			for (ub = lbstart; ub < lbend; ++ub){
-				ik = Ucb_indptr[lk][ub].lbnum; /* Local block number, row-wise. */
-				usub = Llu->Ufstnz_br_ptr[ik];
-				uval = Llu->Unzval_br_ptr[ik];
-				i = Ucb_indptr[lk][ub].indpos; /* Start of the block in usub[]. */
-				i += UB_DESCRIPTOR;
-				il = LSUM_BLK( ik );
-				gik = ik * grid->nprow + myrow;/* Global block number, row-wise. */
-				iknsupc = SuperSize( gik );
-				ikfrow = FstBlockC( gik );
-				iklrow = FstBlockC( gik+1 );				
-				
-				RHS_ITERATE(j) {
-					dest = &lsum[il + j*iknsupc+sizelsum*thread_id1];
-					y = &xk[j*knsupc];
-					uptr = Ucb_valptr[lk][ub]; /* Start of the block in uval[]. */
-					for (jj = 0; jj < knsupc; ++jj) {
-						fnz = usub[i + jj];
-						if ( fnz < iklrow ) { /* Nonzero segment. */
-							/* AXPY */
-							for (irow = fnz; irow < iklrow; ++irow)
-								dest[irow - ikfrow] -= uval[uptr++] * y[jj];
-							stat[thread_id1]->ops[SOLVE] += 2 * (iklrow - fnz);
-						}
-					} /* for jj ... */
-				}
-			}
-#if ( PROFlevel>=1 )
-			TOC(t2, t1);
-			stat[thread_id1]->utime[SOL_GEMM] += t2;
-#endif	
-		}
-				
-	}else{
-#ifdef _OPENMP				 
-		thread_id1 = omp_get_thread_num ();
-#else
-		thread_id1 = 0;
-#endif	
-		rtemp_loc = &rtemp[sizertemp* thread_id1];
-#if ( PROFlevel>=1 )
-		TIC(t1);
-#endif	
-		for (ub = 0; ub < nub; ++ub) {
-			ik = Ucb_indptr[lk][ub].lbnum; /* Local block number, row-wise. */
-			usub = Llu->Ufstnz_br_ptr[ik];
-			uval = Llu->Unzval_br_ptr[ik];
-			i = Ucb_indptr[lk][ub].indpos; /* Start of the block in usub[]. */
-			i += UB_DESCRIPTOR;
-			il = LSUM_BLK( ik );
-			gik = ik * grid->nprow + myrow;/* Global block number, row-wise. */
-			iknsupc = SuperSize( gik );
-			ikfrow = FstBlockC( gik );
-			iklrow = FstBlockC( gik+1 );
-				
-			RHS_ITERATE(j) {
-				dest = &lsum[il + j*iknsupc+sizelsum*thread_id1];
-				y = &xk[j*knsupc];
-				uptr = Ucb_valptr[lk][ub]; /* Start of the block in uval[]. */
-				for (jj = 0; jj < knsupc; ++jj) {
-					fnz = usub[i + jj];
-					if ( fnz < iklrow ) { /* Nonzero segment. */
-						/* AXPY */
-						for (irow = fnz; irow < iklrow; ++irow)
-							dest[irow - ikfrow] -= uval[uptr++] * y[jj];
-						stat[thread_id1]->ops[SOLVE] += 2 * (iklrow - fnz);
-					}
-				} /* for jj ... */
-			}			
-		}	
-#if ( PROFlevel>=1 )
-		TOC(t2, t1);
-		stat[thread_id1]->utime[SOL_GEMM] += t2;
-#endif				
-	}
-
-	
-	
-#ifdef _OPENMP				 
-	thread_id1 = omp_get_thread_num ();
-#else
-	thread_id1 = 0;
-#endif	
-	rtemp_loc = &rtemp[sizertemp* thread_id1];	
-	for (ub = 0; ub < nub; ++ub){
-		ik = Ucb_indptr[lk][ub].lbnum; /* Local block number, row-wise. */
-		il = LSUM_BLK( ik );
-		gik = ik * grid->nprow + myrow;/* Global block number, row-wise. */
-		iknsupc = SuperSize( gik );
-
-	// #ifdef _OPENMP
-	// #pragma omp atomic capture
-	// #endif		
-		bmod_tmp=--bmod[ik];
-		
-		if ( bmod_tmp == 0 ) { /* Local accumulation done. */
-			gikcol = PCOL( gik, grid );
-			p = PNUM( myrow, gikcol, grid );
-			if ( iam != p ) {
-				for (ii=1;ii<num_thread;ii++)
-					// if(ii!=thread_id1)
-					for (jj=0;jj<iknsupc*nrhs;jj++)
-						lsum[il + jj] += lsum[il + jj + ii*sizelsum];			
-				RdTree_forwardMessageSimple(URtree_ptr[ik],&lsum[il - LSUM_H ]);
-
-#if ( DEBUGlevel>=2 )
-				printf("(%2d) Sent LSUM[%2.0f], size %2d, to P %2d\n",
-						iam, lsum[il-LSUM_H], iknsupc*nrhs+LSUM_H, p);
-#endif
-			} else { /* Diagonal process: X[i] += lsum[i]. */
-				
-#if ( PROFlevel>=1 )
-				TIC(t1);
-#endif								
-				for (ii=1;ii<num_thread;ii++)
-					// if(ii!=thread_id1)
-					for (jj=0;jj<iknsupc*nrhs;jj++)
-						lsum[il + jj] += lsum[il + jj + ii*sizelsum];					
-
-				ii = X_BLK( ik );
-				dest = &x[ii];
-						
-				RHS_ITERATE(j)
-					for (i = 0; i < iknsupc; ++i)
-						dest[i + j*iknsupc] += lsum[i + il + j*iknsupc];
-				// if ( !brecv[ik] ) { /* Becomes a leaf node. */
-					// bmod[ik] = -1; /* Do not solve X[k] in the future. */
-					lk1 = LBj( gik, grid ); /* Local block number. */
-					lsub = Llu->Lrowind_bc_ptr[lk1];
-					lusup = Llu->Lnzval_bc_ptr[lk1];
-					nsupr = lsub[1];
-
-					if(Llu->inv == 1){
-						Uinv = Llu->Uinv_bc_ptr[lk1];  
-#ifdef _CRAY
-						SGEMM( ftcs2, ftcs2, &iknsupc, &nrhs, &iknsupc,
-								&alpha, Uinv, &iknsupc, &x[ii],
-								&iknsupc, &beta, rtemp_loc, &iknsupc );
-#elif defined (USE_VENDOR_BLAS)
-						dgemm_( "N", "N", &iknsupc, &nrhs, &iknsupc,
-								&alpha, Uinv, &iknsupc, &x[ii],
-								&iknsupc, &beta, rtemp_loc, &iknsupc, 1, 1 );
-#else
-						dgemm_( "N", "N", &iknsupc, &nrhs, &iknsupc,
-								&alpha, Uinv, &iknsupc, &x[ii],
-								&iknsupc, &beta, rtemp_loc, &iknsupc );
-#endif	   
-						for (i=0 ; i<iknsupc*nrhs ; i++){
-							x[ii+i] = rtemp_loc[i];
-						}		
-					}else{
-#ifdef _CRAY
-						STRSM(ftcs1, ftcs3, ftcs2, ftcs2, &iknsupc, &nrhs, &alpha,
-								lusup, &nsupr, &x[ii], &iknsupc);
-#elif defined (USE_VENDOR_BLAS)
-						dtrsm_("L", "U", "N", "N", &iknsupc, &nrhs, &alpha, 
-								lusup, &nsupr, &x[ii], &iknsupc, 1, 1, 1, 1);	
-#else
-						dtrsm_("L", "U", "N", "N", &iknsupc, &nrhs, &alpha, 
-								lusup, &nsupr, &x[ii], &iknsupc);
-#endif
-					}
-			
-#if ( PROFlevel>=1 )
-					TOC(t2, t1);
-					stat[thread_id1]->utime[SOL_TRSM] += t2;
-#endif	
-					stat[thread_id1]->ops[SOLVE] += iknsupc * (iknsupc + 1) * nrhs;					
-					
-#if ( DEBUGlevel>=2 )
-					printf("(%2d) Solve X[%2d]\n", iam, gik);
-#endif
-
-					/*
-					 * Send Xk to process column Pc[k].
-					 */
-
-					 // for (i=0 ; i<iknsupc*nrhs ; i++){
-						// printf("xre: %f\n",x[ii+i]);
-						// fflush(stdout);
-					// }
-					if(UBtree_ptr[lk1]!=NULL){
-					BcTree_forwardMessageSimple(UBtree_ptr[lk1],&x[ii - XK_H]); 
-					} 
-
-					/*
-					 * Perform local block modifications.
-					 */
-					if ( Urbs[lk1] ){
-						// #ifdef _OPENMP
-						// #pragma	omp	task firstprivate (Ucb_indptr,Ucb_valptr,Llu,sizelsum,ii,gik,x,rtemp,bmod,Urbs,Urbs2,lsum,stat,nrhs,grid,xsup) untied 
-						// #endif
-						{
-						dlsum_bmod_inv_master(lsum, x, &x[ii], rtemp, nrhs, gik, bmod, Urbs,Urbs2,
-								Ucb_indptr, Ucb_valptr, xsup, grid, Llu,
-								send_req, stat, sizelsum,sizertemp);
-						}
-					}
-				// } /* if brecv[ik] == 0 */
-			}
-		} /* if bmod[ik] == 0 */		
-	}	
-	
-} /* dlsum_bmod_inv_master */
-
-
-
-
diff --git a/SRC/pdsymbfact_distdata.c b/SRC/pdsymbfact_distdata.c
index a66189b..c301c81 100644
--- a/SRC/pdsymbfact_distdata.c
+++ b/SRC/pdsymbfact_distdata.c
@@ -32,10 +32,6 @@ at the top-level directory.
 #include "superlu_ddefs.h"
 #include "psymbfact.h"
 
-#ifndef CACHELINE
-#define CACHELINE 64  /* bytes, Xeon Phi KNL, Cori haswell, Edision */
-#endif
-
 /*! \brief
  *
  * <pre>
@@ -1191,21 +1187,19 @@ float
 ddist_psymbtonum(fact_t fact, int_t n, SuperMatrix *A,
 		ScalePermstruct_t *ScalePermstruct,
 		Pslu_freeable_t *Pslu_freeable, 
-		LUstruct_t *LUstruct, gridinfo_t *grid, int_t nrhs)
+		LUstruct_t *LUstruct, gridinfo_t *grid)
 {
   Glu_persist_t *Glu_persist = LUstruct->Glu_persist;
   Glu_freeable_t Glu_freeable_n;
   LocalLU_t *Llu = LUstruct->Llu;
-  int_t bnnz, fsupc, i, irow, istart, j, jb,ib, jj, k, k1,  
+  int_t bnnz, fsupc, i, irow, istart, j, jb, jj, k, 
     len, len1, nsupc, nsupc_gb, ii, nprocs;
-  int_t lib;  /* local block row number */
-  int_t nlb;  /* local block rows*/  
   int_t ljb;  /* local block column number */
   int_t nrbl; /* number of L blocks in current block column */
   int_t nrbu; /* number of U blocks in current block column */
   int_t gb;   /* global block number; 0 < gb <= nsuper */
   int_t lb;   /* local block number; 0 < lb <= ceil(NSUPERS/Pr) */
-  int iam, jbrow, jbcol, jcol, kcol, krow, mycol, myrow, pc, pr, ljb_i, ljb_j, p;
+  int iam, jbrow, jbcol, jcol, kcol, mycol, myrow, pc, pr, ljb_i, ljb_j, p;
   int_t mybufmax[NBUFFERS];
   NRformat_loc *Astore;
   double *a;
@@ -1213,7 +1207,7 @@ ddist_psymbtonum(fact_t fact, int_t n, SuperMatrix *A,
   int_t *ainf_colptr, *ainf_rowind, *asup_rowptr, *asup_colind;
   double *asup_val, *ainf_val;
   int_t *xsup, *supno;    /* supernode and column mapping */
-  int_t *lsub, *xlsub, *usub, *usub1, *xusub;
+  int_t *lsub, *xlsub, *usub, *xusub;
   int_t nsupers, nsupers_i, nsupers_j, nsupers_ij;
   int_t next_ind;      /* next available position in index[*] */
   int_t next_val;      /* next available position in nzval[*] */
@@ -1223,25 +1217,10 @@ ddist_psymbtonum(fact_t fact, int_t n, SuperMatrix *A,
   int_t *recvBuf;
   int *ptrToRecv, *nnzToRecv, *ptrToSend, *nnzToSend;
   double **Lnzval_bc_ptr;  /* size ceil(NSUPERS/Pc) */
-  double **Linv_bc_ptr;  /* size ceil(NSUPERS/Pc) */
-  double **Uinv_bc_ptr;  /* size ceil(NSUPERS/Pc) */
   int_t  **Lrowind_bc_ptr; /* size ceil(NSUPERS/Pc) */
-  int_t   **Lindval_loc_bc_ptr; /* size ceil(NSUPERS/Pc)                 */	 
-  int_t *index_srt;         /* indices consist of headers and row subscripts */	
-  double *lusup_srt; /* nonzero values in L and U */  
   double **Unzval_br_ptr;  /* size ceil(NSUPERS/Pr) */
   int_t  **Ufstnz_br_ptr;  /* size ceil(NSUPERS/Pr) */
-
-  BcTree  *LBtree_ptr;       /* size ceil(NSUPERS/Pc)                */
-  RdTree  *LRtree_ptr;		  /* size ceil(NSUPERS/Pr)                */
-  BcTree  *UBtree_ptr;       /* size ceil(NSUPERS/Pc)                */
-  RdTree  *URtree_ptr;		  /* size ceil(NSUPERS/Pr)                */	
-  int msgsize;
-
-  int_t  *Urbs,*Urbs1; /* Number of row blocks in each block column of U. */
-  Ucb_indptr_t **Ucb_indptr;/* Vertical linked list pointing to Uindex[] */
-  int_t  **Ucb_valptr;      /* Vertical linked list pointing to Unzval[] */  
-
+  
   /*-- Counts to be used in factorization. --*/
   int  *ToRecv, *ToSendD, **ToSendR;
   
@@ -1268,8 +1247,10 @@ ddist_psymbtonum(fact_t fact, int_t n, SuperMatrix *A,
   int_t *LUb_number; /* global block number; size nsupers_ij */
   int_t *LUb_valptr; /* pointers to U nzval[]; size ceil(NSUPERS/Pc)      */
   int_t *Lrb_marker;  /* block hit marker; size ceil(NSUPERS/Pr)           */
-
-
+  double *dense, *dense_col; /* SPA */
+  double zero = 0.0;
+  int_t ldaspa;     /* LDA of SPA */
+  int_t iword, dword;
   float memStrLU, memA,
         memDist = 0.; /* memory used for redistributing the data, which does
 		         not include the memory for the numerical values
@@ -1277,35 +1258,10 @@ ddist_psymbtonum(fact_t fact, int_t n, SuperMatrix *A,
   float  memNLU = 0.; /* memory allocated for storing the numerical values of 
 		         L and U, that will be used in the numeric
                          factorization (positive number) */
-  int_t *ActiveFlag;
-  int_t *ActiveFlagAll;
-  int_t Iactive;
-  int *ranks;
-  int_t *idxs;
-  int_t **nzrows;
-  double rseed;
-  int rank_cnt,rank_cnt_ref,Root;
-  double *dense, *dense_col; /* SPA */
-  double zero = 0.0;
-  int_t ldaspa;     /* LDA of SPA */
-  int_t iword, dword;
-  float mem_use = 0.0;
-  int_t *mod_bit;
-  int_t *frecv, *brecv, *lloc; 
-  double *SeedSTD_BC,*SeedSTD_RD;				 
-  int_t idx_indx,idx_lusup;
-  int_t nbrow;
-  int_t  ik, il, lk, rel, knsupc, idx_r;
-  int_t  lptr1_tmp, idx_i, idx_v,m, uu, aln_i;	
-  int_t	nub;
-	
+
 #if ( PRNTlevel>=1 )
   int_t nLblocks = 0, nUblocks = 0;
 #endif
-#if ( PROFlevel>=1 ) 
-	double t, t_u, t_l;
-	int_t u_blks;
-#endif
   
   /* Initialization. */
   iam = grid->iam;
@@ -1321,8 +1277,6 @@ ddist_psymbtonum(fact_t fact, int_t n, SuperMatrix *A,
   iword = sizeof(int_t);
   dword = sizeof(double);
 
-  aln_i = ceil(CACHELINE/(double)iword); 
-  
   if (fact == SamePattern_SameRowPerm) {
     ABORT ("ERROR: call of dist_psymbtonum with fact equals SamePattern_SameRowPerm.");  
   }
@@ -1487,33 +1441,13 @@ ddist_psymbtonum(fact_t fact, int_t n, SuperMatrix *A,
     fprintf(stderr, "Malloc fails for Lnzval_bc_ptr[].");
     return (memDist + memNLU);
   }
-  if ( !(Linv_bc_ptr = 
-	 (double**)SUPERLU_MALLOC(nsupers_j * sizeof(double*))) ) {
-    fprintf(stderr, "Malloc fails for Linv_bc_ptr[].");
-    return (memDist + memNLU);
-  }  
-  if ( !(Uinv_bc_ptr = 
-	 (double**)SUPERLU_MALLOC(nsupers_j * sizeof(double*))) ) {
-    fprintf(stderr, "Malloc fails for Uinv_bc_ptr[].");
-    return (memDist + memNLU);
-  }  
-  
   if ( !(Lrowind_bc_ptr = (int_t**)SUPERLU_MALLOC(nsupers_j * sizeof(int_t*))) ) {
     fprintf(stderr, "Malloc fails for Lrowind_bc_ptr[].");
     return (memDist + memNLU);
   }
-  
-  if ( !(Lindval_loc_bc_ptr = (int_t**)SUPERLU_MALLOC(nsupers_j * sizeof(int_t*))) ){
-    fprintf(stderr, "Malloc fails for Lindval_loc_bc_ptr[].");
-    return (memDist + memNLU);
-  }
-  
-  memNLU += nsupers_j * sizeof(double*) + nsupers_j * sizeof(int_t*)+ nsupers_j * sizeof(int_t*);
+  memNLU += nsupers_j * sizeof(double*) + nsupers_j * sizeof(int_t*);
   Lnzval_bc_ptr[nsupers_j-1] = NULL;
-  Linv_bc_ptr[nsupers_j-1] = NULL;
-  Uinv_bc_ptr[nsupers_j-1] = NULL;
   Lrowind_bc_ptr[nsupers_j-1] = NULL;
-  Lindval_loc_bc_ptr[nsupers_j-1] = NULL;  
   
   /* These lists of processes will be used for triangular solves. */
   if ( !(fsendx_plist = (int_t **) SUPERLU_MALLOC(nsupers_j*sizeof(int_t*))) ) {
@@ -1801,24 +1735,7 @@ ddist_psymbtonum(fact_t fact, int_t n, SuperMatrix *A,
 	  fprintf(stderr, "Malloc fails for Lnzval_bc_ptr[*][] col block " IFMT, jb);
 	  return (memDist + memNLU);
 	}
-
-	if (!(Linv_bc_ptr[ljb_j] = 
-	      doubleCalloc_dist(nsupc*nsupc))) {
-	  fprintf(stderr, "Malloc fails for Linv_bc_ptr[*][] col block " IFMT, jb);
-	  return (memDist + memNLU);
-	}
-	if (!(Uinv_bc_ptr[ljb_j] = 
-	      doubleCalloc_dist(nsupc*nsupc))) {
-	  fprintf(stderr, "Malloc fails for Uinv_bc_ptr[*][] col block " IFMT, jb);
-	  return (memDist + memNLU);
-	}
-
 	memNLU += len1*iword + len*nsupc*dword;
-
-	if ( !(Lindval_loc_bc_ptr[ljb_j] = intCalloc_dist(((nrbl*3 + (aln_i - 1)) / aln_i) * aln_i)) ) 
-		ABORT("Malloc fails for Lindval_loc_bc_ptr[ljb_j][]");
-
-
 	
 	lusup = Lnzval_bc_ptr[ljb_j];
 	mybufmax[0] = SUPERLU_MAX( mybufmax[0], len1 );
@@ -1832,11 +1749,6 @@ ddist_psymbtonum(fact_t fact, int_t n, SuperMatrix *A,
 	  gb = LUb_number[k];
 	  lb = LBi( gb, grid );
 	  len = LUb_length[lb];
-	  
-	  Lindval_loc_bc_ptr[ljb_j][k] = lb;
-	  Lindval_loc_bc_ptr[ljb_j][k+nrbl] = next_ind;
-	  Lindval_loc_bc_ptr[ljb_j][k+nrbl*2] = next_val;			  
-	  
 	  LUb_length[lb] = 0;
 	  index[next_ind++] = gb; /* Descriptor */
 	  index[next_ind++] = len; 
@@ -1865,65 +1777,9 @@ ddist_psymbtonum(fact_t fact, int_t n, SuperMatrix *A,
 	      }
 	    }
 	  } /* for i ... */
-	  
-
-		/* sort Lindval_loc_bc_ptr[ljb_j], Lrowind_bc_ptr[ljb_j] and Lnzval_bc_ptr[ljb_j] here*/
-		if(nrbl>1){
-			krow = PROW( jb, grid );
-			if(myrow==krow){ /* skip the diagonal block */
-				uu=nrbl-2;
-				lloc = &Lindval_loc_bc_ptr[ljb_j][1];
-			}else{
-				uu=nrbl-1;	
-				lloc = Lindval_loc_bc_ptr[ljb_j];
-			}	
-			quickSortM(lloc,0,uu,nrbl,0,3);	
-		}
-
-
-		if ( !(index_srt = intMalloc_dist(len1)) ) 
-			ABORT("Malloc fails for index_srt[]");				
-		if (!(lusup_srt = doubleMalloc_dist(len*nsupc))) 
-			ABORT("Malloc fails for lusup_srt[]");
-
-		idx_indx = BC_HEADER;
-		idx_lusup = 0;
-		for (jj=0;jj<BC_HEADER;jj++)
-			index_srt[jj] = index[jj];
-
-		for(i=0;i<nrbl;i++){
-			nbrow = index[Lindval_loc_bc_ptr[ljb_j][i+nrbl]+1];
-			for (jj=0;jj<LB_DESCRIPTOR+nbrow;jj++){
-				index_srt[idx_indx++] = index[Lindval_loc_bc_ptr[ljb_j][i+nrbl]+jj];
-			}
-
-			Lindval_loc_bc_ptr[ljb_j][i+nrbl] = idx_indx - LB_DESCRIPTOR - nbrow; 
-
-			for (jj=0;jj<nbrow;jj++){
-				k=idx_lusup;
-				k1=Lindval_loc_bc_ptr[ljb_j][i+nrbl*2]+jj;
-				for (j = 0; j < nsupc; ++j) {				
-					lusup_srt[k] = lusup[k1];
-					k += len;
-					k1 += len;
-				}	
-				idx_lusup++;
-			}				
-			Lindval_loc_bc_ptr[ljb_j][i+nrbl*2] = idx_lusup - nbrow;	
-		}
-
-		SUPERLU_FREE(lusup);
-		SUPERLU_FREE(index);
-
-		Lrowind_bc_ptr[ljb_j] = index_srt;
-		Lnzval_bc_ptr[ljb_j] = lusup_srt; 			
-	  
 	} else {
 	  Lrowind_bc_ptr[ljb_j] = NULL;
 	  Lnzval_bc_ptr[ljb_j] = NULL;
-	  Linv_bc_ptr[ljb_j] = NULL;
-	  Uinv_bc_ptr[ljb_j] = NULL;
-	  Lindval_loc_bc_ptr[ljb_j] = NULL;
 	} /* if nrbl ... */		  
       } /* if mycol == pc */
   } /* for jb ... */
@@ -1936,7 +1792,14 @@ ddist_psymbtonum(fact_t fact, int_t n, SuperMatrix *A,
   SUPERLU_FREE(LUb_valptr);
   SUPERLU_FREE(Lrb_marker);
   SUPERLU_FREE(dense);
-
+  
+  /* Free the memory used for storing L and U */
+  SUPERLU_FREE(xlsub); SUPERLU_FREE(xusub);
+  if (lsub != NULL)
+    SUPERLU_FREE(lsub);  
+  if (usub != NULL)
+    SUPERLU_FREE(usub);
+  
   /* Free the memory used for storing A */
   SUPERLU_FREE(ainf_colptr);
   if (ainf_rowind != NULL) {
@@ -2061,699 +1924,6 @@ ddist_psymbtonum(fact_t fact, int_t n, SuperMatrix *A,
     }
   }
   
-
-
-
-			/////////////////////////////////////////////////////////////////
-			
-			/* Set up additional pointers for the index and value arrays of U.
-			   nub is the number of local block columns. */
-			nub = CEILING( nsupers, grid->npcol); /* Number of local block columns. */
-			if ( !(Urbs = (int_t *) intCalloc_dist(2*nub)) )
-				ABORT("Malloc fails for Urbs[]"); /* Record number of nonzero
-									 blocks in a block column. */
-			Urbs1 = Urbs + nub;
-			if ( !(Ucb_indptr = SUPERLU_MALLOC(nub * sizeof(Ucb_indptr_t *))) )
-				ABORT("Malloc fails for Ucb_indptr[]");
-			if ( !(Ucb_valptr = SUPERLU_MALLOC(nub * sizeof(int_t *))) )
-				ABORT("Malloc fails for Ucb_valptr[]");
-			nlb = CEILING( nsupers, grid->nprow ); /* Number of local block rows. */
-
-			/* Count number of row blocks in a block column. 
-			   One pass of the skeleton graph of U. */
-			for (lk = 0; lk < nlb; ++lk) {
-				usub1 = Ufstnz_br_ptr[lk];
-				if ( usub1 ) { /* Not an empty block row. */
-					/* usub1[0] -- number of column blocks in this block row. */
-					i = BR_HEADER; /* Pointer in index array. */
-					for (lb = 0; lb < usub1[0]; ++lb) { /* For all column blocks. */
-						k = usub1[i];            /* Global block number */
-						++Urbs[LBj(k,grid)];
-						i += UB_DESCRIPTOR + SuperSize( k );
-					}
-				}
-			}
-
-			/* Set up the vertical linked lists for the row blocks.
-			   One pass of the skeleton graph of U. */
-			for (lb = 0; lb < nub; ++lb) {
-				if ( Urbs[lb] ) { /* Not an empty block column. */
-					if ( !(Ucb_indptr[lb]
-								= SUPERLU_MALLOC(Urbs[lb] * sizeof(Ucb_indptr_t))) )
-						ABORT("Malloc fails for Ucb_indptr[lb][]");
-					if ( !(Ucb_valptr[lb] = (int_t *) intMalloc_dist(Urbs[lb])) )
-						ABORT("Malloc fails for Ucb_valptr[lb][]");
-				}
-			}
-			for (lk = 0; lk < nlb; ++lk) { /* For each block row. */
-				usub1 = Ufstnz_br_ptr[lk];
-				if ( usub1 ) { /* Not an empty block row. */
-					i = BR_HEADER; /* Pointer in index array. */
-					j = 0;         /* Pointer in nzval array. */
-
-					for (lb = 0; lb < usub1[0]; ++lb) { /* For all column blocks. */
-						k = usub1[i];          /* Global block number, column-wise. */
-						ljb = LBj( k, grid ); /* Local block number, column-wise. */
-						Ucb_indptr[ljb][Urbs1[ljb]].lbnum = lk;
-
-						Ucb_indptr[ljb][Urbs1[ljb]].indpos = i;
-						Ucb_valptr[ljb][Urbs1[ljb]] = j;
-						
-						++Urbs1[ljb];
-						j += usub1[i+1];
-						i += UB_DESCRIPTOR + SuperSize( k );
-					}
-				}
-			}			
-			
-			
-			
-			
-			/////////////////////////////////////////////////////////////////
-
-			// if(LSUM<nsupers)ABORT("Need increase LSUM."); /* temporary*/
-
-#if ( PROFlevel>=1 )
-				t = SuperLU_timer_();
-#endif				
-			/* construct the Bcast tree for L ... */
-
-			k = CEILING( nsupers, grid->npcol );/* Number of local block columns */
-			if ( !(LBtree_ptr = (BcTree*)SUPERLU_MALLOC(k * sizeof(BcTree))) )
-				ABORT("Malloc fails for LBtree_ptr[].");
-			if ( !(ActiveFlag = intCalloc_dist(grid->nprow*2)) )
-				ABORT("Calloc fails for ActiveFlag[].");	
-			if ( !(ranks = (int*)SUPERLU_MALLOC(grid->nprow * sizeof(int))) )
-				ABORT("Malloc fails for ranks[].");	
-			if ( !(SeedSTD_BC = (double*)SUPERLU_MALLOC(k * sizeof(double))) )
-				ABORT("Malloc fails for SeedSTD_BC[].");	
-
-			for (i=0;i<k;i++){
-				SeedSTD_BC[i]=rand();		
-			}
-
-			MPI_Allreduce(MPI_IN_PLACE,&SeedSTD_BC[0],k,MPI_DOUBLE,MPI_MAX,grid->cscp.comm);					  
-
-			for (ljb = 0; ljb <k ; ++ljb) {
-				LBtree_ptr[ljb]=NULL;
-			}			
-			
-
-			if ( !(ActiveFlagAll = intMalloc_dist(grid->nprow*k)) )
-				ABORT("Calloc fails for ActiveFlag[].");				
-			for (j=0;j<grid->nprow*k;++j)ActiveFlagAll[j]=3*nsupers;	
-			for (ljb = 0; ljb < k; ++ljb) { /* for each local block column ... */
-				jb = mycol+ljb*grid->npcol;  /* not sure */
-				if(jb<nsupers){
-				pc = PCOL( jb, grid );
-				
-				istart = xlsub[ljb];
-				for (i = istart; i < xlsub[ljb+1]; ++i) {
-					irow = lsub[i];
-					gb = BlockNum( irow );
-					pr = PROW( gb, grid );
-					ActiveFlagAll[pr+ljb*grid->nprow]=MIN(ActiveFlagAll[pr+ljb*grid->nprow],gb);
-				} /* for j ... */
-				}
-			}			
-
-			
-			MPI_Allreduce(MPI_IN_PLACE,ActiveFlagAll,grid->nprow*k,mpi_int_t,MPI_MIN,grid->cscp.comm);					  
-			
-			
-			
-			for (ljb = 0; ljb < k; ++ljb) { /* for each local block column ... */
-				
-				jb = mycol+ljb*grid->npcol;  /* not sure */
-				if(jb<nsupers){
-				pc = PCOL( jb, grid );
-
-				for (j=0;j<grid->nprow;++j)ActiveFlag[j]=ActiveFlagAll[j+ljb*grid->nprow];
-				for (j=0;j<grid->nprow;++j)ActiveFlag[j+grid->nprow]=j;
-				for (j=0;j<grid->nprow;++j)ranks[j]=-1;
-
-				Root=-1; 
-				Iactive = 0;				
-				for (j=0;j<grid->nprow;++j){
-					if(ActiveFlag[j]!=3*nsupers){
-					gb = ActiveFlag[j];
-					pr = PROW( gb, grid );
-					if(gb==jb)Root=pr;
-					if(myrow==pr)Iactive=1;		
-					}					
-				}
-				
-
-				quickSortM(ActiveFlag,0,grid->nprow-1,grid->nprow,0,2);	
-
-				if(Iactive==1){
-					// printf("jb %5d damn\n",jb);
-					// fflush(stdout);
-					assert( Root>-1 );
-					rank_cnt = 1;
-					ranks[0]=Root;
-					for (j = 0; j < grid->nprow; ++j){
-						if(ActiveFlag[j]!=3*nsupers && ActiveFlag[j+grid->nprow]!=Root){
-							ranks[rank_cnt]=ActiveFlag[j+grid->nprow];
-							++rank_cnt;
-						}
-					}		
-
-					if(rank_cnt>1){
-
-						for (ii=0;ii<rank_cnt;ii++)   // use global ranks rather than local ranks
-							ranks[ii] = PNUM( ranks[ii], pc, grid );
-
-						// rseed=rand();
-						// rseed=1.0;
-						msgsize = SuperSize( jb )*nrhs+XK_H;
-						LBtree_ptr[ljb] = BcTree_Create(grid->comm, ranks, rank_cnt, msgsize,SeedSTD_BC[ljb]);  	
-						BcTree_SetTag(LBtree_ptr[ljb],BC_L);
-
-						// printf("iam %5d btree rank_cnt %5d \n",iam,rank_cnt);
-						// fflush(stdout);
-
-						// if(iam==15 || iam==3){
-						// printf("iam %5d btree lk %5d tag %5d root %5d\n",iam, ljb,jb,BcTree_IsRoot(LBtree_ptr[ljb]));
-						// fflush(stdout);
-						// }
-
-						// #if ( PRNTlevel>=1 )		
-						if(Root==myrow){
-							rank_cnt_ref=1;
-							for (j = 0; j < grid->nprow; ++j) {
-								if ( fsendx_plist[ljb][j] != EMPTY ) {	
-									++rank_cnt_ref;		
-								}
-							}
-							assert(rank_cnt==rank_cnt_ref);		
-
-							// printf("Partial Bcast Procs: col%7d np%4d\n",jb,rank_cnt);
-
-							// // printf("Partial Bcast Procs: %4d %4d: ",iam, rank_cnt);
-							// // for(j=0;j<rank_cnt;++j)printf("%4d",ranks[j]);
-							// // printf("\n");
-						}
-						// #endif
-					}	
-				}
-				}
-			}
-
-			
-			SUPERLU_FREE(ActiveFlag);
-			SUPERLU_FREE(ActiveFlagAll);
-			SUPERLU_FREE(ranks);
-			SUPERLU_FREE(SeedSTD_BC);
-			
-			
-#if ( PROFlevel>=1 )
-		t = SuperLU_timer_() - t;
-		if ( !iam) printf(".. Construct Bcast tree for L: %.2f\t\n", t);
-#endif			
-	
-
-#if ( PROFlevel>=1 )
-				t = SuperLU_timer_();
-#endif			
-			/* construct the Reduce tree for L ... */
-			/* the following is used as reference */
-			nlb = CEILING( nsupers, grid->nprow );/* Number of local block rows */
-			if ( !(mod_bit = intMalloc_dist(nlb)) )
-				ABORT("Malloc fails for mod_bit[].");
-			if ( !(frecv = intMalloc_dist(nlb)) )
-				ABORT("Malloc fails for frecv[].");
-
-			for (k = 0; k < nlb; ++k) mod_bit[k] = 0;
-			for (k = 0; k < nsupers; ++k) {
-				pr = PROW( k, grid );
-				if ( myrow == pr ) {
-					lib = LBi( k, grid );    /* local block number */
-					kcol = PCOL( k, grid );
-					if (mycol == kcol || fmod[lib] )
-						mod_bit[lib] = 1;  /* contribution from off-diagonal and diagonal*/
-				}
-			}
-			/* Every process receives the count, but it is only useful on the
-			   diagonal processes.  */
-			MPI_Allreduce( mod_bit, frecv, nlb, mpi_int_t, MPI_SUM, grid->rscp.comm);
-
-
-
-			k = CEILING( nsupers, grid->nprow );/* Number of local block rows */
-			if ( !(LRtree_ptr = (RdTree*)SUPERLU_MALLOC(k * sizeof(RdTree))) )
-				ABORT("Malloc fails for LRtree_ptr[].");
-			if ( !(ActiveFlag = intCalloc_dist(grid->npcol*2)) )
-				ABORT("Calloc fails for ActiveFlag[].");	
-			if ( !(ranks = (int*)SUPERLU_MALLOC(grid->npcol * sizeof(int))) )
-				ABORT("Malloc fails for ranks[].");	
-
-			// if ( !(idxs = intCalloc_dist(nsupers)) )
-				// ABORT("Calloc fails for idxs[].");	
-
-			// if ( !(nzrows = (int_t**)SUPERLU_MALLOC(nsupers * sizeof(int_t*))) )
-				// ABORT("Malloc fails for nzrows[].");
-
-			if ( !(SeedSTD_RD = (double*)SUPERLU_MALLOC(k * sizeof(double))) )
-				ABORT("Malloc fails for SeedSTD_RD[].");	
-
-			for (i=0;i<k;i++){
-				SeedSTD_RD[i]=rand();		
-			}
-
-			MPI_Allreduce(MPI_IN_PLACE,&SeedSTD_RD[0],k,MPI_DOUBLE,MPI_MAX,grid->rscp.comm);					  
-
-
-			for (lib = 0; lib <k ; ++lib) {
-				LRtree_ptr[lib]=NULL;
-			}
-
-			
-			if ( !(ActiveFlagAll = intMalloc_dist(grid->npcol*k)) )
-				ABORT("Calloc fails for ActiveFlagAll[].");				
-			for (j=0;j<grid->npcol*k;++j)ActiveFlagAll[j]=-3*nsupers;	
-				
-				
-				
-			for (ljb = 0; ljb < CEILING( nsupers, grid->npcol); ++ljb) { /* for each local block column ... */
-				jb = mycol+ljb*grid->npcol;  /* not sure */
-				if(jb<nsupers){
-					pc = PCOL( jb, grid );
-					for(i=xlsub[ljb];i<xlsub[ljb+1];++i){
-						irow = lsub[i];
-						ib = BlockNum( irow );
-						pr = PROW( ib, grid );
-						if ( myrow == pr ) { /* Block row ib in my process row */
-							lib = LBi( ib, grid ); /* Local block number */
-							ActiveFlagAll[pc+lib*grid->npcol]=MAX(ActiveFlagAll[pc+lib*grid->npcol],jb);
-						}
-					}
-				}
-			}
-
-			MPI_Allreduce(MPI_IN_PLACE,ActiveFlagAll,grid->npcol*k,mpi_int_t,MPI_MAX,grid->rscp.comm);
-			
-			for (lib=0;lib<k;++lib){
-				ib = myrow+lib*grid->nprow;  /* not sure */
-				if(ib<nsupers){
-					pr = PROW( ib, grid );
-					for (j=0;j<grid->npcol;++j)ActiveFlag[j]=ActiveFlagAll[j+lib*grid->npcol];;
-					for (j=0;j<grid->npcol;++j)ActiveFlag[j+grid->npcol]=j;
-					for (j=0;j<grid->npcol;++j)ranks[j]=-1;
-					Root=-1; 
-					Iactive = 0;				
-
-					for (j=0;j<grid->npcol;++j){
-						if(ActiveFlag[j]!=-3*nsupers){
-						jb = ActiveFlag[j];
-						pc = PCOL( jb, grid );
-						if(jb==ib)Root=pc;
-						if(mycol==pc)Iactive=1;		
-						}					
-					}
-				
-				
-					quickSortM(ActiveFlag,0,grid->npcol-1,grid->npcol,1,2);
-
-					if(Iactive==1){
-						assert( Root>-1 );
-						rank_cnt = 1;
-						ranks[0]=Root;
-						for (j = 0; j < grid->npcol; ++j){
-							if(ActiveFlag[j]!=-3*nsupers && ActiveFlag[j+grid->npcol]!=Root){
-								ranks[rank_cnt]=ActiveFlag[j+grid->npcol];
-								++rank_cnt;
-							}
-						}
-						if(rank_cnt>1){
-
-							for (ii=0;ii<rank_cnt;ii++)   // use global ranks rather than local ranks
-								ranks[ii] = PNUM( pr, ranks[ii], grid );		
-
-							// rseed=rand();
-							// rseed=1.0;
-							msgsize = SuperSize( ib )*nrhs+LSUM_H;
-
-							// if(ib==0){
-
-							LRtree_ptr[lib] = RdTree_Create(grid->comm, ranks, rank_cnt, msgsize,SeedSTD_RD[lib]);  	
-							RdTree_SetTag(LRtree_ptr[lib], RD_L);
-							// }
-
-							// printf("iam %5d rtree rank_cnt %5d \n",iam,rank_cnt);
-							// fflush(stdout);
-
-	
-							#if ( PRNTlevel>=1 )
-							if(Root==mycol){
-							assert(rank_cnt==frecv[lib]);
-							// printf("Partial Reduce Procs: row%7d np%4d\n",ib,rank_cnt);
-							// printf("Partial Reduce Procs: %4d %4d: ",iam, rank_cnt);
-							// // for(j=0;j<rank_cnt;++j)printf("%4d",ranks[j]);
-							// printf("\n");
-							}
-							#endif		
-						}
-					}				
-				}	
-			}
-
-			SUPERLU_FREE(mod_bit);
-			SUPERLU_FREE(frecv);
-
-
-			SUPERLU_FREE(ActiveFlag);
-			SUPERLU_FREE(ActiveFlagAll);
-			SUPERLU_FREE(ranks);	
-			// SUPERLU_FREE(idxs);	 
-			SUPERLU_FREE(SeedSTD_RD);	
-			// for(i=0;i<nsupers;++i){
-				// if(nzrows[i])SUPERLU_FREE(nzrows[i]);
-			// }
-			// SUPERLU_FREE(nzrows);
-
-				////////////////////////////////////////////////////////
-
-#if ( PROFlevel>=1 )
-		t = SuperLU_timer_() - t;
-		if ( !iam) printf(".. Construct Reduce tree for L: %.2f\t\n", t);
-#endif					
-
-#if ( PROFlevel>=1 )
-			t = SuperLU_timer_();
-#endif	
-
-			/* construct the Bcast tree for U ... */
-
-			k = CEILING( nsupers, grid->npcol );/* Number of local block columns */
-			if ( !(UBtree_ptr = (BcTree*)SUPERLU_MALLOC(k * sizeof(BcTree))) )
-				ABORT("Malloc fails for UBtree_ptr[].");
-			if ( !(ActiveFlag = intCalloc_dist(grid->nprow*2)) )
-				ABORT("Calloc fails for ActiveFlag[].");	
-			if ( !(ranks = (int*)SUPERLU_MALLOC(grid->nprow * sizeof(int))) )
-				ABORT("Malloc fails for ranks[].");	
-			if ( !(SeedSTD_BC = (double*)SUPERLU_MALLOC(k * sizeof(double))) )
-				ABORT("Malloc fails for SeedSTD_BC[].");	
-
-			for (i=0;i<k;i++){
-				SeedSTD_BC[i]=rand();		
-			}
-
-			MPI_Allreduce(MPI_IN_PLACE,&SeedSTD_BC[0],k,MPI_DOUBLE,MPI_MAX,grid->cscp.comm);					  
-
-
-			for (ljb = 0; ljb <k ; ++ljb) {
-				UBtree_ptr[ljb]=NULL;
-			}	
-
-			if ( !(ActiveFlagAll = intMalloc_dist(grid->nprow*k)) )
-				ABORT("Calloc fails for ActiveFlagAll[].");				
-			for (j=0;j<grid->nprow*k;++j)ActiveFlagAll[j]=-3*nsupers;	
-			
-			
-			
-			for (lib = 0; lib < CEILING( nsupers, grid->nprow); ++lib) { /* for each local block row ... */
-				ib = myrow+lib*grid->nprow;  /* not sure */
-				
-			// if(ib==0)printf("iam %5d ib %5d\n",iam,ib);
-			// fflush(stdout);				
-				
-				if(ib<nsupers){
-					for (i = xusub[lib]; i < xusub[lib+1]; i++) {
-					  jcol = usub[i];
-					  jb = BlockNum( jcol );
-					  ljb = LBj( jb, grid );    /* local block number */
-					  pc = PCOL( jb, grid );
-					  pr = PROW( ib, grid );
-					  if ( mycol == pc ) { /* Block column ib in my process column */		
-						ActiveFlagAll[pr+ljb*grid->nprow]=MAX(ActiveFlagAll[pr+ljb*grid->nprow],ib);			  
-					  }
-					}  /* for i ... */
-					pr = PROW( ib, grid ); // take care of diagonal node stored as L
-					pc = PCOL( ib, grid );
-					if ( mycol == pc ) { /* Block column ib in my process column */					
-						ljb = LBj( ib, grid );    /* local block number */
-						ActiveFlagAll[pr+ljb*grid->nprow]=MAX(ActiveFlagAll[pr+ljb*grid->nprow],ib);					
-						// if(pr+ljb*grid->nprow==0)printf("iam %5d ib %5d ActiveFlagAll %5d pr %5d ljb %5d\n",iam,ib,ActiveFlagAll[pr+ljb*grid->nprow],pr,ljb);
-						// fflush(stdout);	
-					}					
-				}	
-			}
-			
-			// printf("iam %5d ActiveFlagAll %5d\n",iam,ActiveFlagAll[0]);
-			// fflush(stdout);
-			
-			MPI_Allreduce(MPI_IN_PLACE,ActiveFlagAll,grid->nprow*k,mpi_int_t,MPI_MAX,grid->cscp.comm);					  
-						
-			for (ljb = 0; ljb < k; ++ljb) { /* for each block column ... */
-				jb = mycol+ljb*grid->npcol;  /* not sure */
-				if(jb<nsupers){
-				pc = PCOL( jb, grid );
-				// if ( mycol == pc ) { /* Block column jb in my process column */
-
-				for (j=0;j<grid->nprow;++j)ActiveFlag[j]=ActiveFlagAll[j+ljb*grid->nprow];
-				for (j=0;j<grid->nprow;++j)ActiveFlag[j+grid->nprow]=j;
-				for (j=0;j<grid->nprow;++j)ranks[j]=-1;
-
-				Root=-1; 
-				Iactive = 0;				
-				for (j=0;j<grid->nprow;++j){
-					if(ActiveFlag[j]!=-3*nsupers){
-					gb = ActiveFlag[j];
-					pr = PROW( gb, grid );
-					if(gb==jb)Root=pr;
-					if(myrow==pr)Iactive=1;		
-					}
-				}						
-				
-				quickSortM(ActiveFlag,0,grid->nprow-1,grid->nprow,1,2);	
-			// printf("jb: %5d Iactive %5d\n",jb,Iactive);
-			// fflush(stdout);
-				if(Iactive==1){
-					// if(jb==0)printf("root:%5d jb: %5d ActiveFlag %5d \n",Root,jb,ActiveFlag[0]);
-					fflush(stdout);
-					assert( Root>-1 );
-					rank_cnt = 1;
-					ranks[0]=Root;
-					for (j = 0; j < grid->nprow; ++j){
-						if(ActiveFlag[j]!=-3*nsupers && ActiveFlag[j+grid->nprow]!=Root){
-							ranks[rank_cnt]=ActiveFlag[j+grid->nprow];
-							++rank_cnt;
-						}
-					}		
-			// printf("jb: %5d rank_cnt %5d\n",jb,rank_cnt);
-			// fflush(stdout);
-					if(rank_cnt>1){
-						for (ii=0;ii<rank_cnt;ii++)   // use global ranks rather than local ranks
-							ranks[ii] = PNUM( ranks[ii], pc, grid );
-
-						// rseed=rand();
-						// rseed=1.0;
-						msgsize = SuperSize( jb )*nrhs+XK_H;
-						UBtree_ptr[ljb] = BcTree_Create(grid->comm, ranks, rank_cnt, msgsize,SeedSTD_BC[ljb]);  	
-						BcTree_SetTag(UBtree_ptr[ljb],BC_U);
-
-						// printf("iam %5d btree rank_cnt %5d \n",iam,rank_cnt);
-						// fflush(stdout);
-						
-						if(Root==myrow){
-						rank_cnt_ref=1;
-						for (j = 0; j < grid->nprow; ++j) {
-							// printf("ljb %5d j %5d nprow %5d\n",ljb,j,grid->nprow);
-							// fflush(stdout);
-							if ( bsendx_plist[ljb][j] != EMPTY ) {	
-								++rank_cnt_ref;		
-							}
-						}
-						// printf("ljb %5d rank_cnt %5d rank_cnt_ref %5d\n",ljb,rank_cnt,rank_cnt_ref);
-						// fflush(stdout);								
-						assert(rank_cnt==rank_cnt_ref);		
-						}						
-					}
-				}
-				}
-			}	
-			SUPERLU_FREE(ActiveFlag);
-			SUPERLU_FREE(ActiveFlagAll);
-			SUPERLU_FREE(ranks);				
-			SUPERLU_FREE(SeedSTD_BC);				
-				
-#if ( PROFlevel>=1 )
-		t = SuperLU_timer_() - t;
-		if ( !iam) printf(".. Construct Bcast tree for U: %.2f\t\n", t);
-#endif					
-
-#if ( PROFlevel>=1 )
-				t = SuperLU_timer_();
-#endif					
-			/* construct the Reduce tree for U ... */
-			/* the following is used as reference */
-			nlb = CEILING( nsupers, grid->nprow );/* Number of local block rows */
-			if ( !(mod_bit = intMalloc_dist(nlb)) )
-				ABORT("Malloc fails for mod_bit[].");
-			if ( !(brecv = intMalloc_dist(nlb)) )
-				ABORT("Malloc fails for brecv[].");
-
-			for (k = 0; k < nlb; ++k) mod_bit[k] = 0;
-			for (k = 0; k < nsupers; ++k) {
-				pr = PROW( k, grid );
-				if ( myrow == pr ) {
-					lib = LBi( k, grid );    /* local block number */
-					kcol = PCOL( k, grid );
-					if (mycol == kcol || bmod[lib] )
-						mod_bit[lib] = 1;  /* contribution from off-diagonal and diagonal*/
-				}
-			}
-			/* Every process receives the count, but it is only useful on the
-			   diagonal processes.  */
-			MPI_Allreduce( mod_bit, brecv, nlb, mpi_int_t, MPI_SUM, grid->rscp.comm);
-
-
-
-			k = CEILING( nsupers, grid->nprow );/* Number of local block rows */
-			if ( !(URtree_ptr = (RdTree*)SUPERLU_MALLOC(k * sizeof(RdTree))) )
-				ABORT("Malloc fails for URtree_ptr[].");
-			if ( !(ActiveFlag = intCalloc_dist(grid->npcol*2)) )
-				ABORT("Calloc fails for ActiveFlag[].");	
-			if ( !(ranks = (int*)SUPERLU_MALLOC(grid->npcol * sizeof(int))) )
-				ABORT("Malloc fails for ranks[].");	
-
-			// if ( !(idxs = intCalloc_dist(nsupers)) )
-				// ABORT("Calloc fails for idxs[].");	
-
-			// if ( !(nzrows = (int_t**)SUPERLU_MALLOC(nsupers * sizeof(int_t*))) )
-				// ABORT("Malloc fails for nzrows[].");
-
-			if ( !(SeedSTD_RD = (double*)SUPERLU_MALLOC(k * sizeof(double))) )
-				ABORT("Malloc fails for SeedSTD_RD[].");	
-
-			for (i=0;i<k;i++){
-				SeedSTD_RD[i]=rand();		
-			}
-
-			MPI_Allreduce(MPI_IN_PLACE,&SeedSTD_RD[0],k,MPI_DOUBLE,MPI_MAX,grid->rscp.comm);					  
-
-			for (lib = 0; lib <k ; ++lib) {
-				URtree_ptr[lib]=NULL;
-			}
-
-			
-			if ( !(ActiveFlagAll = intMalloc_dist(grid->npcol*k)) )
-				ABORT("Calloc fails for ActiveFlagAll[].");				
-			for (j=0;j<grid->npcol*k;++j)ActiveFlagAll[j]=3*nsupers;	
-							
-			for (lib = 0; lib < CEILING( nsupers, grid->nprow); ++lib) { /* for each local block row ... */
-				ib = myrow+lib*grid->nprow;  /* not sure */
-				if(ib<nsupers){
-					for (i = xusub[lib]; i < xusub[lib+1]; i++) {
-					  jcol = usub[i];
-					  jb = BlockNum( jcol );
-					  pc = PCOL( jb, grid );
-					  if ( mycol == pc ) { /* Block column ib in my process column */	
-						ActiveFlagAll[pc+lib*grid->npcol]=MIN(ActiveFlagAll[pc+lib*grid->npcol],jb);			  
-					  }	
-					}  /* for i ... */
-					pc = PCOL( ib, grid );
-					if ( mycol == pc ) { /* Block column ib in my process column */						
-						ActiveFlagAll[pc+lib*grid->npcol]=MIN(ActiveFlagAll[pc+lib*grid->npcol],ib);
-					}						
-				}	
-			}
-			
-			MPI_Allreduce(MPI_IN_PLACE,ActiveFlagAll,grid->npcol*k,mpi_int_t,MPI_MIN,grid->rscp.comm);	
-			
-			for (lib=0;lib<k;++lib){
-				ib = myrow+lib*grid->nprow;  /* not sure */
-				if(ib<nsupers){
-					pr = PROW( ib, grid );
-					for (j=0;j<grid->npcol;++j)ActiveFlag[j]=ActiveFlagAll[j+lib*grid->npcol];;
-					for (j=0;j<grid->npcol;++j)ActiveFlag[j+grid->npcol]=j;
-					for (j=0;j<grid->npcol;++j)ranks[j]=-1;
-					Root=-1; 
-					Iactive = 0;				
-
-					for (j=0;j<grid->npcol;++j){
-						if(ActiveFlag[j]!=3*nsupers){
-						jb = ActiveFlag[j];
-						pc = PCOL( jb, grid );
-						if(jb==ib)Root=pc;
-						if(mycol==pc)Iactive=1;		
-						}					
-					}
-					
-					quickSortM(ActiveFlag,0,grid->npcol-1,grid->npcol,0,2);
-
-					if(Iactive==1){
-						assert( Root>-1 );
-						rank_cnt = 1;
-						ranks[0]=Root;
-						for (j = 0; j < grid->npcol; ++j){
-							if(ActiveFlag[j]!=3*nsupers && ActiveFlag[j+grid->npcol]!=Root){
-								ranks[rank_cnt]=ActiveFlag[j+grid->npcol];
-								++rank_cnt;
-							}
-						}
-						if(rank_cnt>1){
-
-							for (ii=0;ii<rank_cnt;ii++)   // use global ranks rather than local ranks
-								ranks[ii] = PNUM( pr, ranks[ii], grid );		
-
-							// rseed=rand();
-							// rseed=1.0;
-							msgsize = SuperSize( ib )*nrhs+LSUM_H;
-
-							// if(ib==0){
-
-							URtree_ptr[lib] = RdTree_Create(grid->comm, ranks, rank_cnt, msgsize,SeedSTD_RD[lib]);  	
-							RdTree_SetTag(URtree_ptr[lib], RD_U);
-							// }
-	
-							// #if ( PRNTlevel>=1 )
-							if(Root==mycol){
-							// printf("Partial Reduce Procs: %4d %4d %5d \n",iam, rank_cnt,brecv[lib]);
-							// fflush(stdout);
-							assert(rank_cnt==brecv[lib]);
-							// printf("Partial Reduce Procs: row%7d np%4d\n",ib,rank_cnt);
-							// printf("Partial Reduce Procs: %4d %4d: ",iam, rank_cnt);
-							// // for(j=0;j<rank_cnt;++j)printf("%4d",ranks[j]);
-							// printf("\n");
-							}
-							// #endif		
-						}
-					}
-				}						
-			}
-
-			SUPERLU_FREE(mod_bit);
-			SUPERLU_FREE(brecv);
-
-
-			SUPERLU_FREE(ActiveFlag);
-			SUPERLU_FREE(ActiveFlagAll);
-			SUPERLU_FREE(ranks);	
-			// SUPERLU_FREE(idxs);	
-			SUPERLU_FREE(SeedSTD_RD);	
-			// for(i=0;i<nsupers;++i){
-				// if(nzrows[i])SUPERLU_FREE(nzrows[i]);
-			// }
-			// SUPERLU_FREE(nzrows);				
-				
-#if ( PROFlevel>=1 )
-		t = SuperLU_timer_() - t;
-		if ( !iam) printf(".. Construct Reduce tree for U: %.2f\t\n", t);
-#endif						
-				
-		////////////////////////////////////////////////////////
-    
-  
-  
-  /* Free the memory used for storing L and U */
-  SUPERLU_FREE(xlsub); SUPERLU_FREE(xusub);
-  if (lsub != NULL)
-    SUPERLU_FREE(lsub);  
-  if (usub != NULL)
-    SUPERLU_FREE(usub);  
-  
   SUPERLU_FREE(nnzToRecv);
   SUPERLU_FREE(ptrToRecv);
   SUPERLU_FREE(nnzToSend);
@@ -2761,10 +1931,7 @@ ddist_psymbtonum(fact_t fact, int_t n, SuperMatrix *A,
   SUPERLU_FREE(recvBuf);
   
   Llu->Lrowind_bc_ptr = Lrowind_bc_ptr;
-  Llu->Lindval_loc_bc_ptr = Lindval_loc_bc_ptr;
   Llu->Lnzval_bc_ptr = Lnzval_bc_ptr;
-  Llu->Linv_bc_ptr = Linv_bc_ptr;
-  Llu->Uinv_bc_ptr = Uinv_bc_ptr;
   Llu->Ufstnz_br_ptr = Ufstnz_br_ptr;
   Llu->Unzval_br_ptr = Unzval_br_ptr;
   Llu->ToRecv = ToRecv;
@@ -2781,14 +1948,6 @@ ddist_psymbtonum(fact_t fact, int_t n, SuperMatrix *A,
   Llu->ilsum = ilsum;
   Llu->ldalsum = ldaspa;
   LUstruct->Glu_persist = Glu_persist;	
-  Llu->LRtree_ptr = LRtree_ptr;
-  Llu->LBtree_ptr = LBtree_ptr;
-  Llu->URtree_ptr = URtree_ptr;
-  Llu->UBtree_ptr = UBtree_ptr;
-  Llu->Urbs = Urbs; 
-  Llu->Ucb_indptr = Ucb_indptr; 
-  Llu->Ucb_valptr = Ucb_valptr; 
-
 #if ( PRNTlevel>=1 )
   if ( !iam ) printf(".. # L blocks " IFMT "\t# U blocks " IFMT "\n",
 		     nLblocks, nUblocks);
diff --git a/SRC/pdutil.c b/SRC/pdutil.c
index a27f223..05975b0 100644
--- a/SRC/pdutil.c
+++ b/SRC/pdutil.c
@@ -533,17 +533,6 @@ void pdinf_norm_error(int iam, int_t n, int_t nrhs, double x[], int_t ldx,
 
       err = err / xnorm;
       if ( !iam ) printf("\tSol %2d: ||X-Xtrue||/||X|| = %e\n", j, err);
-	  fflush(stdout);
-	  
-	  // while(1);
-	  
-      // if(err>1e-5){
-		// if( !iam ) printf("Wrong solution! \n");
-		// fflush(stdout);
-		// while(1);
-
-		// ABORT("Wrong solution! \n");
-// }
     }
 }
 
diff --git a/SRC/psymbfact.h b/SRC/psymbfact.h
index 3330695..549e51e 100644
--- a/SRC/psymbfact.h
+++ b/SRC/psymbfact.h
@@ -299,6 +299,3 @@ typedef struct {
 
 
 #endif /* __SUPERLU_DIST_PSYMBFACT */
-
-
-
diff --git a/SRC/pzgssvx.c b/SRC/pzgssvx.c
index dc382bc..b56147b 100644
--- a/SRC/pzgssvx.c
+++ b/SRC/pzgssvx.c
@@ -239,7 +239,7 @@ at the top-level directory.
  *      The user must also supply 
  *
  *        o  A, the unfactored matrix, only in the case that iterative
- *              refinment is to be done (specifically A must be the output
+ *              refinement is to be done (specifically A must be the output
  *              A from the previous call, so that it has been scaled and permuted)
  *        o  all of ScalePermstruct
  *        o  all of LUstruct, including the actual numerical values of
@@ -342,7 +342,7 @@ at the top-level directory.
  *           = SLU_DOUBLE: accumulate residual in double precision.
  *           = SLU_EXTRA:  accumulate residual in extra precision.
  *
- *         NOTE: all options must be indentical on all processes when
+ *         NOTE: all options must be identical on all processes when
  *               calling this routine.
  *
  * A (input/output) SuperMatrix* (local)
@@ -467,7 +467,7 @@ at the top-level directory.
  * SOLVEstruct (input/output) SOLVEstruct_t*
  *         The data structure to hold the communication pattern used
  *         in the phases of triangular solution and iterative refinement.
- *         This pattern should be intialized only once for repeated solutions.
+ *         This pattern should be initialized only once for repeated solutions.
  *         If options->SolveInitialized = YES, it is an input argument.
  *         If options->SolveInitialized = NO and nrhs != 0, it is an output
  *         argument. See superlu_zdefs.h for the definition of 'SOLVEstruct_t'.
@@ -649,8 +649,10 @@ pzgssvx(superlu_dist_options_t *options, SuperMatrix *A,
     }
 
     /* ------------------------------------------------------------
-       Diagonal scaling to equilibrate the matrix. (simple scheme)
-       ------------------------------------------------------------*/
+     * Diagonal scaling to equilibrate the matrix. (simple scheme)
+     *   for row i = 1:n,  A(i,:) <- A(i,:) / max(abs(A(i,:));
+     *   for column j = 1:n,  A(:,j) <- A(:, j) / max(abs(A(:,j))
+     * ------------------------------------------------------------*/
     if ( Equil ) {
 #if ( DEBUGlevel>=1 )
 	CHECK_MALLOC(iam, "Enter equil");
diff --git a/SRC/pzgssvx_ABglobal.c b/SRC/pzgssvx_ABglobal.c
index 247f9e8..c42dbe3 100644
--- a/SRC/pzgssvx_ABglobal.c
+++ b/SRC/pzgssvx_ABglobal.c
@@ -209,7 +209,7 @@ at the top-level directory.
  *
  *      The user must also supply 
  *
- *      -  A, the unfactored matrix, only in the case that iterative refinment
+ *      -  A, the unfactored matrix, only in the case that iterative refinement
  *            is to be done (specifically A must be the output A from 
  *            the previous call, so that it has been scaled and permuted)
  *      -  all of ScalePermstruct
@@ -312,7 +312,7 @@ at the top-level directory.
  *           = SLU_DOUBLE: accumulate residual in double precision.
  *           = SLU_EXTRA:  accumulate residual in extra precision.
  *
- *         NOTE: all options must be indentical on all processes when
+ *         NOTE: all options must be identical on all processes when
  *               calling this routine.
  *
  * A (input/output) SuperMatrix*
diff --git a/SRC/pzgstrf2.c b/SRC/pzgstrf2.c
index c14d5dc..e95eed3 100644
--- a/SRC/pzgstrf2.c
+++ b/SRC/pzgstrf2.c
@@ -13,10 +13,13 @@ at the top-level directory.
  * \brief Performs panel LU factorization.
  *
  * <pre>
- * -- Distributed SuperLU routine (version 4.0) --
+ * -- Distributed SuperLU routine (version 5.2) --
  * Lawrence Berkeley National Lab, Univ. of California Berkeley.
  * August 15, 2014
  *
+ * Modified:
+ *   September 30, 2017
+ *
  * <pre>
  * Purpose
  * =======
@@ -96,6 +99,7 @@ pzgstrf2_trsm
     int_t Pr;
     MPI_Status status;
     MPI_Comm comm = (grid->cscp).comm;
+    double t1, t2;
 
     /* Initialization. */
     iam = grid->iam;
@@ -127,16 +131,25 @@ pzgstrf2_trsm
     if ( U_diag_blk_send_req && 
 	 U_diag_blk_send_req[myrow] != MPI_REQUEST_NULL ) {
         /* There are pending sends - wait for all Isend to complete */
-        for (pr = 0; pr < Pr; ++pr)
+#if ( PROFlevel>=1 )
+	TIC (t1);
+#endif
+        for (pr = 0; pr < Pr; ++pr) {
             if (pr != myrow) {
                 MPI_Wait (U_diag_blk_send_req + pr, &status);
             }
-
+	}
+#if ( PROFlevel>=1 )
+	TOC (t2, t1);
+	stat->utime[COMM] += t2;
+	stat->utime[COMM_DIAG] += t2;
+#endif
 	/* flag no more outstanding send request. */
 	U_diag_blk_send_req[myrow] = MPI_REQUEST_NULL;
     }
 
     if (iam == pkk) {            /* diagonal process */
+	/* ++++ First step compute diagonal block ++++++++++ */
         for (j = 0; j < jlst - jfst; ++j) {  /* for each column in panel */
             /* Diagonal pivot */
             i = luptr;
@@ -197,13 +210,16 @@ pzgstrf2_trsm
 
         }                       /* for column j ...  first loop */
 
-	/* ++++++++++second step ====== */
+	/* ++++ Second step compute off-diagonal block with communication  ++*/
 
         ublk_ptr = ujrow = Llu->ujrow;
 
-        if (U_diag_blk_send_req && iam == pkk)  { /* Send the U block */
+        if (U_diag_blk_send_req && iam == pkk)  { /* Send the U block downward */
             /** ALWAYS SEND TO ALL OTHERS - TO FIX **/
-            for (pr = 0; pr < Pr; ++pr)
+#if ( PROFlevel>=1 )
+	    TIC (t1);
+#endif
+            for (pr = 0; pr < Pr; ++pr) {
                 if (pr != krow) {
                     /* tag = ((k0<<2)+2) % tag_ub;        */
                     /* tag = (4*(nsupers+k0)+2) % tag_ub; */
@@ -212,6 +228,12 @@ pzgstrf2_trsm
                                comm, U_diag_blk_send_req + pr);
 
                 }
+            }
+#if ( PROFlevel>=1 )
+	    TOC (t2, t1);
+	    stat->utime[COMM] += t2;
+	    stat->utime[COMM_DIAG] += t2;
+#endif
 
 	    /* flag outstanding Isend */
             U_diag_blk_send_req[krow] = (MPI_Request) TRUE; /* Sherry */
@@ -219,8 +241,6 @@ pzgstrf2_trsm
 
         /* pragma below would be changed by an MKL call */
 
-        char uplo = 'u', side = 'r', transa = 'n', diag = 'n';
-
         l = nsupr - nsupc;
         // n = nsupc;
 	doublecomplex alpha = {1.0, 0.0};
@@ -239,27 +259,33 @@ pzgstrf2_trsm
 #endif
 	stat->ops[FACT] += 4.0 * ((flops_t) nsupc * (nsupc+1) * l);
     } else {  /* non-diagonal process */
-        /* ================================================ *
-         * Receive the diagonal block of U                  *
-         * for panel factorization of L(:,k)                *
-         * note: we block for panel factorization of L(:,k) *
-         * but panel factorization of U(:,k) don't          *
-         * ================================================ */
+        /* ================================================================== *
+         * Receive the diagonal block of U for panel factorization of L(:,k). * 
+         * Note: we block for panel factorization of L(:,k), but panel        *
+	 * factorization of U(:,k) do not block                               *
+         * ================================================================== */
 
         /* tag = ((k0<<2)+2) % tag_ub;        */
         /* tag = (4*(nsupers+k0)+2) % tag_ub; */
         // printf("hello message receiving%d %d\n",(nsupc*(nsupc+1))>>1,SLU_MPI_TAG(4,k0));
+#if ( PROFlevel>=1 )
+	TIC (t1);
+#endif
         MPI_Recv (ublk_ptr, (nsupc * nsupc), SuperLU_MPI_DOUBLE_COMPLEX, krow,
                   SLU_MPI_TAG (4, k0) /* tag */ ,
                   comm, &status);
+#if ( PROFlevel>=1 )
+	TOC (t2, t1);
+	stat->utime[COMM] += t2;
+	stat->utime[COMM_DIAG] += t2;
+#endif
         if (nsupr > 0) {
-            char uplo = 'u', side = 'r', transa = 'n', diag = 'n';
             doublecomplex alpha = {1.0, 0.0};
 
 #ifdef PI_DEBUG
             printf ("ztrsm non diagonal param 11:  %d \n", nsupr);
             if (!lusup)
-                printf (" Rank :%d \t Empty block column occured :\n", iam);
+                printf (" Rank :%d \t Empty block column occurred :\n", iam);
 #endif
 #if defined (USE_VENDOR_BLAS)
             ztrsm_ ("R", "U", "N", "N", &nsupr, &nsupc,
@@ -298,12 +324,10 @@ void pzgstrs2_omp
     int_t *usub;
     doublecomplex *lusup, *uval;
 
-#ifdef _OPENMP
-    int thread_id = omp_get_thread_num ();
-    int num_thread = omp_get_num_threads ();
-#else
-    int thread_id = 0;
-    int num_thread = 1;
+#if 0
+    //#ifdef USE_VTUNE
+    __SSC_MARK(0x111);// start SDE tracing, note uses 2 underscores
+    __itt_resume(); // start VTune, again use 2 underscores
 #endif
 
     /* Quick return. */
@@ -313,15 +337,12 @@ void pzgstrs2_omp
     /* Initialization. */
     iam = grid->iam;
     pkk = PNUM (PROW (k, grid), PCOL (k, grid), grid);
-    int k_row_cycle = k / grid->nprow;  /* for which cycle k exist (to assign rowwise thread blocking) */
-    int gb_col_cycle;  /* cycle through block columns  */
+    //int k_row_cycle = k / grid->nprow;  /* for which cycle k exist (to assign rowwise thread blocking) */
+    //int gb_col_cycle;  /* cycle through block columns  */
     klst = FstBlockC (k + 1);
     knsupc = SuperSize (k);
     usub = Llu->Ufstnz_br_ptr[lk];  /* index[] of block row U(k,:) */
     uval = Llu->Unzval_br_ptr[lk];
-    nb = usub[0];
-    iukp = BR_HEADER;
-    rukp = 0;
     if (iam == pkk) {
         lk = LBj (k, grid);
         nsupr = Llu->Lrowind_bc_ptr[lk][1]; /* LDA of lusup[] */
@@ -331,28 +352,45 @@ void pzgstrs2_omp
         lusup = Llu->Lval_buf_2[k0 % (1 + stat->num_look_aheads)];
     }
 
-    /* Loop through all the row blocks. */
-    for (b = 0; b < nb; ++b)  {
-        /* assuming column cyclic distribution of data among threads */
-        gb = usub[iukp];
-        gb_col_cycle = gb / grid->npcol;
-        nsupc = SuperSize (gb);
-        iukp += UB_DESCRIPTOR;
+    /////////////////////new-test//////////////////////////
+    /* !! Taken from Carl/SuperLU_DIST_5.1.0/EXAMPLE/pdgstrf2_v3.c !! */
+
+    /* Master thread: set up pointers to each block in the row */
+    nb = usub[0];
+    iukp = BR_HEADER;
+    rukp = 0;
+    
+    int* blocks_index_pointers = SUPERLU_MALLOC (3 * nb * sizeof(int));
+    int* blocks_value_pointers = blocks_index_pointers + nb;
+    int* nsupc_temp = blocks_value_pointers + nb;
+    for (b = 0; b < nb; b++) { /* set up pointers to each block */
+	blocks_index_pointers[b] = iukp + UB_DESCRIPTOR;
+	blocks_value_pointers[b] = rukp;
+	gb = usub[iukp];
+	rukp += usub[iukp+1];
+	nsupc = SuperSize( gb );
+	nsupc_temp[b] = nsupc;
+	iukp += (UB_DESCRIPTOR + nsupc);  /* move to the next block */
+    }
+
+    // Sherry: this version is more NUMA friendly compared to pdgstrf2_v2.c
+    // https://stackoverflow.com/questions/13065943/task-based-programming-pragma-omp-task-versus-pragma-omp-parallel-for
+#pragma omp parallel for schedule(static) default(shared) \
+    private(b,j,iukp,rukp,segsize)
+    /* Loop through all the blocks in the row. */
+    for (b = 0; b < nb; ++b) {
+	iukp = blocks_index_pointers[b];
+	rukp = blocks_value_pointers[b];
 
         /* Loop through all the segments in the block. */
-        for (j = 0; j < nsupc; ++j) {
-#ifdef PI_DEBUG
-            printf("segsize %d klst %d usub[%d] : %d",segsize,klst ,iukp,usub[iukp]);
-#endif 
+        for (j = 0; j < nsupc_temp[b]; j++) {
             segsize = klst - usub[iukp++];
-            if (segsize) {    /* Nonzero segment. */
-                luptr = (knsupc - segsize) * (nsupr + 1);
+	    if (segsize) {
+#pragma omp task default(shared) firstprivate(segsize,rukp) if (segsize > 30)
+		{ /* Nonzero segment. */
+		    int_t luptr = (knsupc - segsize) * (nsupr + 1);
+		    //printf("[2] segsize %d, nsupr %d\n", segsize, nsupr);
 
-		/* if gb belongs to present thread then do the factorize */
-                if ((gb_col_cycle + k_row_cycle + 1) % num_thread == thread_id) {
-#ifdef PI_DEBUG
-                    printf ("dtrsv param 4 %d param 6 %d\n", segsize, nsupr);
-#endif
 #if defined (USE_VENDOR_BLAS)
                     ztrsv_ ("L", "N", "U", &segsize, &lusup[luptr], &nsupr,
                             &uval[rukp], &incx, 1, 1, 1);
@@ -360,12 +398,22 @@ void pzgstrs2_omp
                     ztrsv_ ("L", "N", "U", &segsize, &lusup[luptr], &nsupr,
                             &uval[rukp], &incx);
 #endif
-                }
-                rukp += segsize;
-		stat->ops[FACT] += 4.0 * (flops_t) segsize * (segsize + 1);
-            } /* end if segsize > 0 */
-        } /* end for j ... */
-    } /* end for b ... */
+		} /* end task */
+		rukp += segsize;
+		stat->ops[FACT] += segsize * (segsize + 1);
+	    } /* end if segsize > 0 */
+	} /* end for j in parallel ... */
+/* #pragma omp taskwait */
+    }  /* end for b ... */
+
+    /* Deallocate memory */
+    SUPERLU_FREE(blocks_index_pointers);
+
+#if 0
+    //#ifdef USE_VTUNE
+    __itt_pause(); // stop VTune
+    __SSC_MARK(0x222); // stop SDE tracing
+#endif
 
 } /* PZGSTRS2_omp */
 
diff --git a/SRC/pzgstrs.c b/SRC/pzgstrs.c
index 5a11f84..a537073 100644
--- a/SRC/pzgstrs.c
+++ b/SRC/pzgstrs.c
@@ -1174,7 +1174,7 @@ pzgstrs(int_t n, LUstruct_t *LUstruct,
 
 
     /*
-     * Compute the internal nodes asychronously by all processes.
+     * Compute the internal nodes asynchronously by all processes.
      */
     while ( nbrecvx || nbrecvmod ) { /* While not finished. */
 
diff --git a/SRC/pzgstrs1.c b/SRC/pzgstrs1.c
index 8924de8..f2621c9 100644
--- a/SRC/pzgstrs1.c
+++ b/SRC/pzgstrs1.c
@@ -779,7 +779,7 @@ void pzgstrs1(int_t n, LUstruct_t *LUstruct, gridinfo_t *grid,
 
 
     /*
-     * Compute the internal nodes asychronously by all processes.
+     * Compute the internal nodes asynchronously by all processes.
      */
     while ( nbrecvx || nbrecvmod ) { /* While not finished. */
 
diff --git a/SRC/pzgstrs_Bglobal.c b/SRC/pzgstrs_Bglobal.c
index e769f35..47ba50f 100644
--- a/SRC/pzgstrs_Bglobal.c
+++ b/SRC/pzgstrs_Bglobal.c
@@ -826,7 +826,7 @@ pzgstrs_Bglobal(int_t n, LUstruct_t *LUstruct, gridinfo_t *grid,
 
 
     /*
-     * Compute the internal nodes asychronously by all processes.
+     * Compute the internal nodes asynchronously by all processes.
      */
     while ( nbrecvx || nbrecvmod ) { /* While not finished. */
 
diff --git a/SRC/superlu_ddefs.h b/SRC/superlu_ddefs.h
index 7e6ccf2..27b3487 100644
--- a/SRC/superlu_ddefs.h
+++ b/SRC/superlu_ddefs.h
@@ -46,16 +46,8 @@ typedef struct {
 typedef struct {
     int_t   **Lrowind_bc_ptr; /* size ceil(NSUPERS/Pc)                 */
     double  **Lnzval_bc_ptr;  /* size ceil(NSUPERS/Pc)                 */
-    double  **Linv_bc_ptr;  /* size ceil(NSUPERS/Pc)                 */
-	int_t   **Lindval_loc_bc_ptr; /* size ceil(NSUPERS/Pc)  pointers to locations in Lrowind_bc_ptr and Lnzval_bc_ptr               */
-	int_t 	**Lrowind_bc_2_lsum; /* size ceil(NSUPERS/Pc)  map indices of Lrowind_bc_ptr to indices of lsum  */  
-    double  **Uinv_bc_ptr;  /* size ceil(NSUPERS/Pc)                 */
     int_t   **Ufstnz_br_ptr;  /* size ceil(NSUPERS/Pr)                 */
     double  **Unzval_br_ptr;  /* size ceil(NSUPERS/Pr)                 */
-	BcTree  *LBtree_ptr;       /* size ceil(NSUPERS/Pc)                */
-	RdTree  *LRtree_ptr;		  /* size ceil(NSUPERS/Pr)                */
-	BcTree  *UBtree_ptr;       /* size ceil(NSUPERS/Pc)                */
-	RdTree  *URtree_ptr;		  /* size ceil(NSUPERS/Pr)                */	
 #if 0
     int_t   *Lsub_buf;        /* Buffer for the remote subscripts of L */
     double  *Lval_buf;        /* Buffer for the remote nonzeros of L   */
@@ -100,6 +92,7 @@ typedef struct {
     int_t   SolveMsgSent;     /* Number of actual messages sent in LU-solve */
     int_t   SolveMsgVol;      /* Volume of messages sent in the solve phase */
 
+
     /*********************/	
     /* The following variables are used in the hybrid solver */
 
@@ -125,7 +118,6 @@ typedef struct {
     int_t n;
     int_t nleaf;
     int_t nfrecvmod;
-	int_t inv; /* whether the diagonal block is inverted*/	
 } LocalLU_t;
 
 
@@ -220,10 +212,6 @@ extern int     dcreate_matrix_rb(SuperMatrix *, int, double **, int *,
 extern int     dcreate_matrix_dat(SuperMatrix *, int, double **, int *, 
 			      double **, int *, FILE *, gridinfo_t *);
 
-extern int 	   dcreate_matrix_postfix(SuperMatrix *, int, double **, int *, 
-				  double **, int *, FILE *, char *, gridinfo_t *);				  
-				  
-				  
 /* Driver related */
 extern void    dgsequ_dist (SuperMatrix *, double *, double *, double *,
 			    double *, double *, int_t *);
@@ -254,14 +242,11 @@ extern void  pdgssvx_ABglobal(superlu_dist_options_t *, SuperMatrix *,
 			      SuperLUStat_t *, int *);
 extern float pddistribute(fact_t, int_t, SuperMatrix *, 
 			 ScalePermstruct_t *, Glu_freeable_t *, 
-			 LUstruct_t *, gridinfo_t *, int_t);
+			 LUstruct_t *, gridinfo_t *);
 extern void  pdgssvx(superlu_dist_options_t *, SuperMatrix *, 
 		     ScalePermstruct_t *, double *,
 		     int, int, gridinfo_t *, LUstruct_t *,
 		     SOLVEstruct_t *, double *, SuperLUStat_t *, int *);
-
-extern void  pdCompute_Diag_Inv(int_t, LUstruct_t *,gridinfo_t *, SuperLUStat_t *, int *);
-		 
 extern int  dSolveInit(superlu_dist_options_t *, SuperMatrix *, int_t [], int_t [],
 		       int_t, LUstruct_t *, gridinfo_t *, SOLVEstruct_t *);
 extern void dSolveFinalize(superlu_dist_options_t *, SOLVEstruct_t *);
@@ -296,22 +281,6 @@ extern void dlsum_bmod(double *, double *, double *,
                        int, int_t, int_t *, int_t *, Ucb_indptr_t **,
                        int_t **, int_t *, gridinfo_t *, LocalLU_t *,
 		       MPI_Request [], SuperLUStat_t *);
-extern void dlsum_fmod_inv(double *, double *, double *, double *,
-		       int, int, int_t , int_t *, int_t,
-		       int_t *, gridinfo_t *, LocalLU_t *, 
-		       SuperLUStat_t **, int_t *, int_t *, int_t, int_t, int_t);
-extern void dlsum_fmod_inv_master(double *, double *, double *, double *,
-		       int, int, int_t , int_t *, int_t, 
-		       int_t *, gridinfo_t *, LocalLU_t *, 
-		       SuperLUStat_t **, int_t, int_t, int_t);
-extern void dlsum_bmod_inv(double *, double *, double *, double *,
-                       int, int_t, int_t *, int_t *, int_t *, Ucb_indptr_t **,
-                       int_t **, int_t *, gridinfo_t *, LocalLU_t *,
-		       MPI_Request [], SuperLUStat_t **, int_t *, int_t *, int_t, int_t);
-extern void dlsum_bmod_inv_master(double *, double *, double *, double *,
-                       int, int_t, int_t *, int_t *, int_t *, Ucb_indptr_t **,
-                       int_t **, int_t *, gridinfo_t *, LocalLU_t *,
-		       MPI_Request [], SuperLUStat_t **, int_t, int_t);			   
 extern void pdgsrfs(int_t, SuperMatrix *, double, LUstruct_t *,
 		    ScalePermstruct_t *, gridinfo_t *,
 		    double [], int_t, double [], int_t, int,
@@ -354,12 +323,11 @@ extern void  dreadrb_dist(int, FILE *, int_t *, int_t *, int_t *,
 		     double **, int_t **, int_t **);
 extern void  dreadMM_dist(FILE *, int_t *, int_t *, int_t *,
 	                  double **, int_t **, int_t **);
-extern int  dread_binary(FILE *, int_t *, int_t *, int_t *,
-	                  double **, int_t **, int_t **);													 
+
 /* Distribute the data for numerical factorization */
 extern float ddist_psymbtonum(fact_t, int_t, SuperMatrix *,
                                 ScalePermstruct_t *, Pslu_freeable_t *, 
-                                LUstruct_t *, gridinfo_t *, int_t nrhs);
+                                LUstruct_t *, gridinfo_t *);
 extern void pdGetDiagU(int_t, LUstruct_t *, gridinfo_t *, double *);
 
 
@@ -390,13 +358,6 @@ extern void dgemv_(char *, int *, int *, double *, double *a, int *,
 extern void dger_(int*, int*, double*, double*, int*,
                  double*, int*, double*, int*);
 
-extern int daxpy_(int *, double *, double *, int *, double *, int *);				 
-
-				 
-extern void dtrtri_(char*, char*, int*, double*, int*,int*);				 
-
-			 	
-				 
 #else
 extern int dgemm_(const char*, const char*, const int*, const int*, const int*,
                    const double*,  const double*,  const int*,  const double*,
@@ -409,8 +370,6 @@ extern int dgemv_(char *, int *, int *, double *, double *a, int *,
                   double *, int *, double *, double *, int *);
 extern void dger_(int*, int*, double*, double*, int*,
                  double*, int*, double*, int*);
-				 
-extern int daxpy_(int *, double *, double *, int *, double *, int *);				 
 
 #endif
 
diff --git a/SRC/superlu_defs.h b/SRC/superlu_defs.h
index 553cbb9..1925acb 100644
--- a/SRC/superlu_defs.h
+++ b/SRC/superlu_defs.h
@@ -42,17 +42,13 @@ at the top-level directory.
 #include <stdio.h>
 #include <limits.h>
 #include <string.h>
-#include <stdatomic.h>
-#include <math.h>
-
-// /* Following is for vtune */
-// #if 0
-// #include <ittnotify.h>
-// #define USE_VTUNE
-// #endif
-#if ( VTUNE>=1 )
+
+/* Following is for vtune */
+#if 0
 #include <ittnotify.h>
+#define USE_VTUNE
 #endif
+
 /*************************************************************************
  * Constants
  **************************************************************************/
@@ -69,9 +65,9 @@ at the top-level directory.
 #define SUPERLU_DIST_MAJOR_VERSION     5
 #define SUPERLU_DIST_MINOR_VERSION     3
 #define SUPERLU_DIST_PATCH_VERSION     0
-#define SUPERLU_DIST_RELEASE_DATE      "January 28, 2018"					  
+#define SUPERLU_DIST_RELEASE_DATE      "January 28, 2018"
 
-#include "superlu_dist_config.h"							   
+#include "superlu_dist_config.h"
 /* Define my integer size int_t */
 #ifdef _CRAY
   typedef short int_t;
@@ -87,21 +83,6 @@ at the top-level directory.
   #define IFMT "%8d"
 #endif
 
- 
-
-
-// /* Define atomic int_t */
-// #ifdef _CRAY
-  // typedef atomic_short int_t_ato  ;
-// #elif defined (_LONGINT)
-  // typedef atomic_llong int_t_ato  ;
-// #else /* Default */
-  // typedef atomic_int int_t_ato  ;
-// #endif
-
-
-
-
 #include "superlu_enum_consts.h"
 #include "Cnames.h"
 #include "supermatrix.h"
@@ -192,13 +173,7 @@ at the top-level directory.
 #define GSUM     20 
 #define Xk       21
 #define Yk       22
-#define LSUM     23    
-
- 
-static const int BC_L=1;	/* MPI tag for x in L-solve*/	
-static const int RD_L=2;	/* MPI tag for lsum in L-solve*/	
-static const int BC_U=3;	/* MPI tag for x in U-solve*/
-static const int RD_U=4;	/* MPI tag for lsum in U-solve*/	
+#define LSUM     23
 
 /* 
  * Communication scopes
@@ -246,10 +221,6 @@ static const int RD_U=4;	/* MPI tag for lsum in U-solve*/
 #define SuperLU_timer_  SuperLU_timer_dist_
 #define LOG2(x)   (log10((double) x) / log10(2.0))
 
-#define MIN(a,b) ((a) <= (b) ? (a) : (b))
-#define MAX(a,b) ((a) >= (b) ? (a) : (b))
-
-
 
 #if ( VAMPIR>=1 ) 
 #define VT_TRACEON    VT_traceon()
@@ -272,20 +243,6 @@ static const int RD_U=4;	/* MPI tag for lsum in U-solve*/
 #endif /* MSVC */
 #endif /* SUPERLU_DIST_EXPORT */
 
-#ifdef __cplusplus
-extern "C" {
-#endif
-
-
-#ifndef max
-    #define cmax(a,b) ((a) > (b) ? (a) : (b))
-#endif
-
-#ifdef __cplusplus
-  }
-#endif
-
-
 /***********************************************************************
  * New data types
  ***********************************************************************/
@@ -622,7 +579,6 @@ typedef struct {
 typedef struct {
     fact_t        Fact;
     yes_no_t      Equil;
-    yes_no_t      DiagInv;
     colperm_t     ColPerm;
     trans_t       Trans;
     IterRefine_t  IterRefine;
@@ -768,10 +724,6 @@ extern void  PStatPrint(superlu_dist_options_t *, SuperLUStat_t *, gridinfo_t *)
 extern void  log_memory(long long, SuperLUStat_t *);
 extern void  print_memorylog(SuperLUStat_t *, char *);
 extern int   superlu_dist_GetVersionNumber(int *, int *, int *);
-extern void  quickSort( int_t*, int_t, int_t, int_t);
-extern void  quickSortM( int_t*, int_t, int_t, int_t, int_t, int_t);
-extern int_t partition( int_t*, int_t, int_t, int_t);
-extern int_t partitionM( int_t*, int_t, int_t, int_t, int_t, int_t);
 
 /* Prototypes for parallel symbolic factorization */
 extern float symbfact_dist
@@ -826,45 +778,6 @@ extern int   file_PrintInt10(FILE *, char *, int_t, int_t *);
 extern int   file_PrintInt32(FILE *, char *, int, int *);
 extern int   file_PrintLong10(FILE *, char *, int_t, int_t *);
 
-
-/* Routines for Async_tree communication*/
-
-#ifndef __SUPERLU_ASYNC_TREE /* allow multiple inclusions */
-#define __SUPERLU_ASYNC_TREE
-typedef void* BcTree;
-typedef void* RdTree;
-typedef void* StdList;
-#endif
-
-// typedef enum {NO, YES}  yes_no_t;
-extern RdTree   RdTree_Create(MPI_Comm comm, int* ranks, int rank_cnt, int msgSize, double rseed);  
-extern void   	RdTree_Destroy(RdTree Tree);
-extern void 	RdTree_SetTag(RdTree Tree, int tag);
-extern yes_no_t RdTree_IsRoot(RdTree Tree);
-extern void 	RdTree_forwardMessageSimple(RdTree Tree, void* localBuffer);
-extern void 	RdTree_allocateRequest(RdTree Tree);
-extern int  	RdTree_GetDestCount(RdTree Tree);
-extern void 	RdTree_waitSendRequest(RdTree Tree);
-
-extern BcTree   BcTree_Create(MPI_Comm comm, int* ranks, int rank_cnt, int msgSize, double rseed);  
-extern void   	BcTree_Destroy(BcTree Tree);
-extern void 	BcTree_SetTag(BcTree Tree, int tag);
-extern yes_no_t BcTree_IsRoot(BcTree Tree);
-extern void 	BcTree_forwardMessageSimple(BcTree Tree, void* localBuffer);
-extern void 	BcTree_allocateRequest(BcTree Tree);
-extern int 		BcTree_getDestCount(BcTree Tree); 
-extern void 	BcTree_waitSendRequest(BcTree Tree);
- 
-extern StdList 	StdList_Init();
-extern void 	StdList_Pushback(StdList lst, int_t dat);
-extern void 	StdList_Pushfront(StdList lst, int_t dat);
-extern int_t 		StdList_Popfront(StdList lst);
-extern yes_no_t StdList_Find(StdList lst, int_t dat);
-extern int_t 	   	StdList_Size(StdList lst);
-yes_no_t 		StdList_Empty(StdList lst);
-
-
-
 #ifdef __cplusplus
   }
 #endif
diff --git a/SRC/superlu_enum_consts.h b/SRC/superlu_enum_consts.h
index ceac433..628eecf 100644
--- a/SRC/superlu_enum_consts.h
+++ b/SRC/superlu_enum_consts.h
@@ -14,7 +14,7 @@ at the top-level directory.
  * -- SuperLU routine (version 4.1) --
  * Lawrence Berkeley National Lab, Univ. of California Berkeley, 
  * October 1, 2010
- * January 28, 2018				   
+ * January 28, 2018
  *
  */
 
@@ -33,7 +33,7 @@ typedef enum {NOTRANS, TRANS, CONJ}                             trans_t;
 typedef enum {NOEQUIL, ROW, COL, BOTH}                          DiagScale_t;
 typedef enum {NOREFINE, SLU_SINGLE=1, SLU_DOUBLE, SLU_EXTRA}    IterRefine_t;
 //typedef enum {LUSUP, UCOL, LSUB, USUB, LLVL, ULVL, NO_MEMTYPE}  MemType;
-typedef enum {USUB, LSUB, UCOL, LUSUP, LLVL, ULVL, NO_MEMTYPE}  MemType;																		
+typedef enum {USUB, LSUB, UCOL, LUSUP, LLVL, ULVL, NO_MEMTYPE}  MemType;
 typedef enum {HEAD, TAIL}                                       stack_end_t;
 typedef enum {SYSTEM, USER}                                     LU_space_t;
 typedef enum {ONE_NORM, TWO_NORM, INF_NORM}			norm_t;
@@ -71,11 +71,8 @@ typedef enum {
     COMM,    /* communication for factorization */
     COMM_DIAG, /* Bcast diagonal block to process column */
     COMM_RIGHT, /* communicate L panel */
-    COMM_DOWN, /* communicate U panel */										
+    COMM_DOWN, /* communicate U panel */
     SOL_COMM,/* communication for solve */
-    SOL_GEMM,/* gemm for solve */
-    SOL_TRSM,/* trsm for solve */
-	SOL_L,	/* LU-solve time*/
     RCOND,   /* estimate reciprocal condition number */
     SOLVE,   /* forward and back solves */
     REFINE,  /* perform iterative refinement */
@@ -85,4 +82,5 @@ typedef enum {
     NPHASES  /* total number of phases */
 } PhaseType;
 
+
 #endif /* __SUPERLU_ENUM_CONSTS */
diff --git a/SRC/util.c b/SRC/util.c
index fc5933f..afceea0 100644
--- a/SRC/util.c
+++ b/SRC/util.c
@@ -22,7 +22,6 @@ at the top-level directory.
  */
 
 #include <math.h>
-#include <unistd.h>
 #include "superlu_ddefs.h"
 
 /*! \brief Deallocate the structure pointing to the actual storage of the matrix. */
@@ -145,64 +144,6 @@ Destroy_LU(int_t n, gridinfo_t *grid, LUstruct_t *LUstruct)
     SUPERLU_FREE(Llu->bsendx_plist);
     SUPERLU_FREE(Llu->mod_bit);
 
-	
-	
-    nb = CEILING(nsupers, grid->npcol);
-    for (i = 0; i < nb; ++i) 
-	if ( Llu->Lindval_loc_bc_ptr[i] ) {
-	    SUPERLU_FREE (Llu->Lindval_loc_bc_ptr[i]);
-	}	
-	SUPERLU_FREE(Llu->Lindval_loc_bc_ptr);
-	
- 
-	nb = CEILING(nsupers, grid->npcol);
-	for (i=0;i<nb;++i){
-		if(Llu->LBtree_ptr[i]!=NULL){
-			BcTree_Destroy(Llu->LBtree_ptr[i]);
-		}
-		if(Llu->UBtree_ptr[i]!=NULL){
-			BcTree_Destroy(Llu->UBtree_ptr[i]);
-		}		
-	}
-	SUPERLU_FREE(Llu->LBtree_ptr);
-	SUPERLU_FREE(Llu->UBtree_ptr);
-	
- 	nb = CEILING(nsupers, grid->nprow);
-	for (i=0;i<nb;++i){
-		if(Llu->LRtree_ptr[i]!=NULL){
-			RdTree_Destroy(Llu->LRtree_ptr[i]);
-		}
-		if(Llu->URtree_ptr[i]!=NULL){
-			RdTree_Destroy(Llu->URtree_ptr[i]);
-		}		
-	}
-	SUPERLU_FREE(Llu->LRtree_ptr);
-	SUPERLU_FREE(Llu->URtree_ptr);
-
-	nb = CEILING(nsupers, grid->npcol);
-	for (i=0;i<nb;++i){
-		if(Llu->Linv_bc_ptr[i]!=NULL){
-			SUPERLU_FREE(Llu->Linv_bc_ptr[i]);
-		}
-		if(Llu->Uinv_bc_ptr[i]!=NULL){
-			SUPERLU_FREE(Llu->Uinv_bc_ptr[i]);
-		}	
-	}
-	SUPERLU_FREE(Llu->Linv_bc_ptr);
-	SUPERLU_FREE(Llu->Uinv_bc_ptr);
-	
-	
-	nb = CEILING(nsupers, grid->npcol);
-    for (i = 0; i < nb; ++i)
-	if ( Llu->Urbs[i] ) {
-	    SUPERLU_FREE(Llu->Ucb_indptr[i]);
-	    SUPERLU_FREE(Llu->Ucb_valptr[i]);
-	}
-    SUPERLU_FREE(Llu->Ucb_indptr);
-    SUPERLU_FREE(Llu->Ucb_valptr);	
-	SUPERLU_FREE(Llu->Urbs);
-
-	
     SUPERLU_FREE(Glu_persist->xsup);
     SUPERLU_FREE(Glu_persist->supno);
 
@@ -252,7 +193,6 @@ void LUstructInit(const int_t n, LUstruct_t *LUstruct)
     if ( !(LUstruct->Llu = (LocalLU_t *)
 	   SUPERLU_MALLOC(sizeof(LocalLU_t))) )
 	ABORT("Malloc fails for LocalLU_t.");
-	LUstruct->Llu->inv = 0;
 }
 
 /*! \brief Deallocate LUstruct */
@@ -387,7 +327,7 @@ void set_default_options_dist(superlu_dist_options_t *options)
     options->ColPerm           = METIS_AT_PLUS_A;
 #else
     options->ColPerm            = MMD_AT_PLUS_A;
-#endif	  
+#endif
     options->RowPerm           = LargeDiag;
     options->ReplaceTinyPivot  = NO;
     options->IterRefine        = SLU_DOUBLE;
@@ -398,7 +338,6 @@ void set_default_options_dist(superlu_dist_options_t *options)
     options->num_lookaheads    = 10;
     options->lookahead_etree   = NO;
     options->SymPattern        = NO;
-    options->DiagInv           = NO;
 }
 
 /*! \brief Print the options setting.
@@ -681,121 +620,60 @@ PStatPrint(superlu_dist_options_t *options, SuperLUStat_t *stat, gridinfo_t *gri
 	       0, grid->comm);
     solveflop = flopcnt;
     if ( !iam ) {
-	printf("\tSOLVE time         %8.3f\n", utime[SOLVE]);
+	printf("\tSOLVE time         %8.2f\n", utime[SOLVE]);
 	if ( utime[SOLVE] != 0.0 )
 	    printf("\tSolve flops\t%e\tMflops \t%8.2f\n",
 		   flopcnt,
 		   flopcnt*1e-6/utime[SOLVE]);
 	if ( options->IterRefine != NOREFINE ) {
-	    printf("\tREFINEMENT time    %8.3f\tSteps%8d\n\n",
+	    printf("\tREFINEMENT time    %8.2f\tSteps%8d\n\n",
 		   utime[REFINE], stat->RefineSteps);
 	}
 	printf("**************************************************\n");
     }
 
-	double  *utime1,*utime2,*utime3,*utime4;
-	flops_t  *ops1;
 #if ( PROFlevel>=1 )
-	
+    fflush(stdout);
     MPI_Barrier( grid->comm );
 
     {
 	int_t i, P = grid->nprow*grid->npcol;
 	flops_t b, maxflop;
-	
-		
-	if ( !iam )utime1=doubleMalloc_dist(P);
-	if ( !iam )utime2=doubleMalloc_dist(P);
-	if ( !iam )utime3=doubleMalloc_dist(P);
-	if ( !iam )utime4=doubleMalloc_dist(P);
-	if ( !iam )ops1=(flops_t *) SUPERLU_MALLOC(P * sizeof(flops_t));
-
-	
-	// fflush(stdout); 
-	// if ( !iam ) printf("\n.. Tree max sizes:\tbtree\trtree\n");
-	// fflush(stdout);
-	// sleep(2.0); 	
-	// MPI_Barrier( grid->comm );
-	// for (i = 0; i < P; ++i) {
-	    // if ( iam == i) {
-		// printf("\t\t%d %5d %5d\n", iam, stat->MaxActiveBTrees,stat->MaxActiveRTrees);
-		// fflush(stdout);
-	    // }
-	    // MPI_Barrier( grid->comm );
-	// }	
-	
-	// sleep(2.0); 	
-
-	
-	MPI_Barrier( grid->comm );	
-	
 	if ( !iam ) printf("\n.. FACT time breakdown:\tcomm\ttotal\n");
-
-    MPI_Gather(&utime[COMM], 1, MPI_DOUBLE,utime1, 1 , MPI_DOUBLE, 0, grid->comm);	
-    MPI_Gather(&utime[FACT], 1, MPI_DOUBLE,utime2, 1 , MPI_DOUBLE, 0, grid->comm);	
-	if ( !iam ) 
 	for (i = 0; i < P; ++i) {
-		printf("\t\t(%d)%8.2f%8.2f\n", i, utime1[i], utime2[i]);
+	    if ( iam == i) {
+		printf("\t\t(%d)%8.2f%8.2f\n", iam, utime[COMM], utime[FACT]);
+		fflush(stdout);
+	    }
+	    MPI_Barrier( grid->comm );
 	}
-	fflush(stdout);
-	MPI_Barrier( grid->comm );	
-	
 	if ( !iam ) printf("\n.. FACT ops distribution:\n");
-    MPI_Gather(&ops[FACT], 1, MPI_FLOAT,ops1, 1 , MPI_FLOAT, 0, grid->comm);
-	
-	if ( !iam ) 
 	for (i = 0; i < P; ++i) {
-		printf("\t\t(%d)\t%e\n", i, ops1[i]);
+	    if ( iam == i ) {
+		printf("\t\t(%d)\t%e\n", iam, ops[FACT]);
+		fflush(stdout);
+	    }
+	    MPI_Barrier( grid->comm );
 	}
-	fflush(stdout);
-	MPI_Barrier( grid->comm );
-	
 	MPI_Reduce(&ops[FACT], &maxflop, 1, MPI_FLOAT, MPI_MAX, 0, grid->comm);
-
 	if ( !iam ) {
 	    b = factflop/P/maxflop;
 	    printf("\tFACT load balance: %.2f\n", b);
 	}
-	fflush(stdout);
-	MPI_Barrier( grid->comm );
-
-	
-	if ( !iam ) printf("\n.. SOLVE time breakdown:\tcommL \tgemmL\ttrsmL\ttotal\n");
-
-    MPI_Gather(&utime[SOL_COMM], 1, MPI_DOUBLE,utime1, 1 , MPI_DOUBLE, 0, grid->comm);	
-    MPI_Gather(&utime[SOL_GEMM], 1, MPI_DOUBLE,utime2, 1 , MPI_DOUBLE, 0, grid->comm);		
-    MPI_Gather(&utime[SOL_TRSM], 1, MPI_DOUBLE,utime3, 1 , MPI_DOUBLE, 0, grid->comm);		
-    MPI_Gather(&utime[SOL_L], 1, MPI_DOUBLE,utime4, 1 , MPI_DOUBLE, 0, grid->comm);		
-	if ( !iam ) 	
+	if ( !iam ) printf("\n.. SOLVE ops distribution:\n");
 	for (i = 0; i < P; ++i) {
-		printf("\t\t\t%d%10.5f%10.5f%10.5f%10.5f\n", i,utime1[i],utime2[i],utime3[i], utime4[i]);
-	}
-	fflush(stdout); 
-	MPI_Barrier( grid->comm );	
-	
-	if ( !iam ) printf("\n.. SOLVE ops distribution:\n"); 
-    MPI_Gather(&ops[SOLVE], 1, MPI_FLOAT,ops1, 1 , MPI_FLOAT, 0, grid->comm);	
-	if ( !iam ) 
-	for (i = 0; i < P; ++i) {
-		printf("\t\t%d\t%e\n", i, ops1[i]);
+	    if ( iam == i ) {
+		printf("\t\t%d\t%e\n", iam, ops[SOLVE]);
+		fflush(stdout);
+	    }
+	    MPI_Barrier( grid->comm );
 	}
 	MPI_Reduce(&ops[SOLVE], &maxflop, 1, MPI_FLOAT, MPI_MAX, 0,grid->comm);
 	if ( !iam ) {
 	    b = solveflop/P/maxflop;
 	    printf("\tSOLVE load balance: %.2f\n", b);
-		fflush(stdout);
 	}
-	
     }
-	
-	if ( !iam ){
-	SUPERLU_FREE(utime1);
-	SUPERLU_FREE(utime2);
-	SUPERLU_FREE(utime3);
-	SUPERLU_FREE(utime4);
-	SUPERLU_FREE(ops1);
-	}
-	
 #endif
 
 /*  if ( !iam ) fflush(stdout);  CRASH THE SYSTEM pierre.  */
@@ -1207,14 +1085,13 @@ arrive_at_ublock (int_t j,      /* j-th block in a U panel */
         /* Reinitilize the pointers to the beginning of the 
 	 * k-th column/row of L/U factors.
 	 * usub[] - index array for panel U(k,:)
-		*/        
-	    // printf("iukp %d \n",*iukp );
-		*jb = usub[*iukp];      /* Global block number of block U(k,j). */
+	 */
+        // printf("iukp %d \n",*iukp );
+        *jb = usub[*iukp];      /* Global block number of block U(k,j). */
         // printf("jb %d \n",*jb );
         *nsupc = SuperSize (*jb);
         // printf("nsupc %d \n",*nsupc );
         *iukp += UB_DESCRIPTOR; /* Start fstnz of block U(k,j). */
-
         *rukp += usub[*iukp - 1]; /* Jump # of nonzeros in block U(k,jj);
 				     Move to block U(k,jj+1) in nzval[] */ 
         *iukp += *nsupc;
@@ -1265,6 +1142,7 @@ static int_t num_full_cols_U
 			 j, &iukp, &rukp, &jb, &ljb, &nsupc,
 			 iukp0, rukp0, usub, perm_u, xsup, grid
 			 );
+
         for (int_t jj = iukp; jj < iukp + nsupc; ++jj) {
             segsize = klst - usub[jj];
             if ( segsize ) ++temp_ncols;
@@ -1306,113 +1184,8 @@ int_t estimate_bigu_size(int_t nsupers,
     MPI_Allreduce(&ldu, &max_ldu, 1, mpi_int_t, MPI_MAX, grid->cscp.comm);
 
 #if ( PRNTlevel>=1 )
-	if(iam==0)
     printf("max_ncols %d, max_ldu %d, ldt %d, bigu_size=%d\n",
 	   max_ncols, max_ldu, ldt, max_ldu*max_ncols);
 #endif
     return(max_ldu * max_ncols);
 }
-
-
-
-void quickSort( int_t* a, int_t l, int_t r, int_t dir)
-{
-   int_t j;
-
-   if( l < r ) 
-   {
-   	// divide and conquer
-       j = partition( a, l, r, dir);
-       quickSort( a, l, j-1, dir);
-       quickSort( a, j+1, r, dir);
-   }
-	
-}
-
-int_t partition( int_t* a, int_t l, int_t r, int_t dir) {
-   int_t pivot, i, j, t;
-   pivot = a[l];
-   i = l; j = r+1;
-   
-   if(dir==0){		
-	   while( 1)
-	   {
-		do ++i; while( a[i] <= pivot && i <= r );
-		do --j; while( a[j] > pivot );
-		if( i >= j ) break;
-		t = a[i]; a[i] = a[j]; a[j] = t;
-	   }
-	   t = a[l]; a[l] = a[j]; a[j] = t;
-	   return j;
-   }else if(dir==1){
-	   while( 1)
-	   {
-		do ++i; while( a[i] >= pivot && i <= r );
-		do --j; while( a[j] < pivot );
-		if( i >= j ) break;
-		t = a[i]; a[i] = a[j]; a[j] = t;
-	   }
-	   t = a[l]; a[l] = a[j]; a[j] = t;
-	   return j;	   
-   }
-}
-
-
-
-void quickSortM( int_t* a, int_t l, int_t r, int_t lda, int_t dir, int_t dims)
-{
-   int_t j;
-
-   if( l < r ) 
-   {
-	   	// printf("dims: %5d",dims);
-		// fflush(stdout);
-		
-   	// divide and conquer
-       j = partitionM( a, l, r,lda,dir, dims);
-       quickSortM( a, l, j-1,lda,dir,dims);
-       quickSortM( a, j+1, r,lda,dir,dims);
-   }
-	
-}
-
-
-int_t partitionM( int_t* a, int_t l, int_t r, int_t lda, int_t dir, int_t dims) {
-   int_t pivot, i, j, t, dd;
-   pivot = a[l];
-   i = l; j = r+1;
-
-	if(dir==0){
-	   while( 1)
-	   {
-		do ++i; while( a[i] <= pivot && i <= r );
-		do --j; while( a[j] > pivot );
-		if( i >= j ) break; 
-		for(dd=0;dd<dims;dd++){	
-			t = a[i+lda*dd]; a[i+lda*dd] = a[j+lda*dd]; a[j+lda*dd] = t;	
-		}
-	   }
-	   for(dd=0;dd<dims;dd++){	
-		t = a[l+lda*dd]; a[l+lda*dd] = a[j+lda*dd]; a[j+lda*dd] = t;
-	   }	   
-	   return j;		
-	}else if(dir==1){
-	   while( 1)
-	   {
-		do ++i; while( a[i] >= pivot && i <= r );
-		do --j; while( a[j] < pivot );
-		if( i >= j ) break;
-		for(dd=0;dd<dims;dd++){	
-			t = a[i+lda*dd]; a[i+lda*dd] = a[j+lda*dd]; a[j+lda*dd] = t;	
-		}
-	   }
-	   for(dd=0;dd<dims;dd++){	
-		t = a[l+lda*dd]; a[l+lda*dd] = a[j+lda*dd]; a[j+lda*dd] = t;
-	   } 
-	   return j;		
-	}
-}
-
-
-
-
diff --git a/SRC/util_dist.h b/SRC/util_dist.h
index 06ae379..f40b29c 100644
--- a/SRC/util_dist.h
+++ b/SRC/util_dist.h
@@ -90,8 +90,6 @@ typedef struct {
     float   current_buffer; /* bytes allocated for buffer in numerical factorization */
     float   peak_buffer;    /* monitor the peak buffer size (bytes) */
     float   gpu_buffer;     /* monitor the buffer allocated on GPU (bytes) */
-	int_t MaxActiveBTrees;
-	int_t MaxActiveRTrees;	
 } SuperLUStat_t;
 
 /* Headers for 2 types of dynamatically managed memory */
@@ -119,7 +117,7 @@ typedef struct {
 			      4: llvl; level number in L for ILU(k)
 			      5: ulvl; level number in U for ILU(k)
                            */
-#endif	  
+#endif
 
 /* Macros to manipulate stack */
 #define StackFull(x)         ( x + stack.used >= stack.size )
diff --git a/TEST/pztest.sh b/TEST/pztest.sh
old mode 100644
new mode 100755
diff --git a/a.out b/a.out
deleted file mode 100644
index 3f6e7f9..0000000
--- a/a.out
+++ /dev/null
@@ -1,18180 +0,0 @@
-diff --git a/.gitignore b/.gitignore
-index adcaf5c..d7739ac 100644
---- a/.gitignore
-+++ b/.gitignore
-@@ -1,12 +1,8 @@
- *~
- 
--# You have to ignore this generated file or git will complain that it is an
-+# You have to ignore this genrated file or git will complain that it is an
- # unknown file!
- /make.inc
- 
- # If the instructions are telling people to create this build dir under the
- # source tree, you had better put in an ignore for this.
--/build/
--
--# Ignore Testing/ folder
--Testing/
-diff --git a/.travis.yml b/.travis.yml
-deleted file mode 100644
-index eeb8d2c..0000000
---- a/.travis.yml
-+++ /dev/null
-@@ -1,103 +0,0 @@
--language: cpp
--
--compiler: gcc
--
--os: linux
--
--sudo: required
--
--branches:
--  only:
--  - master
--
--notifications:
--  slack: 
--    rooms:
--      - ecpsparsesolvers:nBWC0jcAd7B1j9whHUYcaVJO
--    on_failure: always
--    on_success: never
--
--env:
--  matrix:
--  - TEST_NUMBER=1"
--  - TEST_NUMBER=2"
--  - TEST_NUMBER=3"
--  - TEST_NUMBER=4"
--  - TEST_NUMBER=5"
--  - TEST_NUMBER=6"
--  - TEST_NUMBER=7"
--  - TEST_NUMBER=8"
--  - TEST_NUMBER=9"
--  - TEST_NUMBER=10"
--  - TEST_NUMBER=11"
--  - TEST_NUMBER=12"
--  - TEST_NUMBER=13"
--  - TEST_NUMBER=14"
--
--git:
--  depth: 1
--
--before_install:
--  - export BLUE="\033[34;1m"
--  - mkdir installDir
--
--  - printf "${BLUE} GC; Installing gcc-6 via apt\n"
--  - sudo apt-get update
--  - sudo apt-get install build-essential software-properties-common -y
--  - sudo add-apt-repository ppa:ubuntu-toolchain-r/test -y
--  - sudo apt-get update
--  - sudo apt-get install gcc-6 g++-6 -y
--  - sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-6 60 --slave /usr/bin/g++ g++ /usr/bin/g++-6
--  - export CXX="g++-6"
--  - export CC="gcc-6"
--  - printf "${BLUE} GC; Done installing gcc-6 via apt\n"
--
--  - printf "${BLUE} GC; Installing gfortran via apt\n"
--  - sudo apt-get install gfortran-6 -y
--  - sudo update-alternatives --install /usr/bin/gfortran gfortran /usr/bin/gfortran-6 60
--  - printf "${BLUE} GC; Done installing gfortran via apt\n"
--
--  - printf "${BLUE} GC; Installing openmpi\n"
--  - sudo apt-get install openmpi-bin libopenmpi-dev
--  - printf "${BLUE} GC; Done installing openmpi\n"
--
--  - printf "${BLUE} GC; Installing BLASfrom apt\n"
--  - sudo apt-get install libblas-dev
--  - export BLAS_LIB=/usr/lib/libblas/libblas.so
--  - printf "${BLUE} GC; Done installing BLASfrom apt\n"
--
--  - printf "${BLUE} GC; Installing ParMetis-4.0 from source\n"
--  - cd $TRAVIS_BUILD_DIR/installDir
--  - wget http://glaros.dtc.umn.edu/gkhome/fetch/sw/parmetis/parmetis-4.0.3.tar.gz
--  - tar -xf parmetis-4.0.3.tar.gz
--  - cd parmetis-4.0.3/
--  - mkdir install
--  - make config shared=1 cc=mpicc cxx=mpic++ prefix=$PWD/install
--  - make install > make_parmetis_install.log 2>&1
--  - printf "${BLUE} GC; Done installing ParMetis-4.0 from source\n"
--
--install:
--  - export BLUE="\033[34;1m"
--  - printf "${BLUE} GC; Installing superlu_dist from source\n"
--  - cd $TRAVIS_BUILD_DIR
--  - mkdir build
--  - cd build
--  - |
--    cmake .. \
--    -DTPL_PARMETIS_INCLUDE_DIRS="$TRAVIS_BUILD_DIR/installDir/parmetis-4.0.3/metis/include;$TRAVIS_BUILD_DIR/installDir/parmetis-4.0.3/install/include" \
--    -DTPL_PARMETIS_LIBRARIES="$TRAVIS_BUILD_DIR/installDir/parmetis-4.0.3/install/lib/libparmetis.so" \
--    -DCMAKE_C_FLAGS="-std=c99 -DPRNTlevel=0 -DPROFlevel=0 -DDEBUGlevel=0" \
--    -DTPL_BLAS_LIBRARIES="$BLAS_LIB" \
--    -Denable_blaslib=OFF \
--    -DBUILD_SHARED_LIBS=OFF \
--    -DCMAKE_C_COMPILER=cc \
--    -DCMAKE_INSTALL_PREFIX=. \
--    -DCMAKE_BUILD_TYPE=Release \
--    -DCMAKE_VERBOSE_MAKEFILE:BOOL=OFF
--  - make
--  - make install
--  - printf "${BLUE} GC; Done installing superlu_dist from source\n"
--
--script: 
--  - cd $TRAVIS_BUILD_DIR
--  - ./.travis_tests.sh
-diff --git a/.travis_tests.sh b/.travis_tests.sh
-deleted file mode 100755
-index d9504b1..0000000
---- a/.travis_tests.sh
-+++ /dev/null
-@@ -1,28 +0,0 @@
--#!/bin/sh
--set -e
--
--export RED="\033[31;1m"
--export BLUE="\033[34;1m"
--printf "${BLUE} GC; Entered tests file:\n"
--
--export DATA_FOLDER=$TRAVIS_BUILD_DIR/EXAMPLE
--export EXAMPLE_FOLDER=$TRAVIS_BUILD_DIR/build/EXAMPLE
--export TEST_FOLDER=$TRAVIS_BUILD_DIR/build/TEST
--
--case "${TEST_NUMBER}" in
--1)  mpirun "-n" "1" "$TEST_FOLDER/pdtest" "-r" "1" "-c" "1" "-s" "1" "-b" "2" "-x" "8" "-m" "20" "-f" "$DATA_FOLDER/g20.rua" ;;
--2)  mpirun "-n" "1" "$TEST_FOLDER/pdtest" "-r" "1" "-c" "1" "-s" "3" "-b" "2" "-x" "8" "-m" "20" "-f" "$DATA_FOLDER/g20.rua" ;;
--3)  mpirun "-n" "3" "$TEST_FOLDER/pdtest" "-r" "1" "-c" "3" "-s" "1" "-b" "2" "-x" "8" "-m" "20" "-f" "$DATA_FOLDER/g20.rua" ;;
--4)  mpirun "-n" "3" "$TEST_FOLDER/pdtest" "-r" "1" "-c" "3" "-s" "3" "-b" "2" "-x" "8" "-m" "20" "-f" "$DATA_FOLDER/g20.rua" ;;
--5)  mpirun "-n" "2" "$TEST_FOLDER/pdtest" "-r" "2" "-c" "1" "-s" "1" "-b" "2" "-x" "8" "-m" "20" "-f" "$DATA_FOLDER/g20.rua" ;;
--6)  mpirun "-n" "2" "$TEST_FOLDER/pdtest" "-r" "2" "-c" "1" "-s" "3" "-b" "2" "-x" "8" "-m" "20" "-f" "$DATA_FOLDER/g20.rua" ;;
--7)  mpirun "-n" "6" "$TEST_FOLDER/pdtest" "-r" "2" "-c" "3" "-s" "1" "-b" "2" "-x" "8" "-m" "20" "-f" "$DATA_FOLDER/g20.rua" ;;
--8)  mpirun "-n" "6" "$TEST_FOLDER/pdtest" "-r" "2" "-c" "3" "-s" "3" "-b" "2" "-x" "8" "-m" "20" "-f" "$DATA_FOLDER/g20.rua" ;;
--9)  mpirun "-n" "4" "$EXAMPLE_FOLDER/pddrive1" "-r" "2" "-c" "2" "$DATA_FOLDER/big.rua" ;;
--10) mpirun "-n" "4" "$EXAMPLE_FOLDER/pddrive2" "-r" "2" "-c" "2" "$DATA_FOLDER/big.rua" ;;
--11) mpirun "-n" "4" "$EXAMPLE_FOLDER/pddrive3" "-r" "2" "-c" "2" "$DATA_FOLDER/big.rua" ;;
--12) mpirun "-n" "4" "$EXAMPLE_FOLDER/pzdrive1" "-r" "2" "-c" "2" "$DATA_FOLDER/cg20.cua" ;;
--13) mpirun "-n" "4" "$EXAMPLE_FOLDER/pzdrive2" "-r" "2" "-c" "2" "$DATA_FOLDER/cg20.cua" ;;
--14) mpirun "-n" "4" "$EXAMPLE_FOLDER/pzdrive3" "-r" "2" "-c" "2" "$DATA_FOLDER/cg20.cua" ;;
--*) printf "${RED} ###GC: Unknown test\n" ;;
--esac
-diff --git a/CMakeLists.txt b/CMakeLists.txt
-index d870ea9..95f101b 100644
---- a/CMakeLists.txt
-+++ b/CMakeLists.txt
-@@ -15,7 +15,6 @@ set(VERSION_BugFix "0")
- set(PROJECT_VERSION ${VERSION_MAJOR}.${VERSION_MINOR}.${VERSION_BugFix})
- 
- list(APPEND CMAKE_MODULE_PATH "${PROJECT_SOURCE_DIR}/cmake")
--
- ######################################################################
- #
- # IDEAS: xSDK standards module
-@@ -52,7 +51,6 @@ SET(CMAKE_INSTALL_RPATH_USE_LINK_PATH TRUE)
- #----
- 
- SET(BUILD_STATIC_LIBS TRUE CACHE BOOL "Include static libs when building shared")
--
- if (BUILD_SHARED_LIBS)
-   message("-- SuperLU_DIST will be built as a shared library.")
-   set(PROJECT_NAME_LIB_EXPORT libsuperlu_dist.so)
-@@ -65,6 +63,8 @@ else()
- endif()
- 
- enable_language (C)
-+enable_language (CXX)
-+set(CMAKE_CXX_STANDARD 11)
- if (XSDK_ENABLE_Fortran)
-   enable_language (Fortran)
-   set(NOFORTRAN FALSE)
-@@ -110,7 +110,6 @@ set(INSTALL_INC_DIR "${default_install_inc_dir}" CACHE STRING "The folder where
- set(INSTALL_LIB_DIR "${default_install_lib_dir}" CACHE STRING "The folder where libraries will be installed.")
- set(INSTALL_BIN_DIR "${default_install_bin_dir}" CACHE STRING "The folder where runtime files will be installed.")
- 
--
- # Set up required compiler defines and options.
- ## get_directory_property( DirDefs COMPILE_DEFINITIONS )
- # set(CMAKE_C_FLAGS "-DDEBUGlevel=0 -DPRNTlevel=0 ${CMAKE_C_FLAGS}")
-@@ -220,7 +219,6 @@ if(PARMETIS_FOUND)
-   set(HAVE_PARMETIS TRUE)
- endif()
- 
--
- ######################################################################
- #
- # Include directories
-@@ -233,7 +231,6 @@ if (TPL_PARMETIS_INCLUDE_DIRS)
-   include_directories(${TPL_PARMETIS_INCLUDE_DIRS})  ## parmetis
- endif ()
- include_directories(${MPI_C_INCLUDE_PATH})
--
- ######################################################################
- #
- # Add subdirectories
-diff --git a/DoxyConfig b/DoxyConfig
-index 36b49c5..5bbc5a0 100644
---- a/DoxyConfig
-+++ b/DoxyConfig
-@@ -31,7 +31,7 @@ PROJECT_NAME           = SuperLU Distributed
- # This could be handy for archiving the generated documentation or 
- # if some version control system is used.
- 
--PROJECT_NUMBER         = 5.3.0
-+PROJECT_NUMBER         = 5.0.0
- e
- # The OUTPUT_DIRECTORY tag is used to specify the (relative or absolute) 
- # base path where the generated documentation will be put. 
-@@ -513,7 +513,7 @@ WARN_LOGFILE           =
- # directories like "/usr/src/myproject". Separate the files or directories 
- # with spaces.
- 
--INPUT                  = SRC/ EXAMPLE/ FORTRAN/ TEST/
-+INPUT                  = SRC/ EXAMPLE/ FORTRAN/
- 
- # This tag can be used to specify the character encoding of the source files 
- # that doxygen parses. Internally doxygen uses the UTF-8 encoding, which is 
-diff --git a/EXAMPLE/Makefile b/EXAMPLE/Makefile
-index 79ece5a..7984801 100644
---- a/EXAMPLE/Makefile
-+++ b/EXAMPLE/Makefile
-@@ -133,7 +133,8 @@ pzdrive4_ABglobal: $(ZEXMG4) $(DSUPERLULIB)
- #	$(CC) $(CFLAGS) $(CDEFS) $(BLASDEF) $(INCLUDEDIR) -c pdgstrf.c $(VERBOSE)
- .c.o:
- 	$(CC) $(CFLAGS) $(CDEFS) $(BLASDEF) $(INCLUDEDIR) -c $< $(VERBOSE)
--
-+.cpp.o:
-+	$(CPP) $(CPPFLAGS) $(CDEFS) $(BLASDEF) $(INCLUDEDIR) -c $< $(VERBOSE)
- .f.o:
- 	$(FORTRAN) $(FFLAGS) -c $< $(VERBOSE)
- 
-diff --git a/EXAMPLE/dcreate_matrix.c b/EXAMPLE/dcreate_matrix.c
-index a622463..d90185e 100644
---- a/EXAMPLE/dcreate_matrix.c
-+++ b/EXAMPLE/dcreate_matrix.c
-@@ -63,7 +63,8 @@ at the top-level directory.
-  * </pre>
-  */
- 
--int dcreate_matrix(SuperMatrix *A, int nrhs, double **rhs,
-+ 
-+ int dcreate_matrix(SuperMatrix *A, int nrhs, double **rhs,
-                    int *ldb, double **x, int *ldx,
-                    FILE *fp, gridinfo_t *grid)
- {
-@@ -89,14 +90,203 @@ int dcreate_matrix(SuperMatrix *A, int nrhs, double **rhs,
- #endif
- 
-     if ( !iam ) {
--        double t = SuperLU_timer_();
-+    double t = SuperLU_timer_();
- 
--        /* Read the matrix stored on disk in Harwell-Boeing format. */
--        dreadhb_dist(iam, fp, &m, &n, &nnz, &nzval, &rowind, &colptr);
-+	/* Read the matrix stored on disk in Harwell-Boeing format. */
-+	dreadhb_dist(iam, fp, &m, &n, &nnz, &nzval, &rowind, &colptr);
-+	
-+	printf("Time to read and distribute matrix %.2f\n", 
-+	        SuperLU_timer_() - t);  fflush(stdout);	
-+		
-+	/* Broadcast matrix A to the other PEs. */
-+	MPI_Bcast( &m,     1,   mpi_int_t,  0, grid->comm );
-+	MPI_Bcast( &n,     1,   mpi_int_t,  0, grid->comm );
-+	MPI_Bcast( &nnz,   1,   mpi_int_t,  0, grid->comm );
-+	MPI_Bcast( nzval,  nnz, MPI_DOUBLE, 0, grid->comm );
-+	MPI_Bcast( rowind, nnz, mpi_int_t,  0, grid->comm );
-+	MPI_Bcast( colptr, n+1, mpi_int_t,  0, grid->comm );
-+    } else {
-+	/* Receive matrix A from PE 0. */
-+	MPI_Bcast( &m,   1,   mpi_int_t,  0, grid->comm );
-+	MPI_Bcast( &n,   1,   mpi_int_t,  0, grid->comm );
-+	MPI_Bcast( &nnz, 1,   mpi_int_t,  0, grid->comm );
-+
-+	/* Allocate storage for compressed column representation. */
-+	dallocateA_dist(n, nnz, &nzval, &rowind, &colptr);
-+
-+	MPI_Bcast( nzval,   nnz, MPI_DOUBLE, 0, grid->comm );
-+	MPI_Bcast( rowind,  nnz, mpi_int_t,  0, grid->comm );
-+	MPI_Bcast( colptr,  n+1, mpi_int_t,  0, grid->comm );
-+    }
-+
-+#if 0
-+    nzval[0]=0.1;
-+#endif
-+
-+    /* Compute the number of rows to be distributed to local process */
-+    m_loc = m / (grid->nprow * grid->npcol); 
-+    m_loc_fst = m_loc;
-+    /* When m / procs is not an integer */
-+    if ((m_loc * grid->nprow * grid->npcol) != m) {
-+        /*m_loc = m_loc+1;
-+          m_loc_fst = m_loc;*/
-+      if (iam == (grid->nprow * grid->npcol - 1)) /* last proc. gets all*/
-+	  m_loc = m - m_loc * (grid->nprow * grid->npcol - 1);
-+    }
-+
-+    /* Create compressed column matrix for GA. */
-+    dCreate_CompCol_Matrix_dist(&GA, m, n, nnz, nzval, rowind, colptr,
-+				SLU_NC, SLU_D, SLU_GE);
-+
-+    /* Generate the exact solution and compute the right-hand side. */
-+    if ( !(b_global = doubleMalloc_dist(m*nrhs)) )
-+        ABORT("Malloc fails for b[]");
-+    if ( !(xtrue_global = doubleMalloc_dist(n*nrhs)) )
-+        ABORT("Malloc fails for xtrue[]");
-+    *trans = 'N';
-+
-+    dGenXtrue_dist(n, nrhs, xtrue_global, n);
-+    dFillRHS_dist(trans, nrhs, xtrue_global, n, &GA, b_global, m);
-+
-+    /*************************************************
-+     * Change GA to a local A with NR_loc format     *
-+     *************************************************/
-+
-+    rowptr = (int_t *) intMalloc_dist(m_loc+1);
-+    marker = (int_t *) intCalloc_dist(n);
-+
-+    /* Get counts of each row of GA */
-+    for (i = 0; i < n; ++i)
-+      for (j = colptr[i]; j < colptr[i+1]; ++j) ++marker[rowind[j]];
-+    /* Set up row pointers */
-+    rowptr[0] = 0;
-+    fst_row = iam * m_loc_fst;
-+    nnz_loc = 0;
-+    for (j = 0; j < m_loc; ++j) {
-+      row = fst_row + j;
-+      rowptr[j+1] = rowptr[j] + marker[row];
-+      marker[j] = rowptr[j];
-+    }
-+    nnz_loc = rowptr[m_loc];
-+
-+    nzval_loc = (double *) doubleMalloc_dist(nnz_loc);
-+    colind = (int_t *) intMalloc_dist(nnz_loc);
-+
-+    /* Transfer the matrix into the compressed row storage */
-+    for (i = 0; i < n; ++i) {
-+      for (j = colptr[i]; j < colptr[i+1]; ++j) {
-+	row = rowind[j];
-+	if ( (row>=fst_row) && (row<fst_row+m_loc) ) {
-+	  row = row - fst_row;
-+	  relpos = marker[row];
-+	  colind[relpos] = i;
-+	  nzval_loc[relpos] = nzval[j];
-+	  ++marker[row];
-+	}
-+      }
-+    }
-+
-+#if ( DEBUGlevel>=2 )
-+    if ( !iam ) dPrint_CompCol_Matrix_dist(&GA);
-+#endif   
-+
-+    /* Destroy GA */
-+    Destroy_CompCol_Matrix_dist(&GA);
-+
-+    /******************************************************/
-+    /* Change GA to a local A with NR_loc format */
-+    /******************************************************/
-+
-+    /* Set up the local A in NR_loc format */
-+    dCreate_CompRowLoc_Matrix_dist(A, m, n, nnz_loc, m_loc, fst_row,
-+				   nzval_loc, colind, rowptr,
-+				   SLU_NR_loc, SLU_D, SLU_GE);
-+    
-+    /* Get the local B */
-+    if ( !((*rhs) = doubleMalloc_dist(m_loc*nrhs)) )
-+        ABORT("Malloc fails for rhs[]");
-+    for (j =0; j < nrhs; ++j) {
-+	for (i = 0; i < m_loc; ++i) {
-+	    row = fst_row + i;
-+	    (*rhs)[j*m_loc+i] = b_global[j*n+row];
-+	}
-+    }
-+    *ldb = m_loc;
-+
-+    /* Set the true X */    
-+    *ldx = m_loc;
-+    if ( !((*x) = doubleMalloc_dist(*ldx * nrhs)) )
-+        ABORT("Malloc fails for x_loc[]");
-+
-+    /* Get the local part of xtrue_global */
-+    for (j = 0; j < nrhs; ++j) {
-+      for (i = 0; i < m_loc; ++i)
-+	(*x)[i + j*(*ldx)] = xtrue_global[i + fst_row + j*n];
-+    }
-+
-+    SUPERLU_FREE(b_global);
-+    SUPERLU_FREE(xtrue_global);
-+    SUPERLU_FREE(marker);
-+
-+#if ( DEBUGlevel>=1 )
-+    printf("sizeof(NRforamt_loc) %lu\n", sizeof(NRformat_loc));
-+    CHECK_MALLOC(iam, "Exit dcreate_matrix()");
-+#endif
-+    return 0;
-+}
-+
-+ 
-+ 
-+ 
-+int dcreate_matrix_postfix(SuperMatrix *A, int nrhs, double **rhs,
-+                   int *ldb, double **x, int *ldx,
-+                   FILE *fp, char * postfix, gridinfo_t *grid)
-+{
-+    SuperMatrix GA;              /* global A */
-+    double   *b_global, *xtrue_global;  /* replicated on all processes */
-+    int_t    *rowind, *colptr;	 /* global */
-+    double   *nzval;             /* global */
-+    double   *nzval_loc;         /* local */
-+    int_t    *colind, *rowptr;	 /* local */
-+    int_t    m, n, nnz;
-+    int_t    m_loc, fst_row, nnz_loc;
-+    int_t    m_loc_fst; /* Record m_loc of the first p-1 processors,
-+			   when mod(m, p) is not zero. */ 
-+    int_t    row, col, i, j, relpos;
-+    int      iam;
-+    char     trans[1];
-+    int_t      *marker;
-+
-+    iam = grid->iam;
-+
-+#if ( DEBUGlevel>=1 )
-+    CHECK_MALLOC(iam, "Enter dcreate_matrix()");
-+#endif
-+
-+    if ( !iam ) {
-+    double t = SuperLU_timer_();       
-+	if(!strcmp(postfix,"rua")){
-+		/* Read the matrix stored on disk in Harwell-Boeing format. */
-+		dreadhb_dist(iam, fp, &m, &n, &nnz, &nzval, &rowind, &colptr);
-+	}else if(!strcmp(postfix,"mtx")){
-+		/* Read the matrix stored on disk in Matrix Market format. */
-+		dreadMM_dist(fp, &m, &n, &nnz, &nzval, &rowind, &colptr);
-+	}else if(!strcmp(postfix,"rb")){
-+		/* Read the matrix stored on disk in Rutherford-Boeing format. */
-+		dreadrb_dist(iam, fp, &m, &n, &nnz, &nzval, &rowind, &colptr);		
-+	}else if(!strcmp(postfix,"dat")){
-+		/* Read the matrix stored on disk in triplet format. */
-+		dreadtriple_dist(fp, &m, &n, &nnz, &nzval, &rowind, &colptr);
-+	}else if(!strcmp(postfix,"bin")){
-+		/* Read the matrix stored on disk in binary format. */
-+		dread_binary(fp, &m, &n, &nnz, &nzval, &rowind, &colptr);		
-+	}else {
-+		ABORT("File format not known");
-+	}
- 
- 	printf("Time to read and distribute matrix %.2f\n", 
- 	        SuperLU_timer_() - t);  fflush(stdout);
--
-+		
- 	/* Broadcast matrix A to the other PEs. */
- 	MPI_Bcast( &m,     1,   mpi_int_t,  0, grid->comm );
- 	MPI_Bcast( &n,     1,   mpi_int_t,  0, grid->comm );
-diff --git a/EXAMPLE/dcreate_matrix_perturbed.c b/EXAMPLE/dcreate_matrix_perturbed.c
-index d4ea5e1..4c6ae01 100644
---- a/EXAMPLE/dcreate_matrix_perturbed.c
-+++ b/EXAMPLE/dcreate_matrix_perturbed.c
-@@ -228,3 +228,192 @@ int dcreate_matrix_perturbed(SuperMatrix *A, int nrhs, double **rhs,
- #endif
-     return 0;
- }
-+
-+
-+
-+int dcreate_matrix_perturbed_postfix(SuperMatrix *A, int nrhs, double **rhs,
-+                   int *ldb, double **x, int *ldx,
-+                   FILE *fp, char *postfix, gridinfo_t *grid)
-+{
-+    SuperMatrix GA;              /* global A */
-+    double   *b_global, *xtrue_global;  /* replicated on all processes */
-+    int_t    *rowind, *colptr;	 /* global */
-+    double   *nzval;             /* global */
-+    double   *nzval_loc;         /* local */
-+    int_t    *colind, *rowptr;	 /* local */
-+    int_t    m, n, nnz;
-+    int_t    m_loc, fst_row, nnz_loc;
-+    int_t    m_loc_fst; /* Record m_loc of the first p-1 processors,
-+			   when mod(m, p) is not zero. */ 
-+    int_t    row, col, i, j, relpos;
-+    int      iam;
-+    char     trans[1];
-+    int_t      *marker;
-+
-+    iam = grid->iam;
-+
-+#if ( DEBUGlevel>=1 )
-+    CHECK_MALLOC(iam, "Enter dcreate_matrix()");
-+#endif
-+
-+    if ( !iam ) {
-+    double t = SuperLU_timer_();       
-+	if(!strcmp(postfix,"rua")){
-+		/* Read the matrix stored on disk in Harwell-Boeing format. */
-+		dreadhb_dist(iam, fp, &m, &n, &nnz, &nzval, &rowind, &colptr);
-+	}else if(!strcmp(postfix,"mtx")){
-+		/* Read the matrix stored on disk in Matrix Market format. */
-+		dreadMM_dist(fp, &m, &n, &nnz, &nzval, &rowind, &colptr);
-+	}else if(!strcmp(postfix,"rb")){
-+		/* Read the matrix stored on disk in Rutherford-Boeing format. */
-+		dreadrb_dist(iam, fp, &m, &n, &nnz, &nzval, &rowind, &colptr);		
-+	}else if(!strcmp(postfix,"dat")){
-+		/* Read the matrix stored on disk in triplet format. */
-+		dreadtriple_dist(fp, &m, &n, &nnz, &nzval, &rowind, &colptr);
-+	}else if(!strcmp(postfix,"bin")){
-+		/* Read the matrix stored on disk in binary format. */
-+		dread_binary(fp, &m, &n, &nnz, &nzval, &rowind, &colptr);		
-+	}else {
-+		ABORT("File format not known");
-+	}
-+
-+	printf("Time to read and distribute matrix %.2f\n", 
-+	        SuperLU_timer_() - t);  fflush(stdout);
-+
-+	/* Broadcast matrix A to the other PEs. */
-+	MPI_Bcast( &m,     1,   mpi_int_t,  0, grid->comm );
-+	MPI_Bcast( &n,     1,   mpi_int_t,  0, grid->comm );
-+	MPI_Bcast( &nnz,   1,   mpi_int_t,  0, grid->comm );
-+	MPI_Bcast( nzval,  nnz, MPI_DOUBLE, 0, grid->comm );
-+	MPI_Bcast( rowind, nnz, mpi_int_t,  0, grid->comm );
-+	MPI_Bcast( colptr, n+1, mpi_int_t,  0, grid->comm );
-+    } else {
-+	/* Receive matrix A from PE 0. */
-+	MPI_Bcast( &m,   1,   mpi_int_t,  0, grid->comm );
-+	MPI_Bcast( &n,   1,   mpi_int_t,  0, grid->comm );
-+	MPI_Bcast( &nnz, 1,   mpi_int_t,  0, grid->comm );
-+
-+	/* Allocate storage for compressed column representation. */
-+	dallocateA_dist(n, nnz, &nzval, &rowind, &colptr);
-+
-+	MPI_Bcast( nzval,   nnz, MPI_DOUBLE, 0, grid->comm );
-+	MPI_Bcast( rowind,  nnz, mpi_int_t,  0, grid->comm );
-+	MPI_Bcast( colptr,  n+1, mpi_int_t,  0, grid->comm );
-+    }
-+
-+    /* Perturbed the 1st and last diagonal of the matrix to lower
-+       values. Intention is to change perm_r[].   */
-+    nzval[0] *= 0.01;
-+    nzval[nnz-1] *= 0.0001; 
-+
-+    /* Compute the number of rows to be distributed to local process */
-+    m_loc = m / (grid->nprow * grid->npcol); 
-+    m_loc_fst = m_loc;
-+    /* When m / procs is not an integer */
-+    if ((m_loc * grid->nprow * grid->npcol) != m) {
-+        /*m_loc = m_loc+1;
-+          m_loc_fst = m_loc;*/
-+      if (iam == (grid->nprow * grid->npcol - 1)) /* last proc. gets all*/
-+	  m_loc = m - m_loc * (grid->nprow * grid->npcol - 1);
-+    }
-+
-+    /* Create compressed column matrix for GA. */
-+    dCreate_CompCol_Matrix_dist(&GA, m, n, nnz, nzval, rowind, colptr,
-+				SLU_NC, SLU_D, SLU_GE);
-+
-+    /* Generate the exact solution and compute the right-hand side. */
-+    if ( !(b_global = doubleMalloc_dist(m*nrhs)) )
-+        ABORT("Malloc fails for b[]");
-+    if ( !(xtrue_global = doubleMalloc_dist(n*nrhs)) )
-+        ABORT("Malloc fails for xtrue[]");
-+    *trans = 'N';
-+
-+    dGenXtrue_dist(n, nrhs, xtrue_global, n);
-+    dFillRHS_dist(trans, nrhs, xtrue_global, n, &GA, b_global, m);
-+
-+    /*************************************************
-+     * Change GA to a local A with NR_loc format     *
-+     *************************************************/
-+
-+    rowptr = (int_t *) intMalloc_dist(m_loc+1);
-+    marker = (int_t *) intCalloc_dist(n);
-+
-+    /* Get counts of each row of GA */
-+    for (i = 0; i < n; ++i)
-+      for (j = colptr[i]; j < colptr[i+1]; ++j) ++marker[rowind[j]];
-+    /* Set up row pointers */
-+    rowptr[0] = 0;
-+    fst_row = iam * m_loc_fst;
-+    nnz_loc = 0;
-+    for (j = 0; j < m_loc; ++j) {
-+      row = fst_row + j;
-+      rowptr[j+1] = rowptr[j] + marker[row];
-+      marker[j] = rowptr[j];
-+    }
-+    nnz_loc = rowptr[m_loc];
-+
-+    nzval_loc = (double *) doubleMalloc_dist(nnz_loc);
-+    colind = (int_t *) intMalloc_dist(nnz_loc);
-+
-+    /* Transfer the matrix into the compressed row storage */
-+    for (i = 0; i < n; ++i) {
-+      for (j = colptr[i]; j < colptr[i+1]; ++j) {
-+	row = rowind[j];
-+	if ( (row>=fst_row) && (row<fst_row+m_loc) ) {
-+	  row = row - fst_row;
-+	  relpos = marker[row];
-+	  colind[relpos] = i;
-+	  nzval_loc[relpos] = nzval[j];
-+	  ++marker[row];
-+	}
-+      }
-+    }
-+
-+#if ( DEBUGlevel>=2 )
-+    if ( !iam ) dPrint_CompCol_Matrix_dist(&GA);
-+#endif   
-+
-+    /* Destroy GA */
-+    Destroy_CompCol_Matrix_dist(&GA);
-+
-+    /******************************************************/
-+    /* Change GA to a local A with NR_loc format */
-+    /******************************************************/
-+
-+    /* Set up the local A in NR_loc format */
-+    dCreate_CompRowLoc_Matrix_dist(A, m, n, nnz_loc, m_loc, fst_row,
-+				   nzval_loc, colind, rowptr,
-+				   SLU_NR_loc, SLU_D, SLU_GE);
-+    
-+    /* Get the local B */
-+    if ( !((*rhs) = doubleMalloc_dist(m_loc*nrhs)) )
-+        ABORT("Malloc fails for rhs[]");
-+    for (j =0; j < nrhs; ++j) {
-+	for (i = 0; i < m_loc; ++i) {
-+	    row = fst_row + i;
-+	    (*rhs)[j*m_loc+i] = b_global[j*n+row];
-+	}
-+    }
-+    *ldb = m_loc;
-+
-+    /* Set the true X */    
-+    *ldx = m_loc;
-+    if ( !((*x) = doubleMalloc_dist(*ldx * nrhs)) )
-+        ABORT("Malloc fails for x_loc[]");
-+
-+    /* Get the local part of xtrue_global */
-+    for (j = 0; j < nrhs; ++j) {
-+      for (i = 0; i < m_loc; ++i)
-+	(*x)[i + j*(*ldx)] = xtrue_global[i + fst_row + j*n];
-+    }
-+
-+    SUPERLU_FREE(b_global);
-+    SUPERLU_FREE(xtrue_global);
-+    SUPERLU_FREE(marker);
-+
-+#if ( DEBUGlevel>=1 )
-+    printf("sizeof(NRforamt_loc) %lu\n", sizeof(NRformat_loc));
-+    CHECK_MALLOC(iam, "Exit dcreate_matrix()");
-+#endif
-+    return 0;
-+}
-\ No newline at end of file
-diff --git a/EXAMPLE/pddrive.c b/EXAMPLE/pddrive.c
-index 7d6073a..867ec76 100644
---- a/EXAMPLE/pddrive.c
-+++ b/EXAMPLE/pddrive.c
-@@ -22,7 +22,7 @@ at the top-level directory.
-  */
- 
- #include <math.h>
--#include <superlu_dist_config.h>
-+#include <superlu_dist_config.h>								
- #include "superlu_ddefs.h"
- 
- /*! \brief
-@@ -62,18 +62,30 @@ int main(int argc, char *argv[])
-     int    m, n;
-     int      nprow, npcol;
-     int      iam, info, ldb, ldx, nrhs;
--    char     **cpp, c;
-+    char     **cpp, c, *postfix;
-     FILE *fp, *fopen();
-     int cpp_defs();
--
-+	int ii;
-+	int omp_mpi_level;
-+	
-     nprow = 1;  /* Default process rows.      */
-     npcol = 1;  /* Default process columns.   */
--    nrhs = 1;   /* Number of right-hand side. */
--
-+    nrhs =1;   /* Number of right-hand side. */
-+ 
-     /* ------------------------------------------------------------
-        INITIALIZE MPI ENVIRONMENT. 
-        ------------------------------------------------------------*/
-     MPI_Init( &argc, &argv );
-+    //MPI_Init_thread( &argc, &argv, MPI_THREAD_MULTIPLE, &omp_mpi_level); 
-+	
-+
-+#if ( VAMPIR>=1 )
-+    VT_traceoff(); 
-+#endif
-+
-+#if ( VTUNE>=1 )
-+	__itt_pause();
-+#endif
- 
-     /* Parse command line argv[]. */
-     for (cpp = argv+1; *cpp; ++cpp) {
-@@ -105,6 +117,28 @@ int main(int argc, char *argv[])
-        ------------------------------------------------------------*/
-     superlu_gridinit(MPI_COMM_WORLD, nprow, npcol, &grid);
- 
-+	if(grid.iam==0){
-+	MPI_Query_thread(&omp_mpi_level);
-+    switch (omp_mpi_level) {
-+      case MPI_THREAD_SINGLE:
-+		printf("MPI_Query_thread with MPI_THREAD_SINGLE\n");
-+		fflush(stdout);
-+	break;
-+      case MPI_THREAD_FUNNELED:
-+		printf("MPI_Query_thread with MPI_THREAD_FUNNELED\n");
-+		fflush(stdout);
-+	break;
-+      case MPI_THREAD_SERIALIZED:
-+		printf("MPI_Query_thread with MPI_THREAD_SERIALIZED\n");
-+		fflush(stdout);
-+	break;
-+      case MPI_THREAD_MULTIPLE:
-+		printf("MPI_Query_thread with MPI_THREAD_MULTIPLE\n");
-+		fflush(stdout);
-+	break;
-+    }
-+	}
-+
-     /* Bail out if I do not belong in the grid. */
-     iam = grid.iam;
-     if ( iam >= nprow * npcol )	goto out;
-@@ -126,10 +160,18 @@ int main(int argc, char *argv[])
-     CHECK_MALLOC(iam, "Enter main()");
- #endif
- 
-+	for(ii = 0;ii<strlen(*cpp);ii++){
-+		if((*cpp)[ii]=='.'){
-+			postfix = &((*cpp)[ii+1]);
-+		}
-+	}
-+	// printf("%s\n", postfix);
-+	
-+
-     /* ------------------------------------------------------------
-        GET THE MATRIX FROM FILE AND SETUP THE RIGHT HAND SIDE. 
-        ------------------------------------------------------------*/
--    dcreate_matrix(&A, nrhs, &b, &ldb, &xtrue, &ldx, fp, &grid);
-+    dcreate_matrix_postfix(&A, nrhs, &b, &ldb, &xtrue, &ldx, fp, postfix, &grid);
- 
-     if ( !(berr = doubleMalloc_dist(nrhs)) )
- 	ABORT("Malloc fails for berr[].");
-@@ -154,12 +196,21 @@ int main(int argc, char *argv[])
-     set_default_options_dist(&options);
- #if 0
-     options.ColPerm = PARMETIS;
--    options.ParSymbFact = YES;
-+    options.ParSymbFact = YES;							  
-     options.RowPerm = NOROWPERM;
-     options.IterRefine = NOREFINE;
-+    options.ColPerm = NATURAL;
-     options.Equil = NO; 
- #endif
- 
-+//	options.ParSymbFact       = YES;
-+//	options.ColPerm           = PARMETIS;
-+//	options.RowPerm = NOROWPERM;
-+	options.IterRefine       = 0;
-+//	options.DiagInv       = YES;
-+	options.ReplaceTinyPivot = NO;	
-+	options.SymPattern = YES;						  
-+	
-     if (!iam) {
- 	print_sp_ienv_dist(&options);
- 	print_options_dist(&options);
-diff --git a/EXAMPLE/pddrive1.c b/EXAMPLE/pddrive1.c
-index 06c6329..cf43cd9 100644
---- a/EXAMPLE/pddrive1.c
-+++ b/EXAMPLE/pddrive1.c
-@@ -22,7 +22,7 @@ at the top-level directory.
-  */
- 
- #include <math.h>
--#include <superlu_dist_config.h>
-+#include <superlu_dist_config.h>								
- #include "superlu_ddefs.h"
- 
- /*! \brief
-@@ -56,10 +56,10 @@ int main(int argc, char *argv[])
-     gridinfo_t grid;
-     double   *berr;
-     double   *b, *xtrue, *b1;
--    int    i, j, m, n;
-+    int    i, j, m, n, ii;
-     int    nprow, npcol;
-     int    iam, info, ldb, ldx, nrhs;
--    char     **cpp, c;
-+    char     **cpp, c, *postfix;
-     FILE *fp, *fopen();
-     int cpp_defs();
- 
-@@ -123,11 +123,19 @@ int main(int argc, char *argv[])
-     CHECK_MALLOC(iam, "Enter main()");
- #endif
- 
-+	for(ii = 0;ii<strlen(*cpp);ii++){
-+		if((*cpp)[ii]=='.'){
-+			postfix = &((*cpp)[ii+1]);
-+		}
-+	}	
-+	// printf("%s\n", postfix);
-+	 
-     /* ------------------------------------------------------------
-        GET THE MATRIX FROM FILE AND SETUP THE RIGHT HAND SIDE. 
-        ------------------------------------------------------------*/
--    dcreate_matrix(&A, nrhs, &b, &ldb, &xtrue, &ldx, fp, &grid);
--    if ( !(b1 = doubleMalloc_dist(ldb * nrhs)) )
-+    dcreate_matrix_postfix(&A, nrhs, &b, &ldb, &xtrue, &ldx, fp, postfix, &grid);
-+	
-+	if ( !(b1 = doubleMalloc_dist(ldb * nrhs)) )
-         ABORT("Malloc fails for b1[]");
-     for (j = 0; j < nrhs; ++j)
-         for (i = 0; i < ldb; ++i) b1[i+j*ldb] = b[i+j*ldb];
-@@ -152,7 +160,7 @@ int main(int argc, char *argv[])
-         options.PrintStat = YES;
-      */
-     set_default_options_dist(&options);
--    printf("options.ColPerm = %d\n", options.ColPerm);
-+    printf("options.ColPerm = %d\n", options.ColPerm);													  
- 
-     if (!iam) {
- 	print_sp_ienv_dist(&options);
-diff --git a/EXAMPLE/pddrive2.c b/EXAMPLE/pddrive2.c
-index c79a8a6..7e45dcf 100644
---- a/EXAMPLE/pddrive2.c
-+++ b/EXAMPLE/pddrive2.c
-@@ -23,7 +23,7 @@ at the top-level directory.
-  */
- 
- #include <math.h>
--#include <superlu_dist_config.h>
-+#include <superlu_dist_config.h>								
- #include "superlu_ddefs.h"
- 
- /*! \brief
-@@ -60,17 +60,20 @@ int main(int argc, char *argv[])
-     double   *berr;
-     double   *b, *b1, *xtrue, *xtrue1;
-     int_t    *colind, *colind1, *rowptr, *rowptr1;
--    int_t    i, j, m, n, nnz_loc, m_loc;
-+    int_t    i, j, ii, m, n, nnz_loc, m_loc;
-     int      nprow, npcol;
-     int      iam, info, ldb, ldx, nrhs;
--    char     **cpp, c;
-+    char     **cpp, c, *postfix;
-     FILE *fp, *fopen();
-     int cpp_defs();
- 
-     /* prototypes */
-     extern int dcreate_matrix_perturbed
-         (SuperMatrix *, int, double **, int *, double **, int *,
--         FILE *, gridinfo_t *);
-+         FILE *, gridinfo_t *);    
-+	extern int dcreate_matrix_perturbed_postfix
-+        (SuperMatrix *, int, double **, int *, double **, int *,
-+         FILE *, char *, gridinfo_t *);
- 
-     nprow = 1;  /* Default process rows.      */
-     npcol = 1;  /* Default process columns.   */
-@@ -128,10 +131,17 @@ int main(int argc, char *argv[])
-     CHECK_MALLOC(iam, "Enter main()");
- #endif
- 
-+	for(ii = 0;ii<strlen(*cpp);ii++){
-+		if((*cpp)[ii]=='.'){
-+			postfix = &((*cpp)[ii+1]);
-+		}
-+	}	
-+	// printf("%s\n", postfix);
-+	 
-     /* ------------------------------------------------------------
--       GET THE MATRIX FROM FILE AND SETUP THE RIGHT-HAND SIDE. 
-+       GET THE MATRIX FROM FILE AND SETUP THE RIGHT HAND SIDE. 
-        ------------------------------------------------------------*/
--    dcreate_matrix(&A, nrhs, &b, &ldb, &xtrue, &ldx, fp, &grid);
-+    dcreate_matrix_postfix(&A, nrhs, &b, &ldb, &xtrue, &ldx, fp, postfix, &grid);
- 
-     if ( !(berr = doubleMalloc_dist(nrhs)) )
- 	ABORT("Malloc fails for berr[].");
-@@ -203,7 +213,7 @@ int main(int argc, char *argv[])
-     /* Get the matrix from file, perturbed some diagonal entries to force
-        a different perm_r[]. Set up the right-hand side.   */
-     if ( !(fp = fopen(*cpp, "r")) ) ABORT("File does not exist");
--    dcreate_matrix_perturbed(&A, nrhs, &b1, &ldb, &xtrue1, &ldx, fp, &grid);
-+    dcreate_matrix_perturbed_postfix(&A, nrhs, &b1, &ldb, &xtrue1, &ldx, fp, postfix, &grid);
- 
-     PStatInit(&stat); /* Initialize the statistics variables. */
- 
-diff --git a/EXAMPLE/pddrive3.c b/EXAMPLE/pddrive3.c
-index 11c80fe..f2e40aa 100644
---- a/EXAMPLE/pddrive3.c
-+++ b/EXAMPLE/pddrive3.c
-@@ -22,7 +22,7 @@ at the top-level directory.
-  */
- 
- #include <math.h>
--#include <superlu_dist_config.h>
-+#include <superlu_dist_config.h>								
- #include "superlu_ddefs.h"
- 
- /*! \brief
-@@ -65,10 +65,10 @@ int main(int argc, char *argv[])
-     double   *berr;
-     double   *b, *b1, *xtrue, *nzval, *nzval1;
-     int_t    *colind, *colind1, *rowptr, *rowptr1;
--    int_t    i, j, m, n, nnz_loc, m_loc, fst_row;
-+    int_t    i, j, ii, m, n, nnz_loc, m_loc, fst_row;
-     int      nprow, npcol;
-     int      iam, info, ldb, ldx, nrhs;
--    char     **cpp, c;
-+    char     **cpp, c, *postfix;;
-     FILE *fp, *fopen();
-     int cpp_defs();
- 
-@@ -128,10 +128,17 @@ int main(int argc, char *argv[])
-     CHECK_MALLOC(iam, "Enter main()");
- #endif
- 
-+	for(ii = 0;ii<strlen(*cpp);ii++){
-+		if((*cpp)[ii]=='.'){
-+			postfix = &((*cpp)[ii+1]);
-+		}
-+	}	
-+	// printf("%s\n", postfix);
-+	 
-     /* ------------------------------------------------------------
-        GET THE MATRIX FROM FILE AND SETUP THE RIGHT HAND SIDE. 
-        ------------------------------------------------------------*/
--    dcreate_matrix(&A, nrhs, &b, &ldb, &xtrue, &ldx, fp, &grid);
-+    dcreate_matrix_postfix(&A, nrhs, &b, &ldb, &xtrue, &ldx, fp, postfix, &grid);
- 
-     if ( !(b1 = doubleMalloc_dist(ldb * nrhs)) )
-         ABORT("Malloc fails for b1[]");
-diff --git a/EXAMPLE/pddrive4.c b/EXAMPLE/pddrive4.c
-index 1a03add..82a0b14 100644
---- a/EXAMPLE/pddrive4.c
-+++ b/EXAMPLE/pddrive4.c
-@@ -59,12 +59,12 @@ int main(int argc, char *argv[])
-     double   *berr;
-     double   *a, *b, *xtrue;
-     int_t    *asub, *xa;
--    int_t    i, j, m, n;
-+    int_t    i, j, ii, m, n;
-     int      nprow, npcol, ldumap, p;
-     int_t    usermap[6];
-     int      iam, info, ldb, ldx, nprocs;
-     int      nrhs = 1;   /* Number of right-hand side. */
--    char     **cpp, c;
-+    char     **cpp, c, *postfix;;
-     FILE *fp, *fopen();
-     int cpp_defs();
- 
-@@ -134,14 +134,24 @@ int main(int argc, char *argv[])
-     CHECK_MALLOC(iam, "Enter main()");
- #endif
- 
-+
-+	for(ii = 0;ii<strlen(*cpp);ii++){
-+		if((*cpp)[ii]=='.'){
-+			postfix = &((*cpp)[ii+1]);
-+		}
-+	}
-+	// printf("%s\n", postfix);
-+	
-+
-     if ( iam >= 0 && iam < 6 ) { /* I am in grid 1. */
- 	iam = grid1.iam;  /* Get the logical number in the new grid. */
- 
-         /* ------------------------------------------------------------
-            GET THE MATRIX FROM FILE AND SETUP THE RIGHT HAND SIDE. 
-            ------------------------------------------------------------*/
--        dcreate_matrix(&A, nrhs, &b, &ldb, &xtrue, &ldx, fp, &grid1);
--	
-+		dcreate_matrix_postfix(&A, nrhs, &b, &ldb, &xtrue, &ldx, fp, postfix, &grid1);
-+
-+		
- 	if ( !(berr = doubleMalloc_dist(nrhs)) )
- 	    ABORT("Malloc fails for berr[].");
- 
-@@ -210,7 +220,7 @@ int main(int argc, char *argv[])
-         /* ------------------------------------------------------------
-            GET THE MATRIX FROM FILE AND SETUP THE RIGHT HAND SIDE. 
-            ------------------------------------------------------------*/
--        dcreate_matrix(&A, nrhs, &b, &ldb, &xtrue, &ldx, fp, &grid2);
-+		dcreate_matrix_postfix(&A, nrhs, &b, &ldb, &xtrue, &ldx, fp, postfix, &grid2);
- 
- 	if ( !(berr = doubleMalloc_dist(nrhs)) )
- 	    ABORT("Malloc fails for berr[].");
-diff --git a/EXAMPLE/sp_ienv.c b/EXAMPLE/sp_ienv.c
-index 08d1e8f..4173ee7 100644
---- a/EXAMPLE/sp_ienv.c
-+++ b/EXAMPLE/sp_ienv.c
-@@ -35,7 +35,7 @@ at the top-level directory.
- 
-     Arguments   
-     =========   
--
-+ 
-     ISPEC   (input) int
-             Specifies the parameter to be returned as the value of SP_IENV_DIST.   
-             = 1: the panel size w; a panel consists of w consecutive
-@@ -62,11 +62,9 @@ at the top-level directory.
- </pre>
- */
- 
--
- #include <stdlib.h>
- #include <stdio.h>
- 
--
- int_t
- sp_ienv_dist(int_t ispec)
- {
-@@ -100,7 +98,7 @@ sp_ienv_dist(int_t ispec)
-                 return(atoi(ttemp));
-             }
-             else
--            return 128;
-+            return 128;	
- 
- #endif
-         case 6: 
-diff --git a/README.md b/README.md
-deleted file mode 100644
-index 733e19b..0000000
---- a/README.md
-+++ /dev/null
-@@ -1,308 +0,0 @@
--# SuperLU_DIST (version 5.3)
--
--[![Build Status](https://travis-ci.org/xiaoyeli/superlu_dist.svg?branch=master)](https://travis-ci.org/xiaoyeli/superlu_dist) 
--[Nightly tests](http://my.cdash.org/index.php?project=superlu_dist)
--
--SuperLU_DIST contains a set of subroutines to solve a sparse linear system 
--A*X=B. It uses Gaussian elimination with static pivoting (GESP). 
--Static pivoting is a technique that combines the numerical stability of
--partial pivoting with the scalability of Cholesky (no pivoting),
--to run accurately and efficiently on large numbers of processors. 
--
--SuperLU_DIST is a parallel extension to the serial SuperLU library.
--It is targeted for the distributed memory parallel machines.
--SuperLU_DIST is implemented in ANSI C, and MPI for communications.
--Currently, the LU factorization and triangular solution routines,
--which are the most time-consuming part of the solution process,
--are parallelized. The other routines, such as static pivoting and 
--column preordering for sparsity are performed sequentially. 
--This "alpha" release contains double-precision real and double-precision
--complex data types.
--
--### The distribution contains the following directory structure:
--
--```
--SuperLU_DIST/README    instructions on installation
--SuperLU_DIST/CBLAS/    needed BLAS routines in C, not necessarily fast
--	 	       (NOTE: this version is single threaded. If you use the
--		       library with multiple OpenMP threads, performance
--		       relies on a good multithreaded BLAS implementation.)
--SuperLU_DIST/DOC/      the Users' Guide
--SuperLU_DIST/EXAMPLE/  example programs
--SuperLU_DIST/INSTALL/  test machine dependent parameters
--SuperLU_DIST/SRC/      C source code, to be compiled into libsuperlu_dist.a
--SuperLU_DIST/TEST/     testing code
--SuperLU_DIST/lib/      contains library archive libsuperlu_dist.a
--SuperLU_DIST/Makefile  top-level Makefile that does installation and testing
--SuperLU_DIST/make.inc  compiler, compiler flags, library definitions and C
--	               preprocessor definitions, included in all Makefiles.
--	               (You may need to edit it to suit your system
--	               before compiling the whole package.)
--SuperLU_DIST/MAKE_INC/ sample machine-specific make.inc files
--```
--
--## INSTALLATION
--
--There are two ways to install the package. One requires users to 
--edit makefile manually, the other uses CMake build system.
--The procedures are described below.
--
--### Installation option 1: Manual installation with makefile.
--Before installing the package, please examine the three things dependent 
--on your system setup:
--
--1.1 Edit the make.inc include file.
--
--This make include file is referenced inside each of the Makefiles
--in the various subdirectories. As a result, there is no need to 
--edit the Makefiles in the subdirectories. All information that is
--machine specific has been defined in this include file. 
--
--Sample machine-specific make.inc are provided in the MAKE_INC/
--directory for several platforms, such as Cray XT5, Linux, Mac-OS, and CUDA.
--When you have selected the machine to which you wish to install
--SuperLU_DIST, copy the appropriate sample include file 
--(if one is present) into make.inc.
--
--For example, if you wish to run SuperLU_DIST on a Cray XT5,  you can do
--`cp MAKE_INC/make.xt5  make.inc`
--
--For the systems other than listed above, some porting effort is needed
--for parallel factorization routines. Please refer to the Users' Guide 
--for detailed instructions on porting.
--
--The following CPP definitions can be set in CFLAGS.
--```
---DXSDK_INDEX_SIZE=64
--use 64-bit integers for indexing sparse matrices. (default 32 bit)
--
---DPRNTlevel=[0,1,2,...]
--printing level to show solver's execution details. (default 0)
--
---DDEBUGlevel=[0,1,2,...]
--diagnostic printing level for debugging purpose. (default 0)
--```      
--
--1.2. The BLAS library.
--The parallel routines in SuperLU_DIST use some BLAS routines on each MPI
--process. Moreover, if you enable OpenMP with multiple threads, you need to
--link with a multithreaded BLAS library. Otherwise performance will be poor.
--A good public domain BLAS library is OpenBLAS (http://www.openblas.net),
--which has OpenMP support.
--
--If you have a BLAS library your machine, you may define the following in
--the file make.inc:
--```
--BLASDEF = -DUSE_VENDOR_BLAS
--BLASLIB = <BLAS library you wish to link with>
--```
--The CBLAS/ subdirectory contains the part of the C BLAS (single threaded) 
--needed by SuperLU_DIST package. However, these codes are intended for use
--only if there is no faster implementation of the BLAS already
--available on your machine. In this case, you should go to the
--top-level SuperLU_DIST/ directory and do the following:
--
--1) In make.inc, undefine (comment out) BLASDEF, and define:
--` BLASLIB = ../lib/libblas$(PLAT).a`
--
--2) Type: `make blaslib`
--to make the BLAS library from the routines in the
--` CBLAS/ subdirectory.`
--
--1.3. External libraries: Metis and ParMetis.
--
--If you will use Metis or ParMetis ordering, you will
--need to install them yourself. Since ParMetis package already
--contains the source code for the Metis library, you can just
--download and compile ParMetis from:
--[http://glaros.dtc.umn.edu/gkhome/metis/parmetis/download](http://glaros.dtc.umn.edu/gkhome/metis/parmetis/download)
--
--After you have installed it, you should define the following in make.inc:
--```
--METISLIB = -L<metis directory> -lmetis
--PARMETISLIB = -L<parmetis directory> -lparmetis
--I_PARMETIS = -I<parmetis directory>/include -I<parmetis directory>/metis/include
--```
--You can disable ParMetis with the following line in SRC/superlu_dist_config.h:
--```
--#undef HAVE_PARMETIS
--```
--1.4. C preprocessor definition CDEFS.
--In the header file SRC/Cnames.h, we use macros to determine how
--C routines should be named so that they are callable by Fortran.
--(Some vendor-supplied BLAS libraries do not have C interfaces. So the 
--re-naming is needed in order for the SuperLU BLAS calls (in C) to 
--interface with the Fortran-style BLAS.)
--The possible options for CDEFS are:
--```
---DAdd_: Fortran expects a C routine to have an underscore
--  postfixed to the name;
--  (This is set as the default)
---DNoChange: Fortran expects a C routine name to be identical to
--      that compiled by C;
---DUpCase: Fortran expects a C routine name to be all uppercase.
--```
--1.5. Multicore and GPU (optional).
--
--To use OpenMP parallelism, need to link with an OpenMP library, and
--set the number of threads you wish to use as follows (bash):
--
--`export OMP_NUM_THREADS=<##>`
--
--To enable NVIDIA GPU access, need to take the following 2 step:
--1) Set the following Linux environment variable:
--`export ACC=GPU`
--
--2) Add the CUDA library location in make.inc:
--```
--ifeq "${ACC}" "GPU"
--CFLAGS += -DGPU_ACC
--INCS += -I<CUDA directory>/include
--LIBS += -L<CUDA directory>/lib64 -lcublas -lcudart 
--endif
--```
--A Makefile is provided in each subdirectory. The installation can be done
--completely automatically by simply typing "make" at the top level.
--
--### Installation option 2: Using CMake build system.
--You will need to create a build tree from which to invoke CMake.
--
--First, in order to use parallel symbolic factorization function, you
--need to install ParMETIS parallel ordering package and define the
--two environment variables: PARMETIS_ROOT and PARMETIS_BUILD_DIR
--
--```
--export PARMETIS_ROOT=<Prefix directory of the ParMETIS installation>
--export PARMETIS_BUILD_DIR=${PARMETIS_ROOT}/build/Linux-x86_64
--```
--Then, the installation procedure is the following.
--
--From the top level directory, do:
--```
--mkdir build ; cd build
--cmake .. \
---DTPL_PARMETIS_LIBRARIES="${PARMETIS_BUILD_DIR}/libparmetis/libparmetis.a;${PARMETIS_BUILD_DIR}/libmetis/libmetis.a" \
---DTPL_PARMETIS_INCLUDE_DIRS="${PARMETIS_ROOT}/include;${PARMETIS_ROOT}/metis/include"
--
--( Example cmake script: see run_cmake_build.sh
--
--export PARMETIS_ROOT=~/lib/dynamic/parmetis-4.0.3 
--export PARMETIS_BUILD_DIR=${PARMETIS_ROOT}/build/Linux-x86_64 
--cmake .. \
---DTPL_PARMETIS_INCLUDE_DIRS="${PARMETIS_ROOT}/include;${PARMETIS_ROOT}/metis/include" \
---DTPL_PARMETIS_LIBRARIES="${PARMETIS_BUILD_DIR}/libparmetis/libparmetis.a;${PARMETIS_BUILD_DIR}/libmetis/libmetis.a" \
---DCMAKE_C_FLAGS="-std=c99 -g" \
---Denable_blaslib=OFF \
---DBUILD_SHARED_LIBS=OFF \
---DCMAKE_C_COMPILER=mpicc \
---DCMAKE_INSTALL_PREFIX=.
--
--)
--```
--To actually build, type:
--`make`
--
--To install the libraries, type:
--`make install`
--
--To run the installation test, type:
--`ctest`
--(The outputs are in file: `build/Testing/Temporary/LastTest.log`)
--or,
--`ctest -D Experimental`
--or,
--`ctest -D Nightly`
--
--**NOTE:**
--The parallel execution in ctest is invoked by "mpiexec" command which is
--from MPICH environment. If your MPI is not MPICH/mpiexec based, the test
--execution may fail. You can always go to TEST/ directory to perform
--testing manually.
--
--**Note on the C-Fortran name mangling handled by C preprocessor definition:**  
--In the default setting, we assume that Fortran expects a C routine
--to have an underscore postfixed to the name. Depending on the
--compiler, you may need to define one of the following flags in
--during the cmake build to overwrite default setting:
--```
--cmake .. -DCMAKE_C_FLAGS="-DNoChange" 
--cmake .. -DCMAKE_C_FLAGS="-DUpCase"
--```
--## Windows Usage
--Prerequisites: CMake, Visual Studio, Microsoft HPC Pack
--This has been tested with Visual Studio 2017, without Parmetis,
--without Fortran, and with OpenMP disabled. 
--
--The cmake configuration line used was
--```
--'/winsame/contrib-vs2017/cmake-3.9.4-ser/bin/cmake' \
--  -DCMAKE_INSTALL_PREFIX:PATH=C:/winsame/volatile-vs2017/superlu_dist-master.r147-parcomm \
--  -DCMAKE_BUILD_TYPE:STRING=Release \
--  -DCMAKE_COLOR_MAKEFILE:BOOL=FALSE \
--  -DCMAKE_VERBOSE_MAKEFILE:BOOL=TRUE \
--  -Denable_openmp:BOOL=FALSE \
--  -DCMAKE_C_COMPILER:FILEPATH='C:/Program Files (x86)/Microsoft Visual Studio/2017/Professional/VC/Tools/MSVC/14.11.25503/bin/HostX64/x64/cl.exe' \
--  -DCMAKE_C_FLAGS:STRING='/DWIN32 /D_WINDOWS /W3' \
--  -Denable_parmetislib:BOOL=FALSE \
--  -DXSDK_ENABLE_Fortran=OFF \
--  -G 'NMake Makefiles JOM' \
--  C:/path/to/superlu_dist
--```
--
--After configuring, simply do
--```
--  jom # or nmake
--  jom install  # or nmake install
--```
--
--Libraries will be installed under
--C:/winsame/volatile-vs2017/superlu_dist-master.r147-parcomm/lib
--for the above configuration.
--
--If you wish to test:
--  `ctest`
--
--## READING SPARSE MATRIX FILES
--
--The SRC/ directory contains the following routines to read different file 
--formats, they all have the similar calling sequence.
--```
--$ ls -l dread*.c
--dreadMM.c              : Matrix Market, files with suffix .mtx
--dreadhb.c              : Harrell-Boeing, files with suffix .rua
--dreadrb.c              : Rutherford-Boeing, files with suffix .rb
--dreadtriple.c          : triplet, with header
--dreadtriple_noheader.c : triplet, no header, which is also readable in Matlab
--```
--
--## REFERENCES
--
--**[1]** SuperLU_DIST: A Scalable Distributed-Memory Sparse Direct Solver for Unsymmetric Linear Systems. Xiaoye S. Li and James W. Demmel. ACM Trans. on Math. Software, Vol. 29, No. 2, June 2003, pp. 110-140.  
--**[2]** Parallel Symbolic Factorization for Sparse LU with Static Pivoting. L. Grigori, J. Demmel and X.S. Li. SIAM J. Sci. Comp., Vol. 29, Issue 3, 1289-1314, 2007.  
--**[3]** A distributed CPU-GPU sparse direct solver. P. Sao, R. Vuduc and X.S. Li, Proc. of EuroPar-2014 Parallel Processing, August 25-29, 2014. Porto, Portugal.  
--
--**Xiaoye S. Li**, Lawrence Berkeley National Lab, [xsli@lbl.gov](xsli@lbl.gov)  
--**Laura Grigori**, INRIA, France, [laura.grigori@inria.fr](laura.grigori@inria.fr)  
--**Piyush Sao**, Georgia Institute of Technology, [piyush.feynman@gmail.com](piyush.feynman@gmail.com)  
--**Ichitaro Yamazaki**, Univ. of Tennessee, [ic.yamazaki@gmail.com](ic.yamazaki@gmail.com)  
--
--## RELEASE VERSIONS
--```
--October 15, 2003   Version 2.0  
--October 1,  2007   Version 2.1  
--Feburary 20, 2008   Version 2.2  
--October 15, 2008   Version 2.3  
--June 9, 2010        Version 2.4  
--November 23, 2010   Version 2.5  
--March 31, 2013      Version 3.3  
--October 1, 2014     Version 4.0  
--July 15, 2014       Version 4.1  
--September 25, 2015  Version 4.2  
--December 31, 2015   Version 4.3  
--April 8, 2016       Version 5.0.0  
--May 15, 2016        Version 5.1.0  
--October 4, 2016     Version 5.1.1  
--December 31, 2016   Version 5.1.3  
--September 30, 2017  Version 5.2.0  
--January 28, 2018    Version 5.3.0
--```
-diff --git a/SRC/CMakeLists.txt b/SRC/CMakeLists.txt
-index 88c0d75..400921d 100644
---- a/SRC/CMakeLists.txt
-+++ b/SRC/CMakeLists.txt
-@@ -9,6 +9,11 @@ set(headers
-     supermatrix.h
-     util_dist.h
-     colamd.h
-+    environment.hpp
-+    TreeBcast_v2.hpp
-+	TreeReduce_v2.hpp	
-+    TreeBcast_v2_impl.hpp
-+    TreeReduce_v2_impl.hpp	
-     ${CMAKE_CURRENT_BINARY_DIR}/superlu_dist_config.h
- )
- if (MSVC)
-@@ -17,6 +22,8 @@ endif ()
- 
- # first: precision-independent files
- set(sources
-+  global.cpp
-+  TreeInterface.cpp
-   sp_ienv.c
-   etree.c 
-   sp_colorder.c
-@@ -38,7 +45,7 @@ set(sources
-   smach_dist.c
-   dmach_dist.c
-   colamd.c
--  superlu_dist_version.c
-+  superlu_dist_version.c						
- )
- if (MSVC)
-   list(APPEND sources wingetopt.c)
-@@ -62,6 +69,7 @@ if(enable_double)
-     dreadhb.c
-     dreadrb.c
-     dreadtriple.c
-+	dbinary_io.c	
-     dreadMM.c
-     pdgsequ.c
-     pdlaqgs.c
-@@ -167,4 +175,4 @@ install(TARGETS ${targets}
- install(FILES ${headers}
- # DESTINATION ${CMAKE_INSTALL_PREFIX}/include)
-   DESTINATION ${CMAKE_INSTALL_INCLUDEDIR}
--)
-+)
-\ No newline at end of file
-diff --git a/SRC/Cnames.h b/SRC/Cnames.h
-index b446c46..792f514 100644
---- a/SRC/Cnames.h
-+++ b/SRC/Cnames.h
-@@ -148,6 +148,7 @@ at the top-level directory.
- #define dtrsv_    DTRSV
- #define dgemm_    DGEMM
- #define dtrsm_    DTRSM
-+#define dtrtri_   DTRTRI
- 
- #define scasum_   SCASUM
- #define icamax_   ICAMAX
-@@ -275,6 +276,7 @@ at the top-level directory.
- #define dtrsv_    dtrsv
- #define dgemm_    dgemm
- #define dtrsm_    dtrsm
-+#define dtrtri_   dtrtri
- 
- #define scasum_   scasum
- #define icamax_   icamax
-diff --git a/SRC/Makefile b/SRC/Makefile
-index 532274e..eb0c002 100644
---- a/SRC/Makefile
-+++ b/SRC/Makefile
-@@ -29,7 +29,7 @@ include ../make.inc
- #
- # Precision independent routines
- #
--ALLAUX 	= sp_ienv.o etree.o sp_colorder.o get_perm_c.o \
-+ALLAUX 	= global.o TreeInterface.o sp_ienv.o etree.o sp_colorder.o get_perm_c.o \
- 	  colamd.o mmd.o comm.o memory.o util.o superlu_grid.o \
- 	  pxerr_dist.o superlu_timer.o symbfact.o \
- 	  psymbfact.o psymbfact_util.o get_perm_c_parmetis.o mc64ad_dist.o \
-@@ -52,7 +52,7 @@ ZSLUSRC	= dcomplex_dist.o zlangs_dist.o zgsequ_dist.o zlaqgs_dist.o \
- #
- # Routines for double precision parallel SuperLU
- DPLUSRC = pdgssvx.o pdgssvx_ABglobal.o \
--	  dreadhb.o dreadrb.o dreadtriple.o dreadMM.o \
-+	  dreadhb.o dreadrb.o dreadtriple.o dreadMM.o dbinary_io.o \
- 	  pdgsequ.o pdlaqgs.o dldperm_dist.o pdlangs.o pdutil.o \
- 	  pdsymbfact_distdata.o ddistribute.o pddistribute.o \
- 	  pdgstrf.o pdgstrf2.o pdGetDiagU.o \
-@@ -98,6 +98,9 @@ pzgstrf.o: zscatter.c zlook_ahead_update.c zSchCompUdt-2Ddynamic.c pzgstrf.c
- .c.o:
- 	$(CC) $(CFLAGS) $(CDEFS) $(BLASDEF) -c $< $(VERBOSE)
- 
-+.cpp.o:
-+	$(CPP) $(CPPFLAGS) $(CDEFS) $(BLASDEF) -c $< $(VERBOSE)
-+
- .f.o:
- 	$(FORTRAN) $(FFLAGS) -c $< $(VERBOSE)
- 
-diff --git a/SRC/TreeBcast_v2.hpp b/SRC/TreeBcast_v2.hpp
-new file mode 100644
-index 0000000..68f95ea
---- /dev/null
-+++ b/SRC/TreeBcast_v2.hpp
-@@ -0,0 +1,149 @@
-+#ifndef _PEXSI_TREE_V2_HPP_
-+#define _PEXSI_TREE_V2_HPP_
-+
-+#include "environment.hpp"
-+// #include "blas.hpp"
-+// #include "timer.h"
-+#include "superlu_defs.h"
-+
-+
-+#include <vector>
-+#include <list>
-+#include <map>
-+#include <algorithm>
-+#include <string>
-+#include <memory>
-+//#include <random>
-+
-+// options to switch from a flat bcast/reduce tree to a binary tree
-+#ifndef FTREE_LIMIT
-+#define FTREE_LIMIT 8
-+#endif
-+
-+
-+
-+namespace ASYNCOMM{
-+
-+
-+  extern std::map< MPI_Comm , std::vector<int> > commGlobRanks;
-+
-+
-+
-+  template< typename T>
-+    class TreeBcast_v2{
-+      protected:
-+        std::vector<MPI_Request> recvRequests_;
-+        std::vector<MPI_Status> recvStatuses_;
-+        std::vector<int> recvDoneIdx_;
-+        std::vector<T *> recvDataPtrs_;
-+        std::vector<T> recvTempBuffer_;
-+        Int recvPostedCount_;
-+        Int recvCount_;
-+
-+        std::vector<MPI_Request> sendRequests_;
-+        std::vector<MPI_Status> sendStatuses_;
-+        std::vector<int> sendDoneIdx_;
-+        std::vector<T *> sendDataPtrs_;
-+        std::vector<T> sendTempBuffer_;
-+        Int sendPostedCount_;
-+        Int sendCount_;
-+
-+        bool done_;
-+        bool fwded_;
-+        bool isReady_;
-+
-+        MPI_Comm comm_;
-+        Int myRoot_;
-+        //not sure about this one
-+        Int mainRoot_;
-+        std::vector<Int> myDests_;
-+
-+        Int myRank_;
-+        Int msgSize_;
-+        Int tag_;
-+
-+        MPI_Datatype type_;
-+
-+
-+      protected:
-+        virtual void buildTree(Int * ranks, Int rank_cnt)=0;
-+
-+
-+      public:
-+        static TreeBcast_v2<T> * Create(const MPI_Comm & pComm, Int * ranks, Int rank_cnt, Int msgSize,double rseed);
-+        TreeBcast_v2();
-+        TreeBcast_v2(const MPI_Comm & pComm, Int * ranks, Int rank_cnt,Int msgSize);
-+        TreeBcast_v2(const TreeBcast_v2 & Tree);
-+        virtual ~TreeBcast_v2();
-+        virtual TreeBcast_v2 * clone() const = 0; 
-+        virtual void Copy(const TreeBcast_v2 & Tree);
-+        virtual void Reset();
-+
-+
-+        virtual inline Int GetNumMsgToRecv();
-+        virtual inline Int GetNumRecvMsg();
-+        virtual inline Int GetNumMsgToSend();
-+        virtual inline Int GetNumSendMsg();
-+        inline void SetDataReady(bool rdy);
-+        inline void SetTag(Int tag);
-+        inline Int GetTag();
-+        Int * GetDests();
-+        Int GetDest(Int i);
-+        Int GetDestCount();
-+        Int GetRoot();
-+        bool IsRoot();
-+        void SetMsgSize(Int msgSize){ this->msgSize_ = msgSize;}
-+        Int GetMsgSize();
-+        bool IsReady(){ return this->isReady_;}
-+
-+        //async wait and forward
-+		virtual void AllocateBuffer();															
-+
-+
-+        virtual void cleanupBuffers();
-+
-+		virtual void allocateRequest();
-+		virtual void forwardMessageSimple(T * locBuffer);	
-+		virtual void waitSendRequest();	
-+
-+    };
-+
-+
-+  template< typename T>
-+    class FTreeBcast2: public TreeBcast_v2<T>{
-+      protected:
-+        virtual void buildTree(Int * ranks, Int rank_cnt);
-+
-+      public:
-+        FTreeBcast2(const MPI_Comm & pComm, Int * ranks, Int rank_cnt, Int msgSize);
-+        virtual FTreeBcast2<T> * clone() const;
-+    };
-+
-+  template< typename T>
-+    class BTreeBcast2: public TreeBcast_v2<T>{
-+      protected:
-+        virtual void buildTree(Int * ranks, Int rank_cnt);
-+
-+      public:
-+        BTreeBcast2(const MPI_Comm & pComm, Int * ranks, Int rank_cnt, Int msgSize);
-+        virtual BTreeBcast2<T> * clone() const;
-+    };
-+
-+  template< typename T>
-+    class ModBTreeBcast2: public TreeBcast_v2<T>{
-+      protected:
-+        double rseed_;
-+        virtual void buildTree(Int * ranks, Int rank_cnt);
-+
-+      public:
-+        ModBTreeBcast2(const MPI_Comm & pComm, Int * ranks, Int rank_cnt, Int msgSize, double rseed);
-+        virtual ModBTreeBcast2<T> * clone() const;
-+    };
-+
-+
-+
-+
-+}//namespace ASYNCOMM
-+
-+#include "TreeBcast_v2_impl.hpp"
-+#endif
-diff --git a/SRC/TreeBcast_v2_impl.hpp b/SRC/TreeBcast_v2_impl.hpp
-new file mode 100644
-index 0000000..6ca7f63
---- /dev/null
-+++ b/SRC/TreeBcast_v2_impl.hpp
-@@ -0,0 +1,526 @@
-+#ifndef _PEXSI_TREE_IMPL_V2_HPP_
-+#define _PEXSI_TREE_IMPL_V2_HPP_
-+
-+// #define CHECK_MPI_ERROR
-+
-+// #include "TreeBcast_v2.hpp"
-+
-+
-+namespace ASYNCOMM{
-+
-+
-+  template< typename T> 
-+    TreeBcast_v2<T>::TreeBcast_v2(){
-+      comm_ = MPI_COMM_NULL;
-+      myRank_=-1;
-+      myRoot_ = -1; 
-+      msgSize_ = -1;
-+      recvCount_ = -1;
-+      sendCount_ = -1;
-+      recvPostedCount_ = -1;
-+      sendPostedCount_ = -1;
-+      tag_=-1;
-+      mainRoot_=-1;
-+      isReady_ = false;
-+      recvDataPtrs_.assign(1,NULL);
-+      recvRequests_.assign(1,MPI_REQUEST_NULL);
-+      fwded_=false;
-+      done_ = false;
-+
-+
-+      MPI_Type_contiguous( sizeof(T), MPI_BYTE, &type_ );
-+      MPI_Type_commit( &type_ );
-+
-+    }
-+
-+
-+  template< typename T> 
-+    TreeBcast_v2<T>::TreeBcast_v2(const MPI_Comm & pComm, Int * ranks, Int rank_cnt,Int msgSize):TreeBcast_v2(){
-+      comm_ = pComm;
-+      MPI_Comm_rank(comm_,&myRank_);
-+      msgSize_ = msgSize;
-+      recvCount_ = 0;
-+      sendCount_ = 0;
-+      recvPostedCount_ = 0;
-+      sendPostedCount_ = 0;
-+      mainRoot_=ranks[0];
-+#ifdef CHECK_MPI_ERROR
-+          MPI_Errhandler_set(this->comm_, MPI_ERRORS_RETURN);
-+          MPI_Errhandler_set(MPI_COMM_WORLD, MPI_ERRORS_RETURN);
-+#endif
-+    }
-+
-+
-+  template< typename T> 
-+    TreeBcast_v2<T>::TreeBcast_v2(const TreeBcast_v2 & Tree){
-+      this->Copy(Tree);
-+    }
-+
-+  template< typename T> 
-+    inline void TreeBcast_v2<T>::Copy(const TreeBcast_v2 & Tree){
-+      this->comm_ = Tree.comm_;
-+      this->myRank_ = Tree.myRank_;
-+      this->myRoot_ = Tree.myRoot_; 
-+      this->msgSize_ = Tree.msgSize_;
-+
-+      this->recvCount_ = Tree.recvCount_;
-+      this->sendCount_ = Tree.sendCount_;
-+      this->recvPostedCount_ = Tree.recvPostedCount_;
-+      this->sendPostedCount_ = Tree.sendPostedCount_;
-+      this->tag_= Tree.tag_;
-+      this->mainRoot_= Tree.mainRoot_;
-+      this->isReady_ = Tree.isReady_;
-+      this->myDests_ = Tree.myDests_;
-+
-+      this->recvRequests_ = Tree.recvRequests_;
-+      this->recvTempBuffer_ = Tree.recvTempBuffer_;
-+      this->sendRequests_ = Tree.sendRequests_;
-+      this->recvDataPtrs_ = Tree.recvDataPtrs_;
-+      if(Tree.recvDataPtrs_[0]==(T*)Tree.recvTempBuffer_.data()){
-+        this->recvDataPtrs_[0]=(T*)this->recvTempBuffer_.data();
-+      }
-+
-+      this->fwded_= Tree.fwded_;
-+      this->done_= Tree.done_;
-+    }
-+
-+  template< typename T> 
-+    inline void TreeBcast_v2<T>::Reset(){
-+      assert(done_);
-+      cleanupBuffers();
-+      done_=false;
-+      fwded_=false;
-+      recvDataPtrs_.assign(GetNumMsgToRecv(),NULL);
-+      recvRequests_.assign(GetNumMsgToRecv(),MPI_REQUEST_NULL);
-+      sendDataPtrs_.assign(GetNumMsgToSend(),NULL);
-+      sendRequests_.assign(GetNumMsgToSend(),MPI_REQUEST_NULL);
-+      // isAllocated_=false;
-+      isReady_=false;
-+      recvCount_ = 0;
-+      sendCount_ = 0;
-+      recvPostedCount_ = 0;
-+      sendPostedCount_ = 0;
-+    }
-+
-+
-+  template< typename T> 
-+    TreeBcast_v2<T>::~TreeBcast_v2(){
-+      cleanupBuffers();
-+      MPI_Type_free( &type_ );
-+    }
-+
-+  template< typename T> 
-+    inline Int TreeBcast_v2<T>::GetNumRecvMsg(){
-+      return recvCount_;
-+    }
-+  template< typename T> 
-+    inline Int TreeBcast_v2<T>::GetNumMsgToSend(){
-+      return this->GetDestCount();
-+    }
-+
-+  template< typename T> 
-+    inline Int TreeBcast_v2<T>::GetNumMsgToRecv(){
-+      return 1;//always one even for root//myRank_==myRoot_?0:1;
-+    }
-+
-+  template< typename T> 
-+    inline Int TreeBcast_v2<T>::GetNumSendMsg(){
-+      return sendCount_;
-+    }
-+  template< typename T> 
-+    inline void TreeBcast_v2<T>::SetDataReady(bool rdy){ 
-+      isReady_=rdy;
-+    }
-+  template< typename T> 
-+    inline void TreeBcast_v2<T>::SetTag(Int tag){
-+      tag_ = tag;
-+    }
-+  template< typename T> 
-+    inline Int TreeBcast_v2<T>::GetTag(){
-+      return tag_;
-+    }
-+
-+
-+  template< typename T> 
-+    inline Int * TreeBcast_v2<T>::GetDests(){
-+      return &myDests_[0];
-+    }
-+  template< typename T> 
-+    inline Int TreeBcast_v2<T>::GetDest(Int i){
-+      return myDests_[i];
-+    }
-+  template< typename T> 
-+    inline Int TreeBcast_v2<T>::GetDestCount(){
-+      return this->myDests_.size();
-+    }
-+  template< typename T> 
-+    inline Int TreeBcast_v2<T>::GetRoot(){
-+      return this->myRoot_;
-+    }
-+
-+  template< typename T> 
-+    inline bool TreeBcast_v2<T>::IsRoot(){
-+      return this->myRoot_==this->myRank_;
-+    }
-+	
-+
-+  template< typename T> 
-+    inline Int TreeBcast_v2<T>::GetMsgSize(){
-+      return this->msgSize_;
-+    }
-+
-+	
-+	
-+  template< typename T> 
-+    inline void TreeBcast_v2<T>::forwardMessageSimple(T * locBuffer){
-+        MPI_Status status;
-+		for( Int idxRecv = 0; idxRecv < this->myDests_.size(); ++idxRecv ){
-+          Int iProc = this->myDests_[idxRecv];
-+          // Use Isend to send to multiple targets
-+          int error_code = MPI_Isend( locBuffer, this->msgSize_, this->type_, 
-+              iProc, this->tag_,this->comm_, &this->sendRequests_[idxRecv] );
-+			  // MPI_Wait(&this->sendRequests_[idxRecv],&status) ; 
-+			  // std::cout<<this->myRank_<<" FWD to "<<iProc<<" on tag "<<this->tag_<<std::endl;
-+        } // for (iProc)
-+    }	  
-+
-+  template< typename T> 
-+    inline void TreeBcast_v2<T>::waitSendRequest(){
-+        MPI_Status status;
-+		for( Int idxRecv = 0; idxRecv < this->myDests_.size(); ++idxRecv ){
-+			  MPI_Wait(&this->sendRequests_[idxRecv],&status) ; 
-+        } // for (iProc)
-+    }	
-+	
-+	
-+
-+ 
-+
-+  template< typename T> 
-+    inline void TreeBcast_v2<T>::allocateRequest(){
-+        if(this->sendRequests_.size()!=this->GetDestCount()){
-+          this->sendRequests_.assign(this->GetDestCount(),MPI_REQUEST_NULL);
-+        }
-+    }
-+	
-+	
-+
-+	
-+	
-+	
-+  template< typename T> 
-+    inline void TreeBcast_v2<T>::cleanupBuffers(){
-+      this->recvRequests_.clear();
-+      this->recvStatuses_.clear();
-+      this->recvDoneIdx_.clear();
-+      this->recvDataPtrs_.clear();
-+      this->recvTempBuffer_.clear();
-+      this->sendRequests_.clear();
-+      this->sendStatuses_.clear();
-+      this->sendDoneIdx_.clear();
-+      this->sendDataPtrs_.clear();
-+      this->sendTempBuffer_.clear();
-+
-+      this->recvRequests_.shrink_to_fit();
-+      this->recvStatuses_.shrink_to_fit();
-+      this->recvDoneIdx_.shrink_to_fit();
-+      this->recvDataPtrs_.shrink_to_fit();
-+      this->recvTempBuffer_.shrink_to_fit();
-+      this->sendRequests_.shrink_to_fit();
-+      this->sendStatuses_.shrink_to_fit();
-+      this->sendDoneIdx_.shrink_to_fit();
-+      this->sendDataPtrs_.shrink_to_fit();
-+      this->sendTempBuffer_.shrink_to_fit();
-+	  
-+	  this->myDests_.clear();
-+	  
-+    }
-+
-+
-+  template< typename T> 
-+    inline void TreeBcast_v2<T>::AllocateBuffer()
-+    {
-+
-+      if(!this->IsRoot()){
-+
-+        if(this->recvDataPtrs_[0]==NULL){
-+          this->recvTempBuffer_.resize(this->msgSize_);
-+          this->recvDataPtrs_[0] = (T*)this->recvTempBuffer_.data();
-+        }
-+      }
-+    }	
-+	
-+  template< typename T>
-+    inline TreeBcast_v2<T> * TreeBcast_v2<T>::Create(const MPI_Comm & pComm, Int * ranks, Int rank_cnt, Int msgSize, double rseed){
-+      //get communicator size
-+      Int nprocs = 0;
-+      MPI_Comm_size(pComm, &nprocs);
-+
-+#if defined(FTREE)
-+      return new FTreeBcast2<T>(pComm,ranks,rank_cnt,msgSize);
-+#elif defined(MODBTREE)
-+      return new ModBTreeBcast2<T>(pComm,ranks,rank_cnt,msgSize,rseed);
-+#elif defined(BTREE)
-+      return new BTreeBcast2<T>(pComm,ranks,rank_cnt,msgSize);
-+#endif
-+
-+
-+      if(nprocs<=FTREE_LIMIT){
-+#if ( _DEBUGlevel_ >= 1 ) || defined(REDUCE_VERBOSE)
-+        statusOFS<<"FLAT TREE USED"<<std::endl;
-+#endif
-+
-+        return new FTreeBcast2<T>(pComm,ranks,rank_cnt,msgSize);
-+
-+      }
-+      else{
-+#if ( _DEBUGlevel_ >= 1 ) || defined(REDUCE_VERBOSE)
-+        statusOFS<<"BINARY TREE USED"<<std::endl;
-+#endif
-+        // return new ModBTreeBcast2<T>(pComm,ranks,rank_cnt,msgSize, rseed);
-+		return new BTreeBcast2<T>(pComm,ranks,rank_cnt,msgSize);
-+      }
-+    }
-+
-+
-+
-+  template< typename T>
-+    inline void FTreeBcast2<T>::buildTree(Int * ranks, Int rank_cnt){
-+      Int idxStart = 0;
-+      Int idxEnd = rank_cnt;
-+      this->myRoot_ = ranks[0];
-+      if(this->IsRoot() ){
-+        this->myDests_.insert(this->myDests_.end(),&ranks[1],&ranks[0]+rank_cnt);
-+      }
-+#if (defined(BCAST_VERBOSE)) 
-+      statusOFS<<"My root is "<<this->myRoot_<<std::endl;
-+      statusOFS<<"My dests are ";
-+      for(int i =0;i<this->myDests_.size();++i){statusOFS<<this->myDests_[i]<<" ";}
-+      statusOFS<<std::endl;
-+#endif
-+    }
-+
-+
-+
-+  template< typename T>
-+    FTreeBcast2<T>::FTreeBcast2(const MPI_Comm & pComm, Int * ranks, Int rank_cnt, Int msgSize):TreeBcast_v2<T>(pComm,ranks,rank_cnt,msgSize){
-+      //build the binary tree;
-+      buildTree(ranks,rank_cnt);
-+    }
-+
-+
-+  template< typename T>
-+    inline FTreeBcast2<T> * FTreeBcast2<T>::clone() const{
-+      FTreeBcast2 * out = new FTreeBcast2(*this);
-+      return out;
-+    } 
-+
-+
-+
-+
-+
-+
-+
-+
-+  template< typename T>
-+    inline BTreeBcast2<T>::BTreeBcast2(const MPI_Comm & pComm, Int * ranks, Int rank_cnt, Int msgSize):TreeBcast_v2<T>(pComm,ranks,rank_cnt,msgSize){
-+      //build the binary tree;
-+      buildTree(ranks,rank_cnt);
-+    }
-+
-+  template< typename T>
-+    inline BTreeBcast2<T> * BTreeBcast2<T>::clone() const{
-+      BTreeBcast2<T> * out = new BTreeBcast2<T>(*this);
-+      return out;
-+    }
-+
-+
-+
-+  // template< typename T>
-+    // inline void BTreeBcast2<T>::buildTree(Int * ranks, Int rank_cnt){
-+
-+      // Int idxStart = 0;
-+      // Int idxEnd = rank_cnt;
-+
-+
-+
-+      // Int prevRoot = ranks[0];
-+      // while(idxStart<idxEnd){
-+        // Int curRoot = ranks[idxStart];
-+        // Int listSize = idxEnd - idxStart;
-+
-+        // if(listSize == 1){
-+          // if(curRoot == this->myRank_){
-+            // this->myRoot_ = prevRoot;
-+            // break;
-+          // }
-+        // }
-+        // else{
-+          // Int halfList = floor(ceil(double(listSize) / 2.0));
-+          // Int idxStartL = idxStart+1;
-+          // Int idxStartH = idxStart+halfList;
-+
-+          // if(curRoot == this->myRank_){
-+            // if ((idxEnd - idxStartH) > 0 && (idxStartH - idxStartL)>0){
-+              // Int childL = ranks[idxStartL];
-+              // Int childR = ranks[idxStartH];
-+
-+              // this->myDests_.push_back(childL);
-+              // this->myDests_.push_back(childR);
-+            // }
-+            // else if ((idxEnd - idxStartH) > 0){
-+              // Int childR = ranks[idxStartH];
-+              // this->myDests_.push_back(childR);
-+            // }
-+            // else{
-+              // Int childL = ranks[idxStartL];
-+              // this->myDests_.push_back(childL);
-+            // }
-+            // this->myRoot_ = prevRoot;
-+            // break;
-+          // } 
-+
-+          // if( this->myRank_ < ranks[idxStartH]){
-+            // idxStart = idxStartL;
-+            // idxEnd = idxStartH;
-+          // }
-+          // else{
-+            // idxStart = idxStartH;
-+          // }
-+          // prevRoot = curRoot;
-+        // }
-+
-+      // }
-+
-+// #if (defined(BCAST_VERBOSE))
-+      // statusOFS<<"My root is "<<this->myRoot_<<std::endl;
-+      // statusOFS<<"My dests are ";
-+      // for(int i =0;i<this->myDests_.size();++i){statusOFS<<this->myDests_[i]<<" ";}
-+      // statusOFS<<std::endl;
-+// #endif
-+    // }
-+
-+  template< typename T>
-+    inline void BTreeBcast2<T>::buildTree(Int * ranks, Int rank_cnt){
-+
-+      Int myIdx = 0;
-+      Int ii=0; 
-+	  Int child,root;
-+	  for (ii=0;ii<rank_cnt;ii++)
-+		  if(this->myRank_ == ranks[ii]){
-+			  myIdx = ii;
-+			  break;
-+		  }
-+
-+	  if(myIdx*2+1<rank_cnt){
-+		   child = ranks[myIdx*2+1];
-+           this->myDests_.push_back(child);
-+	  }
-+	  if(myIdx*2+2<rank_cnt){
-+		   child = ranks[myIdx*2+2];
-+           this->myDests_.push_back(child);
-+	  }	 
-+
-+	  if(myIdx!=0){
-+		  this->myRoot_ = ranks[(Int)floor((double)(myIdx-1.0)/2.0)];
-+	  }else{
-+		  this->myRoot_ = this->myRank_;
-+	  } 
-+	  
-+    }
-+
-+
-+
-+  template< typename T>
-+    ModBTreeBcast2<T>::ModBTreeBcast2(const MPI_Comm & pComm, Int * ranks, Int rank_cnt, Int msgSize, double rseed):TreeBcast_v2<T>(pComm,ranks,rank_cnt,msgSize){
-+      //build the binary tree;
-+      MPI_Comm_rank(this->comm_,&this->myRank_);
-+      this->rseed_ = rseed;
-+      buildTree(ranks,rank_cnt);
-+    }
-+
-+  template< typename T>
-+    inline ModBTreeBcast2<T> * ModBTreeBcast2<T>::clone() const{
-+      ModBTreeBcast2 * out = new ModBTreeBcast2(*this);
-+      return out;
-+    }
-+
-+  template< typename T>
-+    inline void ModBTreeBcast2<T>::buildTree(Int * ranks, Int rank_cnt){
-+
-+      Int idxStart = 0;
-+      Int idxEnd = rank_cnt;
-+
-+      //sort the ranks with the modulo like operation
-+      if(rank_cnt>1){
-+        Int new_idx = (Int)this->rseed_ % (rank_cnt - 1) + 1; 
-+        Int * new_start = &ranks[new_idx];
-+        std::rotate(&ranks[1], new_start, &ranks[0]+rank_cnt);
-+      }
-+
-+      Int prevRoot = ranks[0];
-+      while(idxStart<idxEnd){
-+        Int curRoot = ranks[idxStart];
-+        Int listSize = idxEnd - idxStart;
-+
-+        if(listSize == 1){
-+          if(curRoot == this->myRank_){
-+            this->myRoot_ = prevRoot;
-+            break;
-+          }
-+        }
-+        else{
-+          Int halfList = floor(ceil(double(listSize) / 2.0));
-+          Int idxStartL = idxStart+1;
-+          Int idxStartH = idxStart+halfList;
-+
-+          if(curRoot == this->myRank_){
-+            if ((idxEnd - idxStartH) > 0 && (idxStartH - idxStartL)>0){
-+              Int childL = ranks[idxStartL];
-+              Int childR = ranks[idxStartH];
-+
-+              this->myDests_.push_back(childL);
-+              this->myDests_.push_back(childR);
-+            }
-+            else if ((idxEnd - idxStartH) > 0){
-+              Int childR = ranks[idxStartH];
-+              this->myDests_.push_back(childR);
-+            }
-+            else{
-+              Int childL = ranks[idxStartL];
-+              this->myDests_.push_back(childL);
-+            }
-+            this->myRoot_ = prevRoot;
-+            break;
-+          } 
-+
-+          //not true anymore ?
-+          //first half to 
-+          TIMER_START(FIND_RANK);
-+          Int * pos = std::find(&ranks[idxStartL], &ranks[idxStartH], this->myRank_);
-+          TIMER_STOP(FIND_RANK);
-+          if( pos != &ranks[idxStartH]){
-+            idxStart = idxStartL;
-+            idxEnd = idxStartH;
-+          }
-+          else{
-+            idxStart = idxStartH;
-+          }
-+          prevRoot = curRoot;
-+        }
-+
-+      }
-+
-+#if (defined(REDUCE_VERBOSE))
-+      statusOFS<<"My root is "<<this->myRoot_<<std::endl;
-+      statusOFS<<"My dests are ";
-+      for(int i =0;i<this->myDests_.size();++i){statusOFS<<this->myDests_[i]<<" ";}
-+      statusOFS<<std::endl;
-+#endif
-+    }
-+
-+
-+} //namespace ASYNCOMM
-+
-+
-+#endif
-diff --git a/SRC/TreeInterface.cpp b/SRC/TreeInterface.cpp
-new file mode 100644
-index 0000000..ecff034
---- /dev/null
-+++ b/SRC/TreeInterface.cpp
-@@ -0,0 +1,159 @@
-+#include "TreeReduce_v2.hpp"
-+
-+
-+namespace ASYNCOMM{
-+	
-+	
-+	
-+#ifdef __cplusplus
-+	extern "C" {
-+#endif
-+
-+	BcTree BcTree_Create(MPI_Comm comm, Int* ranks, Int rank_cnt, Int msgSize, double rseed){
-+		assert(msgSize>0);
-+		TreeBcast_v2<double>* BcastTree = TreeBcast_v2<double>::Create(comm,ranks,rank_cnt,msgSize,rseed);
-+		return (BcTree) BcastTree;
-+	}
-+
-+	void BcTree_Destroy(BcTree Tree){
-+		TreeBcast_v2<double>* BcastTree = (TreeBcast_v2<double>*) Tree;
-+		delete BcastTree; 
-+	}		
-+	
-+	void BcTree_SetTag(BcTree Tree, Int tag){
-+		TreeBcast_v2<double>* BcastTree = (TreeBcast_v2<double>*) Tree;
-+		BcastTree->SetTag(tag); 
-+	}
-+
-+
-+	yes_no_t BcTree_IsRoot(BcTree Tree){
-+		TreeBcast_v2<double>* BcastTree = (TreeBcast_v2<double>*) Tree;
-+		return BcastTree->IsRoot()?YES:NO;
-+	}
-+
-+	
-+	void BcTree_forwardMessageSimple(BcTree Tree, void* localBuffer){
-+		TreeBcast_v2<double>* BcastTree = (TreeBcast_v2<double>*) Tree;
-+		BcastTree->forwardMessageSimple((double*)localBuffer);		
-+	}
-+
-+	void BcTree_waitSendRequest(BcTree Tree){
-+		TreeBcast_v2<double>* BcastTree = (TreeBcast_v2<double>*) Tree;
-+		BcastTree->waitSendRequest();		
-+	}
-+	
-+	
-+	
-+	void BcTree_allocateRequest(BcTree Tree){
-+		TreeBcast_v2<double>* BcastTree = (TreeBcast_v2<double>*) Tree;
-+		BcastTree->allocateRequest();			
-+	}	
-+	
-+	int BcTree_getDestCount(BcTree Tree){
-+		TreeBcast_v2<double>* BcastTree = (TreeBcast_v2<double>*) Tree;
-+		return BcastTree->GetDestCount();			
-+	}	
-+
-+
-+	StdList StdList_Init(){
-+		std::list<int_t>* lst = new std::list<int_t>();
-+		return (StdList) lst;
-+	}
-+	void StdList_Pushback(StdList lst, int_t dat){
-+		std::list<int_t>* list = (std::list<int_t>*) lst;
-+		list->push_back(dat);
-+	}
-+	
-+	void StdList_Pushfront(StdList lst, int_t dat){
-+		std::list<int_t>* list = (std::list<int_t>*) lst;
-+		list->push_front(dat);
-+	}
-+	
-+	int_t StdList_Popfront(StdList lst){
-+		std::list<int_t>* list = (std::list<int_t>*) lst;
-+		int_t dat = -1;
-+		if((*list).begin()!=(*list).end()){
-+			dat = (*list).front();
-+			list->pop_front();
-+		}
-+		return dat;		
-+	}	
-+	
-+	yes_no_t StdList_Find(StdList lst, int_t dat){
-+		std::list<int_t>* list = (std::list<int_t>*) lst;
-+		for (std::list<int_t>::iterator itr = (*list).begin(); itr != (*list).end(); /*nothing*/){
-+			if(*itr==dat)return YES;
-+			++itr;
-+		}
-+		return NO;
-+	}
-+
-+	int_t StdList_Size(StdList lst){
-+		std::list<int_t>* list = (std::list<int_t>*) lst;
-+		return list->size();
-+	}	
-+
-+
-+	yes_no_t StdList_Empty(StdList lst){
-+		std::list<int_t>* list = (std::list<int_t>*) lst;
-+		return (*list).begin()==(*list).end()?YES:NO;
-+	}		
-+	
-+
-+	RdTree RdTree_Create(MPI_Comm comm, Int* ranks, Int rank_cnt, Int msgSize, double rseed){
-+		assert(msgSize>0);
-+		TreeReduce_v2<double>* ReduceTree = TreeReduce_v2<double>::Create(comm,ranks,rank_cnt,msgSize,rseed);
-+		return (RdTree) ReduceTree;
-+	}
-+	
-+	void RdTree_Destroy(RdTree Tree){
-+		TreeReduce_v2<double>* ReduceTree = (TreeReduce_v2<double>*) Tree;
-+		delete ReduceTree; 
-+	}	
-+	
-+
-+	void RdTree_SetTag(RdTree Tree, Int tag){
-+		TreeReduce_v2<double>* ReduceTree = (TreeReduce_v2<double>*) Tree;
-+		ReduceTree->SetTag(tag); 
-+	}
-+
-+	int  RdTree_GetDestCount(RdTree Tree){
-+		TreeReduce_v2<double>* ReduceTree = (TreeReduce_v2<double>*) Tree;
-+		return ReduceTree->GetDestCount();
-+	}	
-+	
-+
-+	yes_no_t RdTree_IsRoot(RdTree Tree){
-+		TreeReduce_v2<double>* ReduceTree = (TreeReduce_v2<double>*) Tree;
-+		return ReduceTree->IsRoot()?YES:NO;
-+	}
-+
-+
-+	void RdTree_forwardMessageSimple(RdTree Tree, void* localBuffer){
-+		TreeReduce_v2<double>* ReduceTree = (TreeReduce_v2<double>*) Tree;
-+		ReduceTree->forwardMessageSimple((double*)localBuffer);		
-+	}
-+	void RdTree_allocateRequest(RdTree Tree){
-+		TreeReduce_v2<double>* ReduceTree = (TreeReduce_v2<double>*) Tree;
-+		ReduceTree->allocateRequest();			
-+	}
-+
-+	void RdTree_waitSendRequest(RdTree Tree){
-+		TreeReduce_v2<double>* ReduceTree = (TreeReduce_v2<double>*) Tree;
-+		ReduceTree->waitSendRequest();		
-+	}
-+	
-+	
-+
-+
-+#ifdef __cplusplus
-+	}
-+#endif
-+	
-+	
-+	
-+
-+
-+
-+} //namespace ASYNCOMM
-+
-diff --git a/SRC/TreeReduce_v2.hpp b/SRC/TreeReduce_v2.hpp
-new file mode 100644
-index 0000000..63404cb
---- /dev/null
-+++ b/SRC/TreeReduce_v2.hpp
-@@ -0,0 +1,114 @@
-+#ifndef _PEXSI_REDUCE_TREE_V2_HPP_
-+#define _PEXSI_REDUCE_TREE_V2_HPP_
-+
-+#include "environment.hpp"
-+// #include "timer.h"
-+#include "TreeBcast_v2.hpp"
-+
-+#include <vector>
-+#include <map>
-+#include <algorithm>
-+#include <string>
-+//#include <random>
-+
-+
-+
-+namespace ASYNCOMM{
-+
-+
-+
-+  template< typename T>
-+    class TreeReduce_v2: public TreeBcast_v2<T>{
-+      protected:
-+        bool isAllocated_;
-+        bool isBufferSet_;
-+
-+      public:
-+        static TreeReduce_v2<T> * Create(const MPI_Comm & pComm, Int * ranks, Int rank_cnt, Int msgSize,double rseed);
-+
-+        TreeReduce_v2();
-+        TreeReduce_v2(const MPI_Comm & pComm, Int * ranks, Int rank_cnt, Int msgSize);
-+        TreeReduce_v2(const TreeReduce_v2 & Tree);
-+
-+        virtual ~TreeReduce_v2();
-+        virtual TreeReduce_v2 * clone() const = 0; 
-+        virtual void Copy(const TreeReduce_v2 & Tree);
-+        virtual void Reset();
-+
-+
-+        bool IsAllocated(){return this->isAllocated_;}
-+        virtual inline Int GetNumMsgToSend(){return this->myRank_==this->myRoot_?0:1;}
-+        virtual inline Int GetNumMsgToRecv(){return this->GetDestCount();}
-+
-+        virtual T * GetLocalBuffer();
-+
-+
-+		
-+		virtual void forwardMessageSimple(T * locBuffer);
-+		virtual void allocateRequest();	
-+		virtual void waitSendRequest();
-+    };
-+
-+
-+template< typename T>
-+class FTreeReduce_v2: public TreeReduce_v2<T>{
-+protected:
-+  virtual void buildTree(Int * ranks, Int rank_cnt);
-+public:
-+  FTreeReduce_v2(const MPI_Comm & pComm, Int * ranks, Int rank_cnt, Int msgSize);
-+  virtual FTreeReduce_v2<T> * clone() const;
-+};
-+
-+
-+
-+template< typename T>
-+class BTreeReduce_v2: public TreeReduce_v2<T>{
-+protected:
-+  virtual void buildTree(Int * ranks, Int rank_cnt);
-+
-+public:
-+  BTreeReduce_v2(const MPI_Comm & pComm, Int * ranks, Int rank_cnt, Int msgSize);
-+
-+
-+  virtual BTreeReduce_v2<T> * clone() const;
-+
-+
-+
-+};
-+
-+
-+template< typename T>
-+class ModBTreeReduce_v2: public TreeReduce_v2<T>{
-+protected:
-+  double rseed_;
-+  virtual void buildTree(Int * ranks, Int rank_cnt);
-+  
-+public:
-+  ModBTreeReduce_v2(const MPI_Comm & pComm, Int * ranks, Int rank_cnt, Int msgSize, double rseed);
-+  virtual void Copy(const ModBTreeReduce_v2<T> & Tree);
-+  virtual ModBTreeReduce_v2<T> * clone() const;
-+
-+};
-+
-+
-+template< typename T>
-+class PalmTreeReduce_v2: public TreeReduce_v2<T>{
-+protected:
-+
-+  virtual void buildTree(Int * ranks, Int rank_cnt);
-+
-+
-+public:
-+  PalmTreeReduce_v2(const MPI_Comm & pComm, Int * ranks, Int rank_cnt, Int msgSize);
-+  virtual PalmTreeReduce_v2<T> * clone() const;
-+
-+
-+
-+
-+};
-+
-+
-+}//namespace ASYNCOMM
-+
-+#include "TreeReduce_v2_impl.hpp"
-+#endif
-diff --git a/SRC/TreeReduce_v2_impl.hpp b/SRC/TreeReduce_v2_impl.hpp
-new file mode 100644
-index 0000000..b6dd4f1
---- /dev/null
-+++ b/SRC/TreeReduce_v2_impl.hpp
-@@ -0,0 +1,431 @@
-+#ifndef _PEXSI_REDUCE_TREE_IMPL_V2_HPP_
-+#define _PEXSI_REDUCE_TREE_IMPL_V2_HPP_
-+
-+#define _SELINV_TAG_COUNT_ 17
-+
-+// #include "TreeReduce_v2.hpp"
-+
-+
-+namespace ASYNCOMM{
-+	
-+  template<typename T>
-+    TreeReduce_v2<T>::TreeReduce_v2(const MPI_Comm & pComm, Int * ranks, Int rank_cnt, Int msgSize):TreeBcast_v2<T>(pComm,ranks,rank_cnt,msgSize){
-+      this->sendDataPtrs_.assign(1,NULL);
-+      this->sendRequests_.assign(1,MPI_REQUEST_NULL);
-+      this->isAllocated_=false;
-+      this->isBufferSet_=false;
-+    }
-+
-+
-+
-+
-+  template<typename T>
-+    TreeReduce_v2<T>::TreeReduce_v2(const TreeReduce_v2<T> & Tree){
-+      this->Copy(Tree);
-+    }
-+
-+  template<typename T>
-+    TreeReduce_v2<T>::TreeReduce_v2():TreeBcast_v2<T>(){
-+    }
-+
-+  template<typename T>
-+    TreeReduce_v2<T>::~TreeReduce_v2(){
-+      this->cleanupBuffers();
-+    }
-+
-+  template<typename T>
-+    inline void TreeReduce_v2<T>::Copy(const TreeReduce_v2<T> & Tree){
-+      ((TreeBcast_v2<T>*)this)->Copy(*(const TreeBcast_v2<T>*)&Tree);
-+
-+      this->sendDataPtrs_.assign(1,NULL);
-+      this->sendRequests_.assign(1,MPI_REQUEST_NULL);
-+      this->isAllocated_= Tree.isAllocated_;
-+      this->isBufferSet_= Tree.isBufferSet_;
-+
-+      this->cleanupBuffers();
-+    }
-+	
-+  template< typename T> 
-+    inline void TreeReduce_v2<T>::forwardMessageSimple(T * locBuffer){
-+        MPI_Status status;
-+		if(this->myRank_!=this->myRoot_){
-+			// if(this->recvCount_== this->GetDestCount()){		
-+			  //forward to my root if I have reseived everything
-+			  Int iProc = this->myRoot_;
-+			  // Use Isend to send to multiple targets
-+
-+			  int error_code = MPI_Isend(locBuffer, this->msgSize_, this->type_, 
-+				  iProc, this->tag_,this->comm_, &this->sendRequests_[0] );
-+				  
-+				  // std::cout<<this->myRank_<<" FWD to "<<iProc<<" on tag "<<this->tag_<<std::endl;
-+				  
-+				 // MPI_Wait(&this->sendRequests_[0],&status) ; 
-+				  
-+			// }
-+		}
-+      }
-+	
-+ 
-+
-+  template< typename T> 
-+    inline void TreeReduce_v2<T>::allocateRequest(){
-+        if(this->sendRequests_.size()==0){
-+          this->sendRequests_.assign(1,MPI_REQUEST_NULL);
-+        }
-+    }
-+		
-+	
-+  template< typename T> 
-+    inline void TreeReduce_v2<T>::waitSendRequest(){
-+        MPI_Status status;		
-+        if(this->sendRequests_.size()>0){
-+		  MPI_Wait(&this->sendRequests_[0],&status) ; 
-+        }	
-+	}	
-+
-+
-+
-+  template< typename T> 
-+    inline T * TreeReduce_v2<T>::GetLocalBuffer(){ 
-+      return this->sendDataPtrs_[0];
-+    }
-+
-+
-+  template< typename T> 
-+    inline void TreeReduce_v2<T>::Reset(){
-+      TreeBcast_v2<T>::Reset();
-+      this->isAllocated_=false;
-+      this->isBufferSet_=false;
-+    }
-+
-+
-+  template< typename T>
-+    inline TreeReduce_v2<T> * TreeReduce_v2<T>::Create(const MPI_Comm & pComm, Int * ranks, Int rank_cnt, Int msgSize, double rseed){
-+      //get communicator size
-+      Int nprocs = 0;
-+      MPI_Comm_size(pComm, &nprocs);
-+
-+#if defined(FTREE)
-+      return new FTreeReduce_v2<T>(pComm,ranks,rank_cnt,msgSize);
-+#elif defined(MODBTREE)
-+      return new ModBTreeReduce_v2<T>(pComm,ranks,rank_cnt,msgSize, rseed);
-+#elif defined(BTREE)
-+      return new BTreeReduce<T>(pComm,ranks,rank_cnt,msgSize);
-+#elif defined(PALMTREE)
-+      return new PalmTreeReduce_v2<T>(pComm,ranks,rank_cnt,msgSize);
-+#endif
-+
-+
-+      if(nprocs<=FTREE_LIMIT){
-+#if ( _DEBUGlevel_ >= 1 ) || defined(REDUCE_VERBOSE)
-+        statusOFS<<"FLAT TREE USED"<<std::endl;
-+#endif
-+        return new FTreeReduce_v2<T>(pComm,ranks,rank_cnt,msgSize);
-+      }
-+      else{
-+#if ( _DEBUGlevel_ >= 1 ) || defined(REDUCE_VERBOSE)
-+        statusOFS<<"BINARY TREE USED"<<std::endl;
-+#endif
-+        // return new ModBTreeReduce_v2<T>(pComm,ranks,rank_cnt,msgSize, rseed);
-+		return new BTreeReduce_v2<T>(pComm,ranks,rank_cnt,msgSize);
-+      }
-+    }
-+
-+  template< typename T>
-+    FTreeReduce_v2<T>::FTreeReduce_v2(const MPI_Comm & pComm, Int * ranks, Int rank_cnt, Int msgSize):TreeReduce_v2<T>(pComm, ranks, rank_cnt, msgSize){
-+      buildTree(ranks,rank_cnt);
-+    }
-+
-+  template< typename T>
-+    inline FTreeReduce_v2<T> * FTreeReduce_v2<T>::clone() const{
-+      FTreeReduce_v2<T> * out = new FTreeReduce_v2<T>(*this);
-+      return out;
-+    }
-+
-+
-+
-+  template< typename T>
-+    inline void FTreeReduce_v2<T>::buildTree(Int * ranks, Int rank_cnt){
-+
-+      Int idxStart = 0;
-+      Int idxEnd = rank_cnt;
-+
-+      this->myRoot_ = ranks[0];
-+
-+      if(this->myRank_==this->myRoot_){
-+        this->myDests_.insert(this->myDests_.end(),&ranks[1],&ranks[0]+rank_cnt);
-+      }
-+
-+#if (defined(REDUCE_VERBOSE))
-+      statusOFS<<"My root is "<<this->myRoot_<<std::endl;
-+      statusOFS<<"My dests are ";
-+      for(int i =0;i<this->myDests_.size();++i){statusOFS<<this->myDests_[i]<<" ";}
-+      statusOFS<<std::endl;
-+#endif
-+    }
-+
-+  template< typename T>
-+    BTreeReduce_v2<T>::BTreeReduce_v2(const MPI_Comm & pComm, Int * ranks, Int rank_cnt, Int msgSize):TreeReduce_v2<T>(pComm, ranks, rank_cnt, msgSize){
-+      buildTree(ranks,rank_cnt);
-+    }
-+
-+  template< typename T>
-+    inline BTreeReduce_v2<T> * BTreeReduce_v2<T>::clone() const{
-+      BTreeReduce_v2<T> * out = new BTreeReduce_v2<T>(*this);
-+      return out;
-+    }
-+
-+  // template< typename T>
-+    // inline void BTreeReduce_v2<T>::buildTree(Int * ranks, Int rank_cnt){
-+      // Int idxStart = 0;
-+      // Int idxEnd = rank_cnt;
-+
-+
-+
-+      // Int prevRoot = ranks[0];
-+      // while(idxStart<idxEnd){
-+        // Int curRoot = ranks[idxStart];
-+        // Int listSize = idxEnd - idxStart;
-+
-+        // if(listSize == 1){
-+          // if(curRoot == this->myRank_){
-+            // this->myRoot_ = prevRoot;
-+            // break;
-+          // }
-+        // }
-+        // else{
-+          // Int halfList = floor(ceil(double(listSize) / 2.0));
-+          // Int idxStartL = idxStart+1;
-+          // Int idxStartH = idxStart+halfList;
-+
-+          // if(curRoot == this->myRank_){
-+            // if ((idxEnd - idxStartH) > 0 && (idxStartH - idxStartL)>0){
-+              // Int childL = ranks[idxStartL];
-+              // Int childR = ranks[idxStartH];
-+
-+              // this->myDests_.push_back(childL);
-+              // this->myDests_.push_back(childR);
-+            // }
-+            // else if ((idxEnd - idxStartH) > 0){
-+              // Int childR = ranks[idxStartH];
-+              // this->myDests_.push_back(childR);
-+            // }
-+            // else{
-+              // Int childL = ranks[idxStartL];
-+              // this->myDests_.push_back(childL);
-+            // }
-+            // this->myRoot_ = prevRoot;
-+            // break;
-+          // } 
-+
-+          // if( this->myRank_ < ranks[idxStartH]){
-+            // idxStart = idxStartL;
-+            // idxEnd = idxStartH;
-+          // }
-+          // else{
-+            // idxStart = idxStartH;
-+          // }
-+          // prevRoot = curRoot;
-+        // }
-+
-+      // }
-+
-+	  
-+// #if (defined(REDUCE_VERBOSE))
-+      // statusOFS<<"My root is "<<this->myRoot_<<std::endl;
-+      // statusOFS<<"My dests are ";
-+      // for(int i =0;i<this->myDests_.size();++i){statusOFS<<this->myDests_[i]<<" ";}
-+      // statusOFS<<std::endl;
-+// #endif
-+    // }  
-+	
-+	template< typename T>
-+    inline void BTreeReduce_v2<T>::buildTree(Int * ranks, Int rank_cnt){
-+      Int myIdx = 0;
-+      Int ii=0; 
-+	  Int child,root;
-+	  for (ii=0;ii<rank_cnt;ii++)
-+		  if(this->myRank_ == ranks[ii]){
-+			  myIdx = ii;
-+			  break;
-+		  }
-+
-+	  if(myIdx*2+1<rank_cnt){
-+		   child = ranks[myIdx*2+1];
-+           this->myDests_.push_back(child);
-+	  }
-+	  if(myIdx*2+2<rank_cnt){
-+		   child = ranks[myIdx*2+2];
-+           this->myDests_.push_back(child);
-+	  }	 
-+
-+	  if(myIdx!=0){
-+		  this->myRoot_ = ranks[(Int)floor((double)(myIdx-1.0)/2.0)];
-+	  }else{
-+		  this->myRoot_ = this->myRank_;
-+	  } 
-+	  
-+	  // for(int i =0;i<this->myDests_.size();++i){std::cout<<this->myRank_<<" "<<this->myDests_[i]<<" "<<std::endl;}
-+
-+	  // {std::cout<<this->myRank_<<" "<<this->myRoot_<<" "<<std::endl;}	  
-+	  
-+    }
-+
-+
-+  template< typename T>
-+    ModBTreeReduce_v2<T>::ModBTreeReduce_v2(const MPI_Comm & pComm, Int * ranks, Int rank_cnt, Int msgSize, double rseed):TreeReduce_v2<T>(pComm, ranks, rank_cnt, msgSize){
-+      this->rseed_ = rseed;
-+      buildTree(ranks,rank_cnt);
-+    }
-+
-+  template< typename T>
-+    inline void ModBTreeReduce_v2<T>::Copy(const ModBTreeReduce_v2<T> & Tree){
-+      ((TreeReduce_v2<T>*)this)->Copy(*((const TreeReduce_v2<T>*)&Tree));
-+      this->rseed_ = Tree.rseed_;
-+    }
-+
-+  template< typename T>
-+    inline ModBTreeReduce_v2<T> * ModBTreeReduce_v2<T>::clone() const{
-+      ModBTreeReduce_v2<T> * out = new ModBTreeReduce_v2<T>(*this);
-+      return out;
-+    }
-+
-+  template< typename T>
-+    inline void ModBTreeReduce_v2<T>::buildTree(Int * ranks, Int rank_cnt){
-+
-+      Int idxStart = 0;
-+      Int idxEnd = rank_cnt;
-+
-+      //sort the ranks with the modulo like operation
-+      if(rank_cnt>1){
-+        //generate a random position in [1 .. rand_cnt]
-+        //Int new_idx = (int)((rand()+1.0) * (double)rank_cnt / ((double)RAND_MAX+1.0));
-+        //srand(ranks[0]+rank_cnt);
-+        //Int new_idx = rseed_%(rank_cnt-1)+1;
-+
-+        //Int new_idx = (int)((rank_cnt - 0) * ( (double)this->rseed_ / (double)RAND_MAX ) + 0);// (this->rseed_)%(rank_cnt-1)+1;
-+        //Int new_idx = (Int)rseed_ % (rank_cnt - 1) + 1; 
-+        //      Int new_idx = (int)((rank_cnt - 0) * ( (double)this->rseed_ / (double)RAND_MAX ) + 0);// (this->rseed_)%(rank_cnt-1)+1;
-+        Int new_idx = (int)(this->rseed_)%(rank_cnt-1)+1;
-+
-+        Int * new_start = &ranks[new_idx];
-+        //        for(int i =0;i<rank_cnt;++i){statusOFS<<ranks[i]<<" ";} statusOFS<<std::endl;
-+
-+        //        Int * new_start = std::lower_bound(&ranks[1],&ranks[0]+rank_cnt,ranks[0]);
-+        //just swap the two chunks   r[0] | r[1] --- r[new_start-1] | r[new_start] --- r[end]
-+        // becomes                   r[0] | r[new_start] --- r[end] | r[1] --- r[new_start-1] 
-+        std::rotate(&ranks[1], new_start, &ranks[0]+rank_cnt);
-+        //        for(int i =0;i<rank_cnt;++i){statusOFS<<ranks[i]<<" ";} statusOFS<<std::endl;
-+      }
-+
-+      Int prevRoot = ranks[0];
-+      while(idxStart<idxEnd){
-+        Int curRoot = ranks[idxStart];
-+        Int listSize = idxEnd - idxStart;
-+
-+        if(listSize == 1){
-+          if(curRoot == this->myRank_){
-+            this->myRoot_ = prevRoot;
-+            break;
-+          }
-+        }
-+        else{
-+          Int halfList = floor(ceil(double(listSize) / 2.0));
-+          Int idxStartL = idxStart+1;
-+          Int idxStartH = idxStart+halfList;
-+
-+          if(curRoot == this->myRank_){
-+            if ((idxEnd - idxStartH) > 0 && (idxStartH - idxStartL)>0){
-+              Int childL = ranks[idxStartL];
-+              Int childR = ranks[idxStartH];
-+
-+              this->myDests_.push_back(childL);
-+              this->myDests_.push_back(childR);
-+            }
-+            else if ((idxEnd - idxStartH) > 0){
-+              Int childR = ranks[idxStartH];
-+              this->myDests_.push_back(childR);
-+            }
-+            else{
-+              Int childL = ranks[idxStartL];
-+              this->myDests_.push_back(childL);
-+            }
-+            this->myRoot_ = prevRoot;
-+            break;
-+          } 
-+
-+          //not true anymore ?
-+          //first half to 
-+          TIMER_START(FIND_RANK);
-+          Int * pos = std::find(&ranks[idxStartL], &ranks[idxStartH], this->myRank_);
-+          TIMER_STOP(FIND_RANK);
-+          if( pos != &ranks[idxStartH]){
-+            idxStart = idxStartL;
-+            idxEnd = idxStartH;
-+          }
-+          else{
-+            idxStart = idxStartH;
-+          }
-+          prevRoot = curRoot;
-+        }
-+
-+      }
-+
-+#if (defined(REDUCE_VERBOSE)) 
-+      statusOFS<<"My root is "<<this->myRoot_<<std::endl;
-+      statusOFS<<"My dests are ";
-+      for(int i =0;i<this->myDests_.size();++i){statusOFS<<this->myDests_[i]<<" ";}
-+      statusOFS<<std::endl;
-+#endif
-+    }
-+
-+  template< typename T>
-+    PalmTreeReduce_v2<T>::PalmTreeReduce_v2(const MPI_Comm & pComm, Int * ranks, Int rank_cnt, Int msgSize):TreeReduce_v2<T>(pComm,ranks,rank_cnt,msgSize){
-+      //build the binary tree;
-+      buildTree(ranks,rank_cnt);
-+    }
-+
-+  template< typename T>
-+    inline PalmTreeReduce_v2<T> * PalmTreeReduce_v2<T>::clone() const{
-+      PalmTreeReduce_v2<T> * out = new PalmTreeReduce_v2<T>(*this);
-+      return out;
-+    }
-+
-+
-+  template< typename T>
-+    inline void PalmTreeReduce_v2<T>::buildTree(Int * ranks, Int rank_cnt){
-+      Int numLevel = floor(log2(rank_cnt));
-+      Int numRoots = 0;
-+      for(Int level=0;level<numLevel;++level){
-+        numRoots = std::min( rank_cnt, numRoots + (Int)pow(2,level));
-+        Int numNextRoots = std::min(rank_cnt,numRoots + (Int)pow(2,(level+1)));
-+        Int numReceivers = numNextRoots - numRoots;
-+        for(Int ip = 0; ip<numRoots;++ip){
-+          Int p = ranks[ip];
-+          for(Int ir = ip; ir<numReceivers;ir+=numRoots){
-+            Int r = ranks[numRoots+ir];
-+            if(r==this->myRank_){
-+              this->myRoot_ = p;
-+            }
-+
-+            if(p==this->myRank_){
-+              this->myDests_.push_back(r);
-+            }
-+          }
-+        }
-+      }
-+
-+#if (defined(BCAST_VERBOSE))
-+      statusOFS<<"My root is "<<this->myRoot_<<std::endl;
-+      statusOFS<<"My dests are ";
-+      for(int i =0;i<this->myDests_.size();++i){statusOFS<<this->myDests_[i]<<" ";}
-+      statusOFS<<std::endl;
-+#endif
-+    }
-+
-+
-+
-+
-+
-+} //namespace ASYNCOMM
-+#endif
-diff --git a/SRC/environment.hpp b/SRC/environment.hpp
-new file mode 100644
-index 0000000..fd58724
---- /dev/null
-+++ b/SRC/environment.hpp
-@@ -0,0 +1,227 @@
-+/// @file environment.hpp
-+/// @brief Environmental variables.
-+/// @date 2012-08-10
-+#ifndef _PEXSI_ENVIRONMENT_HPP_
-+#define _PEXSI_ENVIRONMENT_HPP_
-+
-+// STL libraries
-+#include <iostream> 
-+#include <iomanip> 
-+#include <fstream>
-+#include <sstream>
-+#include <unistd.h>
-+
-+#include <cfloat>
-+#include <complex>
-+#include <string>
-+
-+#include <set>
-+#include <map>
-+#include <stack>
-+#include <vector>
-+
-+#include <algorithm>
-+#include <cmath>
-+
-+#include <cassert>
-+#include <stdexcept>
-+#include <execinfo.h>
-+//#include <signal.h>
-+#include <exception>
-+
-+// For 32-bit and 64-bit integers
-+#include <stdint.h>
-+
-+// MPI
-+#include <mpi.h>
-+
-+
-+
-+// *********************************************************************
-+// Redefine the global macros
-+// *********************************************************************
-+
-+
-+// FIXME Always use complex data for pexsi and ppexsi.
-+#define _USE_COMPLEX_
-+
-+// The verbose level of debugging information
-+#ifdef  DEBUG
-+#define _DEBUGlevel_ DEBUG
-+#endif
-+
-+// Release mode. For speed up the calculation and reduce verbose level.
-+// Note that RELEASE overwrites DEBUG level.
-+#ifdef RELEASE
-+#define _RELEASE_
-+#define _DEBUGlevel -1
-+#endif
-+
-+/***********************************************************************
-+ *  Data types and constants
-+ **********************************************************************/
-+
-+/// @namespace ASYNCOMM
-+/// @brief The main namespace.
-+
-+namespace ASYNCOMM{
-+
-+// Basic data types
-+
-+#ifndef Add_
-+#define FORTRAN(name) name
-+#define BLAS(name) name
-+#define LAPACK(name) name
-+#else
-+#define FORTRAN(name) name##_
-+#define BLAS(name) name##_
-+#define LAPACK(name) name##_
-+#endif
-+typedef    int                   Int;
-+typedef    int64_t               LongInt;
-+typedef    double                Real;
-+typedef    std::complex<double>  Complex; // Must use elemental form of complex
-+#ifdef _USE_COMPLEX_
-+typedef    std::complex<double>  Scalar;  // Must use elemental form of complex
-+#else
-+typedef    double                Scalar;
-+#endif
-+
-+// IO
-+extern  std::ofstream  statusOFS;
-+
-+// *********************************************************************
-+// Define constants
-+// *********************************************************************
-+// Commonly used
-+const Int I_ZERO = 0;
-+const Int I_ONE  = 1;
-+const Int I_MINUS_ONE  = -1;
-+const Real D_ZERO = 0.0;
-+const Real D_ONE  = 1.0;
-+const Real D_MINUS_ONE  = -1.0;
-+const Complex Z_ZERO = Complex(0.0, 0.0);
-+const Complex Z_ONE  = Complex(1.0, 0.0);
-+const Complex Z_MINUS_ONE  = Complex(-1.0, 0.0);
-+const Complex Z_I    = Complex(0.0, 1.0);
-+const Complex Z_MINUS_I    = Complex(0.0, -1.0);
-+const Scalar SCALAR_ZERO    = static_cast<Scalar>(0.0);
-+const Scalar SCALAR_ONE     = static_cast<Scalar>(1.0);
-+const Scalar SCALAR_MINUS_ONE = static_cast<Scalar>(-1.0);
-+
-+template<typename T>
-+const T ZERO(){ return static_cast<T>(0.0);};
-+template<typename T>
-+const T ONE(){ return static_cast<T>(1.0);};
-+template<typename T>
-+const T MINUS_ONE(){ return static_cast<T>(-1.0);};
-+
-+const char UPPER = 'U';
-+const char LOWER = 'L';
-+
-+// Physical constants
-+
-+const Real au2K = 315774.67;
-+const Real PI = 3.141592653589793;
-+
-+} // namespace ASYNCOMM
-+
-+/***********************************************************************
-+ *  Error handling
-+ **********************************************************************/
-+
-+namespace ASYNCOMM{
-+
-+
-+
-+
-+
-+
-+  inline void gdb_lock(){
-+    volatile int lock = 1;
-+    statusOFS<<"LOCKED"<<std::endl;
-+    while (lock == 1){ }
-+  }
-+
-+
-+
-+
-+
-+
-+
-+#ifndef _RELEASE_
-+  void PushCallStack( std::string s );
-+  void PopCallStack();
-+  void DumpCallStack();
-+#endif // ifndef _RELEASE_
-+
-+  // We define an output stream that does nothing. This is done so that the 
-+  // root process can be used to print data to a file's ostream while all other 
-+  // processes use a null ostream. 
-+  struct NullStream : std::ostream
-+  {            
-+    struct NullStreamBuffer : std::streambuf
-+    {
-+      Int overflow( Int c ) { return traits_type::not_eof(c); }
-+    } nullStreamBuffer_;
-+
-+    NullStream() 
-+      : std::ios(&nullStreamBuffer_), std::ostream(&nullStreamBuffer_)
-+      { }
-+  };  
-+
-+  /////////////////////////////////////////////
-+
-+  class ExceptionTracer
-+  {
-+  public:
-+    ExceptionTracer()
-+    {
-+      void * array[25];
-+      int nSize = backtrace(array, 25);
-+      char ** symbols = backtrace_symbols(array, nSize);
-+
-+      for (int i = 0; i < nSize; i++)
-+      {
-+	std::cout << symbols[i] << std::endl;
-+      }
-+
-+      free(symbols);
-+    }
-+  };
-+
-+  // *********************************************************************
-+  // Global utility functions 
-+  // These utility functions do not depend on local definitions
-+  // *********************************************************************
-+  // Return the closest integer to a real number
-+	inline Int iround(Real a){ 
-+		Int b = 0;
-+		if(a>0) b = (a-Int(a)<0.5)?Int(a):(Int(a)+1);
-+		else b = (Int(a)-a<0.5)?Int(a):(Int(a)-1);
-+		return b; 
-+	}
-+
-+  // Read the options from command line
-+	inline void OptionsCreate(Int argc, char** argv, std::map<std::string,std::string>& options){
-+		options.clear();
-+		for(Int k=1; k<argc; k=k+2) {
-+			options[ std::string(argv[k]) ] = std::string(argv[k+1]);
-+		}
-+	}
-+
-+	// Size information.
-+	// Like sstm.str().length() but without making the copy
-+	inline Int Size( std::stringstream& sstm ){
-+		Int length;
-+		sstm.seekg (0, std::ios::end);
-+		length = sstm.tellg();
-+		sstm.seekg (0, std::ios::beg);
-+		return length;
-+	}
-+
-+
-+} // namespace ASYNCOMM
-+
-+
-+#endif // _PEXSI_ENVIRONMENT_HPP_
-diff --git a/SRC/global.cpp b/SRC/global.cpp
-new file mode 100644
-index 0000000..6d2f451
---- /dev/null
-+++ b/SRC/global.cpp
-@@ -0,0 +1,38 @@
-+#include "environment.hpp"
-+  #include <deque>
-+
-+namespace ASYNCOMM{
-+
-+// *********************************************************************
-+// IO
-+// *********************************************************************
-+  std::ofstream  statusOFS;
-+
-+
-+// *********************************************************************
-+// Error handling
-+// *********************************************************************
-+	// If we are not in RELEASE mode, then implement wrappers for a
-+	// CallStack
-+#ifndef _RELEASE_
-+	std::stack<std::string> callStack;	
-+
-+	void PushCallStack( std::string s )
-+	{ callStack.push(s); }
-+
-+	void PopCallStack()
-+	{ callStack.pop(); }
-+
-+	void DumpCallStack()
-+	{
-+		std::ostringstream msg;
-+		while( ! callStack.empty() )
-+		{
-+			msg << "Stack[" << callStack.size() << "]: " << callStack.top() << "\n";
-+			callStack.pop();
-+		}
-+		std::cerr << msg.str() << std::endl;
-+	}
-+
-+#endif // ifndef _RELEASE_
-+} // namespace ASYNCOMM
-diff --git a/SRC/pddistribute.c b/SRC/pddistribute.c
-index bcb0d49..cd0d18c 100644
---- a/SRC/pddistribute.c
-+++ b/SRC/pddistribute.c
-@@ -7,7 +7,7 @@ All rights reserved.
- 
- The source code is distributed under BSD license, see the file License.txt
- at the top-level directory.
--*/
-+ */
- 
- 
- /*! @file 
-@@ -21,6 +21,10 @@ at the top-level directory.
- 
- #include "superlu_ddefs.h"
- 
-+#ifndef CACHELINE
-+#define CACHELINE 64  /* bytes, Xeon Phi KNL, Cori haswell, Edision */
-+#endif
-+
- 
- /*! \brief
-  *
-@@ -57,1015 +61,1928 @@ at the top-level directory.
-  * ============
-  * </pre>
-  */
--int_t
-+	int_t
- dReDistribute_A(SuperMatrix *A, ScalePermstruct_t *ScalePermstruct,
--                Glu_freeable_t *Glu_freeable, int_t *xsup, int_t *supno,
--                gridinfo_t *grid, int_t *colptr[], int_t *rowind[],
--                double *a[])
-+		Glu_freeable_t *Glu_freeable, int_t *xsup, int_t *supno,
-+		gridinfo_t *grid, int_t *colptr[], int_t *rowind[],
-+		double *a[])
- {
--    NRformat_loc *Astore;
--    int_t  *perm_r; /* row permutation vector */
--    int_t  *perm_c; /* column permutation vector */
--    int_t  i, irow, fst_row, j, jcol, k, gbi, gbj, n, m_loc, jsize;
--    int_t  nnz_loc;    /* number of local nonzeros */
--    int_t  SendCnt; /* number of remote nonzeros to be sent */
--    int_t  RecvCnt; /* number of remote nonzeros to be sent */
--    int_t  *nnzToSend, *nnzToRecv, maxnnzToRecv;
--    int_t  *ia, *ja, **ia_send, *index, *itemp;
--    int_t  *ptr_to_send;
--    double *aij, **aij_send, *nzval, *dtemp;
--    double *nzval_a;
--    int    iam, it, p, procs;
--    MPI_Request *send_req;
--    MPI_Status  status;
--    
--
--    /* ------------------------------------------------------------
--       INITIALIZATION.
--       ------------------------------------------------------------*/
--    iam = grid->iam;
-+	NRformat_loc *Astore;
-+	int_t  *perm_r; /* row permutation vector */
-+	int_t  *perm_c; /* column permutation vector */
-+	int_t  i, irow, fst_row, j, jcol, k, gbi, gbj, n, m_loc, jsize;
-+	int_t  nnz_loc;    /* number of local nonzeros */
-+	int_t  SendCnt; /* number of remote nonzeros to be sent */
-+	int_t  RecvCnt; /* number of remote nonzeros to be sent */
-+	int_t  *nnzToSend, *nnzToRecv, maxnnzToRecv;
-+	int_t  *ia, *ja, **ia_send, *index, *itemp;
-+	int_t  *ptr_to_send;
-+	double *aij, **aij_send, *nzval, *dtemp;
-+	double *nzval_a;
-+	int    iam, it, p, procs;
-+	MPI_Request *send_req;
-+	MPI_Status  status;
-+
-+	/* ------------------------------------------------------------
-+	   INITIALIZATION.
-+	   ------------------------------------------------------------*/
-+	iam = grid->iam;
- #if ( DEBUGlevel>=1 )
--    CHECK_MALLOC(iam, "Enter dReDistribute_A()");
-+	CHECK_MALLOC(iam, "Enter dReDistribute_A()");
- #endif
--    perm_r = ScalePermstruct->perm_r;
--    perm_c = ScalePermstruct->perm_c;
--    procs = grid->nprow * grid->npcol;
--    Astore = (NRformat_loc *) A->Store;
--    n = A->ncol;
--    m_loc = Astore->m_loc;
--    fst_row = Astore->fst_row;
--    nnzToRecv = intCalloc_dist(2*procs);
--    nnzToSend = nnzToRecv + procs;
--
--
--    /* ------------------------------------------------------------
--       COUNT THE NUMBER OF NONZEROS TO BE SENT TO EACH PROCESS,
--       THEN ALLOCATE SPACE.
--       THIS ACCOUNTS FOR THE FIRST PASS OF A.
--       ------------------------------------------------------------*/
--    for (i = 0; i < m_loc; ++i) {
--        for (j = Astore->rowptr[i]; j < Astore->rowptr[i+1]; ++j) {
--  	    irow = perm_c[perm_r[i+fst_row]];  /* Row number in Pc*Pr*A */
--	    jcol = Astore->colind[j];
--	    gbi = BlockNum( irow );
--	    gbj = BlockNum( jcol );
--	    p = PNUM( PROW(gbi,grid), PCOL(gbj,grid), grid );
--	    ++nnzToSend[p]; 
-+	perm_r = ScalePermstruct->perm_r;
-+	perm_c = ScalePermstruct->perm_c;
-+	procs = grid->nprow * grid->npcol;
-+	Astore = (NRformat_loc *) A->Store;
-+	n = A->ncol;
-+	m_loc = Astore->m_loc;
-+	fst_row = Astore->fst_row;
-+	nnzToRecv = intCalloc_dist(2*procs);
-+	nnzToSend = nnzToRecv + procs;
-+
-+
-+	/* ------------------------------------------------------------
-+	   COUNT THE NUMBER OF NONZEROS TO BE SENT TO EACH PROCESS,
-+	   THEN ALLOCATE SPACE.
-+	   THIS ACCOUNTS FOR THE FIRST PASS OF A.
-+	   ------------------------------------------------------------*/
-+	for (i = 0; i < m_loc; ++i) {
-+		for (j = Astore->rowptr[i]; j < Astore->rowptr[i+1]; ++j) {
-+			irow = perm_c[perm_r[i+fst_row]];  /* Row number in Pc*Pr*A */
-+			jcol = Astore->colind[j];
-+			gbi = BlockNum( irow );
-+			gbj = BlockNum( jcol );
-+			p = PNUM( PROW(gbi,grid), PCOL(gbj,grid), grid );
-+			++nnzToSend[p]; 
-+		}
- 	}
--    }
- 
--    /* All-to-all communication */
--    MPI_Alltoall( nnzToSend, 1, mpi_int_t, nnzToRecv, 1, mpi_int_t,
--		  grid->comm);
-+	/* All-to-all communication */
-+	MPI_Alltoall( nnzToSend, 1, mpi_int_t, nnzToRecv, 1, mpi_int_t,
-+			grid->comm);
- 
--    maxnnzToRecv = 0;
--    nnz_loc = SendCnt = RecvCnt = 0;
-+	maxnnzToRecv = 0;
-+	nnz_loc = SendCnt = RecvCnt = 0;
- 
--    for (p = 0; p < procs; ++p) {
--	if ( p != iam ) {
--	    SendCnt += nnzToSend[p];
--	    RecvCnt += nnzToRecv[p];
--	    maxnnzToRecv = SUPERLU_MAX( nnzToRecv[p], maxnnzToRecv );
--	} else {
--	    nnz_loc += nnzToRecv[p];
--	    /*assert(nnzToSend[p] == nnzToRecv[p]);*/
-+	for (p = 0; p < procs; ++p) {
-+		if ( p != iam ) {
-+			SendCnt += nnzToSend[p];
-+			RecvCnt += nnzToRecv[p];
-+			maxnnzToRecv = SUPERLU_MAX( nnzToRecv[p], maxnnzToRecv );
-+		} else {
-+			nnz_loc += nnzToRecv[p];
-+			/*assert(nnzToSend[p] == nnzToRecv[p]);*/
-+		}
- 	}
--    }
--    k = nnz_loc + RecvCnt; /* Total nonzeros ended up in my process. */
--
--    /* Allocate space for storing the triplets after redistribution. */
--    if ( k ) { /* count can be zero. */
--        if ( !(ia = intMalloc_dist(2*k)) )
--            ABORT("Malloc fails for ia[].");
--        if ( !(aij = doubleMalloc_dist(k)) )
--            ABORT("Malloc fails for aij[].");
--    }
--    ja = ia + k;
--
--    /* Allocate temporary storage for sending/receiving the A triplets. */
--    if ( procs > 1 ) {
--      if ( !(send_req = (MPI_Request *)
--	     SUPERLU_MALLOC(2*procs *sizeof(MPI_Request))) )
--	ABORT("Malloc fails for send_req[].");
--      if ( !(ia_send = (int_t **) SUPERLU_MALLOC(procs*sizeof(int_t*))) )
--        ABORT("Malloc fails for ia_send[].");
--      if ( !(aij_send = (double **)SUPERLU_MALLOC(procs*sizeof(double*))) )
--        ABORT("Malloc fails for aij_send[].");
--      if ( SendCnt ) { /* count can be zero */
--          if ( !(index = intMalloc_dist(2*SendCnt)) )
--              ABORT("Malloc fails for index[].");
--          if ( !(nzval = doubleMalloc_dist(SendCnt)) )
--              ABORT("Malloc fails for nzval[].");
--      }
--      if ( !(ptr_to_send = intCalloc_dist(procs)) )
--        ABORT("Malloc fails for ptr_to_send[].");
--      if ( maxnnzToRecv ) { /* count can be zero */
--          if ( !(itemp = intMalloc_dist(2*maxnnzToRecv)) )
--              ABORT("Malloc fails for itemp[].");
--          if ( !(dtemp = doubleMalloc_dist(maxnnzToRecv)) )
--              ABORT("Malloc fails for dtemp[].");
--      }
--
--      for (i = 0, j = 0, p = 0; p < procs; ++p) {
--          if ( p != iam ) {
--	      ia_send[p] = &index[i];
--	      i += 2 * nnzToSend[p]; /* ia/ja indices alternate */
--	      aij_send[p] = &nzval[j];
--	      j += nnzToSend[p];
--	  }
--      }
--    } /* if procs > 1 */
--      
--    if ( !(*colptr = intCalloc_dist(n+1)) )
--        ABORT("Malloc fails for *colptr[].");
--
--    /* ------------------------------------------------------------
--       LOAD THE ENTRIES OF A INTO THE (IA,JA,AIJ) STRUCTURES TO SEND.
--       THIS ACCOUNTS FOR THE SECOND PASS OF A.
--       ------------------------------------------------------------*/
--    nnz_loc = 0; /* Reset the local nonzero count. */
--    nzval_a = Astore->nzval;
--    for (i = 0; i < m_loc; ++i) {
--        for (j = Astore->rowptr[i]; j < Astore->rowptr[i+1]; ++j) {
--  	    irow = perm_c[perm_r[i+fst_row]];  /* Row number in Pc*Pr*A */
--	    jcol = Astore->colind[j];
--	    gbi = BlockNum( irow );
--	    gbj = BlockNum( jcol );
--	    p = PNUM( PROW(gbi,grid), PCOL(gbj,grid), grid );
--
--	    if ( p != iam ) { /* remote */
--	        k = ptr_to_send[p];
--	        ia_send[p][k] = irow;
--	        ia_send[p][k + nnzToSend[p]] = jcol;
--		aij_send[p][k] = nzval_a[j];
--		++ptr_to_send[p]; 
--	    } else {          /* local */
--	        ia[nnz_loc] = irow;
--	        ja[nnz_loc] = jcol;
--		aij[nnz_loc] = nzval_a[j];
--		++nnz_loc;
--		++(*colptr)[jcol]; /* Count nonzeros in each column */
--	    }
-+	k = nnz_loc + RecvCnt; /* Total nonzeros ended up in my process. */
-+
-+	/* Allocate space for storing the triplets after redistribution. */
-+	if ( k ) { /* count can be zero. */
-+		if ( !(ia = intMalloc_dist(2*k)) )
-+			ABORT("Malloc fails for ia[].");
-+		if ( !(aij = doubleMalloc_dist(k)) )
-+			ABORT("Malloc fails for aij[].");
- 	}
--    }
--
--    /* ------------------------------------------------------------
--       PERFORM REDISTRIBUTION. THIS INVOLVES ALL-TO-ALL COMMUNICATION.
--       NOTE: Can possibly use MPI_Alltoallv.
--       ------------------------------------------------------------*/
--    for (p = 0; p < procs; ++p) {
--        if ( p != iam ) {
--	    it = 2*nnzToSend[p];
--	    MPI_Isend( ia_send[p], it, mpi_int_t,
--		       p, iam, grid->comm, &send_req[p] );
--	    it = nnzToSend[p];
--	    MPI_Isend( aij_send[p], it, MPI_DOUBLE,
--	               p, iam+procs, grid->comm, &send_req[procs+p] ); 
-+	ja = ia + k;
-+
-+	/* Allocate temporary storage for sending/receiving the A triplets. */
-+	if ( procs > 1 ) {
-+		if ( !(send_req = (MPI_Request *)
-+					SUPERLU_MALLOC(2*procs *sizeof(MPI_Request))) )
-+			ABORT("Malloc fails for send_req[].");
-+		if ( !(ia_send = (int_t **) SUPERLU_MALLOC(procs*sizeof(int_t*))) )
-+			ABORT("Malloc fails for ia_send[].");
-+		if ( !(aij_send = (double **)SUPERLU_MALLOC(procs*sizeof(double*))) )
-+			ABORT("Malloc fails for aij_send[].");
-+		if ( SendCnt ) { /* count can be zero */
-+			if ( !(index = intMalloc_dist(2*SendCnt)) )
-+				ABORT("Malloc fails for index[].");
-+			if ( !(nzval = doubleMalloc_dist(SendCnt)) )
-+				ABORT("Malloc fails for nzval[].");
-+		}
-+		if ( !(ptr_to_send = intCalloc_dist(procs)) )
-+			ABORT("Malloc fails for ptr_to_send[].");
-+		if ( maxnnzToRecv ) { /* count can be zero */
-+			if ( !(itemp = intMalloc_dist(2*maxnnzToRecv)) )
-+				ABORT("Malloc fails for itemp[].");
-+			if ( !(dtemp = doubleMalloc_dist(maxnnzToRecv)) )
-+				ABORT("Malloc fails for dtemp[].");
-+		}
-+
-+		for (i = 0, j = 0, p = 0; p < procs; ++p) {
-+			if ( p != iam ) {
-+				ia_send[p] = &index[i];
-+				i += 2 * nnzToSend[p]; /* ia/ja indices alternate */
-+				aij_send[p] = &nzval[j];
-+				j += nnzToSend[p];
-+			}
-+		}
-+	} /* if procs > 1 */
-+
-+	if ( !(*colptr = intCalloc_dist(n+1)) )
-+		ABORT("Malloc fails for *colptr[].");
-+
-+	/* ------------------------------------------------------------
-+	   LOAD THE ENTRIES OF A INTO THE (IA,JA,AIJ) STRUCTURES TO SEND.
-+	   THIS ACCOUNTS FOR THE SECOND PASS OF A.
-+	   ------------------------------------------------------------*/
-+	nnz_loc = 0; /* Reset the local nonzero count. */
-+	nzval_a = Astore->nzval;
-+	for (i = 0; i < m_loc; ++i) {
-+		for (j = Astore->rowptr[i]; j < Astore->rowptr[i+1]; ++j) {
-+			irow = perm_c[perm_r[i+fst_row]];  /* Row number in Pc*Pr*A */
-+			jcol = Astore->colind[j];
-+			gbi = BlockNum( irow );
-+			gbj = BlockNum( jcol );
-+			p = PNUM( PROW(gbi,grid), PCOL(gbj,grid), grid );
-+
-+			if ( p != iam ) { /* remote */
-+				k = ptr_to_send[p];
-+				ia_send[p][k] = irow;
-+				ia_send[p][k + nnzToSend[p]] = jcol;
-+				aij_send[p][k] = nzval_a[j];
-+				++ptr_to_send[p]; 
-+			} else {          /* local */
-+				ia[nnz_loc] = irow;
-+				ja[nnz_loc] = jcol;
-+				aij[nnz_loc] = nzval_a[j];
-+				++nnz_loc;
-+				++(*colptr)[jcol]; /* Count nonzeros in each column */
-+			}
-+		}
- 	}
--    }
--
--    for (p = 0; p < procs; ++p) {
--        if ( p != iam ) {
--	    it = 2*nnzToRecv[p];
--	    MPI_Recv( itemp, it, mpi_int_t, p, p, grid->comm, &status ); 
--	    it = nnzToRecv[p];
--            MPI_Recv( dtemp, it, MPI_DOUBLE, p, p+procs,
--		      grid->comm, &status );
--	    for (i = 0; i < nnzToRecv[p]; ++i) {
--	        ia[nnz_loc] = itemp[i];
--		jcol = itemp[i + nnzToRecv[p]];
--		/*assert(jcol<n);*/
--	        ja[nnz_loc] = jcol;
--		aij[nnz_loc] = dtemp[i];
--		++nnz_loc;
--		++(*colptr)[jcol]; /* Count nonzeros in each column */ 
--	    }
-+
-+	/* ------------------------------------------------------------
-+	   PERFORM REDISTRIBUTION. THIS INVOLVES ALL-TO-ALL COMMUNICATION.
-+NOTE: Can possibly use MPI_Alltoallv.
-+------------------------------------------------------------*/
-+	for (p = 0; p < procs; ++p) {
-+		if ( p != iam ) {
-+			it = 2*nnzToSend[p];
-+			MPI_Isend( ia_send[p], it, mpi_int_t,
-+					p, iam, grid->comm, &send_req[p] );
-+			it = nnzToSend[p];
-+			MPI_Isend( aij_send[p], it, MPI_DOUBLE,
-+					p, iam+procs, grid->comm, &send_req[procs+p] ); 
-+		}
-+	}
-+
-+	for (p = 0; p < procs; ++p) {
-+		if ( p != iam ) {
-+			it = 2*nnzToRecv[p];
-+			MPI_Recv( itemp, it, mpi_int_t, p, p, grid->comm, &status ); 
-+			it = nnzToRecv[p];
-+			MPI_Recv( dtemp, it, MPI_DOUBLE, p, p+procs,
-+					grid->comm, &status );
-+			for (i = 0; i < nnzToRecv[p]; ++i) {
-+				ia[nnz_loc] = itemp[i];
-+				jcol = itemp[i + nnzToRecv[p]];
-+				/*assert(jcol<n);*/
-+				ja[nnz_loc] = jcol;
-+				aij[nnz_loc] = dtemp[i];
-+				++nnz_loc;
-+				++(*colptr)[jcol]; /* Count nonzeros in each column */ 
-+			}
-+		}
- 	}
--    }
- 
--    for (p = 0; p < procs; ++p) {
--        if ( p != iam ) {
--	    MPI_Wait( &send_req[p], &status);
--	    MPI_Wait( &send_req[procs+p], &status);
-+	for (p = 0; p < procs; ++p) {
-+		if ( p != iam ) {
-+			MPI_Wait( &send_req[p], &status);
-+			MPI_Wait( &send_req[procs+p], &status);
-+		}
-+	}
-+
-+	/* ------------------------------------------------------------
-+	   DEALLOCATE TEMPORARY STORAGE
-+	   ------------------------------------------------------------*/
-+
-+	SUPERLU_FREE(nnzToRecv);
-+
-+	if ( procs > 1 ) {
-+		SUPERLU_FREE(send_req);
-+		SUPERLU_FREE(ia_send);
-+		SUPERLU_FREE(aij_send);
-+		if ( SendCnt ) {
-+			SUPERLU_FREE(index);
-+			SUPERLU_FREE(nzval);
-+		}
-+		SUPERLU_FREE(ptr_to_send);
-+		if ( maxnnzToRecv ) {
-+			SUPERLU_FREE(itemp);
-+			SUPERLU_FREE(dtemp);
-+		}
-+	}
-+
-+	/* ------------------------------------------------------------
-+	   CONVERT THE TRIPLET FORMAT INTO THE CCS FORMAT.
-+	   ------------------------------------------------------------*/
-+	if ( nnz_loc ) { /* nnz_loc can be zero */
-+		if ( !(*rowind = intMalloc_dist(nnz_loc)) )
-+			ABORT("Malloc fails for *rowind[].");
-+		if ( !(*a = doubleMalloc_dist(nnz_loc)) )
-+			ABORT("Malloc fails for *a[].");
-+	}
-+
-+	/* Initialize the array of column pointers */
-+	k = 0;
-+	jsize = (*colptr)[0];
-+	(*colptr)[0] = 0;
-+	for (j = 1; j < n; ++j) {
-+		k += jsize;
-+		jsize = (*colptr)[j];
-+		(*colptr)[j] = k;
-+	}
-+
-+	/* Copy the triplets into the column oriented storage */
-+	for (i = 0; i < nnz_loc; ++i) {
-+		j = ja[i];
-+		k = (*colptr)[j];
-+		(*rowind)[k] = ia[i];
-+		(*a)[k] = aij[i];
-+		++(*colptr)[j];
-+	}
-+
-+	/* Reset the column pointers to the beginning of each column */
-+	for (j = n; j > 0; --j) (*colptr)[j] = (*colptr)[j-1];
-+	(*colptr)[0] = 0;
-+
-+	if ( nnz_loc ) {
-+		SUPERLU_FREE(ia);
-+		SUPERLU_FREE(aij);
- 	}
--    }
--
--    /* ------------------------------------------------------------
--       DEALLOCATE TEMPORARY STORAGE
--       ------------------------------------------------------------*/
--
--    SUPERLU_FREE(nnzToRecv);
--
--    if ( procs > 1 ) {
--	SUPERLU_FREE(send_req);
--	SUPERLU_FREE(ia_send);
--	SUPERLU_FREE(aij_send);
--	if ( SendCnt ) {
--            SUPERLU_FREE(index);
--            SUPERLU_FREE(nzval);
--        }
--	SUPERLU_FREE(ptr_to_send);
--        if ( maxnnzToRecv ) {
--            SUPERLU_FREE(itemp);
--            SUPERLU_FREE(dtemp);
--        }
--    }
--
--    /* ------------------------------------------------------------
--       CONVERT THE TRIPLET FORMAT INTO THE CCS FORMAT.
--       ------------------------------------------------------------*/
--    if ( nnz_loc ) { /* nnz_loc can be zero */
--        if ( !(*rowind = intMalloc_dist(nnz_loc)) )
--            ABORT("Malloc fails for *rowind[].");
--        if ( !(*a = doubleMalloc_dist(nnz_loc)) )
--            ABORT("Malloc fails for *a[].");
--    }
--
--    /* Initialize the array of column pointers */
--    k = 0;
--    jsize = (*colptr)[0];
--    (*colptr)[0] = 0;
--    for (j = 1; j < n; ++j) {
--	k += jsize;
--	jsize = (*colptr)[j];
--	(*colptr)[j] = k;
--    }
--    
--    /* Copy the triplets into the column oriented storage */
--    for (i = 0; i < nnz_loc; ++i) {
--	j = ja[i];
--	k = (*colptr)[j];
--	(*rowind)[k] = ia[i];
--	(*a)[k] = aij[i];
--	++(*colptr)[j];
--    }
--
--    /* Reset the column pointers to the beginning of each column */
--    for (j = n; j > 0; --j) (*colptr)[j] = (*colptr)[j-1];
--    (*colptr)[0] = 0;
--
--    if ( nnz_loc ) {
--        SUPERLU_FREE(ia);
--        SUPERLU_FREE(aij);
--    }
- 
- #if ( DEBUGlevel>=1 )
--    CHECK_MALLOC(iam, "Exit dReDistribute_A()");
-+	CHECK_MALLOC(iam, "Exit dReDistribute_A()");
- #endif
-- 
--    return 0;
-+
-+	return 0;
- } /* dReDistribute_A */
- 
--float
-+	float
- pddistribute(fact_t fact, int_t n, SuperMatrix *A,
--	     ScalePermstruct_t *ScalePermstruct,
--	     Glu_freeable_t *Glu_freeable, LUstruct_t *LUstruct,
--	     gridinfo_t *grid)
--/*
-- * -- Distributed SuperLU routine (version 2.0) --
-- * Lawrence Berkeley National Lab, Univ. of California Berkeley.
-- * March 15, 2003
-- *
-- *
-- * Purpose
-- * =======
-- *   Distribute the matrix onto the 2D process mesh.
-- * 
-- * Arguments
-- * =========
-- * 
-- * fact (input) fact_t
-- *        Specifies whether or not the L and U structures will be re-used.
-- *        = SamePattern_SameRowPerm: L and U structures are input, and
-- *                                   unchanged on exit.
-- *        = DOFACT or SamePattern: L and U structures are computed and output.
-- *
-- * n      (input) int
-- *        Dimension of the matrix.
-- *
-- * A      (input) SuperMatrix*
-- *	  The distributed input matrix A of dimension (A->nrow, A->ncol).
-- *        A may be overwritten by diag(R)*A*diag(C)*Pc^T. The type of A can be:
-- *        Stype = SLU_NR_loc; Dtype = SLU_D; Mtype = SLU_GE.
-- *
-- * ScalePermstruct (input) ScalePermstruct_t*
-- *        The data structure to store the scaling and permutation vectors
-- *        describing the transformations performed to the original matrix A.
-- *
-- * Glu_freeable (input) *Glu_freeable_t
-- *        The global structure describing the graph of L and U.
-- * 
-- * LUstruct (input) LUstruct_t*
-- *        Data structures for L and U factors.
-- *
-- * grid   (input) gridinfo_t*
-- *        The 2D process mesh.
-- *
-- * Return value
-- * ============
-- *   > 0, working storage required (in bytes).
-- *
-- */
-+		ScalePermstruct_t *ScalePermstruct,
-+		Glu_freeable_t *Glu_freeable, LUstruct_t *LUstruct,
-+		gridinfo_t *grid, int_t nrhs)
-+	/*
-+	 * -- Distributed SuperLU routine (version 2.0) --
-+	 * Lawrence Berkeley National Lab, Univ. of California Berkeley.
-+	 * March 15, 2003
-+	 *
-+	 *
-+	 * Purpose
-+	 * =======
-+	 *   Distribute the matrix onto the 2D process mesh.
-+	 * 
-+	 * Arguments
-+	 * =========
-+	 * 
-+	 * fact (input) fact_t
-+	 *        Specifies whether or not the L and U structures will be re-used.
-+	 *        = SamePattern_SameRowPerm: L and U structures are input, and
-+	 *                                   unchanged on exit.
-+	 *        = DOFACT or SamePattern: L and U structures are computed and output.
-+	 *
-+	 * n      (input) int
-+	 *        Dimension of the matrix.
-+	 *
-+	 * A      (input) SuperMatrix*
-+	 *	  The distributed input matrix A of dimension (A->nrow, A->ncol).
-+	 *        A may be overwritten by diag(R)*A*diag(C)*Pc^T. The type of A can be:
-+	 *        Stype = SLU_NR_loc; Dtype = SLU_D; Mtype = SLU_GE.
-+	 *
-+	 * ScalePermstruct (input) ScalePermstruct_t*
-+	 *        The data structure to store the scaling and permutation vectors
-+	 *        describing the transformations performed to the original matrix A.
-+	 *
-+	 * Glu_freeable (input) *Glu_freeable_t
-+	 *        The global structure describing the graph of L and U.
-+	 * 
-+	 * LUstruct (input) LUstruct_t*
-+	 *        Data structures for L and U factors.
-+	 *
-+	 * grid   (input) gridinfo_t*
-+	 *        The 2D process mesh.
-+	 *
-+	 * Return value
-+	 * ============
-+	 *   > 0, working storage required (in bytes).
-+	 *
-+	 */
- {
--    Glu_persist_t *Glu_persist = LUstruct->Glu_persist;
--    LocalLU_t *Llu = LUstruct->Llu;
--    int_t bnnz, fsupc, fsupc1, i, ii, irow, istart, j, jb, jj, k, 
--          len, len1, nsupc;
--    int_t ljb;  /* local block column number */
--    int_t nrbl; /* number of L blocks in current block column */
--    int_t nrbu; /* number of U blocks in current block column */
--    int_t gb;   /* global block number; 0 < gb <= nsuper */
--    int_t lb;   /* local block number; 0 < lb <= ceil(NSUPERS/Pr) */
--    int iam, jbrow, kcol, mycol, myrow, pc, pr;
--    int_t mybufmax[NBUFFERS];
--    NRformat_loc *Astore;
--    double *a;
--    int_t *asub, *xa;
--    int_t *xsup = Glu_persist->xsup;    /* supernode and column mapping */
--    int_t *supno = Glu_persist->supno;   
--    int_t *lsub, *xlsub, *usub, *xusub;
--    int_t nsupers;
--    int_t next_lind;      /* next available position in index[*] */
--    int_t next_lval;      /* next available position in nzval[*] */
--    int_t *index;         /* indices consist of headers and row subscripts */
--    int   *index1;        /* temporary pointer to array of int */
--    double *lusup, *uval; /* nonzero values in L and U */
--    double **Lnzval_bc_ptr;  /* size ceil(NSUPERS/Pc) */
--    int_t  **Lrowind_bc_ptr; /* size ceil(NSUPERS/Pc) */
--    double **Unzval_br_ptr;  /* size ceil(NSUPERS/Pr) */
--    int_t  **Ufstnz_br_ptr;  /* size ceil(NSUPERS/Pr) */
--
--    /*-- Counts to be used in factorization. --*/
--    int  *ToRecv, *ToSendD, **ToSendR;
--
--    /*-- Counts to be used in lower triangular solve. --*/
--    int_t  *fmod;          /* Modification count for L-solve.        */
--    int_t  **fsendx_plist; /* Column process list to send down Xk.   */
--    int_t  nfrecvx = 0;    /* Number of Xk I will receive.           */
--    int_t  nfsendx = 0;    /* Number of Xk I will send               */
--    int_t  kseen;
--
--    /*-- Counts to be used in upper triangular solve. --*/
--    int_t  *bmod;          /* Modification count for U-solve.        */
--    int_t  **bsendx_plist; /* Column process list to send down Xk.   */
--    int_t  nbrecvx = 0;    /* Number of Xk I will receive.           */
--    int_t  nbsendx = 0;    /* Number of Xk I will send               */
--    int_t  *ilsum;         /* starting position of each supernode in 
--			      the full array (local)                 */
--
--    /*-- Auxiliary arrays; freed on return --*/
--    int_t *rb_marker;  /* block hit marker; size ceil(NSUPERS/Pr)           */
--    int_t *Urb_length; /* U block length; size ceil(NSUPERS/Pr)             */
--    int_t *Urb_indptr; /* pointers to U index[]; size ceil(NSUPERS/Pr)      */
--    int_t *Urb_fstnz;  /* # of fstnz in a block row; size ceil(NSUPERS/Pr)  */
--    int_t *Ucbs;       /* number of column blocks in a block row            */
--    int_t *Lrb_length; /* L block length; size ceil(NSUPERS/Pr)             */
--    int_t *Lrb_number; /* global block number; size ceil(NSUPERS/Pr)        */
--    int_t *Lrb_indptr; /* pointers to L index[]; size ceil(NSUPERS/Pr)      */
--    int_t *Lrb_valptr; /* pointers to L nzval[]; size ceil(NSUPERS/Pr)      */
--    double *dense, *dense_col; /* SPA */
--    double zero = 0.0;
--    int_t ldaspa;     /* LDA of SPA */
--    int_t iword, dword;
--    float mem_use = 0.0;
--
-+	Glu_persist_t *Glu_persist = LUstruct->Glu_persist;
-+	LocalLU_t *Llu = LUstruct->Llu;
-+	int_t bnnz, fsupc, fsupc1, i, ii, irow, istart, j, ib, jb, jj, k, k1, 
-+	      len, len1, nsupc;
-+	int_t lib;  /* local block row number */
-+	int_t nlb;  /* local block rows*/
-+	int_t ljb;  /* local block column number */
-+	int_t nrbl; /* number of L blocks in current block column */
-+	int_t nrbu; /* number of U blocks in current block column */
-+	int_t gb;   /* global block number; 0 < gb <= nsuper */
-+	int_t lb;   /* local block number; 0 < lb <= ceil(NSUPERS/Pr) */
-+	int iam, jbrow, kcol,krow, mycol, myrow, pc, pr;
-+	int_t mybufmax[NBUFFERS];
-+	NRformat_loc *Astore;
-+	double *a;
-+	int_t *asub, *xa;
-+	int_t *xsup = Glu_persist->xsup;    /* supernode and column mapping */
-+	int_t *supno = Glu_persist->supno;   
-+	int_t *lsub, *xlsub, *usub, *usub1, *xusub;
-+	int_t nsupers;
-+	int_t next_lind;      /* next available position in index[*] */
-+	int_t next_lval;      /* next available position in nzval[*] */
-+	int_t *index;         /* indices consist of headers and row subscripts */
-+	int_t *index_srt;         /* indices consist of headers and row subscripts */	
-+	int   *index1;        /* temporary pointer to array of int */
-+	double *lusup, *lusup_srt, *uval; /* nonzero values in L and U */
-+	double **Lnzval_bc_ptr;  /* size ceil(NSUPERS/Pc) */
-+	int_t  **Lrowind_bc_ptr; /* size ceil(NSUPERS/Pc) */
-+	int_t   **Lindval_loc_bc_ptr; /* size ceil(NSUPERS/Pc)                 */		
-+	double **Unzval_br_ptr;  /* size ceil(NSUPERS/Pr) */
-+	int_t  **Ufstnz_br_ptr;  /* size ceil(NSUPERS/Pr) */
-+	BcTree  *LBtree_ptr;       /* size ceil(NSUPERS/Pc)                */
-+	RdTree  *LRtree_ptr;		  /* size ceil(NSUPERS/Pr)                */
-+	BcTree  *UBtree_ptr;       /* size ceil(NSUPERS/Pc)                */
-+	RdTree  *URtree_ptr;		  /* size ceil(NSUPERS/Pr)                */	
-+	int msgsize;
-+
-+    int_t  *Urbs,*Urbs1; /* Number of row blocks in each block column of U. */
-+    Ucb_indptr_t **Ucb_indptr;/* Vertical linked list pointing to Uindex[] */
-+    int_t  **Ucb_valptr;      /* Vertical linked list pointing to Unzval[] */  	
-+	
-+	/*-- Counts to be used in factorization. --*/
-+	int  *ToRecv, *ToSendD, **ToSendR;
-+
-+	/*-- Counts to be used in lower triangular solve. --*/
-+	int_t  *fmod;          /* Modification count for L-solve.        */
-+	int_t  **fsendx_plist; /* Column process list to send down Xk.   */
-+	int_t  nfrecvx = 0;    /* Number of Xk I will receive.           */
-+	int_t  nfsendx = 0;    /* Number of Xk I will send               */
-+	int_t  kseen;
-+
-+	/*-- Counts to be used in upper triangular solve. --*/
-+	int_t  *bmod;          /* Modification count for U-solve.        */
-+	int_t  **bsendx_plist; /* Column process list to send down Xk.   */
-+	int_t  nbrecvx = 0;    /* Number of Xk I will receive.           */
-+	int_t  nbsendx = 0;    /* Number of Xk I will send               */
-+	int_t  *ilsum;         /* starting position of each supernode in 
-+				  the full array (local)                 */
-+
-+	/*-- Auxiliary arrays; freed on return --*/
-+	int_t *rb_marker;  /* block hit marker; size ceil(NSUPERS/Pr)           */
-+	int_t *Urb_length; /* U block length; size ceil(NSUPERS/Pr)             */
-+	int_t *Urb_indptr; /* pointers to U index[]; size ceil(NSUPERS/Pr)      */
-+	int_t *Urb_fstnz;  /* # of fstnz in a block row; size ceil(NSUPERS/Pr)  */
-+	int_t *Ucbs;       /* number of column blocks in a block row            */
-+	int_t *Lrb_length; /* L block length; size ceil(NSUPERS/Pr)             */
-+	int_t *Lrb_number; /* global block number; size ceil(NSUPERS/Pr)        */
-+	int_t *Lrb_indptr; /* pointers to L index[]; size ceil(NSUPERS/Pr)      */
-+	int_t *Lrb_valptr; /* pointers to L nzval[]; size ceil(NSUPERS/Pr)      */
-+	int_t *ActiveFlag;
-+	int_t *ActiveFlagAll;
-+	int_t Iactive;
-+	int *ranks;
-+	int_t *idxs;
-+	int_t **nzrows;
-+	double rseed;
-+	int rank_cnt,rank_cnt_ref,Root;
-+	double *dense, *dense_col; /* SPA */
-+	double zero = 0.0;
-+	int_t ldaspa;     /* LDA of SPA */
-+	int_t iword, dword;
-+	float mem_use = 0.0;
-+	int_t *mod_bit;
-+	int_t *frecv, *brecv, *lloc;
-+	double **Linv_bc_ptr;  /* size ceil(NSUPERS/Pc) */
-+	double **Uinv_bc_ptr;  /* size ceil(NSUPERS/Pc) */
-+	double *SeedSTD_BC,*SeedSTD_RD;				 
-+	int_t idx_indx,idx_lusup;
-+	int_t nbrow;
-+	int_t  ik, il, lk, rel, knsupc, idx_r;
-+	int_t  lptr1_tmp, idx_i, idx_v,m, uu, aln_i;
-+	int_t nub;
-+	int tag;
-+	
- #if ( PRNTlevel>=1 )
--    int_t nLblocks = 0, nUblocks = 0;
-+	int_t nLblocks = 0, nUblocks = 0;
- #endif
- #if ( PROFlevel>=1 ) 
--    double t, t_u, t_l;
--    int_t u_blks;
-+	double t, t_u, t_l;
-+	int_t u_blks;
- #endif
- 
--    /* Initialization. */
--    iam = grid->iam;
--    myrow = MYROW( iam, grid );
--    mycol = MYCOL( iam, grid );
--    for (i = 0; i < NBUFFERS; ++i) mybufmax[i] = 0;
--    nsupers  = supno[n-1] + 1;
--    Astore   = (NRformat_loc *) A->Store;
-+	/* Initialization. */
-+	iam = grid->iam;
-+	myrow = MYROW( iam, grid );
-+	mycol = MYCOL( iam, grid );
-+	for (i = 0; i < NBUFFERS; ++i) mybufmax[i] = 0;
-+	nsupers  = supno[n-1] + 1;
-+	Astore   = (NRformat_loc *) A->Store;
- 
--#if ( PRNTlevel>=1 )
--    iword = sizeof(int_t);
--    dword = sizeof(double);
--#endif
-+	// #if ( PRNTlevel>=1 )
-+	iword = sizeof(int_t);
-+	dword = sizeof(double);
-+
-+	aln_i = ceil(CACHELINE/(double)iword);
-+
-+	// #endif
- 
- #if ( DEBUGlevel>=1 )
--    CHECK_MALLOC(iam, "Enter pddistribute()");
-+	CHECK_MALLOC(iam, "Enter pddistribute()");
- #endif
- #if ( PROFlevel>=1 )
--    t = SuperLU_timer_();
-+	t = SuperLU_timer_();
- #endif
- 
--    dReDistribute_A(A, ScalePermstruct, Glu_freeable, xsup, supno,
--		      grid, &xa, &asub, &a);
-+	dReDistribute_A(A, ScalePermstruct, Glu_freeable, xsup, supno,
-+			grid, &xa, &asub, &a);
- 
- #if ( PROFlevel>=1 )
--    t = SuperLU_timer_() - t;
--    if ( !iam ) printf("--------\n"
--		       ".. Phase 1 - ReDistribute_A time: %.2f\t\n", t);
-+	t = SuperLU_timer_() - t;
-+	if ( !iam ) printf("--------\n"
-+			".. Phase 1 - ReDistribute_A time: %.2f\t\n", t);
- #endif
- 
--    if ( fact == SamePattern_SameRowPerm ) {
-+	if ( fact == SamePattern_SameRowPerm ) {
- 
- #if ( PROFlevel>=1 )
--	t_l = t_u = 0; u_blks = 0;
-+		t_l = t_u = 0; u_blks = 0;
- #endif
--	/* We can propagate the new values of A into the existing
--	   L and U data structures.            */
--	ilsum = Llu->ilsum;
--	ldaspa = Llu->ldalsum;
--	if ( !(dense = doubleCalloc_dist(ldaspa * sp_ienv_dist(3))) )
--	    ABORT("Calloc fails for SPA dense[].");
--	nrbu = CEILING( nsupers, grid->nprow ); /* No. of local block rows */
--	if ( !(Urb_length = intCalloc_dist(nrbu)) )
--	    ABORT("Calloc fails for Urb_length[].");
--	if ( !(Urb_indptr = intMalloc_dist(nrbu)) )
--	    ABORT("Malloc fails for Urb_indptr[].");
--	Lrowind_bc_ptr = Llu->Lrowind_bc_ptr;
--	Lnzval_bc_ptr = Llu->Lnzval_bc_ptr;
--	Ufstnz_br_ptr = Llu->Ufstnz_br_ptr;
--	Unzval_br_ptr = Llu->Unzval_br_ptr;
-+		/* We can propagate the new values of A into the existing
-+		   L and U data structures.            */
-+		ilsum = Llu->ilsum;
-+		ldaspa = Llu->ldalsum;
-+		if ( !(dense = doubleCalloc_dist(ldaspa * sp_ienv_dist(3))) )
-+			ABORT("Calloc fails for SPA dense[].");
-+		nrbu = CEILING( nsupers, grid->nprow ); /* No. of local block rows */
-+		if ( !(Urb_length = intCalloc_dist(nrbu)) )
-+			ABORT("Calloc fails for Urb_length[].");
-+		if ( !(Urb_indptr = intMalloc_dist(nrbu)) )
-+			ABORT("Malloc fails for Urb_indptr[].");
-+		Lrowind_bc_ptr = Llu->Lrowind_bc_ptr;
-+		Lindval_loc_bc_ptr = Llu->Lindval_loc_bc_ptr;
-+		Lnzval_bc_ptr = Llu->Lnzval_bc_ptr;
-+		Ufstnz_br_ptr = Llu->Ufstnz_br_ptr;
-+		Unzval_br_ptr = Llu->Unzval_br_ptr;
- #if ( PRNTlevel>=1 )
--	mem_use += 2.0*nrbu*iword + ldaspa*sp_ienv_dist(3)*dword;
-+		mem_use += 2.0*nrbu*iword + ldaspa*sp_ienv_dist(3)*dword;
- #endif
- #if ( PROFlevel>=1 )
--	t = SuperLU_timer_();
-+		t = SuperLU_timer_();
- #endif
- 
--	/* Initialize Uval to zero. */
--	for (lb = 0; lb < nrbu; ++lb) {
--	    Urb_indptr[lb] = BR_HEADER; /* Skip header in U index[]. */
--	    index = Ufstnz_br_ptr[lb];
--	    if ( index ) {
--		uval = Unzval_br_ptr[lb];
--		len = index[1];
--		for (i = 0; i < len; ++i) uval[i] = zero;
--	    } /* if index != NULL */
--	} /* for lb ... */
--
--	for (jb = 0; jb < nsupers; ++jb) { /* Loop through each block column */
--	    pc = PCOL( jb, grid );
--	    if ( mycol == pc ) { /* Block column jb in my process column */
--		fsupc = FstBlockC( jb );
--		nsupc = SuperSize( jb );
--
-- 		/* Scatter A into SPA (for L), or into U directly. */
--		for (j = fsupc, dense_col = dense; j < FstBlockC(jb+1); ++j) {
--		    for (i = xa[j]; i < xa[j+1]; ++i) {
--			irow = asub[i];
--			gb = BlockNum( irow );
--			if ( myrow == PROW( gb, grid ) ) {
--			    lb = LBi( gb, grid );
-- 			    if ( gb < jb ) { /* in U */
-- 				index = Ufstnz_br_ptr[lb];
-- 				uval = Unzval_br_ptr[lb];
-- 				while (  (k = index[Urb_indptr[lb]]) < jb ) {
-- 				    /* Skip nonzero values in this block */
-- 				    Urb_length[lb] += index[Urb_indptr[lb]+1];
-- 				    /* Move pointer to the next block */
-- 				    Urb_indptr[lb] += UB_DESCRIPTOR
-- 					+ SuperSize( k );
-- 				}
-- 				/*assert(k == jb);*/
-- 				/* start fstnz */
-- 				istart = Urb_indptr[lb] + UB_DESCRIPTOR;
-- 				len = Urb_length[lb];
-- 				fsupc1 = FstBlockC( gb+1 );
-- 				k = j - fsupc;
-- 				/* Sum the lengths of the leading columns */
-- 				for (jj = 0; jj < k; ++jj)
--				    len += fsupc1 - index[istart++];
--				/*assert(irow>=index[istart]);*/
--				uval[len + irow - index[istart]] = a[i];
--			    } else { /* in L; put in SPA first */
--  				irow = ilsum[lb] + irow - FstBlockC( gb );
--  				dense_col[irow] = a[i];
--  			    }
--  			}
--		    } /* for i ... */
--  		    dense_col += ldaspa;
--		} /* for j ... */
-+		/* Initialize Uval to zero. */
-+		for (lb = 0; lb < nrbu; ++lb) {
-+			Urb_indptr[lb] = BR_HEADER; /* Skip header in U index[]. */
-+			index = Ufstnz_br_ptr[lb];
-+			if ( index ) {
-+				uval = Unzval_br_ptr[lb];
-+				len = index[1];
-+				for (i = 0; i < len; ++i) uval[i] = zero;
-+			} /* if index != NULL */
-+		} /* for lb ... */
-+
-+		for (jb = 0; jb < nsupers; ++jb) { /* Loop through each block column */
-+			pc = PCOL( jb, grid );
-+			if ( mycol == pc ) { /* Block column jb in my process column */
-+				fsupc = FstBlockC( jb );
-+				nsupc = SuperSize( jb );
-+
-+				/* Scatter A into SPA (for L), or into U directly. */
-+				for (j = fsupc, dense_col = dense; j < FstBlockC(jb+1); ++j) {
-+					for (i = xa[j]; i < xa[j+1]; ++i) {
-+						irow = asub[i];
-+						gb = BlockNum( irow );
-+						if ( myrow == PROW( gb, grid ) ) {
-+							lb = LBi( gb, grid );
-+							if ( gb < jb ) { /* in U */
-+								index = Ufstnz_br_ptr[lb];
-+								uval = Unzval_br_ptr[lb];
-+								while (  (k = index[Urb_indptr[lb]]) < jb ) {
-+									/* Skip nonzero values in this block */
-+									Urb_length[lb] += index[Urb_indptr[lb]+1];
-+									/* Move pointer to the next block */
-+									Urb_indptr[lb] += UB_DESCRIPTOR
-+										+ SuperSize( k );
-+								}
-+								/*assert(k == jb);*/
-+								/* start fstnz */
-+								istart = Urb_indptr[lb] + UB_DESCRIPTOR;
-+								len = Urb_length[lb];
-+								fsupc1 = FstBlockC( gb+1 );
-+								k = j - fsupc;
-+								/* Sum the lengths of the leading columns */
-+								for (jj = 0; jj < k; ++jj)
-+									len += fsupc1 - index[istart++];
-+								/*assert(irow>=index[istart]);*/
-+								uval[len + irow - index[istart]] = a[i];
-+							} else { /* in L; put in SPA first */
-+								irow = ilsum[lb] + irow - FstBlockC( gb );
-+								dense_col[irow] = a[i];
-+							}
-+						}
-+					} /* for i ... */
-+					dense_col += ldaspa;
-+				} /* for j ... */
- 
- #if ( PROFlevel>=1 )
--		t_u += SuperLU_timer_() - t;
--		t = SuperLU_timer_();
-+				t_u += SuperLU_timer_() - t;
-+				t = SuperLU_timer_();
- #endif
- 
--		/* Gather the values of A from SPA into Lnzval[]. */
--		ljb = LBj( jb, grid ); /* Local block number */
--		index = Lrowind_bc_ptr[ljb];
--		if ( index ) {
--		    nrbl = index[0];   /* Number of row blocks. */
--		    len = index[1];    /* LDA of lusup[]. */
--		    lusup = Lnzval_bc_ptr[ljb];
--		    next_lind = BC_HEADER;
--		    next_lval = 0;
--		    for (jj = 0; jj < nrbl; ++jj) {
--			gb = index[next_lind++];
--			len1 = index[next_lind++]; /* Rows in the block. */
--			lb = LBi( gb, grid );
--			for (bnnz = 0; bnnz < len1; ++bnnz) {
--			    irow = index[next_lind++]; /* Global index. */
--			    irow = ilsum[lb] + irow - FstBlockC( gb );
--			    k = next_lval++;
--			    for (j = 0, dense_col = dense; j < nsupc; ++j) {
--				lusup[k] = dense_col[irow];
--				dense_col[irow] = zero;
--				k += len;
--				dense_col += ldaspa;
--			    }
--			} /* for bnnz ... */
--		    } /* for jj ... */
--		} /* if index ... */
-+				/* Gather the values of A from SPA into Lnzval[]. */
-+				ljb = LBj( jb, grid ); /* Local block number */
-+				index = Lrowind_bc_ptr[ljb];
-+				if ( index ) {
-+					nrbl = index[0];   /* Number of row blocks. */
-+					len = index[1];    /* LDA of lusup[]. */
-+					lusup = Lnzval_bc_ptr[ljb];
-+					next_lind = BC_HEADER;
-+					next_lval = 0;
-+					for (jj = 0; jj < nrbl; ++jj) {
-+						gb = index[next_lind++];
-+						len1 = index[next_lind++]; /* Rows in the block. */
-+						lb = LBi( gb, grid );
-+						for (bnnz = 0; bnnz < len1; ++bnnz) {
-+							irow = index[next_lind++]; /* Global index. */
-+							irow = ilsum[lb] + irow - FstBlockC( gb );
-+							k = next_lval++;
-+							for (j = 0, dense_col = dense; j < nsupc; ++j) {
-+								lusup[k] = dense_col[irow];
-+								dense_col[irow] = zero;
-+								k += len;
-+								dense_col += ldaspa;
-+							}
-+						} /* for bnnz ... */
-+					} /* for jj ... */
-+				} /* if index ... */
- #if ( PROFlevel>=1 )
--		t_l += SuperLU_timer_() - t;
-+				t_l += SuperLU_timer_() - t;
- #endif
--	    } /* if mycol == pc */
--	} /* for jb ... */
-+			} /* if mycol == pc */
-+		} /* for jb ... */
- 
--	SUPERLU_FREE(dense);
--	SUPERLU_FREE(Urb_length);
--	SUPERLU_FREE(Urb_indptr);
-+		SUPERLU_FREE(dense);
-+		SUPERLU_FREE(Urb_length);
-+		SUPERLU_FREE(Urb_indptr);
- #if ( PROFlevel>=1 )
--	if ( !iam ) printf(".. 2nd distribute time: L %.2f\tU %.2f\tu_blks %d\tnrbu %d\n",
--			   t_l, t_u, u_blks, nrbu);
-+		if ( !iam ) printf(".. 2nd distribute time: L %.2f\tU %.2f\tu_blks %d\tnrbu %d\n",
-+				t_l, t_u, u_blks, nrbu);
- #endif
- 
--    } else {
--        /* ------------------------------------------------------------
--	   FIRST TIME CREATING THE L AND U DATA STRUCTURES.
--	   ------------------------------------------------------------*/
-+	} else {
-+		/* ------------------------------------------------------------
-+		   FIRST TIME CREATING THE L AND U DATA STRUCTURES.
-+		   ------------------------------------------------------------*/
- 
- #if ( PROFlevel>=1 )
--	t_l = t_u = 0; u_blks = 0;
-+		t_l = t_u = 0; u_blks = 0;
- #endif
--	/* We first need to set up the L and U data structures and then
--	 * propagate the values of A into them.
--	 */
--	lsub = Glu_freeable->lsub;    /* compressed L subscripts */
--	xlsub = Glu_freeable->xlsub;
--	usub = Glu_freeable->usub;    /* compressed U subscripts */
--	xusub = Glu_freeable->xusub;
--    
--	if ( !(ToRecv = (int *) SUPERLU_MALLOC(nsupers * sizeof(int))) )
--	    ABORT("Malloc fails for ToRecv[].");
--	for (i = 0; i < nsupers; ++i) ToRecv[i] = 0;
--
--	k = CEILING( nsupers, grid->npcol );/* Number of local column blocks */
--	if ( !(ToSendR = (int **) SUPERLU_MALLOC(k*sizeof(int*))) )
--	    ABORT("Malloc fails for ToSendR[].");
--	j = k * grid->npcol;
--	if ( !(index1 = SUPERLU_MALLOC(j * sizeof(int))) )
--	    ABORT("Malloc fails for index[].");
-+		/* We first need to set up the L and U data structures and then
-+		 * propagate the values of A into them.
-+		 */
-+		lsub = Glu_freeable->lsub;    /* compressed L subscripts */
-+		xlsub = Glu_freeable->xlsub;
-+		usub = Glu_freeable->usub;    /* compressed U subscripts */
-+		xusub = Glu_freeable->xusub;
-+
-+		if ( !(ToRecv = (int *) SUPERLU_MALLOC(nsupers * sizeof(int))) )
-+			ABORT("Malloc fails for ToRecv[].");
-+		for (i = 0; i < nsupers; ++i) ToRecv[i] = 0;
-+
-+		k = CEILING( nsupers, grid->npcol );/* Number of local column blocks */
-+		if ( !(ToSendR = (int **) SUPERLU_MALLOC(k*sizeof(int*))) )
-+			ABORT("Malloc fails for ToSendR[].");
-+		j = k * grid->npcol;
-+		if ( !(index1 = SUPERLU_MALLOC(j * sizeof(int))) )
-+			ABORT("Malloc fails for index[].");
- #if ( PRNTlevel>=1 )
--	mem_use += (float) k*sizeof(int_t*) + (j + nsupers)*iword;
-+		mem_use += (float) k*sizeof(int_t*) + (j + nsupers)*iword;
- #endif
--	for (i = 0; i < j; ++i) index1[i] = EMPTY;
--	for (i = 0,j = 0; i < k; ++i, j += grid->npcol) ToSendR[i] = &index1[j];
--	k = CEILING( nsupers, grid->nprow ); /* Number of local block rows */
--
--	/* Pointers to the beginning of each block row of U. */
--	if ( !(Unzval_br_ptr = 
--              (double**)SUPERLU_MALLOC(k * sizeof(double*))) )
--	    ABORT("Malloc fails for Unzval_br_ptr[].");
--	if ( !(Ufstnz_br_ptr = (int_t**)SUPERLU_MALLOC(k * sizeof(int_t*))) )
--	    ABORT("Malloc fails for Ufstnz_br_ptr[].");
--	
--	if ( !(ToSendD = SUPERLU_MALLOC(k * sizeof(int))) )
--	    ABORT("Malloc fails for ToSendD[].");
--	for (i = 0; i < k; ++i) ToSendD[i] = NO;
--	if ( !(ilsum = intMalloc_dist(k+1)) )
--	    ABORT("Malloc fails for ilsum[].");
--
--	/* Auxiliary arrays used to set up U block data structures.
--	   They are freed on return. */
--	if ( !(rb_marker = intCalloc_dist(k)) )
--	    ABORT("Calloc fails for rb_marker[].");
--	if ( !(Urb_length = intCalloc_dist(k)) )
--	    ABORT("Calloc fails for Urb_length[].");
--	if ( !(Urb_indptr = intMalloc_dist(k)) )
--	    ABORT("Malloc fails for Urb_indptr[].");
--	if ( !(Urb_fstnz = intCalloc_dist(k)) )
--	    ABORT("Calloc fails for Urb_fstnz[].");
--	if ( !(Ucbs = intCalloc_dist(k)) )
--	    ABORT("Calloc fails for Ucbs[].");
-+		for (i = 0; i < j; ++i) index1[i] = EMPTY;
-+		for (i = 0,j = 0; i < k; ++i, j += grid->npcol) ToSendR[i] = &index1[j];
-+		k = CEILING( nsupers, grid->nprow ); /* Number of local block rows */
-+
-+		/* Pointers to the beginning of each block row of U. */
-+		if ( !(Unzval_br_ptr = 
-+					(double**)SUPERLU_MALLOC(k * sizeof(double*))) )
-+			ABORT("Malloc fails for Unzval_br_ptr[].");
-+		if ( !(Ufstnz_br_ptr = (int_t**)SUPERLU_MALLOC(k * sizeof(int_t*))) )
-+			ABORT("Malloc fails for Ufstnz_br_ptr[].");
-+
-+		if ( !(ToSendD = SUPERLU_MALLOC(k * sizeof(int))) )
-+			ABORT("Malloc fails for ToSendD[].");
-+		for (i = 0; i < k; ++i) ToSendD[i] = NO;
-+		if ( !(ilsum = intMalloc_dist(k+1)) )
-+			ABORT("Malloc fails for ilsum[].");
-+
-+		/* Auxiliary arrays used to set up U block data structures.
-+		   They are freed on return. */
-+		if ( !(rb_marker = intCalloc_dist(k)) )
-+			ABORT("Calloc fails for rb_marker[].");
-+		if ( !(Urb_length = intCalloc_dist(k)) )
-+			ABORT("Calloc fails for Urb_length[].");
-+		if ( !(Urb_indptr = intMalloc_dist(k)) )
-+			ABORT("Malloc fails for Urb_indptr[].");
-+		if ( !(Urb_fstnz = intCalloc_dist(k)) )
-+			ABORT("Calloc fails for Urb_fstnz[].");
-+		if ( !(Ucbs = intCalloc_dist(k)) )
-+			ABORT("Calloc fails for Ucbs[].");
- #if ( PRNTlevel>=1 )	
--	mem_use += 2.0*k*sizeof(int_t*) + (7*k+1)*iword;
-+		mem_use += 2.0*k*sizeof(int_t*) + (7*k+1)*iword;
- #endif
--	/* Compute ldaspa and ilsum[]. */
--	ldaspa = 0;
--	ilsum[0] = 0;
--	for (gb = 0; gb < nsupers; ++gb) {
--	    if ( myrow == PROW( gb, grid ) ) {
--		i = SuperSize( gb );
--		ldaspa += i;
--		lb = LBi( gb, grid );
--		ilsum[lb + 1] = ilsum[lb] + i;
--	    }
--	}
--	
-+		/* Compute ldaspa and ilsum[]. */
-+		ldaspa = 0;
-+		ilsum[0] = 0;
-+		for (gb = 0; gb < nsupers; ++gb) {
-+			if ( myrow == PROW( gb, grid ) ) {
-+				i = SuperSize( gb );
-+				ldaspa += i;
-+				lb = LBi( gb, grid );
-+				ilsum[lb + 1] = ilsum[lb] + i;
-+			}
-+		}
-+
- #if ( PROFlevel>=1 )
--	t = SuperLU_timer_();
-+		t = SuperLU_timer_();
- #endif
--	/* ------------------------------------------------------------
--	   COUNT NUMBER OF ROW BLOCKS AND THE LENGTH OF EACH BLOCK IN U.
--	   THIS ACCOUNTS FOR ONE-PASS PROCESSING OF G(U).
--	   ------------------------------------------------------------*/
--	
--	/* Loop through each supernode column. */
--	for (jb = 0; jb < nsupers; ++jb) {
--	    pc = PCOL( jb, grid );
--	    fsupc = FstBlockC( jb );
--	    nsupc = SuperSize( jb );
--	    /* Loop through each column in the block. */
--	    for (j = fsupc; j < fsupc + nsupc; ++j) {
--		/* usub[*] contains only "first nonzero" in each segment. */
--		for (i = xusub[j]; i < xusub[j+1]; ++i) {
--		    irow = usub[i]; /* First nonzero of the segment. */
--		    gb = BlockNum( irow );
--		    kcol = PCOL( gb, grid );
--		    ljb = LBj( gb, grid );
--		    if ( mycol == kcol && mycol != pc ) ToSendR[ljb][pc] = YES;
--		    pr = PROW( gb, grid );
--		    lb = LBi( gb, grid );
--		    if ( mycol == pc ) {
--			if  ( myrow == pr ) {
--			    ToSendD[lb] = YES;
--			    /* Count nonzeros in entire block row. */
--			    Urb_length[lb] += FstBlockC( gb+1 ) - irow;
--			    if (rb_marker[lb] <= jb) {/* First see the block */
--				rb_marker[lb] = jb + 1;
--				Urb_fstnz[lb] += nsupc;
--				++Ucbs[lb]; /* Number of column blocks
--					       in block row lb. */
-+		/* ------------------------------------------------------------
-+		   COUNT NUMBER OF ROW BLOCKS AND THE LENGTH OF EACH BLOCK IN U.
-+		   THIS ACCOUNTS FOR ONE-PASS PROCESSING OF G(U).
-+		   ------------------------------------------------------------*/
-+
-+		/* Loop through each supernode column. */
-+		for (jb = 0; jb < nsupers; ++jb) {
-+			pc = PCOL( jb, grid );
-+			fsupc = FstBlockC( jb );
-+			nsupc = SuperSize( jb );
-+			/* Loop through each column in the block. */
-+			for (j = fsupc; j < fsupc + nsupc; ++j) {
-+				/* usub[*] contains only "first nonzero" in each segment. */
-+				for (i = xusub[j]; i < xusub[j+1]; ++i) {
-+					irow = usub[i]; /* First nonzero of the segment. */
-+					gb = BlockNum( irow );
-+					kcol = PCOL( gb, grid );
-+					ljb = LBj( gb, grid );
-+					if ( mycol == kcol && mycol != pc ) ToSendR[ljb][pc] = YES;
-+					pr = PROW( gb, grid );
-+					lb = LBi( gb, grid );
-+					if ( mycol == pc ) {
-+						if  ( myrow == pr ) {
-+							ToSendD[lb] = YES;
-+							/* Count nonzeros in entire block row. */
-+							Urb_length[lb] += FstBlockC( gb+1 ) - irow;
-+							if (rb_marker[lb] <= jb) {/* First see the block */
-+								rb_marker[lb] = jb + 1;
-+								Urb_fstnz[lb] += nsupc;
-+								++Ucbs[lb]; /* Number of column blocks
-+									       in block row lb. */
- #if ( PRNTlevel>=1 )
--				++nUblocks;
-+								++nUblocks;
- #endif
--			    }
--			    ToRecv[gb] = 1;
--			} else ToRecv[gb] = 2; /* Do I need 0, 1, 2 ? */
--		    }
--		} /* for i ... */
--	    } /* for j ... */
--	} /* for jb ... */
--	
--	/* Set up the initial pointers for each block row in U. */
--	nrbu = CEILING( nsupers, grid->nprow );/* Number of local block rows */
--	for (lb = 0; lb < nrbu; ++lb) {
--	    len = Urb_length[lb];
--	    rb_marker[lb] = 0; /* Reset block marker. */
--	    if ( len ) {
--		/* Add room for descriptors */
--		len1 = Urb_fstnz[lb] + BR_HEADER + Ucbs[lb] * UB_DESCRIPTOR;
--		if ( !(index = intMalloc_dist(len1+1)) )
--		    ABORT("Malloc fails for Uindex[].");
--		Ufstnz_br_ptr[lb] = index;
--		if ( !(Unzval_br_ptr[lb] = doubleMalloc_dist(len)) )
--		    ABORT("Malloc fails for Unzval_br_ptr[*][].");
--		mybufmax[2] = SUPERLU_MAX( mybufmax[2], len1 );
--		mybufmax[3] = SUPERLU_MAX( mybufmax[3], len );
--		index[0] = Ucbs[lb]; /* Number of column blocks */
--		index[1] = len;      /* Total length of nzval[] */
--		index[2] = len1;     /* Total length of index[] */
--		index[len1] = -1;    /* End marker */
--	    } else {
--		Ufstnz_br_ptr[lb] = NULL;
--		Unzval_br_ptr[lb] = NULL;
--	    }
--	    Urb_length[lb] = 0; /* Reset block length. */
--	    Urb_indptr[lb] = BR_HEADER; /* Skip header in U index[]. */
-- 	    Urb_fstnz[lb] = BR_HEADER;
--	} /* for lb ... */
--
--	SUPERLU_FREE(Ucbs);
-+							}
-+							ToRecv[gb] = 1;
-+						} else ToRecv[gb] = 2; /* Do I need 0, 1, 2 ? */
-+					}
-+				} /* for i ... */
-+			} /* for j ... */
-+		} /* for jb ... */
-+
-+		/* Set up the initial pointers for each block row in U. */
-+		nrbu = CEILING( nsupers, grid->nprow );/* Number of local block rows */
-+		for (lb = 0; lb < nrbu; ++lb) {
-+			len = Urb_length[lb];
-+			rb_marker[lb] = 0; /* Reset block marker. */
-+			if ( len ) {
-+				/* Add room for descriptors */
-+				len1 = Urb_fstnz[lb] + BR_HEADER + Ucbs[lb] * UB_DESCRIPTOR;
-+				if ( !(index = intMalloc_dist(len1+1)) )
-+					ABORT("Malloc fails for Uindex[].");
-+				Ufstnz_br_ptr[lb] = index;
-+				if ( !(Unzval_br_ptr[lb] = doubleMalloc_dist(len)) )
-+					ABORT("Malloc fails for Unzval_br_ptr[*][].");
-+				mybufmax[2] = SUPERLU_MAX( mybufmax[2], len1 );
-+				mybufmax[3] = SUPERLU_MAX( mybufmax[3], len );
-+				index[0] = Ucbs[lb]; /* Number of column blocks */
-+				index[1] = len;      /* Total length of nzval[] */
-+				index[2] = len1;     /* Total length of index[] */
-+				index[len1] = -1;    /* End marker */
-+			} else {
-+				Ufstnz_br_ptr[lb] = NULL;
-+				Unzval_br_ptr[lb] = NULL;
-+			}
-+			Urb_length[lb] = 0; /* Reset block length. */
-+			Urb_indptr[lb] = BR_HEADER; /* Skip header in U index[]. */
-+			Urb_fstnz[lb] = BR_HEADER;
-+		} /* for lb ... */
-+
-+		SUPERLU_FREE(Ucbs);
- 
- #if ( PROFlevel>=1 )
--	t = SuperLU_timer_() - t;
--	if ( !iam) printf(".. Phase 2 - setup U strut time: %.2f\t\n", t);
-+		t = SuperLU_timer_() - t;
-+		if ( !iam) printf(".. Phase 2 - setup U strut time: %.2f\t\n", t);
- #endif
- #if ( PRNTlevel>=1 )
--        mem_use -= 2.0*k * iword;
-+		mem_use -= 2.0*k * iword;
- #endif
--	/* Auxiliary arrays used to set up L block data structures.
--	   They are freed on return.
--	   k is the number of local row blocks.   */
--	if ( !(Lrb_length = intCalloc_dist(k)) )
--	    ABORT("Calloc fails for Lrb_length[].");
--	if ( !(Lrb_number = intMalloc_dist(k)) )
--	    ABORT("Malloc fails for Lrb_number[].");
--	if ( !(Lrb_indptr = intMalloc_dist(k)) )
--	    ABORT("Malloc fails for Lrb_indptr[].");
--	if ( !(Lrb_valptr = intMalloc_dist(k)) )
--	    ABORT("Malloc fails for Lrb_valptr[].");
--	if ( !(dense = doubleCalloc_dist(ldaspa * sp_ienv_dist(3))) )
--	    ABORT("Calloc fails for SPA dense[].");
--
--	/* These counts will be used for triangular solves. */
--	if ( !(fmod = intCalloc_dist(k)) )
--	    ABORT("Calloc fails for fmod[].");
--	if ( !(bmod = intCalloc_dist(k)) )
--	    ABORT("Calloc fails for bmod[].");
--
--	/* ------------------------------------------------ */
-+		/* Auxiliary arrays used to set up L block data structures.
-+		   They are freed on return.
-+		   k is the number of local row blocks.   */
-+		if ( !(Lrb_length = intCalloc_dist(k)) )
-+			ABORT("Calloc fails for Lrb_length[].");
-+		if ( !(Lrb_number = intMalloc_dist(k)) )
-+			ABORT("Malloc fails for Lrb_number[].");
-+		if ( !(Lrb_indptr = intMalloc_dist(k)) )
-+			ABORT("Malloc fails for Lrb_indptr[].");
-+		if ( !(Lrb_valptr = intMalloc_dist(k)) )
-+			ABORT("Malloc fails for Lrb_valptr[].");
-+		if ( !(dense = doubleCalloc_dist(ldaspa * sp_ienv_dist(3))) )
-+			ABORT("Calloc fails for SPA dense[].");
-+
-+		/* These counts will be used for triangular solves. */
-+		if ( !(fmod = intCalloc_dist(k)) )
-+			ABORT("Calloc fails for fmod[].");
-+		if ( !(bmod = intCalloc_dist(k)) )
-+			ABORT("Calloc fails for bmod[].");
-+
-+		/* ------------------------------------------------ */
- #if ( PRNTlevel>=1 )	
--	mem_use += 6.0*k*iword + ldaspa*sp_ienv_dist(3)*dword;
-+		mem_use += 6.0*k*iword + ldaspa*sp_ienv_dist(3)*dword;
- #endif
--	k = CEILING( nsupers, grid->npcol );/* Number of local block columns */
--
--	/* Pointers to the beginning of each block column of L. */
--	if ( !(Lnzval_bc_ptr = 
--              (double**)SUPERLU_MALLOC(k * sizeof(double*))) )
--	    ABORT("Malloc fails for Lnzval_bc_ptr[].");
--	if ( !(Lrowind_bc_ptr = (int_t**)SUPERLU_MALLOC(k * sizeof(int_t*))) )
--	    ABORT("Malloc fails for Lrowind_bc_ptr[].");
--	Lrowind_bc_ptr[k-1] = NULL;
--
--	/* These lists of processes will be used for triangular solves. */
--	if ( !(fsendx_plist = (int_t **) SUPERLU_MALLOC(k*sizeof(int_t*))) )
--	    ABORT("Malloc fails for fsendx_plist[].");
--	len = k * grid->nprow;
--	if ( !(index = intMalloc_dist(len)) )
--	    ABORT("Malloc fails for fsendx_plist[0]");
--	for (i = 0; i < len; ++i) index[i] = EMPTY;
--	for (i = 0, j = 0; i < k; ++i, j += grid->nprow)
--	    fsendx_plist[i] = &index[j];
--	if ( !(bsendx_plist = (int_t **) SUPERLU_MALLOC(k*sizeof(int_t*))) )
--	    ABORT("Malloc fails for bsendx_plist[].");
--	if ( !(index = intMalloc_dist(len)) )
--	    ABORT("Malloc fails for bsendx_plist[0]");
--	for (i = 0; i < len; ++i) index[i] = EMPTY;
--	for (i = 0, j = 0; i < k; ++i, j += grid->nprow)
--	    bsendx_plist[i] = &index[j];
--	/* -------------------------------------------------------------- */
-+		k = CEILING( nsupers, grid->npcol );/* Number of local block columns */
-+
-+		/* Pointers to the beginning of each block column of L. */
-+		if ( !(Lnzval_bc_ptr = 
-+					(double**)SUPERLU_MALLOC(k * sizeof(double*))) )
-+			ABORT("Malloc fails for Lnzval_bc_ptr[].");
-+		if ( !(Lrowind_bc_ptr = (int_t**)SUPERLU_MALLOC(k * sizeof(int_t*))) )
-+			ABORT("Malloc fails for Lrowind_bc_ptr[].");
-+		Lrowind_bc_ptr[k-1] = NULL;
-+		if ( !(Lindval_loc_bc_ptr = 
-+					(int_t**)SUPERLU_MALLOC(k * sizeof(int_t*))) )
-+			ABORT("Malloc fails for Lindval_loc_bc_ptr[].");
-+		Lindval_loc_bc_ptr[k-1] = NULL;
-+
-+		if ( !(Linv_bc_ptr = 
-+					(double**)SUPERLU_MALLOC(k * sizeof(double*))) ) {
-+			fprintf(stderr, "Malloc fails for Linv_bc_ptr[].");
-+		}  
-+		if ( !(Uinv_bc_ptr = 
-+					(double**)SUPERLU_MALLOC(k * sizeof(double*))) ) {
-+			fprintf(stderr, "Malloc fails for Uinv_bc_ptr[].");
-+		}  
-+		Linv_bc_ptr[k-1] = NULL;
-+		Uinv_bc_ptr[k-1] = NULL;
-+
-+		/* These lists of processes will be used for triangular solves. */
-+		if ( !(fsendx_plist = (int_t **) SUPERLU_MALLOC(k*sizeof(int_t*))) )
-+			ABORT("Malloc fails for fsendx_plist[].");
-+		len = k * grid->nprow;
-+		if ( !(index = intMalloc_dist(len)) )
-+			ABORT("Malloc fails for fsendx_plist[0]");
-+		for (i = 0; i < len; ++i) index[i] = EMPTY;
-+		for (i = 0, j = 0; i < k; ++i, j += grid->nprow)
-+			fsendx_plist[i] = &index[j];
-+		if ( !(bsendx_plist = (int_t **) SUPERLU_MALLOC(k*sizeof(int_t*))) )
-+			ABORT("Malloc fails for bsendx_plist[].");
-+		if ( !(index = intMalloc_dist(len)) )
-+			ABORT("Malloc fails for bsendx_plist[0]");
-+		for (i = 0; i < len; ++i) index[i] = EMPTY;
-+		for (i = 0, j = 0; i < k; ++i, j += grid->nprow)
-+			bsendx_plist[i] = &index[j];
-+		/* -------------------------------------------------------------- */
- #if ( PRNTlevel>=1 )
--	mem_use += 4.0*k*sizeof(int_t*) + 2.0*len*iword;
-+		mem_use += 4.0*k*sizeof(int_t*) + 2.0*len*iword;
- #endif
- 
--	/*------------------------------------------------------------
--	  PROPAGATE ROW SUBSCRIPTS AND VALUES OF A INTO L AND U BLOCKS.
--	  THIS ACCOUNTS FOR ONE-PASS PROCESSING OF A, L AND U.
--	  ------------------------------------------------------------*/
--
--	for (jb = 0; jb < nsupers; ++jb) { /* for each block column ... */
--	    pc = PCOL( jb, grid );
--	    if ( mycol == pc ) { /* Block column jb in my process column */
--		fsupc = FstBlockC( jb );
--		nsupc = SuperSize( jb );
--		ljb = LBj( jb, grid ); /* Local block number */
--		
--		/* Scatter A into SPA. */
--		for (j = fsupc, dense_col = dense; j < FstBlockC(jb+1); ++j) {
--		    for (i = xa[j]; i < xa[j+1]; ++i) {
--			irow = asub[i];
--			gb = BlockNum( irow );
--			if ( myrow == PROW( gb, grid ) ) {
--			    lb = LBi( gb, grid );
--			    irow = ilsum[lb] + irow - FstBlockC( gb );
--			    dense_col[irow] = a[i];
--			}
--		    }
--		    dense_col += ldaspa;
--		} /* for j ... */
--
--		jbrow = PROW( jb, grid );
--
--		/*------------------------------------------------
--		 * SET UP U BLOCKS.
--		 *------------------------------------------------*/
-+		/*------------------------------------------------------------
-+		  PROPAGATE ROW SUBSCRIPTS AND VALUES OF A INTO L AND U BLOCKS.
-+		  THIS ACCOUNTS FOR ONE-PASS PROCESSING OF A, L AND U.
-+		  ------------------------------------------------------------*/
-+
-+		for (jb = 0; jb < nsupers; ++jb) { /* for each block column ... */
-+			pc = PCOL( jb, grid );
-+			if ( mycol == pc ) { /* Block column jb in my process column */
-+				fsupc = FstBlockC( jb );
-+				nsupc = SuperSize( jb );
-+				ljb = LBj( jb, grid ); /* Local block number */
-+
-+				/* Scatter A into SPA. */
-+				for (j = fsupc, dense_col = dense; j < FstBlockC(jb+1); ++j) {
-+					for (i = xa[j]; i < xa[j+1]; ++i) {
-+						irow = asub[i];
-+						gb = BlockNum( irow );
-+						if ( myrow == PROW( gb, grid ) ) {
-+							lb = LBi( gb, grid );
-+							irow = ilsum[lb] + irow - FstBlockC( gb );
-+							dense_col[irow] = a[i];
-+						}
-+					}
-+					dense_col += ldaspa;
-+				} /* for j ... */
-+
-+				jbrow = PROW( jb, grid );
-+
-+				/*------------------------------------------------
-+				 * SET UP U BLOCKS.
-+				 *------------------------------------------------*/
- #if ( PROFlevel>=1 )
--		t = SuperLU_timer_();
-+				t = SuperLU_timer_();
- #endif
--		kseen = 0;
--		dense_col = dense;
--		/* Loop through each column in the block column. */
--		for (j = fsupc; j < FstBlockC( jb+1 ); ++j) {
--		    istart = xusub[j];
--		    /* NOTE: Only the first nonzero index of the segment
--		       is stored in usub[]. */
--		    for (i = istart; i < xusub[j+1]; ++i) {
--			irow = usub[i]; /* First nonzero in the segment. */
--			gb = BlockNum( irow );
--			pr = PROW( gb, grid );
--			if ( pr != jbrow &&
--			     myrow == jbrow &&  /* diag. proc. owning jb */
--			     bsendx_plist[ljb][pr] == EMPTY ) {
--			    bsendx_plist[ljb][pr] = YES;
--			    ++nbsendx;
--                        }
--			if ( myrow == pr ) {
--			    lb = LBi( gb, grid ); /* Local block number */
--			    index = Ufstnz_br_ptr[lb];
--			    uval = Unzval_br_ptr[lb];
--			    fsupc1 = FstBlockC( gb+1 );
--			    if (rb_marker[lb] <= jb) { /* First time see 
--							  the block       */
--				rb_marker[lb] = jb + 1;
--				Urb_indptr[lb] = Urb_fstnz[lb];;
--				index[Urb_indptr[lb]] = jb; /* Descriptor */
--				Urb_indptr[lb] += UB_DESCRIPTOR;
--				/* Record the first location in index[] of the
--				   next block */
--				Urb_fstnz[lb] = Urb_indptr[lb] + nsupc;
--				len = Urb_indptr[lb];/* Start fstnz in index */
--				index[len-1] = 0;
--				for (k = 0; k < nsupc; ++k)
--				    index[len+k] = fsupc1;
--				if ( gb != jb )/* Exclude diagonal block. */
--				    ++bmod[lb];/* Mod. count for back solve */
--				if ( kseen == 0 && myrow != jbrow ) {
--				    ++nbrecvx;
--				    kseen = 1;
--				}
--			    } else { /* Already saw the block */
--				len = Urb_indptr[lb];/* Start fstnz in index */
--			    }
--			    jj = j - fsupc;
--			    index[len+jj] = irow;
--			    /* Load the numerical values */
--			    k = fsupc1 - irow; /* No. of nonzeros in segment */
--			    index[len-1] += k; /* Increment block length in
--						  Descriptor */
--			    irow = ilsum[lb] + irow - FstBlockC( gb );
--			    for (ii = 0; ii < k; ++ii) {
--				uval[Urb_length[lb]++] = dense_col[irow + ii];
--				dense_col[irow + ii] = zero;
--			    }
--			} /* if myrow == pr ... */
--		    } /* for i ... */
--                    dense_col += ldaspa;
--		} /* for j ... */
-+				kseen = 0;
-+				dense_col = dense;
-+				/* Loop through each column in the block column. */
-+				for (j = fsupc; j < FstBlockC( jb+1 ); ++j) {
-+					istart = xusub[j];
-+					/* NOTE: Only the first nonzero index of the segment
-+					   is stored in usub[]. */
-+					for (i = istart; i < xusub[j+1]; ++i) {
-+						irow = usub[i]; /* First nonzero in the segment. */
-+						gb = BlockNum( irow );
-+						pr = PROW( gb, grid );
-+						if ( pr != jbrow &&
-+								myrow == jbrow &&  /* diag. proc. owning jb */
-+								bsendx_plist[ljb][pr] == EMPTY ) {
-+							bsendx_plist[ljb][pr] = YES;
-+							// if(ljb==0){
-+								// printf("no here??\n");
-+								// fflush(stdout);
-+							// }
-+							++nbsendx;
-+						}
-+						if ( myrow == pr ) {
-+							lb = LBi( gb, grid ); /* Local block number */
-+							index = Ufstnz_br_ptr[lb];
-+							uval = Unzval_br_ptr[lb];
-+							fsupc1 = FstBlockC( gb+1 );
-+							if (rb_marker[lb] <= jb) { /* First time see 
-+										      the block       */
-+								rb_marker[lb] = jb + 1;
-+								Urb_indptr[lb] = Urb_fstnz[lb];;
-+								index[Urb_indptr[lb]] = jb; /* Descriptor */
-+								Urb_indptr[lb] += UB_DESCRIPTOR;
-+								/* Record the first location in index[] of the
-+								   next block */
-+								Urb_fstnz[lb] = Urb_indptr[lb] + nsupc;
-+								len = Urb_indptr[lb];/* Start fstnz in index */
-+								index[len-1] = 0;
-+								for (k = 0; k < nsupc; ++k)
-+									index[len+k] = fsupc1;
-+								if ( gb != jb )/* Exclude diagonal block. */
-+									++bmod[lb];/* Mod. count for back solve */
-+								if ( kseen == 0 && myrow != jbrow ) {
-+									++nbrecvx;
-+									kseen = 1;
-+								}
-+							} else { /* Already saw the block */
-+								len = Urb_indptr[lb];/* Start fstnz in index */
-+							}
-+							jj = j - fsupc;
-+							index[len+jj] = irow;
-+							/* Load the numerical values */
-+							k = fsupc1 - irow; /* No. of nonzeros in segment */
-+							index[len-1] += k; /* Increment block length in
-+									      Descriptor */
-+							irow = ilsum[lb] + irow - FstBlockC( gb );
-+							for (ii = 0; ii < k; ++ii) {
-+								uval[Urb_length[lb]++] = dense_col[irow + ii];
-+								dense_col[irow + ii] = zero;
-+							}
-+						} /* if myrow == pr ... */
-+					} /* for i ... */
-+					dense_col += ldaspa;
-+				} /* for j ... */
- 
- #if ( PROFlevel>=1 )
--		t_u += SuperLU_timer_() - t;
--		t = SuperLU_timer_();
-+				t_u += SuperLU_timer_() - t;
-+				t = SuperLU_timer_();
- #endif		
--		/*------------------------------------------------
--		 * SET UP L BLOCKS.
--		 *------------------------------------------------*/
--
--		/* Count number of blocks and length of each block. */
--		nrbl = 0;
--		len = 0; /* Number of row subscripts I own. */
--		kseen = 0;
--		istart = xlsub[fsupc];
--		for (i = istart; i < xlsub[fsupc+1]; ++i) {
--		    irow = lsub[i];
--		    gb = BlockNum( irow ); /* Global block number */
--		    pr = PROW( gb, grid ); /* Process row owning this block */
--		    if ( pr != jbrow &&
--			 myrow == jbrow &&  /* diag. proc. owning jb */
--			 fsendx_plist[ljb][pr] == EMPTY /* first time */ ) {
--			fsendx_plist[ljb][pr] = YES;
--			++nfsendx;
--                    }
--		    if ( myrow == pr ) {
--			lb = LBi( gb, grid );  /* Local block number */
--			if (rb_marker[lb] <= jb) { /* First see this block */
--			    rb_marker[lb] = jb + 1;
--			    Lrb_length[lb] = 1;
--			    Lrb_number[nrbl++] = gb;
--			    if ( gb != jb ) /* Exclude diagonal block. */
--				++fmod[lb]; /* Mod. count for forward solve */
--			    if ( kseen == 0 && myrow != jbrow ) {
--				++nfrecvx;
--				kseen = 1;
--			    }
-+				/*------------------------------------------------
-+				 * SET UP L BLOCKS.
-+				 *------------------------------------------------*/
-+
-+				/* Count number of blocks and length of each block. */
-+				nrbl = 0;
-+				len = 0; /* Number of row subscripts I own. */
-+				kseen = 0;
-+				istart = xlsub[fsupc];
-+				for (i = istart; i < xlsub[fsupc+1]; ++i) {
-+					irow = lsub[i];
-+					gb = BlockNum( irow ); /* Global block number */
-+					pr = PROW( gb, grid ); /* Process row owning this block */
-+					if ( pr != jbrow &&
-+							myrow == jbrow &&  /* diag. proc. owning jb */
-+							fsendx_plist[ljb][pr] == EMPTY /* first time */ ) {
-+						fsendx_plist[ljb][pr] = YES;
-+						++nfsendx;
-+					}
-+					if ( myrow == pr ) {
-+						lb = LBi( gb, grid );  /* Local block number */
-+						if (rb_marker[lb] <= jb) { /* First see this block */
-+							rb_marker[lb] = jb + 1;
-+							Lrb_length[lb] = 1;
-+							Lrb_number[nrbl++] = gb;
-+							// if(gb==747)printf("worita %5d%5d",iam,jb); 
-+							if ( gb != jb ) /* Exclude diagonal block. */
-+								++fmod[lb]; /* Mod. count for forward solve */
-+							if ( kseen == 0 && myrow != jbrow ) {
-+								++nfrecvx;
-+								kseen = 1;
-+							}
- #if ( PRNTlevel>=1 )
--			    ++nLblocks;
-+							++nLblocks;
- #endif
--			} else {
--			    ++Lrb_length[lb];
-+						} else {
-+							++Lrb_length[lb];
-+						}
-+						++len;
-+					}
-+				} /* for i ... */
-+
-+				if ( nrbl ) { /* Do not ensure the blocks are sorted! */
-+					/* Set up the initial pointers for each block in 
-+					   index[] and nzval[]. */
-+					/* Add room for descriptors */
-+					len1 = len + BC_HEADER + nrbl * LB_DESCRIPTOR;
-+					if ( !(index = intMalloc_dist(len1)) ) 
-+						ABORT("Malloc fails for index[]");				
-+					Lrowind_bc_ptr[ljb] = index;								 
-+					if (!(Lnzval_bc_ptr[ljb] = 
-+								doubleMalloc_dist(len*nsupc))) {
-+						fprintf(stderr, "col block " IFMT " ", jb);
-+						ABORT("Malloc fails for Lnzval_bc_ptr[ljb][]");
-+					}
-+					// if ( !(Lindval_loc_bc_ptr[ljb] = intCalloc_dist(nrbl*3)) ) 
-+					if ( !(Lindval_loc_bc_ptr[ljb] = intCalloc_dist(((nrbl*3 + (aln_i - 1)) / aln_i) * aln_i)) ) 
-+						ABORT("Malloc fails for Lindval_loc_bc_ptr[ljb][]");
-+
-+
-+
-+
-+					if (!(Linv_bc_ptr[ljb] = 
-+								doubleCalloc_dist(nsupc*nsupc))) {
-+						fprintf(stderr, "Malloc fails for Linv_bc_ptr[*][] col block " IFMT, jb);
-+					}
-+					if (!(Uinv_bc_ptr[ljb] = 
-+								doubleCalloc_dist(nsupc*nsupc))) {
-+						fprintf(stderr, "Malloc fails for Uinv_bc_ptr[*][] col block " IFMT, jb);
-+					}
-+
-+					mybufmax[0] = SUPERLU_MAX( mybufmax[0], len1 );
-+					mybufmax[1] = SUPERLU_MAX( mybufmax[1], len*nsupc );
-+					mybufmax[4] = SUPERLU_MAX( mybufmax[4], len );
-+					index[0] = nrbl;  /* Number of row blocks */
-+					index[1] = len;   /* LDA of the nzval[] */
-+					next_lind = BC_HEADER;
-+					next_lval = 0;
-+					for (k = 0; k < nrbl; ++k) {
-+						gb = Lrb_number[k];
-+						lb = LBi( gb, grid );
-+						len = Lrb_length[lb];
-+
-+
-+						Lindval_loc_bc_ptr[ljb][k] = lb;
-+						Lindval_loc_bc_ptr[ljb][k+nrbl] = next_lind;
-+						Lindval_loc_bc_ptr[ljb][k+nrbl*2] = next_lval;			
-+
-+						// if(ljb==0){ 
-+						// printf("lb %5d, ind %5d, val %5d\n",lb,next_lind,next_lval);
-+						// fflush(stdout);
-+						// }
-+
-+						Lrb_length[lb] = 0;  /* Reset vector of block length */
-+						index[next_lind++] = gb; /* Descriptor */
-+						index[next_lind++] = len; 
-+						Lrb_indptr[lb] = next_lind;
-+						Lrb_valptr[lb] = next_lval;
-+						next_lind += len;
-+						next_lval += len;
-+					}
-+
-+
-+					/* Propagate the compressed row subscripts to Lindex[],
-+					   and the initial values of A from SPA into Lnzval[]. */
-+					lusup = Lnzval_bc_ptr[ljb];
-+					len = index[1];  /* LDA of lusup[] */
-+					for (i = istart; i < xlsub[fsupc+1]; ++i) {
-+						irow = lsub[i];
-+						gb = BlockNum( irow );
-+						if ( myrow == PROW( gb, grid ) ) {
-+							lb = LBi( gb, grid );
-+							k = Lrb_indptr[lb]++; /* Random access a block */
-+							index[k] = irow;
-+							k = Lrb_valptr[lb]++;
-+							irow = ilsum[lb] + irow - FstBlockC( gb );
-+							for (j = 0, dense_col = dense; j < nsupc; ++j) {
-+								lusup[k] = dense_col[irow];
-+								dense_col[irow] = zero;
-+								k += len;
-+								dense_col += ldaspa;
-+							}
-+						}
-+					} /* for i ... */
-+
-+					Lrowind_bc_ptr[ljb] = index;
-+					Lnzval_bc_ptr[ljb] = lusup; 
-+
-+
-+					/* sort Lindval_loc_bc_ptr[ljb], Lrowind_bc_ptr[ljb] and Lnzval_bc_ptr[ljb] here*/
-+					if(nrbl>1){
-+						krow = PROW( jb, grid );
-+						if(myrow==krow){ /* skip the diagonal block */
-+							uu=nrbl-2;
-+							lloc = &Lindval_loc_bc_ptr[ljb][1];
-+						}else{
-+							uu=nrbl-1;	
-+							lloc = Lindval_loc_bc_ptr[ljb];
-+						}	
-+						quickSortM(lloc,0,uu,nrbl,0,3);	
-+					}
-+
-+
-+					if ( !(index_srt = intMalloc_dist(len1)) ) 
-+						ABORT("Malloc fails for index_srt[]");				
-+					if (!(lusup_srt = doubleMalloc_dist(len*nsupc))) 
-+						ABORT("Malloc fails for lusup_srt[]");
-+
-+					idx_indx = BC_HEADER;
-+					idx_lusup = 0;
-+					for (jj=0;jj<BC_HEADER;jj++)
-+						index_srt[jj] = index[jj];
-+
-+					for(i=0;i<nrbl;i++){
-+						nbrow = index[Lindval_loc_bc_ptr[ljb][i+nrbl]+1];
-+						for (jj=0;jj<LB_DESCRIPTOR+nbrow;jj++){
-+							index_srt[idx_indx++] = index[Lindval_loc_bc_ptr[ljb][i+nrbl]+jj];
-+						}
-+
-+						Lindval_loc_bc_ptr[ljb][i+nrbl] = idx_indx - LB_DESCRIPTOR - nbrow; 
-+
-+						for (jj=0;jj<nbrow;jj++){
-+							k=idx_lusup;
-+							k1=Lindval_loc_bc_ptr[ljb][i+nrbl*2]+jj;
-+							for (j = 0; j < nsupc; ++j) {				
-+								lusup_srt[k] = lusup[k1];
-+								k += len;
-+								k1 += len;
-+							}	
-+							idx_lusup++;
-+						}				
-+						Lindval_loc_bc_ptr[ljb][i+nrbl*2] = idx_lusup - nbrow;	
-+					}
-+
-+					SUPERLU_FREE(lusup);
-+					SUPERLU_FREE(index);
-+
-+					Lrowind_bc_ptr[ljb] = index_srt;
-+					Lnzval_bc_ptr[ljb] = lusup_srt; 			
-+
-+
-+
-+					// if(ljb==0)
-+					// for (jj=0;jj<nrbl*3;jj++){
-+					// printf("iam %5d Lindval %5d\n",iam, Lindval_loc_bc_ptr[ljb][jj]);
-+					// fflush(stdout);
-+
-+					// for (jj=0;jj<nrbl;jj++){
-+					// printf("iam %5d Lindval %5d\n",iam, index[Lindval_loc_bc_ptr[ljb][jj+nrbl]]);
-+					// fflush(stdout);			
-+
-+					// }	
-+
-+				} else {
-+					Lrowind_bc_ptr[ljb] = NULL;
-+					Lnzval_bc_ptr[ljb] = NULL;
-+					Linv_bc_ptr[ljb] = NULL;
-+					Uinv_bc_ptr[ljb] = NULL;
-+					Lindval_loc_bc_ptr[ljb] = NULL;
-+				} /* if nrbl ... */
-+#if ( PROFlevel>=1 )
-+				t_l += SuperLU_timer_() - t;
-+#endif
-+				} /* if mycol == pc */
-+
-+			} /* for jb ... */
-+
-+			// for (j=0;j<19*3;j++){
-+			// printf("Lindval %5d\n",Lindval_loc_bc_ptr[0][j]);
-+			// fflush(stdout);
-+			// }
-+
-+			
-+			/////////////////////////////////////////////////////////////////
-+			
-+			/* Set up additional pointers for the index and value arrays of U.
-+			   nub is the number of local block columns. */
-+			nub = CEILING( nsupers, grid->npcol); /* Number of local block columns. */
-+			if ( !(Urbs = (int_t *) intCalloc_dist(2*nub)) )
-+				ABORT("Malloc fails for Urbs[]"); /* Record number of nonzero
-+									 blocks in a block column. */
-+			Urbs1 = Urbs + nub;
-+			if ( !(Ucb_indptr = SUPERLU_MALLOC(nub * sizeof(Ucb_indptr_t *))) )
-+				ABORT("Malloc fails for Ucb_indptr[]");
-+			if ( !(Ucb_valptr = SUPERLU_MALLOC(nub * sizeof(int_t *))) )
-+				ABORT("Malloc fails for Ucb_valptr[]");
-+			nlb = CEILING( nsupers, grid->nprow ); /* Number of local block rows. */
-+
-+			/* Count number of row blocks in a block column. 
-+			   One pass of the skeleton graph of U. */
-+			for (lk = 0; lk < nlb; ++lk) {
-+				usub1 = Ufstnz_br_ptr[lk];
-+				if ( usub1 ) { /* Not an empty block row. */
-+					/* usub1[0] -- number of column blocks in this block row. */
-+					i = BR_HEADER; /* Pointer in index array. */
-+					for (lb = 0; lb < usub1[0]; ++lb) { /* For all column blocks. */
-+						k = usub1[i];            /* Global block number */
-+						++Urbs[LBj(k,grid)];
-+						i += UB_DESCRIPTOR + SuperSize( k );
-+					}
-+				}
- 			}
--			++len;
--		    }
--		} /* for i ... */
--
--		if ( nrbl ) { /* Do not ensure the blocks are sorted! */
--		    /* Set up the initial pointers for each block in 
--		       index[] and nzval[]. */
--		    /* Add room for descriptors */
--		    len1 = len + BC_HEADER + nrbl * LB_DESCRIPTOR;
--		    if ( !(index = intMalloc_dist(len1)) ) 
--			ABORT("Malloc fails for index[]");
--		    Lrowind_bc_ptr[ljb] = index;
--		    if (!(Lnzval_bc_ptr[ljb] = 
--                         doubleMalloc_dist(len*nsupc))) {
--			fprintf(stderr, "col block " IFMT " ", jb);
--			ABORT("Malloc fails for Lnzval_bc_ptr[*][]");
--		    }
--		    mybufmax[0] = SUPERLU_MAX( mybufmax[0], len1 );
--		    mybufmax[1] = SUPERLU_MAX( mybufmax[1], len*nsupc );
--		    mybufmax[4] = SUPERLU_MAX( mybufmax[4], len );
--		    index[0] = nrbl;  /* Number of row blocks */
--		    index[1] = len;   /* LDA of the nzval[] */
--		    next_lind = BC_HEADER;
--		    next_lval = 0;
--		    for (k = 0; k < nrbl; ++k) {
--			gb = Lrb_number[k];
--			lb = LBi( gb, grid );
--			len = Lrb_length[lb];
--			Lrb_length[lb] = 0;  /* Reset vector of block length */
--			index[next_lind++] = gb; /* Descriptor */
--			index[next_lind++] = len; 
--			Lrb_indptr[lb] = next_lind;
--			Lrb_valptr[lb] = next_lval;
--			next_lind += len;
--			next_lval += len;
--		    }
--		    /* Propagate the compressed row subscripts to Lindex[],
--                       and the initial values of A from SPA into Lnzval[]. */
--		    lusup = Lnzval_bc_ptr[ljb];
--		    len = index[1];  /* LDA of lusup[] */
--		    for (i = istart; i < xlsub[fsupc+1]; ++i) {
--			irow = lsub[i];
--			gb = BlockNum( irow );
--			if ( myrow == PROW( gb, grid ) ) {
--			    lb = LBi( gb, grid );
--			    k = Lrb_indptr[lb]++; /* Random access a block */
--			    index[k] = irow;
--			    k = Lrb_valptr[lb]++;
--			    irow = ilsum[lb] + irow - FstBlockC( gb );
--			    for (j = 0, dense_col = dense; j < nsupc; ++j) {
--				lusup[k] = dense_col[irow];
--				dense_col[irow] = zero;
--				k += len;
--				dense_col += ldaspa;
--			    }
-+
-+			/* Set up the vertical linked lists for the row blocks.
-+			   One pass of the skeleton graph of U. */
-+			for (lb = 0; lb < nub; ++lb) {
-+				if ( Urbs[lb] ) { /* Not an empty block column. */
-+					if ( !(Ucb_indptr[lb]
-+								= SUPERLU_MALLOC(Urbs[lb] * sizeof(Ucb_indptr_t))) )
-+						ABORT("Malloc fails for Ucb_indptr[lb][]");
-+					if ( !(Ucb_valptr[lb] = (int_t *) intMalloc_dist(Urbs[lb])) )
-+						ABORT("Malloc fails for Ucb_valptr[lb][]");
-+				}
- 			}
--		    } /* for i ... */
--		} else {
--		    Lrowind_bc_ptr[ljb] = NULL;
--		    Lnzval_bc_ptr[ljb] = NULL;
--		} /* if nrbl ... */
-+			for (lk = 0; lk < nlb; ++lk) { /* For each block row. */
-+				usub1 = Ufstnz_br_ptr[lk];
-+				if ( usub1 ) { /* Not an empty block row. */
-+					i = BR_HEADER; /* Pointer in index array. */
-+					j = 0;         /* Pointer in nzval array. */
-+
-+					for (lb = 0; lb < usub1[0]; ++lb) { /* For all column blocks. */
-+						k = usub1[i];          /* Global block number, column-wise. */
-+						ljb = LBj( k, grid ); /* Local block number, column-wise. */
-+						Ucb_indptr[ljb][Urbs1[ljb]].lbnum = lk;
-+
-+						Ucb_indptr[ljb][Urbs1[ljb]].indpos = i;
-+						Ucb_valptr[ljb][Urbs1[ljb]] = j;
-+						
-+						++Urbs1[ljb];
-+						j += usub1[i+1];
-+						i += UB_DESCRIPTOR + SuperSize( k );
-+					}
-+				}
-+			}				
-+			
-+			/////////////////////////////////////////////////////////////////
-+
-+			// if(LSUM<nsupers)ABORT("Need increase LSUM."); /* temporary*/
-+
- #if ( PROFlevel>=1 )
--		t_l += SuperLU_timer_() - t;
--#endif
--	    } /* if mycol == pc */
--
--	} /* for jb ... */
--
--	Llu->Lrowind_bc_ptr = Lrowind_bc_ptr;
--	Llu->Lnzval_bc_ptr = Lnzval_bc_ptr;
--	Llu->Ufstnz_br_ptr = Ufstnz_br_ptr;
--	Llu->Unzval_br_ptr = Unzval_br_ptr;
--	Llu->ToRecv = ToRecv;
--	Llu->ToSendD = ToSendD;
--	Llu->ToSendR = ToSendR;
--	Llu->fmod = fmod;
--	Llu->fsendx_plist = fsendx_plist;
--	Llu->nfrecvx = nfrecvx;
--	Llu->nfsendx = nfsendx;
--	Llu->bmod = bmod;
--	Llu->bsendx_plist = bsendx_plist;
--	Llu->nbrecvx = nbrecvx;
--	Llu->nbsendx = nbsendx;
--	Llu->ilsum = ilsum;
--	Llu->ldalsum = ldaspa;
-+				t = SuperLU_timer_();
-+#endif				
-+			/* construct the Bcast tree for L ... */
-+
-+			k = CEILING( nsupers, grid->npcol );/* Number of local block columns */
-+			if ( !(LBtree_ptr = (BcTree*)SUPERLU_MALLOC(k * sizeof(BcTree))) )
-+				ABORT("Malloc fails for LBtree_ptr[].");
-+			if ( !(ActiveFlag = intCalloc_dist(grid->nprow*2)) )
-+				ABORT("Calloc fails for ActiveFlag[].");	
-+			if ( !(ranks = (int*)SUPERLU_MALLOC(grid->nprow * sizeof(int))) )
-+				ABORT("Malloc fails for ranks[].");	
-+			if ( !(SeedSTD_BC = (double*)SUPERLU_MALLOC(k * sizeof(double))) )
-+				ABORT("Malloc fails for SeedSTD_BC[].");	
-+
-+				
-+			for (i=0;i<k;i++){
-+				SeedSTD_BC[i]=rand();		
-+			}
-+
-+			MPI_Allreduce(MPI_IN_PLACE,&SeedSTD_BC[0],k,MPI_DOUBLE,MPI_MAX,grid->cscp.comm);					  
-+
-+			for (ljb = 0; ljb <k ; ++ljb) {
-+				LBtree_ptr[ljb]=NULL;
-+			}			
-+			
-+
-+			if ( !(ActiveFlagAll = intMalloc_dist(grid->nprow*k)) )
-+				ABORT("Calloc fails for ActiveFlag[].");				
-+			for (j=0;j<grid->nprow*k;++j)ActiveFlagAll[j]=3*nsupers;	
-+			for (ljb = 0; ljb < k; ++ljb) { /* for each local block column ... */
-+				jb = mycol+ljb*grid->npcol;  /* not sure */
-+				if(jb<nsupers){
-+				pc = PCOL( jb, grid );
-+				fsupc = FstBlockC( jb );
-+				nsupc = SuperSize( jb );
-+
-+				istart = xlsub[fsupc];
-+				for (i = istart; i < xlsub[fsupc+1]; ++i) {
-+					irow = lsub[i];
-+					gb = BlockNum( irow );
-+					pr = PROW( gb, grid );
-+					ActiveFlagAll[pr+ljb*grid->nprow]=MIN(ActiveFlagAll[pr+ljb*grid->nprow],gb);
-+				} /* for j ... */
-+				}
-+			}			
-+			
-+			for (ljb = 0; ljb < k; ++ljb) { /* for each local block column ... */
-+				
-+				jb = mycol+ljb*grid->npcol;  /* not sure */
-+				if(jb<nsupers){
-+				pc = PCOL( jb, grid );
-+
-+				for (j=0;j<grid->nprow;++j)ActiveFlag[j]=ActiveFlagAll[j+ljb*grid->nprow];
-+				for (j=0;j<grid->nprow;++j)ActiveFlag[j+grid->nprow]=j;
-+				for (j=0;j<grid->nprow;++j)ranks[j]=-1;
-+
-+				Root=-1; 
-+				Iactive = 0;				
-+				for (j=0;j<grid->nprow;++j){
-+					if(ActiveFlag[j]!=3*nsupers){
-+					gb = ActiveFlag[j];
-+					pr = PROW( gb, grid );
-+					if(gb==jb)Root=pr;
-+					if(myrow==pr)Iactive=1;		
-+					}					
-+				}
-+				
-+
-+				quickSortM(ActiveFlag,0,grid->nprow-1,grid->nprow,0,2);	
-+
-+				if(Iactive==1){
-+					// printf("jb %5d damn\n",jb);
-+					// fflush(stdout);
-+					assert( Root>-1 );
-+					rank_cnt = 1;
-+					ranks[0]=Root;
-+					for (j = 0; j < grid->nprow; ++j){
-+						if(ActiveFlag[j]!=3*nsupers && ActiveFlag[j+grid->nprow]!=Root){
-+							ranks[rank_cnt]=ActiveFlag[j+grid->nprow];
-+							++rank_cnt;
-+						}
-+					}		
-+
-+					if(rank_cnt>1){
-+
-+						for (ii=0;ii<rank_cnt;ii++)   // use global ranks rather than local ranks
-+							ranks[ii] = PNUM( ranks[ii], pc, grid );
-+
-+						// rseed=rand();
-+						// rseed=1.0;
-+						msgsize = SuperSize( jb )*nrhs+XK_H;
-+						LBtree_ptr[ljb] = BcTree_Create(grid->comm, ranks, rank_cnt, msgsize,SeedSTD_BC[ljb]);  	
-+						BcTree_SetTag(LBtree_ptr[ljb],BC_L);
-+
-+						// printf("iam %5d btree rank_cnt %5d \n",iam,rank_cnt);
-+						// fflush(stdout);
-+
-+						// if(iam==15 || iam==3){
-+						// printf("iam %5d btree lk %5d tag %5d root %5d\n",iam, ljb,jb,BcTree_IsRoot(LBtree_ptr[ljb]));
-+						// fflush(stdout);
-+						// }
-+
-+						// #if ( PRNTlevel>=1 )		
-+						if(Root==myrow){
-+							rank_cnt_ref=1;
-+							for (j = 0; j < grid->nprow; ++j) {
-+								if ( fsendx_plist[ljb][j] != EMPTY ) {	
-+									++rank_cnt_ref;		
-+								}
-+							}
-+							assert(rank_cnt==rank_cnt_ref);		
-+
-+							// printf("Partial Bcast Procs: col%7d np%4d\n",jb,rank_cnt);
-+
-+							// // printf("Partial Bcast Procs: %4d %4d: ",iam, rank_cnt);
-+							// // for(j=0;j<rank_cnt;++j)printf("%4d",ranks[j]);
-+							// // printf("\n");
-+						}
-+						// #endif
-+					}	
-+				}
-+				}
-+			}
-+
-+			
-+			SUPERLU_FREE(ActiveFlag);
-+			SUPERLU_FREE(ActiveFlagAll);
-+			SUPERLU_FREE(ranks);
-+			SUPERLU_FREE(SeedSTD_BC);
-+			
-+			
-+#if ( PROFlevel>=1 )
-+		t = SuperLU_timer_() - t;
-+		if ( !iam) printf(".. Construct Bcast tree for L: %.2f\t\n", t);
-+#endif			
- 	
-+
-+#if ( PROFlevel>=1 )
-+				t = SuperLU_timer_();
-+#endif			
-+			/* construct the Reduce tree for L ... */
-+			/* the following is used as reference */
-+			nlb = CEILING( nsupers, grid->nprow );/* Number of local block rows */
-+			if ( !(mod_bit = intMalloc_dist(nlb)) )
-+				ABORT("Malloc fails for mod_bit[].");
-+			if ( !(frecv = intMalloc_dist(nlb)) )
-+				ABORT("Malloc fails for frecv[].");
-+
-+			for (k = 0; k < nlb; ++k) mod_bit[k] = 0;
-+			for (k = 0; k < nsupers; ++k) {
-+				pr = PROW( k, grid );
-+				if ( myrow == pr ) {
-+					lib = LBi( k, grid );    /* local block number */
-+					kcol = PCOL( k, grid );
-+					if (mycol == kcol || fmod[lib] )
-+						mod_bit[lib] = 1;  /* contribution from off-diagonal and diagonal*/
-+				}
-+			}
-+			/* Every process receives the count, but it is only useful on the
-+			   diagonal processes.  */
-+			MPI_Allreduce( mod_bit, frecv, nlb, mpi_int_t, MPI_SUM, grid->rscp.comm);
-+
-+
-+
-+			k = CEILING( nsupers, grid->nprow );/* Number of local block rows */
-+			if ( !(LRtree_ptr = (RdTree*)SUPERLU_MALLOC(k * sizeof(RdTree))) )
-+				ABORT("Malloc fails for LRtree_ptr[].");
-+			if ( !(ActiveFlag = intCalloc_dist(grid->npcol*2)) )
-+				ABORT("Calloc fails for ActiveFlag[].");	
-+			if ( !(ranks = (int*)SUPERLU_MALLOC(grid->npcol * sizeof(int))) )
-+				ABORT("Malloc fails for ranks[].");	
-+
-+			// if ( !(idxs = intCalloc_dist(nsupers)) )
-+				// ABORT("Calloc fails for idxs[].");	
-+
-+			// if ( !(nzrows = (int_t**)SUPERLU_MALLOC(nsupers * sizeof(int_t*))) )
-+				// ABORT("Malloc fails for nzrows[].");
-+
-+			if ( !(SeedSTD_RD = (double*)SUPERLU_MALLOC(k * sizeof(double))) )
-+				ABORT("Malloc fails for SeedSTD_RD[].");	
-+
-+			for (i=0;i<k;i++){
-+				SeedSTD_RD[i]=rand();		
-+			}
-+
-+			MPI_Allreduce(MPI_IN_PLACE,&SeedSTD_RD[0],k,MPI_DOUBLE,MPI_MAX,grid->rscp.comm);					  
-+
-+
-+			// for (jb = 0; jb < nsupers; ++jb) { /* for each block column ... */
-+				// fsupc = FstBlockC( jb );
-+				// len=xlsub[fsupc+1]-xlsub[fsupc];
-+				// idxs[jb] = len-1;
-+				// if(len>0){
-+					// if ( !(nzrows[jb] = intMalloc_dist(len)) )
-+						// ABORT("Malloc fails for nzrows[jb]");
-+					// for(i=xlsub[fsupc];i<xlsub[fsupc+1];++i){
-+						// irow = lsub[i];
-+						// nzrows[jb][i-xlsub[fsupc]]=irow;
-+					// }
-+					// quickSort(nzrows[jb],0,len-1,0);
-+				// }
-+				// else{
-+					// nzrows[jb] = NULL;
-+				// }
-+			// }
-+
-+
-+			for (lib = 0; lib <k ; ++lib) {
-+				LRtree_ptr[lib]=NULL;
-+			}
-+
-+			
-+			if ( !(ActiveFlagAll = intMalloc_dist(grid->npcol*k)) )
-+				ABORT("Calloc fails for ActiveFlagAll[].");				
-+			for (j=0;j<grid->npcol*k;++j)ActiveFlagAll[j]=-3*nsupers;	
-+						
-+			for (jb = 0; jb < nsupers; ++jb) { /* for each block column ... */
-+				fsupc = FstBlockC( jb );
-+				pc = PCOL( jb, grid );
-+				for(i=xlsub[fsupc];i<xlsub[fsupc+1];++i){
-+					irow = lsub[i];
-+					ib = BlockNum( irow );
-+					pr = PROW( ib, grid );
-+					if ( myrow == pr ) { /* Block row ib in my process row */
-+						lib = LBi( ib, grid ); /* Local block number */
-+						ActiveFlagAll[pc+lib*grid->npcol]=MAX(ActiveFlagAll[pc+lib*grid->npcol],jb);
-+					}
-+				}
-+			}
-+
-+			
-+			for (lib=0;lib<k;++lib){
-+				ib = myrow+lib*grid->nprow;  /* not sure */
-+				if(ib<nsupers){
-+					pr = PROW( ib, grid );
-+					for (j=0;j<grid->npcol;++j)ActiveFlag[j]=ActiveFlagAll[j+lib*grid->npcol];;
-+					for (j=0;j<grid->npcol;++j)ActiveFlag[j+grid->npcol]=j;
-+					for (j=0;j<grid->npcol;++j)ranks[j]=-1;
-+					Root=-1; 
-+					Iactive = 0;				
-+
-+					for (j=0;j<grid->npcol;++j){
-+						if(ActiveFlag[j]!=-3*nsupers){
-+						jb = ActiveFlag[j];
-+						pc = PCOL( jb, grid );
-+						if(jb==ib)Root=pc;
-+						if(mycol==pc)Iactive=1;		
-+						}					
-+					}
-+				
-+				
-+					quickSortM(ActiveFlag,0,grid->npcol-1,grid->npcol,1,2);
-+
-+					if(Iactive==1){
-+						assert( Root>-1 );
-+						rank_cnt = 1;
-+						ranks[0]=Root;
-+						for (j = 0; j < grid->npcol; ++j){
-+							if(ActiveFlag[j]!=-3*nsupers && ActiveFlag[j+grid->npcol]!=Root){
-+								ranks[rank_cnt]=ActiveFlag[j+grid->npcol];
-+								++rank_cnt;
-+							}
-+						}
-+						if(rank_cnt>1){
-+
-+							for (ii=0;ii<rank_cnt;ii++)   // use global ranks rather than local ranks
-+								ranks[ii] = PNUM( pr, ranks[ii], grid );		
-+
-+							// rseed=rand();
-+							// rseed=1.0;
-+							msgsize = SuperSize( ib )*nrhs+LSUM_H;
-+
-+							// if(ib==0){
-+
-+							LRtree_ptr[lib] = RdTree_Create(grid->comm, ranks, rank_cnt, msgsize,SeedSTD_RD[lib]);  	
-+							RdTree_SetTag(LRtree_ptr[lib], RD_L);
-+							// }
-+
-+							// printf("iam %5d rtree rank_cnt %5d \n",iam,rank_cnt);
-+							// fflush(stdout);
-+
-+							// if(ib==15  || ib ==16){
-+
-+							// if(iam==15 || iam==3){
-+							// printf("iam %5d rtree lk %5d tag %5d root %5d\n",iam,lib,ib,RdTree_IsRoot(LRtree_ptr[lib]));
-+							// fflush(stdout);
-+							// }		
-+
-+
-+							// #if ( PRNTlevel>=1 )
-+							// if(Root==mycol){
-+							// assert(rank_cnt==frecv[lib]);
-+							// printf("Partial Reduce Procs: row%7d np%4d\n",ib,rank_cnt);
-+							// // printf("Partial Reduce Procs: %4d %4d: ",iam, rank_cnt);
-+							// // // for(j=0;j<rank_cnt;++j)printf("%4d",ranks[j]);
-+							// // printf("\n");
-+							// }
-+							// #endif		
-+						}
-+					}				
-+				}	
-+			}
-+
-+			SUPERLU_FREE(mod_bit);
-+			SUPERLU_FREE(frecv);
-+
-+
-+			SUPERLU_FREE(ActiveFlag);
-+			SUPERLU_FREE(ActiveFlagAll);
-+			SUPERLU_FREE(ranks);	
-+			// SUPERLU_FREE(idxs);	 
-+			SUPERLU_FREE(SeedSTD_RD);	
-+			// for(i=0;i<nsupers;++i){
-+				// if(nzrows[i])SUPERLU_FREE(nzrows[i]);
-+			// }
-+			// SUPERLU_FREE(nzrows);
-+
-+				////////////////////////////////////////////////////////
-+
-+#if ( PROFlevel>=1 )
-+		t = SuperLU_timer_() - t;
-+		if ( !iam) printf(".. Construct Reduce tree for L: %.2f\t\n", t);
-+#endif					
-+
-+#if ( PROFlevel>=1 )
-+			t = SuperLU_timer_();
-+#endif	
-+
-+			/* construct the Bcast tree for U ... */
-+
-+			k = CEILING( nsupers, grid->npcol );/* Number of local block columns */
-+			if ( !(UBtree_ptr = (BcTree*)SUPERLU_MALLOC(k * sizeof(BcTree))) )
-+				ABORT("Malloc fails for UBtree_ptr[].");
-+			if ( !(ActiveFlag = intCalloc_dist(grid->nprow*2)) )
-+				ABORT("Calloc fails for ActiveFlag[].");	
-+			if ( !(ranks = (int*)SUPERLU_MALLOC(grid->nprow * sizeof(int))) )
-+				ABORT("Malloc fails for ranks[].");	
-+			if ( !(SeedSTD_BC = (double*)SUPERLU_MALLOC(k * sizeof(double))) )
-+				ABORT("Malloc fails for SeedSTD_BC[].");	
-+
-+			for (i=0;i<k;i++){
-+				SeedSTD_BC[i]=rand();		
-+			}
-+
-+			MPI_Allreduce(MPI_IN_PLACE,&SeedSTD_BC[0],k,MPI_DOUBLE,MPI_MAX,grid->cscp.comm);					  
-+
-+
-+			for (ljb = 0; ljb <k ; ++ljb) {
-+				UBtree_ptr[ljb]=NULL;
-+			}	
-+
-+			if ( !(ActiveFlagAll = intMalloc_dist(grid->nprow*k)) )
-+				ABORT("Calloc fails for ActiveFlagAll[].");				
-+			for (j=0;j<grid->nprow*k;++j)ActiveFlagAll[j]=-3*nsupers;	
-+			
-+			for (ljb = 0; ljb < k; ++ljb) { /* for each local block column ... */
-+				jb = mycol+ljb*grid->npcol;  /* not sure */
-+				if(jb<nsupers){
-+				pc = PCOL( jb, grid );
-+
-+				fsupc = FstBlockC( jb );
-+				for (j = fsupc; j < FstBlockC( jb+1 ); ++j) {
-+					istart = xusub[j];
-+					/* NOTE: Only the first nonzero index of the segment
-+					   is stored in usub[]. */
-+					for (i = istart; i < xusub[j+1]; ++i) {
-+						irow = usub[i]; /* First nonzero in the segment. */
-+						gb = BlockNum( irow );
-+						pr = PROW( gb, grid );
-+						ActiveFlagAll[pr+ljb*grid->nprow]=MAX(ActiveFlagAll[pr+ljb*grid->nprow],gb);
-+					// printf("gb:%5d jb: %5d nsupers: %5d\n",gb,jb,nsupers);
-+					// fflush(stdout);								
-+						//if(gb==jb)Root=pr;
-+					}
-+					
-+					
-+				}
-+				pr = PROW( jb, grid ); // take care of diagonal node stored as L
-+				// printf("jb %5d current: %5d",jb,ActiveFlagAll[pr+ljb*grid->nprow]);
-+				// fflush(stdout);
-+				ActiveFlagAll[pr+ljb*grid->nprow]=MAX(ActiveFlagAll[pr+ljb*grid->nprow],jb);	
-+				}
-+			}	
-+				
-+				
-+				
-+			for (ljb = 0; ljb < k; ++ljb) { /* for each block column ... */
-+				jb = mycol+ljb*grid->npcol;  /* not sure */
-+				if(jb<nsupers){
-+				pc = PCOL( jb, grid );
-+				// if ( mycol == pc ) { /* Block column jb in my process column */
-+
-+				for (j=0;j<grid->nprow;++j)ActiveFlag[j]=ActiveFlagAll[j+ljb*grid->nprow];
-+				for (j=0;j<grid->nprow;++j)ActiveFlag[j+grid->nprow]=j;
-+				for (j=0;j<grid->nprow;++j)ranks[j]=-1;
-+
-+				Root=-1; 
-+				Iactive = 0;				
-+				for (j=0;j<grid->nprow;++j){
-+					if(ActiveFlag[j]!=-3*nsupers){
-+					gb = ActiveFlag[j];
-+					pr = PROW( gb, grid );
-+					if(gb==jb)Root=pr;
-+					if(myrow==pr)Iactive=1;		
-+					}
-+				}						
-+				
-+				quickSortM(ActiveFlag,0,grid->nprow-1,grid->nprow,1,2);	
-+			// printf("jb: %5d Iactive %5d\n",jb,Iactive);
-+			// fflush(stdout);
-+				if(Iactive==1){
-+					// printf("root:%5d jb: %5d\n",Root,jb);
-+					// fflush(stdout);
-+					assert( Root>-1 );
-+					rank_cnt = 1;
-+					ranks[0]=Root;
-+					for (j = 0; j < grid->nprow; ++j){
-+						if(ActiveFlag[j]!=-3*nsupers && ActiveFlag[j+grid->nprow]!=Root){
-+							ranks[rank_cnt]=ActiveFlag[j+grid->nprow];
-+							++rank_cnt;
-+						}
-+					}		
-+			// printf("jb: %5d rank_cnt %5d\n",jb,rank_cnt);
-+			// fflush(stdout);
-+					if(rank_cnt>1){
-+						for (ii=0;ii<rank_cnt;ii++)   // use global ranks rather than local ranks
-+							ranks[ii] = PNUM( ranks[ii], pc, grid );
-+
-+						// rseed=rand();
-+						// rseed=1.0;
-+						msgsize = SuperSize( jb )*nrhs+XK_H;
-+						UBtree_ptr[ljb] = BcTree_Create(grid->comm, ranks, rank_cnt, msgsize,SeedSTD_BC[ljb]);  	
-+						BcTree_SetTag(UBtree_ptr[ljb],BC_U);
-+
-+						// printf("iam %5d btree rank_cnt %5d \n",iam,rank_cnt);
-+						// fflush(stdout);
-+						
-+						if(Root==myrow){
-+						rank_cnt_ref=1;
-+						for (j = 0; j < grid->nprow; ++j) {
-+							// printf("ljb %5d j %5d nprow %5d\n",ljb,j,grid->nprow);
-+							// fflush(stdout);
-+							if ( bsendx_plist[ljb][j] != EMPTY ) {	
-+								++rank_cnt_ref;		
-+							}
-+						}
-+						// printf("ljb %5d rank_cnt %5d rank_cnt_ref %5d\n",ljb,rank_cnt,rank_cnt_ref);
-+						// fflush(stdout);								
-+						assert(rank_cnt==rank_cnt_ref);		
-+						}						
-+					}
-+				}
-+				}
-+			}	
-+			SUPERLU_FREE(ActiveFlag);
-+			SUPERLU_FREE(ActiveFlagAll);
-+			SUPERLU_FREE(ranks);				
-+			SUPERLU_FREE(SeedSTD_BC);				
-+				
-+#if ( PROFlevel>=1 )
-+		t = SuperLU_timer_() - t;
-+		if ( !iam) printf(".. Construct Bcast tree for U: %.2f\t\n", t);
-+#endif					
-+
-+#if ( PROFlevel>=1 )
-+				t = SuperLU_timer_();
-+#endif					
-+			/* construct the Reduce tree for U ... */
-+			/* the following is used as reference */
-+			nlb = CEILING( nsupers, grid->nprow );/* Number of local block rows */
-+			if ( !(mod_bit = intMalloc_dist(nlb)) )
-+				ABORT("Malloc fails for mod_bit[].");
-+			if ( !(brecv = intMalloc_dist(nlb)) )
-+				ABORT("Malloc fails for brecv[].");
-+
-+			for (k = 0; k < nlb; ++k) mod_bit[k] = 0;
-+			for (k = 0; k < nsupers; ++k) {
-+				pr = PROW( k, grid );
-+				if ( myrow == pr ) {
-+					lib = LBi( k, grid );    /* local block number */
-+					kcol = PCOL( k, grid );
-+					if (mycol == kcol || bmod[lib] )
-+						mod_bit[lib] = 1;  /* contribution from off-diagonal and diagonal*/
-+				}
-+			}
-+			/* Every process receives the count, but it is only useful on the
-+			   diagonal processes.  */
-+			MPI_Allreduce( mod_bit, brecv, nlb, mpi_int_t, MPI_SUM, grid->rscp.comm);
-+
-+
-+
-+			k = CEILING( nsupers, grid->nprow );/* Number of local block rows */
-+			if ( !(URtree_ptr = (RdTree*)SUPERLU_MALLOC(k * sizeof(RdTree))) )
-+				ABORT("Malloc fails for URtree_ptr[].");
-+			if ( !(ActiveFlag = intCalloc_dist(grid->npcol*2)) )
-+				ABORT("Calloc fails for ActiveFlag[].");	
-+			if ( !(ranks = (int*)SUPERLU_MALLOC(grid->npcol * sizeof(int))) )
-+				ABORT("Malloc fails for ranks[].");	
-+
-+			// if ( !(idxs = intCalloc_dist(nsupers)) )
-+				// ABORT("Calloc fails for idxs[].");	
-+
-+			// if ( !(nzrows = (int_t**)SUPERLU_MALLOC(nsupers * sizeof(int_t*))) )
-+				// ABORT("Malloc fails for nzrows[].");
-+
-+			if ( !(SeedSTD_RD = (double*)SUPERLU_MALLOC(k * sizeof(double))) )
-+				ABORT("Malloc fails for SeedSTD_RD[].");	
-+
-+			for (i=0;i<k;i++){
-+				SeedSTD_RD[i]=rand();		
-+			}
-+
-+			MPI_Allreduce(MPI_IN_PLACE,&SeedSTD_RD[0],k,MPI_DOUBLE,MPI_MAX,grid->rscp.comm);					  
-+
-+
-+			// for (jb = 0; jb < nsupers; ++jb) { /* for each block column ... */
-+				// fsupc = FstBlockC( jb );
-+				// len=0;  
-+				// for (j = fsupc; j < FstBlockC( jb+1 ); ++j) {
-+					// istart = xusub[j];
-+					// /* NOTE: Only the first nonzero index of the segment
-+					   // is stored in usub[]. */
-+					// len +=  xusub[j+1] - xusub[j];  
-+				// }	
-+						
-+				// idxs[jb] = len-1;
-+
-+				// if(len>0){
-+					// if ( !(nzrows[jb] = intMalloc_dist(len)) )
-+						// ABORT("Malloc fails for nzrows[jb]");
-+					
-+					// fsupc = FstBlockC( jb );
-+					
-+					// len=0; 
-+					
-+					// for (j = fsupc; j < FstBlockC( jb+1 ); ++j) {
-+						// istart = xusub[j];
-+						// /* NOTE: Only the first nonzero index of the segment
-+						   // is stored in usub[]. */
-+						// for (i = istart; i < xusub[j+1]; ++i) {
-+							// irow = usub[i]; /* First nonzero in the segment. */
-+							// nzrows[jb][len]=irow;
-+							// len++;
-+						// }
-+					// }	
-+					// quickSort(nzrows[jb],0,len-1,0);
-+				// }
-+				// else{
-+					// nzrows[jb] = NULL;
-+				// }
-+			// }
-+			
-+
-+			for (lib = 0; lib <k ; ++lib) {
-+				URtree_ptr[lib]=NULL;
-+			}
-+
-+			
-+			if ( !(ActiveFlagAll = intMalloc_dist(grid->npcol*k)) )
-+				ABORT("Calloc fails for ActiveFlagAll[].");				
-+			for (j=0;j<grid->npcol*k;++j)ActiveFlagAll[j]=3*nsupers;	
-+						
-+			for (jb = 0; jb < nsupers; ++jb) { /* for each block column ... */
-+				fsupc = FstBlockC( jb );
-+				pc = PCOL( jb, grid );
-+				
-+				fsupc = FstBlockC( jb );
-+				for (j = fsupc; j < FstBlockC( jb+1 ); ++j) {
-+					istart = xusub[j];
-+					/* NOTE: Only the first nonzero index of the segment
-+					   is stored in usub[]. */
-+					for (i = istart; i < xusub[j+1]; ++i) {
-+						irow = usub[i]; /* First nonzero in the segment. */
-+						ib = BlockNum( irow );
-+						pr = PROW( ib, grid );
-+						if ( myrow == pr ) { /* Block row ib in my process row */
-+							lib = LBi( ib, grid ); /* Local block number */
-+							ActiveFlagAll[pc+lib*grid->npcol]=MIN(ActiveFlagAll[pc+lib*grid->npcol],jb);
-+						}						
-+					}
-+				}
-+				
-+				pr = PROW( jb, grid );
-+				if ( myrow == pr ) { /* Block row ib in my process row */
-+					lib = LBi( jb, grid ); /* Local block number */
-+					ActiveFlagAll[pc+lib*grid->npcol]=MIN(ActiveFlagAll[pc+lib*grid->npcol],jb);
-+				}					
-+			}
-+				
-+
-+			for (lib=0;lib<k;++lib){
-+				ib = myrow+lib*grid->nprow;  /* not sure */
-+				if(ib<nsupers){
-+					pr = PROW( ib, grid );
-+					for (j=0;j<grid->npcol;++j)ActiveFlag[j]=ActiveFlagAll[j+lib*grid->npcol];;
-+					for (j=0;j<grid->npcol;++j)ActiveFlag[j+grid->npcol]=j;
-+					for (j=0;j<grid->npcol;++j)ranks[j]=-1;
-+					Root=-1; 
-+					Iactive = 0;				
-+
-+					for (j=0;j<grid->npcol;++j){
-+						if(ActiveFlag[j]!=3*nsupers){
-+						jb = ActiveFlag[j];
-+						pc = PCOL( jb, grid );
-+						if(jb==ib)Root=pc;
-+						if(mycol==pc)Iactive=1;		
-+						}					
-+					}
-+					
-+					quickSortM(ActiveFlag,0,grid->npcol-1,grid->npcol,0,2);
-+
-+					if(Iactive==1){
-+						assert( Root>-1 );
-+						rank_cnt = 1;
-+						ranks[0]=Root;
-+						for (j = 0; j < grid->npcol; ++j){
-+							if(ActiveFlag[j]!=3*nsupers && ActiveFlag[j+grid->npcol]!=Root){
-+								ranks[rank_cnt]=ActiveFlag[j+grid->npcol];
-+								++rank_cnt;
-+							}
-+						}
-+						if(rank_cnt>1){
-+
-+							for (ii=0;ii<rank_cnt;ii++)   // use global ranks rather than local ranks
-+								ranks[ii] = PNUM( pr, ranks[ii], grid );		
-+
-+							// rseed=rand();
-+							// rseed=1.0;
-+							msgsize = SuperSize( ib )*nrhs+LSUM_H;
-+
-+							// if(ib==0){
-+
-+							URtree_ptr[lib] = RdTree_Create(grid->comm, ranks, rank_cnt, msgsize,SeedSTD_RD[lib]);  	
-+							RdTree_SetTag(URtree_ptr[lib], RD_U);
-+							// }
-+	
-+							// #if ( PRNTlevel>=1 )
-+							if(Root==mycol){
-+							// printf("Partial Reduce Procs: %4d %4d %5d \n",iam, rank_cnt,brecv[lib]);
-+							// fflush(stdout);
-+							assert(rank_cnt==brecv[lib]);
-+							// printf("Partial Reduce Procs: row%7d np%4d\n",ib,rank_cnt);
-+							// printf("Partial Reduce Procs: %4d %4d: ",iam, rank_cnt);
-+							// // for(j=0;j<rank_cnt;++j)printf("%4d",ranks[j]);
-+							// printf("\n");
-+							}
-+							// #endif		
-+						}
-+					}
-+				}						
-+			}
-+			SUPERLU_FREE(mod_bit);
-+			SUPERLU_FREE(brecv);
-+
-+
-+			SUPERLU_FREE(ActiveFlag);
-+			SUPERLU_FREE(ActiveFlagAll);
-+			SUPERLU_FREE(ranks);	
-+			// SUPERLU_FREE(idxs);	
-+			SUPERLU_FREE(SeedSTD_RD);	
-+			// for(i=0;i<nsupers;++i){
-+				// if(nzrows[i])SUPERLU_FREE(nzrows[i]);
-+			// }
-+			// SUPERLU_FREE(nzrows);				
-+				
-+#if ( PROFlevel>=1 )
-+		t = SuperLU_timer_() - t;
-+		if ( !iam) printf(".. Construct Reduce tree for U: %.2f\t\n", t);
-+#endif						
-+				
-+				////////////////////////////////////////////////////////
-+
-+				
-+				Llu->Lrowind_bc_ptr = Lrowind_bc_ptr;
-+				Llu->Lindval_loc_bc_ptr = Lindval_loc_bc_ptr;
-+				Llu->Lnzval_bc_ptr = Lnzval_bc_ptr;
-+				Llu->Ufstnz_br_ptr = Ufstnz_br_ptr;
-+				Llu->Unzval_br_ptr = Unzval_br_ptr;
-+				Llu->ToRecv = ToRecv;
-+				Llu->ToSendD = ToSendD;
-+				Llu->ToSendR = ToSendR;
-+				Llu->fmod = fmod;
-+				Llu->fsendx_plist = fsendx_plist;
-+				Llu->nfrecvx = nfrecvx;
-+				Llu->nfsendx = nfsendx;
-+				Llu->bmod = bmod;
-+				Llu->bsendx_plist = bsendx_plist;
-+				Llu->nbrecvx = nbrecvx;
-+				Llu->nbsendx = nbsendx;
-+				Llu->ilsum = ilsum;
-+				Llu->ldalsum = ldaspa;
-+				
-+				Llu->LRtree_ptr = LRtree_ptr;
-+				Llu->LBtree_ptr = LBtree_ptr;
-+				Llu->URtree_ptr = URtree_ptr;
-+				Llu->UBtree_ptr = UBtree_ptr;
-+				Llu->Linv_bc_ptr = Linv_bc_ptr;
-+				Llu->Uinv_bc_ptr = Uinv_bc_ptr;	
-+				Llu->Urbs = Urbs; 
-+				Llu->Ucb_indptr = Ucb_indptr; 
-+				Llu->Ucb_valptr = Ucb_valptr; 
-+
- #if ( PRNTlevel>=1 )
--	if ( !iam ) printf(".. # L blocks " IFMT "\t# U blocks " IFMT "\n",
--			   nLblocks, nUblocks);
-+				if ( !iam ) printf(".. # L blocks " IFMT "\t# U blocks " IFMT "\n",
-+						nLblocks, nUblocks);
- #endif
- 
--	SUPERLU_FREE(rb_marker);
--	SUPERLU_FREE(Urb_fstnz);
--	SUPERLU_FREE(Urb_length);
--	SUPERLU_FREE(Urb_indptr);
--	SUPERLU_FREE(Lrb_length);
--	SUPERLU_FREE(Lrb_number);
--	SUPERLU_FREE(Lrb_indptr);
--	SUPERLU_FREE(Lrb_valptr);
--	SUPERLU_FREE(dense);
-+				SUPERLU_FREE(rb_marker);
-+				SUPERLU_FREE(Urb_fstnz);
-+				SUPERLU_FREE(Urb_length);
-+				SUPERLU_FREE(Urb_indptr);
-+				SUPERLU_FREE(Lrb_length);
-+				SUPERLU_FREE(Lrb_number);
-+				SUPERLU_FREE(Lrb_indptr);
-+				SUPERLU_FREE(Lrb_valptr);
-+				SUPERLU_FREE(dense);
- 
--	/* Find the maximum buffer size. */
--	MPI_Allreduce(mybufmax, Llu->bufmax, NBUFFERS, mpi_int_t, 
--		      MPI_MAX, grid->comm);
-+				/* Find the maximum buffer size. */
-+				MPI_Allreduce(mybufmax, Llu->bufmax, NBUFFERS, mpi_int_t, 
-+						MPI_MAX, grid->comm);
- 
--	k = CEILING( nsupers, grid->nprow );/* Number of local block rows */
--	if ( !(Llu->mod_bit = intMalloc_dist(k)) )
--	    ABORT("Malloc fails for mod_bit[].");
-+				k = CEILING( nsupers, grid->nprow );/* Number of local block rows */
-+				if ( !(Llu->mod_bit = intMalloc_dist(k)) )
-+					ABORT("Malloc fails for mod_bit[].");
- 
- #if ( PROFlevel>=1 )
--	if ( !iam ) printf(".. 1st distribute time:\n "
--			   "\tL\t%.2f\n\tU\t%.2f\n"
--			   "\tu_blks %d\tnrbu %d\n--------\n",
--  			   t_l, t_u, u_blks, nrbu);
-+				if ( !iam ) printf(".. 1st distribute time:\n "
-+						"\tL\t%.2f\n\tU\t%.2f\n"
-+						"\tu_blks %d\tnrbu %d\n--------\n",
-+						t_l, t_u, u_blks, nrbu);
- #endif
- 
--    } /* else fact != SamePattern_SameRowPerm */
-+			} /* else fact != SamePattern_SameRowPerm */
- 
--    if ( xa[A->ncol] > 0 ) { /* may not have any entries on this process. */
--        SUPERLU_FREE(asub);
--        SUPERLU_FREE(a);
--    }
--    SUPERLU_FREE(xa);
-+			if ( xa[A->ncol] > 0 ) { /* may not have any entries on this process. */
-+				SUPERLU_FREE(asub);
-+				SUPERLU_FREE(a);
-+			}
-+			SUPERLU_FREE(xa);
- 
- #if ( DEBUGlevel>=1 )
--    /* Memory allocated but not freed:
--       ilsum, fmod, fsendx_plist, bmod, bsendx_plist  */
--    CHECK_MALLOC(iam, "Exit pddistribute()");
-+			/* Memory allocated but not freed:
-+			   ilsum, fmod, fsendx_plist, bmod, bsendx_plist  */
-+			CHECK_MALLOC(iam, "Exit pddistribute()");
- #endif
--    
--    return (mem_use);
--} /* PDDISTRIBUTE */
-+
-+			return (mem_use);
-+		} /* PDDISTRIBUTE */
-diff --git a/SRC/pdgssvx.c b/SRC/pdgssvx.c
-index 3e413f2..7d42782 100644
---- a/SRC/pdgssvx.c
-+++ b/SRC/pdgssvx.c
-@@ -551,6 +551,7 @@ pdgssvx(superlu_dist_options_t *options, SuperMatrix *A,
-     int   col, key; /* parameters for creating a new communicator */
-     Pslu_freeable_t Pslu_freeable;
-     float  flinfo;
-+	int blas_flag;
- 
-     /* Initialization. */
-     m       = A->nrow;
-@@ -924,6 +925,8 @@ pdgssvx(superlu_dist_options_t *options, SuperMatrix *A,
- #endif
-     }
- 
-+	
-+
-     /* ------------------------------------------------------------
-        Perform the LU factorization: symbolic factorization, 
-        redistribution, and numerical factorization.
-@@ -975,7 +978,15 @@ pdgssvx(superlu_dist_options_t *options, SuperMatrix *A,
- 	if ( permc_spec != MY_PERMC && Fact == DOFACT ) {
-           /* Reuse perm_c if Fact == SamePattern, or SamePattern_SameRowPerm */
- 	  if ( permc_spec == PARMETIS ) {
--	      /* Get column permutation vector in perm_c.                    *
-+	      
-+		  
-+	// #pragma omp parallel  
-+    // {  	
-+	// #pragma omp master
-+	// {	
-+	
-+		  
-+		  /* Get column permutation vector in perm_c.                    *
- 	       * This routine takes as input the distributed input matrix A  *
- 	       * and does not modify it.  It also allocates memory for       *
- 	       * sizes[] and fstVtxSep[] arrays, that contain information    *
-@@ -983,6 +994,9 @@ pdgssvx(superlu_dist_options_t *options, SuperMatrix *A,
- 	      flinfo = get_perm_c_parmetis(A, perm_r, perm_c, nprocs_num,
-                                   	   noDomains, &sizes, &fstVtxSep,
-                                            grid, &symb_comm);
-+	// }
-+	// }	
-+										   
- 	      if (flinfo > 0) {
- #if ( PRNTlevel>=1 )
- 	          fprintf(stderr, "Insufficient memory for get_perm_c parmetis\n");
-@@ -1105,7 +1119,7 @@ pdgssvx(superlu_dist_options_t *options, SuperMatrix *A,
-   	       distribution routine. */
- 	    t = SuperLU_timer_();
- 	    dist_mem_use = pddistribute(Fact, n, A, ScalePermstruct,
--                                      Glu_freeable, LUstruct, grid);
-+                                      Glu_freeable, LUstruct, grid, nrhs);
- 	    stat->utime[DIST] = SuperLU_timer_() - t;
- 
-   	    /* Deallocate storage used in symbolic factorization. */
-@@ -1122,7 +1136,7 @@ pdgssvx(superlu_dist_options_t *options, SuperMatrix *A,
- 
-     	    t = SuperLU_timer_();
- 	    dist_mem_use = ddist_psymbtonum(Fact, n, A, ScalePermstruct,
--		  			   &Pslu_freeable, LUstruct, grid);
-+		  			   &Pslu_freeable, LUstruct, grid, nrhs);
- 	    if (dist_mem_use > 0)
- 	        ABORT ("Not enough memory available for dist_psymbtonum\n");
-             
-@@ -1133,8 +1147,20 @@ pdgssvx(superlu_dist_options_t *options, SuperMatrix *A,
- 
- 	/* Perform numerical factorization in parallel. */
- 	t = SuperLU_timer_();
-+	
-+	
-+
-+	
-+    // #pragma omp parallel  
-+    // {  	
-+	// #pragma omp master
-+	// {	
-+		
- 	pdgstrf(options, m, n, anorm, LUstruct, grid, stat, info);
--	stat->utime[FACT] = SuperLU_timer_() - t;
-+	stat->utime[FACT] = SuperLU_timer_() - t;	
-+	// }
-+	// }	 
-+	
- 
- #if 0
- 
-@@ -1305,10 +1331,30 @@ pdgssvx(superlu_dist_options_t *options, SuperMatrix *A,
- 	       For repeated call to pdgssvx(), no need to re-initialilze
- 	       the Solve data & communication structures, unless a new
- 	       factorization with Fact == DOFACT or SamePattern is asked for. */
-+		if(options->DiagInv==YES){	
-+
-+	#ifdef _CRAY
-+			  blas_flag=1;
-+	#elif defined (USE_VENDOR_BLAS)
-+			  blas_flag=2;
-+	#else
-+			  blas_flag=0;
-+	#endif	
-+			if(blas_flag==0)
-+			ABORT("DiagInv doesn't works with internal blas\n");
-+			pdCompute_Diag_Inv(n, LUstruct, grid, stat, info);
-+		}	
- 	} 
- 
-+    // #pragma omp parallel  
-+    // {  	
-+	// #pragma omp master
-+	// {
- 	pdgstrs(n, LUstruct, ScalePermstruct, grid, X, m_loc, 
- 		fst_row, ldb, nrhs, SOLVEstruct, stat, info);
-+	// }
-+	// }
-+
- 
- 	/* ------------------------------------------------------------
- 	   Use iterative refinement to improve the computed solution and
-diff --git a/SRC/pdgstrs.c b/SRC/pdgstrs.c
-index 7c47e74..3423ece 100644
---- a/SRC/pdgstrs.c
-+++ b/SRC/pdgstrs.c
-@@ -1,13 +1,13 @@
- /*! \file
--Copyright (c) 2003, The Regents of the University of California, through
--Lawrence Berkeley National Laboratory (subject to receipt of any required 
--approvals from U.S. Dept. of Energy) 
-+  Copyright (c) 2003, The Regents of the University of California, through
-+  Lawrence Berkeley National Laboratory (subject to receipt of any required 
-+  approvals from U.S. Dept. of Energy) 
- 
--All rights reserved. 
-+  All rights reserved. 
- 
--The source code is distributed under BSD license, see the file License.txt
--at the top-level directory.
--*/
-+  The source code is distributed under BSD license, see the file License.txt
-+  at the top-level directory.
-+ */
- 
- 
- /*! @file 
-@@ -20,9 +20,13 @@ at the top-level directory.
-  * October 15, 2008
-  * </pre>
-  */
--
-+#include <math.h>
- #include "superlu_ddefs.h"
- 
-+#ifndef CACHELINE
-+#define CACHELINE 64  /* bytes, Xeon Phi KNL, Cori haswell, Edision */
-+#endif
-+
- /*
-  * Sketch of the algorithm for L-solve:
-  * =======================
-@@ -79,7 +83,7 @@ at the top-level directory.
-  *         | | | | |                 |
-  *	   --------- <---------------|
-  */
--  
-+
- /*#define ISEND_IRECV*/
- 
- /*
-@@ -87,7 +91,7 @@ at the top-level directory.
-  */
- #ifdef _CRAY
- fortran void STRSM(_fcd, _fcd, _fcd, _fcd, int*, int*, double*,
--		   double*, int*, double*, int*);
-+		double*, int*, double*, int*);
- _fcd ftcs1;
- _fcd ftcs2;
- _fcd ftcs3;
-@@ -146,118 +150,200 @@ _fcd ftcs3;
-  * </pre>
-  */
- 
--int_t
-+	int_t
- pdReDistribute_B_to_X(double *B, int_t m_loc, int nrhs, int_t ldb,
--                      int_t fst_row, int_t *ilsum, double *x,
--		      ScalePermstruct_t *ScalePermstruct,
--		      Glu_persist_t *Glu_persist,
--		      gridinfo_t *grid, SOLVEstruct_t *SOLVEstruct)
-+		int_t fst_row, int_t *ilsum, double *x,
-+		ScalePermstruct_t *ScalePermstruct,
-+		Glu_persist_t *Glu_persist,
-+		gridinfo_t *grid, SOLVEstruct_t *SOLVEstruct)
- {
--    int  *SendCnt, *SendCnt_nrhs, *RecvCnt, *RecvCnt_nrhs;
--    int  *sdispls, *sdispls_nrhs, *rdispls, *rdispls_nrhs;
--    int  *ptr_to_ibuf, *ptr_to_dbuf;
--    int_t  *perm_r, *perm_c; /* row and column permutation vectors */
--    int_t  *send_ibuf, *recv_ibuf;
--    double *send_dbuf, *recv_dbuf;
--    int_t  *xsup, *supno;
--    int_t  i, ii, irow, gbi, j, jj, k, knsupc, l, lk;
--    int    p, procs;
--    pxgstrs_comm_t *gstrs_comm = SOLVEstruct->gstrs_comm;
-+	int  *SendCnt, *SendCnt_nrhs, *RecvCnt, *RecvCnt_nrhs;
-+	int  *sdispls, *sdispls_nrhs, *rdispls, *rdispls_nrhs;
-+	int  *ptr_to_ibuf, *ptr_to_dbuf;
-+	int_t  *perm_r, *perm_c; /* row and column permutation vectors */
-+	int_t  *send_ibuf, *recv_ibuf;
-+	double *send_dbuf, *recv_dbuf;
-+	int_t  *xsup, *supno;
-+	int_t  i, ii, irow, gbi, j, jj, k, knsupc, l, lk, nbrow;
-+	int    p, procs;
-+	pxgstrs_comm_t *gstrs_comm = SOLVEstruct->gstrs_comm;
-+
-+	MPI_Request req_i, req_d, *req_send, *req_recv;
-+	MPI_Status status, *status_send, *status_recv;
-+	int Nreq_recv, Nreq_send, pp;
- 
- #if ( DEBUGlevel>=1 )
--    CHECK_MALLOC(grid->iam, "Enter pdReDistribute_B_to_X()");
-+	CHECK_MALLOC(grid->iam, "Enter pdReDistribute_B_to_X()");
- #endif
- 
--    /* ------------------------------------------------------------
--       INITIALIZATION.
--       ------------------------------------------------------------*/
--    perm_r = ScalePermstruct->perm_r;
--    perm_c = ScalePermstruct->perm_c;
--    procs = grid->nprow * grid->npcol;
--    xsup = Glu_persist->xsup;
--    supno = Glu_persist->supno;
--    SendCnt      = gstrs_comm->B_to_X_SendCnt;
--    SendCnt_nrhs = gstrs_comm->B_to_X_SendCnt +   procs;
--    RecvCnt      = gstrs_comm->B_to_X_SendCnt + 2*procs;
--    RecvCnt_nrhs = gstrs_comm->B_to_X_SendCnt + 3*procs;
--    sdispls      = gstrs_comm->B_to_X_SendCnt + 4*procs;
--    sdispls_nrhs = gstrs_comm->B_to_X_SendCnt + 5*procs;
--    rdispls      = gstrs_comm->B_to_X_SendCnt + 6*procs;
--    rdispls_nrhs = gstrs_comm->B_to_X_SendCnt + 7*procs;
--    ptr_to_ibuf  = gstrs_comm->ptr_to_ibuf;
--    ptr_to_dbuf  = gstrs_comm->ptr_to_dbuf;
--
--    /* ------------------------------------------------------------
--       NOW COMMUNICATE THE ACTUAL DATA.
--       ------------------------------------------------------------*/
--    k = sdispls[procs-1] + SendCnt[procs-1]; /* Total number of sends */
--    l = rdispls[procs-1] + RecvCnt[procs-1]; /* Total number of receives */
--    if ( !(send_ibuf = intMalloc_dist(k + l)) )
--        ABORT("Malloc fails for send_ibuf[].");
--    recv_ibuf = send_ibuf + k;
--    if ( !(send_dbuf = doubleMalloc_dist((k + l)* (size_t)nrhs)) )
--        ABORT("Malloc fails for send_dbuf[].");
--    recv_dbuf = send_dbuf + k * nrhs;
--    
--    for (p = 0; p < procs; ++p) {
--        ptr_to_ibuf[p] = sdispls[p];
--        ptr_to_dbuf[p] = sdispls[p] * nrhs;
--    }
--
--    /* Copy the row indices and values to the send buffer. */
--    for (i = 0, l = fst_row; i < m_loc; ++i, ++l) {
--        irow = perm_c[perm_r[l]]; /* Row number in Pc*Pr*B */
--	gbi = BlockNum( irow );
--	p = PNUM( PROW(gbi,grid), PCOL(gbi,grid), grid ); /* Diagonal process */
--	k = ptr_to_ibuf[p];
--	send_ibuf[k] = irow;
--	k = ptr_to_dbuf[p];
--	RHS_ITERATE(j) { /* RHS is stored in row major in the buffer. */
--	    send_dbuf[k++] = B[i + j*ldb];
-+	/* ------------------------------------------------------------
-+	   INITIALIZATION.
-+	   ------------------------------------------------------------*/
-+	perm_r = ScalePermstruct->perm_r;
-+	perm_c = ScalePermstruct->perm_c;
-+	procs = grid->nprow * grid->npcol;
-+	xsup = Glu_persist->xsup;
-+	supno = Glu_persist->supno;
-+	SendCnt      = gstrs_comm->B_to_X_SendCnt;
-+	SendCnt_nrhs = gstrs_comm->B_to_X_SendCnt +   procs;
-+	RecvCnt      = gstrs_comm->B_to_X_SendCnt + 2*procs;
-+	RecvCnt_nrhs = gstrs_comm->B_to_X_SendCnt + 3*procs;
-+	sdispls      = gstrs_comm->B_to_X_SendCnt + 4*procs;
-+	sdispls_nrhs = gstrs_comm->B_to_X_SendCnt + 5*procs;
-+	rdispls      = gstrs_comm->B_to_X_SendCnt + 6*procs;
-+	rdispls_nrhs = gstrs_comm->B_to_X_SendCnt + 7*procs;
-+	ptr_to_ibuf  = gstrs_comm->ptr_to_ibuf;
-+	ptr_to_dbuf  = gstrs_comm->ptr_to_dbuf;
-+
-+	/* ------------------------------------------------------------
-+	   NOW COMMUNICATE THE ACTUAL DATA.
-+	   ------------------------------------------------------------*/
-+	k = sdispls[procs-1] + SendCnt[procs-1]; /* Total number of sends */
-+	l = rdispls[procs-1] + RecvCnt[procs-1]; /* Total number of receives */
-+	if ( !(send_ibuf = intMalloc_dist(k + l)) )
-+		ABORT("Malloc fails for send_ibuf[].");
-+	recv_ibuf = send_ibuf + k;
-+	if ( !(send_dbuf = doubleMalloc_dist((k + l)* (size_t)nrhs)) )
-+		ABORT("Malloc fails for send_dbuf[].");
-+	recv_dbuf = send_dbuf + k * nrhs;
-+	if ( !(req_send = (MPI_Request*) SUPERLU_MALLOC(procs*sizeof(MPI_Request))) )
-+		ABORT("Malloc fails for req_send[].");	
-+	if ( !(req_recv = (MPI_Request*) SUPERLU_MALLOC(procs*sizeof(MPI_Request))) )
-+		ABORT("Malloc fails for req_recv[].");
-+	if ( !(status_send = (MPI_Status*) SUPERLU_MALLOC(procs*sizeof(MPI_Status))) )
-+		ABORT("Malloc fails for status_send[].");
-+	if ( !(status_recv = (MPI_Status*) SUPERLU_MALLOC(procs*sizeof(MPI_Status))) )
-+		ABORT("Malloc fails for status_recv[].");
-+
-+	for (p = 0; p < procs; ++p) {
-+		ptr_to_ibuf[p] = sdispls[p];
-+		ptr_to_dbuf[p] = sdispls[p] * nrhs;
- 	}
--	++ptr_to_ibuf[p];
--	ptr_to_dbuf[p] += nrhs;
--    }
--
--    /* Communicate the (permuted) row indices. */
--    MPI_Alltoallv(send_ibuf, SendCnt, sdispls, mpi_int_t,
--		  recv_ibuf, RecvCnt, rdispls, mpi_int_t, grid->comm);
--
--    /* Communicate the numerical values. */
--    MPI_Alltoallv(send_dbuf, SendCnt_nrhs, sdispls_nrhs, MPI_DOUBLE,
--		  recv_dbuf, RecvCnt_nrhs, rdispls_nrhs, MPI_DOUBLE,
--		  grid->comm);
--    
--    /* ------------------------------------------------------------
--       Copy buffer into X on the diagonal processes.
--       ------------------------------------------------------------*/
--    ii = 0;
--    for (p = 0; p < procs; ++p) {
--        jj = rdispls_nrhs[p];
--        for (i = 0; i < RecvCnt[p]; ++i) {
--	    /* Only the diagonal processes do this; the off-diagonal processes
--	       have 0 RecvCnt. */
--	    irow = recv_ibuf[ii]; /* The permuted row index. */
--	    k = BlockNum( irow );
--	    knsupc = SuperSize( k );
--	    lk = LBi( k, grid );  /* Local block number. */
--	    l = X_BLK( lk );
--	    x[l - XK_H] = k;      /* Block number prepended in the header. */
--	    irow = irow - FstBlockC(k); /* Relative row number in X-block */
--	    RHS_ITERATE(j) {
--	        x[l + irow + j*knsupc] = recv_dbuf[jj++];
--	    }
--	    ++ii;
-+
-+	/* Copy the row indices and values to the send buffer. */
-+	for (i = 0, l = fst_row; i < m_loc; ++i, ++l) {
-+		irow = perm_c[perm_r[l]]; /* Row number in Pc*Pr*B */
-+		gbi = BlockNum( irow );
-+		p = PNUM( PROW(gbi,grid), PCOL(gbi,grid), grid ); /* Diagonal process */
-+		k = ptr_to_ibuf[p];
-+		send_ibuf[k] = irow;
-+		k = ptr_to_dbuf[p];
-+		RHS_ITERATE(j) { /* RHS is stored in row major in the buffer. */
-+			send_dbuf[k++] = B[i + j*ldb];
-+		}
-+		++ptr_to_ibuf[p];
-+		ptr_to_dbuf[p] += nrhs;
-+	}
-+
-+
-+#if 1	
-+	/* Communicate the (permuted) row indices. */
-+	MPI_Alltoallv(send_ibuf, SendCnt, sdispls, mpi_int_t,
-+			recv_ibuf, RecvCnt, rdispls, mpi_int_t, grid->comm);
-+
-+	/* Communicate the numerical values. */
-+	MPI_Alltoallv(send_dbuf, SendCnt_nrhs, sdispls_nrhs, MPI_DOUBLE,
-+			recv_dbuf, RecvCnt_nrhs, rdispls_nrhs, MPI_DOUBLE,
-+			grid->comm);
-+
-+#else	
-+
-+	/* Communicate the (permuted) row indices. */
-+	MPI_Ialltoallv(send_ibuf, SendCnt, sdispls, mpi_int_t,
-+			recv_ibuf, RecvCnt, rdispls, mpi_int_t, grid->comm, &req_i);
-+
-+	/* Communicate the numerical values. */
-+	MPI_Ialltoallv(send_dbuf, SendCnt_nrhs, sdispls_nrhs, MPI_DOUBLE,
-+			recv_dbuf, RecvCnt_nrhs, rdispls_nrhs, MPI_DOUBLE,
-+			grid->comm, &req_d);	
-+	MPI_Wait(&req_i,&status);
-+	MPI_Wait(&req_d,&status);
-+
-+#endif	
-+
-+
-+
-+	// MPI_Barrier( grid->comm );
-+
-+
-+	// Nreq_send=0;
-+	// for (pp=0;pp<procs;pp++){
-+	// if(SendCnt[pp]>0){
-+	// MPI_Isend(&send_ibuf[sdispls[pp]], SendCnt[pp], mpi_int_t, pp, 0, grid->comm,
-+	// &req_send[Nreq_send] );
-+	// Nreq_send++;
-+	// }
-+	// }
-+
-+	// Nreq_recv=0;
-+	// for (pp=0;pp<procs;pp++){
-+	// if(RecvCnt[pp]>0){
-+	// MPI_Irecv(&recv_ibuf[rdispls[pp]], RecvCnt[pp], mpi_int_t, pp, 0, grid->comm,
-+	// &req_recv[Nreq_recv] );
-+	// Nreq_recv++;
-+	// }
-+	// }
-+
-+	// if(Nreq_send>0)MPI_Waitall(Nreq_send,req_send,status_send);
-+	// if(Nreq_recv>0)MPI_Waitall(Nreq_recv,req_recv,status_recv);
-+
-+
-+	// Nreq_send=0;
-+	// for (pp=0;pp<procs;pp++){
-+	// if(SendCnt_nrhs[pp]>0){
-+	// MPI_Isend(&send_dbuf[sdispls_nrhs[pp]], SendCnt_nrhs[pp], MPI_DOUBLE, pp, 1, grid->comm,
-+	// &req_send[Nreq_send] );
-+	// Nreq_send++;
-+	// }
-+	// }
-+	// Nreq_recv=0;
-+	// for (pp=0;pp<procs;pp++){
-+	// if(RecvCnt_nrhs[pp]>0){
-+	// MPI_Irecv(&recv_dbuf[rdispls_nrhs[pp]], RecvCnt_nrhs[pp], MPI_DOUBLE, pp, 1, grid->comm,
-+	// &req_recv[Nreq_recv] );
-+	// Nreq_recv++;
-+	// }
-+	// }
-+
-+	// if(Nreq_send>0)MPI_Waitall(Nreq_send,req_send,status_send);
-+	// if(Nreq_recv>0)MPI_Waitall(Nreq_recv,req_recv,status_recv);
-+
-+
-+
-+	/* ------------------------------------------------------------
-+	   Copy buffer into X on the diagonal processes.
-+	   ------------------------------------------------------------*/
-+	ii = 0;
-+	for (p = 0; p < procs; ++p) {
-+		jj = rdispls_nrhs[p];
-+		for (i = 0; i < RecvCnt[p]; ++i) {
-+			/* Only the diagonal processes do this; the off-diagonal processes
-+			   have 0 RecvCnt. */
-+			irow = recv_ibuf[ii]; /* The permuted row index. */
-+			k = BlockNum( irow );
-+			knsupc = SuperSize( k );
-+			lk = LBi( k, grid );  /* Local block number. */
-+			l = X_BLK( lk );
-+			x[l - XK_H] = k;      /* Block number prepended in the header. */
-+			irow = irow - FstBlockC(k); /* Relative row number in X-block */
-+			RHS_ITERATE(j) {
-+				x[l + irow + j*knsupc] = recv_dbuf[jj++];
-+			}
-+			++ii;
-+		}
- 	}
--    }
- 
--    SUPERLU_FREE(send_ibuf);
--    SUPERLU_FREE(send_dbuf);
--    
-+	SUPERLU_FREE(send_ibuf);
-+	SUPERLU_FREE(send_dbuf);
-+	SUPERLU_FREE(req_send);
-+	SUPERLU_FREE(req_recv);
-+	SUPERLU_FREE(status_send);
-+	SUPERLU_FREE(status_recv);
-+
- #if ( DEBUGlevel>=1 )
--    CHECK_MALLOC(grid->iam, "Exit pdReDistribute_B_to_X()");
-+	CHECK_MALLOC(grid->iam, "Exit pdReDistribute_B_to_X()");
- #endif
--    return 0;
-+	return 0;
- } /* pdReDistribute_B_to_X */
- 
- /*! \brief
-@@ -275,122 +361,320 @@ pdReDistribute_B_to_X(double *B, int_t m_loc, int nrhs, int_t ldb,
-  * </pre>
-  */
- 
--int_t
-+	int_t
- pdReDistribute_X_to_B(int_t n, double *B, int_t m_loc, int_t ldb, int_t fst_row,
--		      int_t nrhs, double *x, int_t *ilsum,
--		      ScalePermstruct_t *ScalePermstruct,
--		      Glu_persist_t *Glu_persist, gridinfo_t *grid,
--		      SOLVEstruct_t *SOLVEstruct)
-+		int_t nrhs, double *x, int_t *ilsum,
-+		ScalePermstruct_t *ScalePermstruct,
-+		Glu_persist_t *Glu_persist, gridinfo_t *grid,
-+		SOLVEstruct_t *SOLVEstruct)
- {
--    int_t  i, ii, irow, j, jj, k, knsupc, nsupers, l, lk;
--    int_t  *xsup, *supno;
--    int  *SendCnt, *SendCnt_nrhs, *RecvCnt, *RecvCnt_nrhs;
--    int  *sdispls, *rdispls, *sdispls_nrhs, *rdispls_nrhs;
--    int  *ptr_to_ibuf, *ptr_to_dbuf;
--    int_t  *send_ibuf, *recv_ibuf;
--    double *send_dbuf, *recv_dbuf;
--    int_t  *row_to_proc = SOLVEstruct->row_to_proc; /* row-process mapping */
--    pxgstrs_comm_t *gstrs_comm = SOLVEstruct->gstrs_comm;
--    int  iam, p, q, pkk, procs;
--    int_t  num_diag_procs, *diag_procs;
-+	int_t  i, ii, irow, j, jj, k, knsupc, nsupers, l, lk;
-+	int_t  *xsup, *supno;
-+	int  *SendCnt, *SendCnt_nrhs, *RecvCnt, *RecvCnt_nrhs;
-+	int  *sdispls, *rdispls, *sdispls_nrhs, *rdispls_nrhs;
-+	int  *ptr_to_ibuf, *ptr_to_dbuf;
-+	int_t  *send_ibuf, *recv_ibuf;
-+	double *send_dbuf, *recv_dbuf;
-+	int_t  *row_to_proc = SOLVEstruct->row_to_proc; /* row-process mapping */
-+	pxgstrs_comm_t *gstrs_comm = SOLVEstruct->gstrs_comm;
-+	int  iam, p, q, pkk, procs;
-+	int_t  num_diag_procs, *diag_procs;
-+	MPI_Request req_i, req_d, *req_send, *req_recv;
-+	MPI_Status status, *status_send, *status_recv;
-+	int Nreq_recv, Nreq_send, pp;
- 
- #if ( DEBUGlevel>=1 )
--    CHECK_MALLOC(grid->iam, "Enter pdReDistribute_X_to_B()");
-+	CHECK_MALLOC(grid->iam, "Enter pdReDistribute_X_to_B()");
- #endif
- 
--    /* ------------------------------------------------------------
--       INITIALIZATION.
--       ------------------------------------------------------------*/
--    xsup = Glu_persist->xsup;
--    supno = Glu_persist->supno;
--    nsupers = Glu_persist->supno[n-1] + 1;
--    iam = grid->iam;
--    procs = grid->nprow * grid->npcol;
-- 
--    SendCnt      = gstrs_comm->X_to_B_SendCnt;
--    SendCnt_nrhs = gstrs_comm->X_to_B_SendCnt +   procs;
--    RecvCnt      = gstrs_comm->X_to_B_SendCnt + 2*procs;
--    RecvCnt_nrhs = gstrs_comm->X_to_B_SendCnt + 3*procs;
--    sdispls      = gstrs_comm->X_to_B_SendCnt + 4*procs;
--    sdispls_nrhs = gstrs_comm->X_to_B_SendCnt + 5*procs;
--    rdispls      = gstrs_comm->X_to_B_SendCnt + 6*procs;
--    rdispls_nrhs = gstrs_comm->X_to_B_SendCnt + 7*procs;
--    ptr_to_ibuf  = gstrs_comm->ptr_to_ibuf;
--    ptr_to_dbuf  = gstrs_comm->ptr_to_dbuf;
--
--    k = sdispls[procs-1] + SendCnt[procs-1]; /* Total number of sends */
--    l = rdispls[procs-1] + RecvCnt[procs-1]; /* Total number of receives */
--    if ( !(send_ibuf = intMalloc_dist(k + l)) )
--        ABORT("Malloc fails for send_ibuf[].");
--    recv_ibuf = send_ibuf + k;
--    if ( !(send_dbuf = doubleMalloc_dist((k + l)*nrhs)) )
--        ABORT("Malloc fails for send_dbuf[].");
--    recv_dbuf = send_dbuf + k * nrhs;
--    for (p = 0; p < procs; ++p) {
--        ptr_to_ibuf[p] = sdispls[p];
--        ptr_to_dbuf[p] = sdispls_nrhs[p];
--    }
--    num_diag_procs = SOLVEstruct->num_diag_procs;
--    diag_procs = SOLVEstruct->diag_procs;
--
--    for (p = 0; p < num_diag_procs; ++p) {  /* For all diagonal processes. */
--	pkk = diag_procs[p];
--	if ( iam == pkk ) {
--	    for (k = p; k < nsupers; k += num_diag_procs) {
--		knsupc = SuperSize( k );
--		lk = LBi( k, grid ); /* Local block number */
--		irow = FstBlockC( k );
--		l = X_BLK( lk );
--		for (i = 0; i < knsupc; ++i) {
-+	/* ------------------------------------------------------------
-+	   INITIALIZATION.
-+	   ------------------------------------------------------------*/
-+	xsup = Glu_persist->xsup;
-+	supno = Glu_persist->supno;
-+	nsupers = Glu_persist->supno[n-1] + 1;
-+	iam = grid->iam;
-+	procs = grid->nprow * grid->npcol;
-+
-+	SendCnt      = gstrs_comm->X_to_B_SendCnt;
-+	SendCnt_nrhs = gstrs_comm->X_to_B_SendCnt +   procs;
-+	RecvCnt      = gstrs_comm->X_to_B_SendCnt + 2*procs;
-+	RecvCnt_nrhs = gstrs_comm->X_to_B_SendCnt + 3*procs;
-+	sdispls      = gstrs_comm->X_to_B_SendCnt + 4*procs;
-+	sdispls_nrhs = gstrs_comm->X_to_B_SendCnt + 5*procs;
-+	rdispls      = gstrs_comm->X_to_B_SendCnt + 6*procs;
-+	rdispls_nrhs = gstrs_comm->X_to_B_SendCnt + 7*procs;
-+	ptr_to_ibuf  = gstrs_comm->ptr_to_ibuf;
-+	ptr_to_dbuf  = gstrs_comm->ptr_to_dbuf;
-+
-+	k = sdispls[procs-1] + SendCnt[procs-1]; /* Total number of sends */
-+	l = rdispls[procs-1] + RecvCnt[procs-1]; /* Total number of receives */
-+	if ( !(send_ibuf = intMalloc_dist(k + l)) )
-+		ABORT("Malloc fails for send_ibuf[].");
-+	recv_ibuf = send_ibuf + k;
-+	if ( !(send_dbuf = doubleMalloc_dist((k + l)*nrhs)) )
-+		ABORT("Malloc fails for send_dbuf[].");
-+	if ( !(req_send = (MPI_Request*) SUPERLU_MALLOC(procs*sizeof(MPI_Request))) )
-+		ABORT("Malloc fails for req_send[].");	
-+	if ( !(req_recv = (MPI_Request*) SUPERLU_MALLOC(procs*sizeof(MPI_Request))) )
-+		ABORT("Malloc fails for req_recv[].");
-+	if ( !(status_send = (MPI_Status*) SUPERLU_MALLOC(procs*sizeof(MPI_Status))) )
-+		ABORT("Malloc fails for status_send[].");
-+	if ( !(status_recv = (MPI_Status*) SUPERLU_MALLOC(procs*sizeof(MPI_Status))) )
-+		ABORT("Malloc fails for status_recv[].");	
-+
-+
-+	recv_dbuf = send_dbuf + k * nrhs;
-+	for (p = 0; p < procs; ++p) {
-+		ptr_to_ibuf[p] = sdispls[p];
-+		ptr_to_dbuf[p] = sdispls_nrhs[p];
-+	}
-+	num_diag_procs = SOLVEstruct->num_diag_procs;
-+	diag_procs = SOLVEstruct->diag_procs;
-+
-+	for (p = 0; p < num_diag_procs; ++p) {  /* For all diagonal processes. */
-+		pkk = diag_procs[p];
-+		if ( iam == pkk ) {
-+			for (k = p; k < nsupers; k += num_diag_procs) {
-+				knsupc = SuperSize( k );
-+				lk = LBi( k, grid ); /* Local block number */
-+				irow = FstBlockC( k );
-+				l = X_BLK( lk );
-+				for (i = 0; i < knsupc; ++i) {
- #if 0
--		    ii = inv_perm_c[irow]; /* Apply X <== Pc'*Y */
-+					ii = inv_perm_c[irow]; /* Apply X <== Pc'*Y */
- #else
--		    ii = irow;
-+					ii = irow;
- #endif
--		    q = row_to_proc[ii];
--		    jj = ptr_to_ibuf[q];
--		    send_ibuf[jj] = ii;
--		    jj = ptr_to_dbuf[q];
--		    RHS_ITERATE(j) { /* RHS stored in row major in buffer. */
--		        send_dbuf[jj++] = x[l + i + j*knsupc];
--		    }
--		    ++ptr_to_ibuf[q];
--		    ptr_to_dbuf[q] += nrhs;
--		    ++irow;
-+					q = row_to_proc[ii];
-+					jj = ptr_to_ibuf[q];
-+					send_ibuf[jj] = ii;
-+					jj = ptr_to_dbuf[q];
-+					RHS_ITERATE(j) { /* RHS stored in row major in buffer. */
-+						send_dbuf[jj++] = x[l + i + j*knsupc];
-+					}
-+					++ptr_to_ibuf[q];
-+					ptr_to_dbuf[q] += nrhs;
-+					++irow;
-+				}
-+			}
- 		}
--	    }
- 	}
--    }
--    
--    /* ------------------------------------------------------------
--        COMMUNICATE THE (PERMUTED) ROW INDICES AND NUMERICAL VALUES.
--       ------------------------------------------------------------*/
--    MPI_Alltoallv(send_ibuf, SendCnt, sdispls, mpi_int_t,
--		  recv_ibuf, RecvCnt, rdispls, mpi_int_t, grid->comm);
--    MPI_Alltoallv(send_dbuf, SendCnt_nrhs, sdispls_nrhs, MPI_DOUBLE, 
--		  recv_dbuf, RecvCnt_nrhs, rdispls_nrhs, MPI_DOUBLE,
--		  grid->comm);
--
--    /* ------------------------------------------------------------
--       COPY THE BUFFER INTO B.
--       ------------------------------------------------------------*/
--    for (i = 0, k = 0; i < m_loc; ++i) {
--	irow = recv_ibuf[i];
--	irow -= fst_row; /* Relative row number */
--	RHS_ITERATE(j) { /* RHS is stored in row major in the buffer. */
--	    B[irow + j*ldb] = recv_dbuf[k++];
-+
-+	/* ------------------------------------------------------------
-+	   COMMUNICATE THE (PERMUTED) ROW INDICES AND NUMERICAL VALUES.
-+	   ------------------------------------------------------------*/
-+
-+#if 1
-+
-+	MPI_Alltoallv(send_ibuf, SendCnt, sdispls, mpi_int_t,
-+			recv_ibuf, RecvCnt, rdispls, mpi_int_t, grid->comm);
-+	MPI_Alltoallv(send_dbuf, SendCnt_nrhs, sdispls_nrhs, MPI_DOUBLE, 
-+			recv_dbuf, RecvCnt_nrhs, rdispls_nrhs, MPI_DOUBLE,
-+			grid->comm);
-+
-+#else
-+	MPI_Ialltoallv(send_ibuf, SendCnt, sdispls, mpi_int_t,
-+			recv_ibuf, RecvCnt, rdispls, mpi_int_t, grid->comm,&req_i);
-+	MPI_Ialltoallv(send_dbuf, SendCnt_nrhs, sdispls_nrhs, MPI_DOUBLE, 
-+			recv_dbuf, RecvCnt_nrhs, rdispls_nrhs, MPI_DOUBLE,
-+			grid->comm,&req_d);
-+
-+	MPI_Wait(&req_i,&status);
-+	MPI_Wait(&req_d,&status);		 
-+#endif	
-+
-+	// MPI_Barrier( grid->comm );
-+	// Nreq_send=0;
-+	// for (pp=0;pp<procs;pp++){
-+	// if(SendCnt[pp]>0){
-+	// MPI_Isend(&send_ibuf[sdispls[pp]], SendCnt[pp], mpi_int_t, pp, 0, grid->comm,
-+	// &req_send[Nreq_send] );
-+	// Nreq_send++;
-+	// }
-+	// }
-+
-+	// Nreq_recv=0;
-+	// for (pp=0;pp<procs;pp++){
-+	// if(RecvCnt[pp]>0){
-+	// MPI_Irecv(&recv_ibuf[rdispls[pp]], RecvCnt[pp], mpi_int_t, pp, 0, grid->comm,
-+	// &req_recv[Nreq_recv] );
-+	// Nreq_recv++;
-+	// }
-+	// }
-+
-+	// if(Nreq_send>0)MPI_Waitall(Nreq_send,req_send,status_send);
-+	// if(Nreq_recv>0)MPI_Waitall(Nreq_recv,req_recv,status_recv);
-+	// // MPI_Barrier( grid->comm );
-+
-+	// Nreq_send=0;
-+	// for (pp=0;pp<procs;pp++){
-+	// if(SendCnt_nrhs[pp]>0){
-+	// MPI_Isend(&send_dbuf[sdispls_nrhs[pp]], SendCnt_nrhs[pp], MPI_DOUBLE, pp, 1, grid->comm,
-+	// &req_send[Nreq_send] );
-+	// Nreq_send++;
-+	// }
-+	// }
-+	// Nreq_recv=0;
-+	// for (pp=0;pp<procs;pp++){
-+	// if(RecvCnt_nrhs[pp]>0){
-+	// MPI_Irecv(&recv_dbuf[rdispls_nrhs[pp]], RecvCnt_nrhs[pp], MPI_DOUBLE, pp, 1, grid->comm,
-+	// &req_recv[Nreq_recv] );
-+	// Nreq_recv++;
-+	// }
-+	// }
-+
-+	// if(Nreq_send>0)MPI_Waitall(Nreq_send,req_send,status_send);
-+	// if(Nreq_recv>0)MPI_Waitall(Nreq_recv,req_recv,status_recv);
-+	// // MPI_Barrier( grid->comm );
-+
-+
-+
-+
-+
-+	/* ------------------------------------------------------------
-+	   COPY THE BUFFER INTO B.
-+	   ------------------------------------------------------------*/
-+	for (i = 0, k = 0; i < m_loc; ++i) {
-+		irow = recv_ibuf[i];
-+		irow -= fst_row; /* Relative row number */
-+		RHS_ITERATE(j) { /* RHS is stored in row major in the buffer. */
-+			B[irow + j*ldb] = recv_dbuf[k++];
-+		}
- 	}
--    }
- 
--    SUPERLU_FREE(send_ibuf);
--    SUPERLU_FREE(send_dbuf);
-+	SUPERLU_FREE(send_ibuf);
-+	SUPERLU_FREE(send_dbuf);
-+	SUPERLU_FREE(req_send);
-+	SUPERLU_FREE(req_recv);
-+	SUPERLU_FREE(status_send);
-+	SUPERLU_FREE(status_recv);
-+
- #if ( DEBUGlevel>=1 )
--    CHECK_MALLOC(grid->iam, "Exit pdReDistribute_X_to_B()");
-+	CHECK_MALLOC(grid->iam, "Exit pdReDistribute_X_to_B()");
- #endif
--    return 0;
-+	return 0;
- 
- } /* pdReDistribute_X_to_B */
- 
-+
-+
-+	void
-+pdCompute_Diag_Inv(int_t n, LUstruct_t *LUstruct,gridinfo_t *grid, SuperLUStat_t *stat, int *info)
-+{
-+	Glu_persist_t *Glu_persist = LUstruct->Glu_persist;
-+	LocalLU_t *Llu = LUstruct->Llu;
-+
-+	double *lusup;
-+	double *recvbuf, *tempv;
-+	double *Linv;/* Inverse of diagonal block */
-+	double *Uinv;/* Inverse of diagonal block */
-+
-+	int_t  kcol, krow, mycol, myrow;
-+	int_t  i, ii, il, j, jj, k, lb, ljb, lk, lptr, luptr;
-+	int_t  nb, nlb,nlb_nodiag, nub, nsupers;
-+	int_t  *xsup, *supno, *lsub, *usub;
-+	int_t  *ilsum;    /* Starting position of each supernode in lsum (LOCAL)*/
-+	int    Pc, Pr, iam;
-+	int    knsupc, nsupr;
-+	int    ldalsum;   /* Number of lsum entries locally owned. */
-+	int    maxrecvsz, p, pi;
-+	int_t  **Lrowind_bc_ptr;
-+	double **Lnzval_bc_ptr;
-+	double **Linv_bc_ptr;
-+	double **Uinv_bc_ptr;
-+	int INFO;
-+	double t;
-+
-+#if ( PROFlevel>=1 )
-+	t = SuperLU_timer_();
-+#endif 
-+
-+	// printf("wocao \n");
-+	// fflush(stdout);
-+	if(grid->iam==0){
-+		printf("computing inverse of diagonal blocks...\n");
-+		fflush(stdout);
-+	}
-+	/*
-+	 * Initialization.
-+	 */
-+	iam = grid->iam;
-+	Pc = grid->npcol;
-+	Pr = grid->nprow;
-+	myrow = MYROW( iam, grid );
-+	mycol = MYCOL( iam, grid );
-+	xsup = Glu_persist->xsup;
-+	supno = Glu_persist->supno;
-+	nsupers = supno[n-1] + 1;
-+	Lrowind_bc_ptr = Llu->Lrowind_bc_ptr;
-+	Linv_bc_ptr = Llu->Linv_bc_ptr;
-+	Uinv_bc_ptr = Llu->Uinv_bc_ptr;
-+	Lnzval_bc_ptr = Llu->Lnzval_bc_ptr;
-+	nlb = CEILING( nsupers, Pr ); /* Number of local block rows. */
-+
-+
-+	Llu->inv = 1;
-+
-+	/*---------------------------------------------------
-+	 * Compute inverse of L(lk,lk).
-+	 *---------------------------------------------------*/
-+
-+	for (k = 0; k < nsupers; ++k) {
-+		krow = PROW( k, grid );
-+		if ( myrow == krow ) {
-+			lk = LBi( k, grid );    /* local block number */
-+			kcol = PCOL( k, grid );
-+			if ( mycol == kcol ) { /* diagonal process */
-+
-+				lk = LBj( k, grid ); /* Local block number, column-wise. */
-+				lsub = Lrowind_bc_ptr[lk];
-+				lusup = Lnzval_bc_ptr[lk];
-+				Linv = Linv_bc_ptr[lk];
-+				Uinv = Uinv_bc_ptr[lk];
-+				nsupr = lsub[1];	
-+				knsupc = SuperSize( k );
-+
-+				for (j=0 ; j<knsupc; j++){
-+					Linv[j*knsupc+j] = 1.0;
-+
-+					for (i=j+1 ; i<knsupc; i++){
-+						Linv[j*knsupc+i] = lusup[j*nsupr+i];	
-+					}
-+
-+					for (i=0 ; i<j+1; i++){
-+						Uinv[j*knsupc+i] = lusup[j*nsupr+i];	
-+					}			
-+
-+				}
-+
-+#ifdef _CRAY
-+				ABORT("Cray blas dtrtri not implemented\n");
-+#elif defined (USE_VENDOR_BLAS)
-+				dtrtri_("L","U",&knsupc,Linv,&knsupc,&INFO);		  	
-+				dtrtri_("U","N",&knsupc,Uinv,&knsupc,&INFO);	
-+#else
-+				ABORT("internal blas dtrtri not implemented\n");
-+#endif			
-+
-+			}
-+		}
-+	}
-+
-+#if ( PROFlevel>=1 )
-+	if(grid->iam==0){
-+		t = SuperLU_timer_() - t;
-+		printf(".. L-diag_inv time\t%10.5f\n", t);
-+		fflush(stdout);
-+	}
-+#endif	
-+
-+	return;
-+}
-+
-+
-+
-+
- /*! \brief
-  *
-  * <pre>
-@@ -462,880 +746,1654 @@ pdReDistribute_X_to_B(int_t n, double *B, int_t m_loc, int_t ldb, int_t fst_row,
-  * </pre>       
-  */
- 
--void
-+	void
- pdgstrs(int_t n, LUstruct_t *LUstruct, 
--	ScalePermstruct_t *ScalePermstruct,
--	gridinfo_t *grid, double *B,
--	int_t m_loc, int_t fst_row, int_t ldb, int nrhs,
--	SOLVEstruct_t *SOLVEstruct,
--	SuperLUStat_t *stat, int *info)
-+		ScalePermstruct_t *ScalePermstruct,
-+		gridinfo_t *grid, double *B,
-+		int_t m_loc, int_t fst_row, int_t ldb, int nrhs,
-+		SOLVEstruct_t *SOLVEstruct,
-+		SuperLUStat_t *stat, int *info)
- {
--    Glu_persist_t *Glu_persist = LUstruct->Glu_persist;
--    LocalLU_t *Llu = LUstruct->Llu;
--    double alpha = 1.0;
--    double zero = 0.0;
--    double *lsum;  /* Local running sum of the updates to B-components */
--    double *x;     /* X component at step k. */
--		    /* NOTE: x and lsum are of same size. */
--    double *lusup, *dest;
--    double *recvbuf, *tempv;
--    double *rtemp; /* Result of full matrix-vector multiply. */
--    int_t  **Ufstnz_br_ptr = Llu->Ufstnz_br_ptr;
--    int_t  *Urbs, *Urbs1; /* Number of row blocks in each block column of U. */
--    Ucb_indptr_t **Ucb_indptr;/* Vertical linked list pointing to Uindex[] */
--    int_t  **Ucb_valptr;      /* Vertical linked list pointing to Unzval[] */
--    int_t  kcol, krow, mycol, myrow;
--    int_t  i, ii, il, j, jj, k, lb, ljb, lk, lptr, luptr;
--    int_t  nb, nlb, nub, nsupers;
--    int_t  *xsup, *supno, *lsub, *usub;
--    int_t  *ilsum;    /* Starting position of each supernode in lsum (LOCAL)*/
--    int    Pc, Pr, iam;
--    int    knsupc, nsupr;
--    int    ldalsum;   /* Number of lsum entries locally owned. */
--    int    maxrecvsz, p, pi;
--    int_t  **Lrowind_bc_ptr;
--    double **Lnzval_bc_ptr;
--    MPI_Status status;
--    MPI_Request *send_req, recv_req;
--    pxgstrs_comm_t *gstrs_comm = SOLVEstruct->gstrs_comm;
--
--    /*-- Counts used for L-solve --*/
--    int_t  *fmod;         /* Modification count for L-solve --
--                             Count the number of local block products to
--                             be summed into lsum[lk]. */
--    int_t  **fsendx_plist = Llu->fsendx_plist;
--    int_t  nfrecvx = Llu->nfrecvx; /* Number of X components to be recv'd. */
--    int_t  *frecv;        /* Count of lsum[lk] contributions to be received
--                             from processes in this row. 
--                             It is only valid on the diagonal processes. */
--    int_t  nfrecvmod = 0; /* Count of total modifications to be recv'd. */
--    int_t  nleaf = 0, nroot = 0;
--
--    /*-- Counts used for U-solve --*/
--    int_t  *bmod;         /* Modification count for U-solve. */
--    int_t  **bsendx_plist = Llu->bsendx_plist;
--    int_t  nbrecvx = Llu->nbrecvx; /* Number of X components to be recv'd. */
--    int_t  *brecv;        /* Count of modifications to be recv'd from
--			     processes in this row. */
--    int_t  nbrecvmod = 0; /* Count of total modifications to be recv'd. */
--    double t;
-+	Glu_persist_t *Glu_persist = LUstruct->Glu_persist;
-+	LocalLU_t *Llu = LUstruct->Llu;
-+	double alpha = 1.0;
-+	double beta = 0.0;
-+	double zero = 0.0;
-+	double *lsum;  /* Local running sum of the updates to B-components */
-+	double *x;     /* X component at step k. */
-+	/* NOTE: x and lsum are of same size. */
-+	double *lusup, *dest;
-+	double *recvbuf,*recvbuf_on, *tempv, *recvbufall, *recvbuf_BC_fwd, *recvbuf0, *xin;
-+	double *rtemp, *rtemp_loc; /* Result of full matrix-vector multiply. */
-+	double *Linv; /* Inverse of diagonal block */
-+	double *Uinv; /* Inverse of diagonal block */
-+	int *ipiv; 
-+	int_t *leaf_send;
-+	int_t nleaf_send, nleaf_send_tmp;
-+	int_t *root_send;
-+	int_t nroot_send, nroot_send_tmp;
-+	
-+	int_t  **Ufstnz_br_ptr = Llu->Ufstnz_br_ptr;
-+	BcTree  *LBtree_ptr = Llu->LBtree_ptr;
-+	RdTree  *LRtree_ptr = Llu->LRtree_ptr;
-+	BcTree  *UBtree_ptr = Llu->UBtree_ptr;
-+	RdTree  *URtree_ptr = Llu->URtree_ptr;	
-+	int_t  *Urbs1, *Urbs2; /* Number of row blocks in each block column of U. */
-+	int_t  *Urbs = Llu->Urbs; /* Number of row blocks in each block column of U. */
-+	Ucb_indptr_t **Ucb_indptr = Llu->Ucb_indptr;/* Vertical linked list pointing to Uindex[] */
-+	int_t  **Ucb_valptr = Llu->Ucb_valptr;      /* Vertical linked list pointing to Unzval[] */
-+	int_t  kcol, krow, mycol, myrow;
-+	int_t  i, ii, il, j, jj, k, kk, lb, ljb, lk, lib, lptr, luptr, gb, nn;
-+	int_t  nb, nlb,nlb_nodiag, nub, nsupers, nsupers_j, nsupers_i;
-+	int_t  *xsup, *supno, *lsub, *usub;
-+	int_t  *ilsum;    /* Starting position of each supernode in lsum (LOCAL)*/
-+	int    Pc, Pr, iam;
-+	int    knsupc, nsupr, nprobe;
-+	int    nbtree, nrtree, outcount;
-+	int    ldalsum;   /* Number of lsum entries locally owned. */
-+	int    maxrecvsz, p, pi;
-+	int_t  **Lrowind_bc_ptr;
-+	double **Lnzval_bc_ptr;
-+	double **Linv_bc_ptr;
-+	double **Uinv_bc_ptr;
-+	double sum;
-+	MPI_Status status,status_on,statusx,statuslsum;
-+	MPI_Request *send_req, recv_req, req;
-+	pxgstrs_comm_t *gstrs_comm = SOLVEstruct->gstrs_comm;
-+	SuperLUStat_t **stat_loc;
-+
-+	double tmax;
-+
-+	/*-- Counts used for L-solve --*/
-+	int_t  *fmod;         /* Modification count for L-solve --
-+				 Count the number of local block products to
-+				 be summed into lsum[lk]. */
-+	int_t fmod_tmp;
-+	int_t  **fsendx_plist = Llu->fsendx_plist;
-+	int_t  nfrecvx = Llu->nfrecvx; /* Number of X components to be recv'd. */
-+	int_t  nfrecvx_buf=0;						 					 
-+	int_t  *frecv;        /* Count of lsum[lk] contributions to be received
-+				 from processes in this row. 
-+				 It is only valid on the diagonal processes. */
-+	int_t  frecv_tmp;
-+	int_t  nfrecvmod = 0; /* Count of total modifications to be recv'd. */
-+	int_t  nfrecv = 0; /* Count of total messages to be recv'd. */
-+	int_t  nbrecv = 0; /* Count of total messages to be recv'd. */
-+	int_t  nleaf = 0, nroot = 0;
-+	int_t  nleaftmp = 0, nroottmp = 0;
-+	int_t  msgsize;
-+	/*-- Counts used for U-solve --*/
-+	int_t  *bmod;         /* Modification count for U-solve. */
-+	int_t bmod_tmp;
-+	int_t  **bsendx_plist = Llu->bsendx_plist;
-+	int_t  nbrecvx = Llu->nbrecvx; /* Number of X components to be recv'd. */
-+	int_t  nbrecvx_buf=0;		
-+	int_t  *brecv;        /* Count of modifications to be recv'd from
-+				 processes in this row. */
-+	int_t  nbrecvmod = 0; /* Count of total modifications to be recv'd. */
-+	int_t flagx,flaglsum,flag;
-+	int_t *LBTree_active, *LRTree_active, *LBTree_finish, *LRTree_finish, *leafsups, *rootsups; 
-+	int_t TAG;
-+	double t1_sol, t2_sol, t;
- #if ( DEBUGlevel>=2 )
--    int_t Ublocks = 0;
-+	int_t Ublocks = 0;
- #endif
- 
--    int_t *mod_bit = Llu->mod_bit; /* flag contribution from each row block */
-- 
--    t = SuperLU_timer_();
--
--    /* Test input parameters. */
--    *info = 0;
--    if ( n < 0 ) *info = -1;
--    else if ( nrhs < 0 ) *info = -9;
--    if ( *info ) {
--	pxerr_dist("PDGSTRS", grid, -*info);
--	return;
--    }
-+	int_t gik,iklrow,fnz;
- 	
--    /*
--     * Initialization.
--     */
--    iam = grid->iam;
--    Pc = grid->npcol;
--    Pr = grid->nprow;
--    myrow = MYROW( iam, grid );
--    mycol = MYCOL( iam, grid );
--    xsup = Glu_persist->xsup;
--    supno = Glu_persist->supno;
--    nsupers = supno[n-1] + 1;
--    Lrowind_bc_ptr = Llu->Lrowind_bc_ptr;
--    Lnzval_bc_ptr = Llu->Lnzval_bc_ptr;
--    nlb = CEILING( nsupers, Pr ); /* Number of local block rows. */
-+	int_t *mod_bit = Llu->mod_bit; /* flag contribution from each row block */
-+	int INFO, pad;
-+	int_t tmpresult;
-+
-+	// #if ( PROFlevel>=1 )
-+	double t1, t2;
-+	float msg_vol = 0, msg_cnt = 0;
-+	// #endif 
-+
-+	int_t *msgcnt=(int_t *) SUPERLU_MALLOC(4 * sizeof(int_t));   /* Count the size of the message xfer'd in each buffer:
-+								      *     0 : transferred in Lsub_buf[]
-+								      *     1 : transferred in Lval_buf[]
-+								      *     2 : transferred in Usub_buf[]
-+								      *     3 : transferred in Uval_buf[]
-+								      */
-+	int iword = sizeof (int_t);
-+	int dword = sizeof (double);	
-+	int Nwork;
-+
-+	yes_no_t done;
-+	yes_no_t startforward;
-+
-+	int nbrow;
-+	int_t  ik, rel, idx_r, jb, nrbl, irow, pc,iknsupc;
-+	int_t  lptr1_tmp, idx_i, idx_v,m; 
-+
-+	int_t thread_id,ready;
-+	yes_no_t empty;
-+	int_t sizelsum,sizertemp,aln_d,aln_i;
-+
-+	aln_d = ceil(CACHELINE/(double)dword);
-+	aln_i = ceil(CACHELINE/(double)iword);
-+
-+
-+	int num_thread = 1;
-+#ifdef _OPENMP
-+#pragma omp parallel default(shared)
-+	{
-+		if (omp_get_thread_num () == 0) {
-+			num_thread = omp_get_num_threads ();
-+		}
-+	}
-+#endif
-+	if(grid->iam==0){
-+		printf("num_thread: %5d\n",num_thread);
-+		fflush(stdout);
-+	}
-+
-+	MPI_Barrier( grid->comm );
-+	TIC(t1_sol);
-+	t = SuperLU_timer_();
-+
-+	/* Test input parameters. */
-+	*info = 0;
-+	if ( n < 0 ) *info = -1;
-+	else if ( nrhs < 0 ) *info = -9;
-+	if ( *info ) {
-+		pxerr_dist("PDGSTRS", grid, -*info);
-+		return;
-+	}
-+
-+	/*
-+	 * Initialization.
-+	 */
-+	iam = grid->iam;
-+	Pc = grid->npcol;
-+	Pr = grid->nprow;
-+	myrow = MYROW( iam, grid );
-+	mycol = MYCOL( iam, grid );
-+	xsup = Glu_persist->xsup;
-+	supno = Glu_persist->supno;
-+	nsupers = supno[n-1] + 1;
-+	Lrowind_bc_ptr = Llu->Lrowind_bc_ptr;
-+	Lnzval_bc_ptr = Llu->Lnzval_bc_ptr;
-+	Linv_bc_ptr = Llu->Linv_bc_ptr;
-+	Uinv_bc_ptr = Llu->Uinv_bc_ptr;
-+	nlb = CEILING( nsupers, Pr ); /* Number of local block rows. */
-+
-+	stat->utime[SOL_COMM] = 0.0;
-+	stat->utime[SOL_GEMM] = 0.0;
-+	stat->utime[SOL_TRSM] = 0.0;
-+	stat->utime[SOL_L] = 0.0;
-+
- 
- #if ( DEBUGlevel>=1 )
--    CHECK_MALLOC(iam, "Enter pdgstrs()");
-+	CHECK_MALLOC(iam, "Enter pdgstrs()");
- #endif
- 
--    stat->ops[SOLVE] = 0.0;
--    Llu->SolveMsgSent = 0;
-+	stat->ops[SOLVE] = 0.0;
-+	Llu->SolveMsgSent = 0;
- 
--    /* Save the count to be altered so it can be used by
--       subsequent call to PDGSTRS. */
--    if ( !(fmod = intMalloc_dist(nlb)) )
--	ABORT("Calloc fails for fmod[].");
--    for (i = 0; i < nlb; ++i) fmod[i] = Llu->fmod[i];
--    if ( !(frecv = intMalloc_dist(nlb)) )
--	ABORT("Malloc fails for frecv[].");
--    Llu->frecv = frecv;
-+	/* Save the count to be altered so it can be used by
-+	   subsequent call to PDGSTRS. */
- 
--    k = SUPERLU_MAX( Llu->nfsendx, Llu->nbsendx ) + nlb;
--    if ( !(send_req = (MPI_Request*) SUPERLU_MALLOC(k*sizeof(MPI_Request))) )
--	ABORT("Malloc fails for send_req[].");
-+	if ( !(fmod = intMalloc_dist(nlb)) )
-+		ABORT("Calloc fails for fmod[].");
-+	for (i = 0; i < nlb; ++i) fmod[i] = Llu->fmod[i];
-+
-+	if ( !(frecv = intCalloc_dist(nlb)) )
-+		ABORT("Malloc fails for frecv[].");
-+	Llu->frecv = frecv;
-+
-+	if ( !(leaf_send = intMalloc_dist(CEILING( nsupers, Pr )+CEILING( nsupers, Pc ))) )
-+		ABORT("Malloc fails for leaf_send[].");
-+	nleaf_send=0;
-+
-+	if ( !(root_send = intMalloc_dist(CEILING( nsupers, Pr )+CEILING( nsupers, Pc ))) )
-+		ABORT("Malloc fails for root_send[].");
-+	nroot_send=0;	
-+	
- 
- #ifdef _CRAY
--    ftcs1 = _cptofcd("L", strlen("L"));
--    ftcs2 = _cptofcd("N", strlen("N"));
--    ftcs3 = _cptofcd("U", strlen("U"));
-+	ftcs1 = _cptofcd("L", strlen("L"));
-+	ftcs2 = _cptofcd("N", strlen("N"));
-+	ftcs3 = _cptofcd("U", strlen("U"));
- #endif
- 
--
--    /* Obtain ilsum[] and ldalsum for process column 0. */
--    ilsum = Llu->ilsum;
--    ldalsum = Llu->ldalsum;
--
--    /* Allocate working storage. */
--    knsupc = sp_ienv_dist(3);
--    maxrecvsz = knsupc * nrhs + SUPERLU_MAX( XK_H, LSUM_H );
--    if ( !(lsum = doubleCalloc_dist(((size_t)ldalsum)*nrhs + nlb*LSUM_H)) )
--	ABORT("Calloc fails for lsum[].");
--    if ( !(x = doubleCalloc_dist(ldalsum * nrhs + nlb * XK_H)) )
--	ABORT("Calloc fails for x[].");
--    if ( !(recvbuf = doubleMalloc_dist(maxrecvsz)) )
--	ABORT("Malloc fails for recvbuf[].");
--    if ( !(rtemp = doubleCalloc_dist(maxrecvsz)) )
--	ABORT("Malloc fails for rtemp[].");
--
--    
--    /*---------------------------------------------------
--     * Forward solve Ly = b.
--     *---------------------------------------------------*/
--    /* Redistribute B into X on the diagonal processes. */
--    pdReDistribute_B_to_X(B, m_loc, nrhs, ldb, fst_row, ilsum, x, 
--			  ScalePermstruct, Glu_persist, grid, SOLVEstruct);
--
--    /* Set up the headers in lsum[]. */
--    ii = 0;
--    for (k = 0; k < nsupers; ++k) {
--	knsupc = SuperSize( k );
--	krow = PROW( k, grid );
--	if ( myrow == krow ) {
--	    lk = LBi( k, grid );   /* Local block number. */
--	    il = LSUM_BLK( lk );
--	    lsum[il - LSUM_H] = k; /* Block number prepended in the header. */
-+	/* Obtain ilsum[] and ldalsum for process column 0. */
-+	ilsum = Llu->ilsum;
-+	ldalsum = Llu->ldalsum;
-+
-+	/* Allocate working storage. */
-+	knsupc = sp_ienv_dist(3);
-+	maxrecvsz = knsupc * nrhs + SUPERLU_MAX( XK_H, LSUM_H );
-+	sizelsum = (((size_t)ldalsum)*nrhs + nlb*LSUM_H);
-+	sizelsum = ((sizelsum + (aln_d - 1)) / aln_d) * aln_d;
-+
-+#ifdef _OPENMP
-+	if ( !(lsum = doubleMalloc_dist(sizelsum*num_thread)))
-+		ABORT("Calloc fails for lsum[].");	
-+#pragma omp parallel default(shared) private(thread_id,ii)
-+	{
-+		thread_id = omp_get_thread_num ();
-+		for(ii=0;ii<sizelsum;ii++)
-+			lsum[thread_id*sizelsum+ii]=0;
- 	}
--	ii += knsupc;
--    }
-+#else	
-+	if ( !(lsum = doubleCalloc_dist(sizelsum*num_thread)))
-+		ABORT("Calloc fails for lsum[].");	
-+#endif	
-+
-+	if ( !(x = doubleCalloc_dist(ldalsum * nrhs + nlb * XK_H)) )
-+		ABORT("Calloc fails for x[].");
-+	// if ( !(recvbuf = doubleMalloc_dist(maxrecvsz)) )
-+		// ABORT("Malloc fails for recvbuf[].");   
-+
-+	sizertemp=ldalsum * nrhs;
-+	sizertemp = ((sizertemp + (aln_d - 1)) / aln_d) * aln_d;
-+#ifdef _OPENMP
-+	if ( !(rtemp = doubleMalloc_dist(sizertemp* num_thread)) )
-+		ABORT("Malloc fails for rtemp[].");
-+#pragma omp parallel default(shared) private(thread_id,ii)
-+	{
-+		thread_id = omp_get_thread_num ();
-+		for(ii=0;ii<sizertemp;ii++)
-+			rtemp[thread_id*sizertemp+ii]=0;
-+	}
-+#else	
-+	if ( !(rtemp = doubleCalloc_dist(sizertemp* num_thread)) )
-+		ABORT("Malloc fails for rtemp[].");
-+#endif	
- 
--    /*
--     * Compute frecv[] and nfrecvmod counts on the diagonal processes.
--     */
--    {
--	superlu_scope_t *scp = &grid->rscp;
-+	if ( !(stat_loc = (SuperLUStat_t**) SUPERLU_MALLOC(num_thread*sizeof(SuperLUStat_t*))) )
-+		ABORT("Malloc fails for stat_loc[].");
- 
--#if 1
--	for (k = 0; k < nlb; ++k) mod_bit[k] = 0;
--	for (k = 0; k < nsupers; ++k) {
--	    krow = PROW( k, grid );
--	    if ( myrow == krow ) {
--		lk = LBi( k, grid );    /* local block number */
--		kcol = PCOL( k, grid );
--		if ( mycol != kcol && fmod[lk] )
--		    mod_bit[lk] = 1;  /* contribution from off-diagonal */
--	    }
-+	for(i=0;i<num_thread;i++){
-+		stat_loc[i] = (SuperLUStat_t*)SUPERLU_MALLOC(sizeof(SuperLUStat_t));
-+		PStatInit(stat_loc[i]);
- 	}
--	/*PrintInt10("mod_bit", nlb, mod_bit);*/
--	
--#if ( PROFlevel>=2 )
--	t_reduce_tmp = SuperLU_timer_();
--#endif
--	/* Every process receives the count, but it is only useful on the
--	   diagonal processes.  */
--	MPI_Allreduce( mod_bit, frecv, nlb, mpi_int_t, MPI_SUM, scp->comm );
- 
--#if ( PROFlevel>=2 )
--	t_reduce += SuperLU_timer_() - t_reduce_tmp;
--#endif
-+	/*---------------------------------------------------
-+	 * Forward solve Ly = b.
-+	 *---------------------------------------------------*/
-+	/* Redistribute B into X on the diagonal processes. */
-+	pdReDistribute_B_to_X(B, m_loc, nrhs, ldb, fst_row, ilsum, x, 
-+			ScalePermstruct, Glu_persist, grid, SOLVEstruct);
-+
- 
-+#if ( PRNTlevel>=1 )
-+	t = SuperLU_timer_() - t;
-+	if ( !iam) printf(".. B to X redistribute time\t%8.4f\n", t);
-+	fflush(stdout);
-+	t = SuperLU_timer_();
-+#endif			  
-+
-+	/* Set up the headers in lsum[]. */
-+	ii = 0;
- 	for (k = 0; k < nsupers; ++k) {
--	    krow = PROW( k, grid );
--	    if ( myrow == krow ) {
--		lk = LBi( k, grid );    /* local block number */
--		kcol = PCOL( k, grid );
--		if ( mycol == kcol ) { /* diagonal process */
--		    nfrecvmod += frecv[lk];
--		    if ( !frecv[lk] && !fmod[lk] ) ++nleaf;
-+		knsupc = SuperSize( k );
-+		krow = PROW( k, grid );
-+		if ( myrow == krow ) {
-+			lk = LBi( k, grid );   /* Local block number. */
-+			il = LSUM_BLK( lk );
-+			lsum[il - LSUM_H] = k; /* Block number prepended in the header. */
- 		}
--	    }
-+		ii += knsupc;
- 	}
- 
--#else /* old */
-+	/* ---------------------------------------------------------
-+	   Precompute mapping from Lrowind_bc_ptr to lsum.
-+	   --------------------------------------------------------- */		
-+
-+	   
-+	   
-+	// nsupers_j = CEILING( nsupers, grid->npcol ); /* Number of local block columns */
-+	// if ( !(Llu->Lrowind_bc_2_lsum = 
-+				// (int_t**)SUPERLU_MALLOC(nsupers_j * sizeof(int_t*))) )
-+		// ABORT("Malloc fails for Lrowind_bc_2_lsum[].");
-+
-+
-+	// for (ljb = 0; ljb < nsupers_j; ++ljb) {
-+
-+		// if(Lrowind_bc_ptr[ljb]!=NULL){
-+
-+			// jb = mycol+ljb*grid->npcol;
-+
-+			// knsupc = SuperSize( jb );
-+			// krow = PROW( jb, grid );
-+			// nrbl = Lrowind_bc_ptr[ljb][0];
-+
-+			// if(myrow==krow){ /* skip the diagonal block */
-+				// nlb_nodiag=nrbl-1;
-+				// idx_i = nlb_nodiag+2;
-+				// m = Lrowind_bc_ptr[ljb][1]-knsupc;
-+			// }else{
-+				// nlb_nodiag=nrbl;
-+				// idx_i = nlb_nodiag;
-+				// m = Lrowind_bc_ptr[ljb][1];
-+			// }	
-+
-+			// if(nlb_nodiag>0){		
-+				// if ( !(Llu->Lrowind_bc_2_lsum[ljb] = intMalloc_dist(((m*nrhs + (aln_i - 1)) / aln_i) * aln_i)) )
-+					// ABORT("Malloc fails for Lrowind_bc_2_lsum[ljb][].");	
-+				// idx_r=0;
-+				// RHS_ITERATE(j)
-+					// for (lb = 0; lb < nlb_nodiag; ++lb) {
-+						// lptr1_tmp = Llu->Lindval_loc_bc_ptr[ljb][lb+idx_i];	
-+						// ik = Lrowind_bc_ptr[ljb][lptr1_tmp]; /* Global block number, row-wise. */
-+						// iknsupc = SuperSize( ik );
-+						// nbrow = Lrowind_bc_ptr[ljb][lptr1_tmp+1];
-+						// lk = LBi( ik, grid ); /* Local block number, row-wise. */
-+						// il = LSUM_BLK( lk );
-+						// rel = xsup[ik]; /* Global row index of block ik. */
-+						// for (ii = 0; ii < nbrow; ++ii) {	
-+							// irow = Lrowind_bc_ptr[ljb][lptr1_tmp+LB_DESCRIPTOR+ii] - rel; /* Relative row. */		
-+							// Llu->Lrowind_bc_2_lsum[ljb][idx_r++] = il+irow+ j*iknsupc;								
-+						// }			
-+					// }		
-+			// }else{
-+				// Llu->Lrowind_bc_2_lsum[ljb]=NULL;
-+			// } 
-+		// }else{
-+			// Llu->Lrowind_bc_2_lsum[ljb]=NULL;		
-+		// }		
-+	// }	
-+
-+	/* ---------------------------------------------------------
-+	   Initialize the async Bcast trees on all processes.
-+	   --------------------------------------------------------- */		
-+	nsupers_j = CEILING( nsupers, grid->npcol ); /* Number of local block columns */
-+
-+	nbtree = 0;
-+	for (lk=0;lk<nsupers_j;++lk){
-+		if(LBtree_ptr[lk]!=NULL){
-+			// printf("LBtree_ptr lk %5d\n",lk); 
-+			if(BcTree_IsRoot(LBtree_ptr[lk])==NO){			
-+				nbtree++;
-+				if(BcTree_getDestCount(LBtree_ptr[lk])>0)nfrecvx_buf++;				  
-+			}
-+			BcTree_allocateRequest(LBtree_ptr[lk]);
-+		}
-+	}
-+
-+	nsupers_i = CEILING( nsupers, grid->nprow ); /* Number of local block rows */
-+	if ( !(	leafsups = (int_t*)intCalloc_dist(nsupers_i)) )
-+		ABORT("Calloc fails for leafsups.");
-+
-+	nrtree = 0;
-+	nleaf=0;
-+	for (lk=0;lk<nsupers_i;++lk){
-+		if(LRtree_ptr[lk]!=NULL){
-+			nrtree++;
-+			RdTree_allocateRequest(LRtree_ptr[lk]);			
-+			frecv[lk] = RdTree_GetDestCount(LRtree_ptr[lk]);
-+			nfrecvmod += frecv[lk];
-+		}else{
-+			gb = myrow+lk*grid->nprow;  /* not sure */
-+			if(gb<nsupers){
-+				kcol = PCOL( gb, grid );
-+				if(mycol==kcol) { /* Diagonal process */
-+					if (fmod[lk]==0){
-+						leafsups[nleaf]=gb;				
-+						++nleaf;
-+					}
-+				}
-+			}
-+		}
-+	}	
-+
-+
-+	for (i = 0; i < nlb; ++i) fmod[i] += frecv[i];
-+
-+	if ( !(recvbuf_BC_fwd = doubleMalloc_dist(maxrecvsz*(nfrecvx+1))) )   // this needs to be optimized for 1D row mapping
-+		ABORT("Malloc fails for recvbuf_BC_fwd[].");	
-+	nfrecvx_buf=0;			
- 
--	for (k = 0; k < nsupers; ++k) {
--	    krow = PROW( k, grid );
--	    if ( myrow == krow ) {
--		lk = LBi( k, grid );    /* Local block number. */
--		kcol = PCOL( k, grid ); /* Root process in this row scope. */
--		if ( mycol != kcol && fmod[lk] )
--		    i = 1;  /* Contribution from non-diagonal process. */
--		else i = 0;
--		MPI_Reduce( &i, &frecv[lk], 1, mpi_int_t,
--			   MPI_SUM, kcol, scp->comm );
--		if ( mycol == kcol ) { /* Diagonal process. */
--		    nfrecvmod += frecv[lk];
--		    if ( !frecv[lk] && !fmod[lk] ) ++nleaf;
- #if ( DEBUGlevel>=2 )
--		    printf("(%2d) frecv[%4d]  %2d\n", iam, k, frecv[lk]);
--		    assert( frecv[lk] < Pc );
-+	printf("(%2d) nfrecvx %4d,  nfrecvmod %4d,  nleaf %4d\n,  nbtree %4d\n,  nrtree %4d\n",
-+			iam, nfrecvx, nfrecvmod, nleaf, nbtree, nrtree);
-+	fflush(stdout);
- #endif
--		}
--	    }
--	}
-+
-+
-+
-+#if ( PRNTlevel>=1 )
-+	t = SuperLU_timer_() - t;
-+	if ( !iam) printf(".. Setup L-solve time\t%8.4f\n", t);
-+	fflush(stdout);
-+	MPI_Barrier( grid->comm );	
-+	t = SuperLU_timer_();
- #endif
--    }
- 
--    /* ---------------------------------------------------------
--       Solve the leaf nodes first by all the diagonal processes.
--       --------------------------------------------------------- */
-+
-+#if ( VAMPIR>=1 )
-+	// VT_initialize(); 
-+	VT_traceon();	
-+#endif
-+
-+
-+	/* ---------------------------------------------------------
-+	   Solve the leaf nodes first by all the diagonal processes.
-+	   --------------------------------------------------------- */
- #if ( DEBUGlevel>=2 )
--    printf("(%2d) nleaf %4d\n", iam, nleaf);
-+	printf("(%2d) nleaf %4d\n", iam, nleaf);
-+	fflush(stdout);
- #endif
--    for (k = 0; k < nsupers && nleaf; ++k) {
--	krow = PROW( k, grid );
--	kcol = PCOL( k, grid );
--	if ( myrow == krow && mycol == kcol ) { /* Diagonal process */
--	    knsupc = SuperSize( k );
--	    lk = LBi( k, grid );
--	    if ( frecv[lk]==0 && fmod[lk]==0 ) {
--		fmod[lk] = -1;  /* Do not solve X[k] in the future. */
--		ii = X_BLK( lk );
--		lk = LBj( k, grid ); /* Local block number, column-wise. */
--		lsub = Lrowind_bc_ptr[lk];
--		lusup = Lnzval_bc_ptr[lk];
--		nsupr = lsub[1];
-+
-+
-+#ifdef _OPENMP
-+#pragma omp parallel default (shared) 
-+#endif
-+	{	
-+#ifdef _OPENMP
-+#pragma omp master
-+#endif
-+		{
-+
-+#ifdef _OPENMP
-+#pragma	omp	taskloop firstprivate (nrhs,beta,alpha,x,rtemp,ldalsum) private (ii,k,knsupc,lk,luptr,lsub,nsupr,lusup,thread_id,t1,t2,Linv,i,lib,rtemp_loc,nleaf_send_tmp) nogroup	
-+#endif
-+			for (jj=0;jj<nleaf;jj++){
-+				k=leafsups[jj];
-+
-+				// #ifdef _OPENMP
-+				// #pragma	omp	task firstprivate (k,nrhs,beta,alpha,x,rtemp,ldalsum) private (ii,knsupc,lk,luptr,lsub,nsupr,lusup,thread_id,t1,t2,Linv,i,lib,rtemp_loc)	 	
-+				// #endif
-+				{
-+
-+#if ( PROFlevel>=1 )
-+					TIC(t1);
-+#endif	 
-+#ifdef _OPENMP
-+					thread_id = omp_get_thread_num ();
-+#else
-+					thread_id = 0;
-+#endif
-+					rtemp_loc = &rtemp[sizertemp* thread_id];
-+
-+
-+					knsupc = SuperSize( k );
-+					lk = LBi( k, grid );
-+
-+					// if ( frecv[lk]==0 && fmod[lk]==0 ) { 
-+					// fmod[lk] = -1;  /* Do not solve X[k] in the future. */
-+					ii = X_BLK( lk );
-+					lk = LBj( k, grid ); /* Local block number, column-wise. */
-+					lsub = Lrowind_bc_ptr[lk];
-+					lusup = Lnzval_bc_ptr[lk];
-+
-+					nsupr = lsub[1];
-+
-+
-+
-+					if(Llu->inv == 1){
-+						Linv = Linv_bc_ptr[lk];
-+#ifdef _CRAY
-+						SGEMM( ftcs2, ftcs2, &knsupc, &nrhs, &knsupc,
-+								&alpha, Linv, &knsupc, &x[ii],
-+								&knsupc, &beta, rtemp_loc, &knsupc );
-+#elif defined (USE_VENDOR_BLAS)
-+						dgemm_( "N", "N", &knsupc, &nrhs, &knsupc,
-+								&alpha, Linv, &knsupc, &x[ii],
-+								&knsupc, &beta, rtemp_loc, &knsupc, 1, 1 );
-+#else
-+						dgemm_( "N", "N", &knsupc, &nrhs, &knsupc,
-+								&alpha, Linv, &knsupc, &x[ii],
-+								&knsupc, &beta, rtemp_loc, &knsupc );
-+#endif		   
-+						for (i=0 ; i<knsupc*nrhs ; i++){
-+							x[ii+i] = rtemp_loc[i];
-+						}		
-+					}else{
- #ifdef _CRAY
--		STRSM(ftcs1, ftcs1, ftcs2, ftcs3, &knsupc, &nrhs, &alpha,
--		      lusup, &nsupr, &x[ii], &knsupc);
-+						STRSM(ftcs1, ftcs1, ftcs2, ftcs3, &knsupc, &nrhs, &alpha,
-+								lusup, &nsupr, &x[ii], &knsupc);
- #elif defined (USE_VENDOR_BLAS)
--		dtrsm_("L", "L", "N", "U", &knsupc, &nrhs, &alpha, 
--		       lusup, &nsupr, &x[ii], &knsupc, 1, 1, 1, 1);
-+						dtrsm_("L", "L", "N", "U", &knsupc, &nrhs, &alpha, 
-+								lusup, &nsupr, &x[ii], &knsupc, 1, 1, 1, 1);	
- #else
--		dtrsm_("L", "L", "N", "U", &knsupc, &nrhs, &alpha, 
--		       lusup, &nsupr, &x[ii], &knsupc);
-+						dtrsm_("L", "L", "N", "U", &knsupc, &nrhs, &alpha, 
-+								lusup, &nsupr, &x[ii], &knsupc);
- #endif
--		stat->ops[SOLVE] += knsupc * (knsupc - 1) * nrhs;
--		--nleaf;
-+					}
-+
-+
-+#if ( PROFlevel>=1 )
-+					TOC(t2, t1);
-+					stat_loc[thread_id]->utime[SOL_TRSM] += t2;
-+
-+#endif	
-+
-+					stat_loc[thread_id]->ops[SOLVE] += knsupc * (knsupc - 1) * nrhs;
-+					// --nleaf;
- #if ( DEBUGlevel>=2 )
--		printf("(%2d) Solve X[%2d]\n", iam, k);
-+					printf("(%2d) Solve X[%2d]\n", iam, k);
- #endif
--		
--		/*
--		 * Send Xk to process column Pc[k].
--		 */
--		for (p = 0; p < Pr; ++p) {
--		    if ( fsendx_plist[lk][p] != EMPTY ) {
--			pi = PNUM( p, kcol, grid );
- 
--			MPI_Isend( &x[ii - XK_H], knsupc * nrhs + XK_H,
--				   MPI_DOUBLE, pi, Xk, grid->comm,
--                                   &send_req[Llu->SolveMsgSent++]);
--#if 0
--			MPI_Send( &x[ii - XK_H], knsupc * nrhs + XK_H,
--				 MPI_DOUBLE, pi, Xk, grid->comm );
--#endif
--#if ( DEBUGlevel>=2 )
--			printf("(%2d) Sent X[%2.0f] to P %2d\n",
--			       iam, x[ii-XK_H], pi);
-+					/*
-+					 * Send Xk to process column Pc[k].
-+					 */
-+
-+					if(LBtree_ptr[lk]!=NULL){ 
-+						lib = LBi( k, grid ); /* Local block number, row-wise. */
-+						ii = X_BLK( lib );	
-+
-+#ifdef _OPENMP
-+#pragma omp atomic capture
- #endif
--		    }
-+						nleaf_send_tmp = ++nleaf_send;
-+						leaf_send[nleaf_send_tmp-1] = lk;
-+						// BcTree_forwardMessageSimple(LBtree_ptr[lk],&x[ii - XK_H]);
-+					}
-+				}		
-+				}
-+			}	
- 		}
--		/*
--		 * Perform local block modifications: lsum[i] -= L_i,k * X[k]
--		 */
--		nb = lsub[0] - 1;
--		lptr = BC_HEADER + LB_DESCRIPTOR + knsupc;
--		luptr = knsupc; /* Skip diagonal block L(k,k). */
--		
--		dlsum_fmod(lsum, x, &x[ii], rtemp, nrhs, knsupc, k,
--			   fmod, nb, lptr, luptr, xsup, grid, Llu, 
--			   send_req, stat);
--	    }
--	} /* if diagonal process ... */
--    } /* for k ... */
--
--    /* -----------------------------------------------------------
--       Compute the internal nodes asynchronously by all processes.
--       ----------------------------------------------------------- */
--#if ( DEBUGlevel>=2 )
--    printf("(%2d) nfrecvx %4d,  nfrecvmod %4d,  nleaf %4d\n",
--	   iam, nfrecvx, nfrecvmod, nleaf);
--#endif
- 
--    while ( nfrecvx || nfrecvmod ) { /* While not finished. */
- 
--	/* Receive a message. */
--	MPI_Recv( recvbuf, maxrecvsz, MPI_DOUBLE,
--                  MPI_ANY_SOURCE, MPI_ANY_TAG, grid->comm, &status );
-+#if ( VTUNE>=1 )
-+		__itt_resume();
-+#endif
- 
--	k = *recvbuf;
-+		jj=0;
-+#ifdef _OPENMP
-+#pragma omp parallel default (shared) private(thread_id)
-+		{
-+			thread_id = omp_get_thread_num ();
-+#else
-+			{
-+				thread_id = 0;
-+#endif
- 
--#if ( DEBUGlevel>=2 )
--	printf("(%2d) Recv'd block %d, tag %2d\n", iam, k, status.MPI_TAG);
-+#ifdef _OPENMP
-+#pragma omp master
- #endif
--	
--	switch ( status.MPI_TAG ) {
--	  case Xk:
--	      --nfrecvx;
--	      lk = LBj( k, grid ); /* Local block number, column-wise. */
--	      lsub = Lrowind_bc_ptr[lk];
--	      lusup = Lnzval_bc_ptr[lk];
--	      if ( lsub ) {
--		  nb   = lsub[0];
--		  lptr = BC_HEADER;
--		  luptr = 0;
--		  knsupc = SuperSize( k );
--
--		  /*
--		   * Perform local block modifications: lsum[i] -= L_i,k * X[k]
--		   */
--		  dlsum_fmod(lsum, x, &recvbuf[XK_H], rtemp, nrhs, knsupc, k,
--			     fmod, nb, lptr, luptr, xsup, grid, Llu, 
--			     send_req, stat);
--	      } /* if lsub */
--
--	      break;
--
--	  case LSUM: /* Receiver must be a diagonal process */
--	      --nfrecvmod;
--	      lk = LBi( k, grid ); /* Local block number, row-wise. */
--	      ii = X_BLK( lk );
--	      knsupc = SuperSize( k );
--	      tempv = &recvbuf[LSUM_H];
--	      RHS_ITERATE(j) {
--		  for (i = 0; i < knsupc; ++i)
--		      x[i + ii + j*knsupc] += tempv[i + j*knsupc];
--	      }
--
--	      if ( (--frecv[lk])==0 && fmod[lk]==0 ) {
--		  fmod[lk] = -1; /* Do not solve X[k] in the future. */
--		  lk = LBj( k, grid ); /* Local block number, column-wise. */
--		  lsub = Lrowind_bc_ptr[lk];
--		  lusup = Lnzval_bc_ptr[lk];
--		  nsupr = lsub[1];
--#ifdef _CRAY
--		  STRSM(ftcs1, ftcs1, ftcs2, ftcs3, &knsupc, &nrhs, &alpha,
--			lusup, &nsupr, &x[ii], &knsupc);
--#elif defined (USE_VENDOR_BLAS)
--		  dtrsm_("L", "L", "N", "U", &knsupc, &nrhs, &alpha, 
--			 lusup, &nsupr, &x[ii], &knsupc, 1, 1, 1, 1);
-+				{
-+
-+#ifdef _OPENMP
-+#pragma	omp	taskloop private (i,k,ii,knsupc,lk,nb,lptr,luptr,lsub,lusup,thread_id) untied num_tasks(num_thread*8) nogroup
-+#endif
-+
-+					for (jj=0;jj<nleaf;jj++){
-+						k=leafsups[jj];		
-+
-+						// #ifdef _OPENMP
-+						// #pragma	omp	task firstprivate (k) private (ii,knsupc,lk,nb,lptr,luptr,lsub,lusup,thread_id) untied	 	
-+						// #endif
-+						{
-+
-+#ifdef _OPENMP
-+							thread_id = omp_get_thread_num ();
- #else
--		  dtrsm_("L", "L", "N", "U", &knsupc, &nrhs, &alpha, 
--			 lusup, &nsupr, &x[ii], &knsupc);
-+							thread_id = 0;
-+#endif								
-+
-+							/* Diagonal process */
-+							knsupc = SuperSize( k );
-+							lk = LBi( k, grid );
-+
-+							// if ( frecv[lk]==0 && fmod[lk]==0 ) { 
-+							// fmod[lk] = -1;  /* Do not solve X[k] in the future. */
-+							ii = X_BLK( lk );
-+							lk = LBj( k, grid ); /* Local block number, column-wise. */
-+							lsub = Lrowind_bc_ptr[lk];
-+							lusup = Lnzval_bc_ptr[lk];
-+
-+							/*
-+							 * Perform local block modifications: lsum[i] -= L_i,k * X[k]
-+							 */
-+							nb = lsub[0] - 1;
-+							dlsum_fmod_inv(lsum, x, &x[ii], rtemp, nrhs, knsupc, k,
-+									fmod, nb, xsup, grid, Llu, 
-+									stat_loc, leaf_send, &nleaf_send,sizelsum,sizertemp,0);	
-+						}
-+
-+						// } /* if diagonal process ... */
-+					} /* for k ... */
-+				}
-+
-+			}
-+
-+			for (i=0;i<nleaf_send;i++){
-+				lk = leaf_send[i];
-+				if(lk>=0){ // this is a bcast forwarding
-+					gb = mycol+lk*grid->npcol;  /* not sure */
-+					lib = LBi( gb, grid ); /* Local block number, row-wise. */
-+					ii = X_BLK( lib );			
-+					BcTree_forwardMessageSimple(LBtree_ptr[lk],&x[ii - XK_H]);
-+				}else{ // this is a reduce forwarding
-+					lk = -lk - 1;
-+					il = LSUM_BLK( lk );
-+					RdTree_forwardMessageSimple(LRtree_ptr[lk],&lsum[il - LSUM_H ]);
-+				}
-+			}
-+
-+
-+
-+#if ( VTUNE>=1 )
-+			__itt_pause();
- #endif
--		  stat->ops[SOLVE] += knsupc * (knsupc - 1) * nrhs;
--#if ( DEBUGlevel>=2 )
--		  printf("(%2d) Solve X[%2d]\n", iam, k);
-+
-+			/* -----------------------------------------------------------
-+			   Compute the internal nodes asynchronously by all processes.
-+			   ----------------------------------------------------------- */
-+
-+#ifdef _OPENMP
-+#pragma omp parallel default (shared) 
- #endif
--		
--		  /*
--		   * Send Xk to process column Pc[k].
--		   */
--		  kcol = PCOL( k, grid );
--		  for (p = 0; p < Pr; ++p) {
--		      if ( fsendx_plist[lk][p] != EMPTY ) {
--			  pi = PNUM( p, kcol, grid );
--
--			  MPI_Isend( &x[ii-XK_H], knsupc * nrhs + XK_H,
--                                     MPI_DOUBLE, pi, Xk, grid->comm,
--                                     &send_req[Llu->SolveMsgSent++]);
--#if 0
--			  MPI_Send( &x[ii - XK_H], knsupc * nrhs + XK_H,
--				    MPI_DOUBLE, pi, Xk, grid->comm );
-+			{	
-+#ifdef _OPENMP
-+#pragma omp master 
- #endif
-+				{									 
-+					for ( nfrecv =0; nfrecv<nfrecvx+nfrecvmod;nfrecv++) { /* While not finished. */
-+						thread_id = 0;
-+#if ( PROFlevel>=1 )
-+						TIC(t1);
-+						// msgcnt[1] = maxrecvsz;
-+#endif	
-+
-+						recvbuf0 = &recvbuf_BC_fwd[nfrecvx_buf*maxrecvsz];
-+
-+						/* Receive a message. */
-+						MPI_Recv( recvbuf0, maxrecvsz, MPI_DOUBLE,
-+								MPI_ANY_SOURCE, MPI_ANY_TAG, grid->comm, &status );	 	
-+						// MPI_Irecv(recvbuf0,maxrecvsz,MPI_DOUBLE,MPI_ANY_SOURCE,MPI_ANY_TAG,grid->comm,&req);
-+						// ready=0;
-+						// while(ready==0){
-+						// MPI_Test(&req,&ready,&status);
-+						// #pragma omp taskyield
-+						// }
-+
-+#if ( PROFlevel>=1 )		 
-+						TOC(t2, t1);
-+						stat_loc[thread_id]->utime[SOL_COMM] += t2;
-+
-+						msg_cnt += 1;
-+						msg_vol += maxrecvsz * dword;			
-+#endif					  
-+
-+						{  
-+
-+							k = *recvbuf0;
- #if ( DEBUGlevel>=2 )
--			  printf("(%2d) Sent X[%2.0f] to P %2d\n",
--				 iam, x[ii-XK_H], pi);
-+							printf("(%2d) Recv'd block %d, tag %2d\n", iam, k, status.MPI_TAG);
-+#endif
-+
-+							if(status.MPI_TAG==BC_L){
-+								// --nfrecvx;
-+								nfrecvx_buf++;
-+								{
-+									lk = LBj( k, grid );    /* local block number */
-+
-+									if(BcTree_getDestCount(LBtree_ptr[lk])>0){
-+
-+										BcTree_forwardMessageSimple(LBtree_ptr[lk],recvbuf0);	
-+										// nfrecvx_buf++;
-+									}
-+
-+									/*
-+									 * Perform local block modifications: lsum[i] -= L_i,k * X[k]
-+									 */	  
-+
-+									lk = LBj( k, grid ); /* Local block number, column-wise. */
-+									lsub = Lrowind_bc_ptr[lk];
-+									lusup = Lnzval_bc_ptr[lk];
-+									if ( lsub ) {
-+										krow = PROW( k, grid );
-+										if(myrow==krow){
-+											nb = lsub[0] - 1;
-+											knsupc = SuperSize( k );
-+											ii = X_BLK( LBi( k, grid ) );
-+											xin = &x[ii];
-+										}else{
-+											nb   = lsub[0];
-+											knsupc = SuperSize( k );
-+											xin = &recvbuf0[XK_H] ;					
-+										}
-+
-+										dlsum_fmod_inv_master(lsum, x, xin, rtemp, nrhs, knsupc, k,
-+												fmod, nb, xsup, grid, Llu,
-+												stat_loc,sizelsum,sizertemp,0);	
-+
-+									} /* if lsub */
-+								}
-+
-+							}else if(status.MPI_TAG==RD_L){
-+								// --nfrecvmod;		  
-+								lk = LBi( k, grid ); /* Local block number, row-wise. */
-+
-+								knsupc = SuperSize( k );
-+								tempv = &recvbuf0[LSUM_H];
-+								il = LSUM_BLK( lk );		  
-+								RHS_ITERATE(j) {
-+									for (i = 0; i < knsupc; ++i)
-+										lsum[i + il + j*knsupc + thread_id*sizelsum] += tempv[i + j*knsupc];
-+								}			
-+
-+								// #ifdef _OPENMP
-+								// #pragma omp atomic capture
-+								// #endif
-+								fmod_tmp=--fmod[lk];
-+								{
-+									thread_id = 0;
-+									rtemp_loc = &rtemp[sizertemp* thread_id];
-+									if ( fmod_tmp==0 ) {	  
-+										if(RdTree_IsRoot(LRtree_ptr[lk])==YES){
-+											// ii = X_BLK( lk );
-+											knsupc = SuperSize( k );
-+											for (ii=1;ii<num_thread;ii++)
-+												for (jj=0;jj<knsupc*nrhs;jj++)
-+													lsum[il+ jj ] += lsum[il + jj + ii*sizelsum];
-+
-+											ii = X_BLK( lk );
-+											RHS_ITERATE(j)
-+												for (i = 0; i < knsupc; ++i)	
-+													x[i + ii + j*knsupc] += lsum[i + il + j*knsupc ];
-+
-+											// fmod[lk] = -1; /* Do not solve X[k] in the future. */
-+											lk = LBj( k, grid ); /* Local block number, column-wise. */
-+											lsub = Lrowind_bc_ptr[lk];
-+											lusup = Lnzval_bc_ptr[lk];
-+											nsupr = lsub[1];
-+
-+#if ( PROFlevel>=1 )
-+											TIC(t1);
-+#endif			  
-+
-+											if(Llu->inv == 1){
-+												Linv = Linv_bc_ptr[lk];		  
-+#ifdef _CRAY
-+												SGEMM( ftcs2, ftcs2, &knsupc, &nrhs, &knsupc,
-+														&alpha, Linv, &knsupc, &x[ii],
-+														&knsupc, &beta, rtemp_loc, &knsupc );
-+#elif defined (USE_VENDOR_BLAS)
-+												dgemm_( "N", "N", &knsupc, &nrhs, &knsupc,
-+														&alpha, Linv, &knsupc, &x[ii],
-+														&knsupc, &beta, rtemp_loc, &knsupc, 1, 1 );
-+#else
-+												dgemm_( "N", "N", &knsupc, &nrhs, &knsupc,
-+														&alpha, Linv, &knsupc, &x[ii],
-+														&knsupc, &beta, rtemp_loc, &knsupc );
-+#endif			   
-+												for (i=0 ; i<knsupc*nrhs ; i++){
-+													x[ii+i] = rtemp_loc[i];
-+												}		
-+											}
-+											else{
-+#ifdef _CRAY
-+												STRSM(ftcs1, ftcs1, ftcs2, ftcs3, &knsupc, &nrhs, &alpha,
-+														lusup, &nsupr, &x[ii], &knsupc);
-+#elif defined (USE_VENDOR_BLAS)
-+												dtrsm_("L", "L", "N", "U", &knsupc, &nrhs, &alpha, 
-+														lusup, &nsupr, &x[ii], &knsupc, 1, 1, 1, 1);		
-+#else
-+												dtrsm_("L", "L", "N", "U", &knsupc, &nrhs, &alpha, 
-+														lusup, &nsupr, &x[ii], &knsupc);
- #endif
--		      }
--                  }
--		  /*
--		   * Perform local block modifications.
--		   */
--		  nb = lsub[0] - 1;
--		  lptr = BC_HEADER + LB_DESCRIPTOR + knsupc;
--		  luptr = knsupc; /* Skip diagonal block L(k,k). */
--
--		  dlsum_fmod(lsum, x, &x[ii], rtemp, nrhs, knsupc, k,
--			     fmod, nb, lptr, luptr, xsup, grid, Llu,
--			     send_req, stat);
--	      } /* if */
--
--	      break;
-+											}
-+
-+
-+#if ( PROFlevel>=1 )
-+											TOC(t2, t1);
-+											stat_loc[thread_id]->utime[SOL_TRSM] += t2;
-+
-+#endif	
-+
-+
- 
-+											stat_loc[thread_id]->ops[SOLVE] += knsupc * (knsupc - 1) * nrhs;
- #if ( DEBUGlevel>=2 )
--	    default:
--	      printf("(%2d) Recv'd wrong message tag %4d\n", iam, status.MPI_TAG);
--	      break;
-+											printf("(%2d) Solve X[%2d]\n", iam, k);
- #endif
--	  } /* switch */
- 
--    } /* while not finished ... */
-+											/*
-+											 * Send Xk to process column Pc[k].
-+											 */						  
-+											if(LBtree_ptr[lk]!=NULL){ 
-+												BcTree_forwardMessageSimple(LBtree_ptr[lk],&x[ii - XK_H]);
-+											}		  
- 
- 
--#if ( PRNTlevel>=2 )
--    t = SuperLU_timer_() - t;
--    if ( !iam ) printf(".. L-solve time\t%8.2f\n", t);
--    t = SuperLU_timer_();
-+											/*
-+											 * Perform local block modifications.
-+											 */
-+											lk = LBj( k, grid ); /* Local block number, column-wise. */
-+											lsub = Lrowind_bc_ptr[lk];
-+											lusup = Lnzval_bc_ptr[lk];
-+											if ( lsub ) {
-+												krow = PROW( k, grid );
-+												nb = lsub[0] - 1;
-+												knsupc = SuperSize( k );
-+												ii = X_BLK( LBi( k, grid ) );
-+												xin = &x[ii];		
-+												dlsum_fmod_inv_master(lsum, x, xin, rtemp, nrhs, knsupc, k,
-+														fmod, nb, xsup, grid, Llu,
-+														stat_loc,sizelsum,sizertemp,0);	
-+											} /* if lsub */
-+											// }
-+
-+									}else{
-+
-+										il = LSUM_BLK( lk );		  
-+										knsupc = SuperSize( k );
-+
-+										for (ii=1;ii<num_thread;ii++)
-+											for (jj=0;jj<knsupc*nrhs;jj++)
-+												lsum[il + jj] += lsum[il + jj + ii*sizelsum];
-+
-+										RdTree_forwardMessageSimple(LRtree_ptr[lk],&lsum[il-LSUM_H]); 
-+									}  
-+
-+								}
-+
-+							}					
-+						} /* check Tag */		  
-+					}
-+
-+				} /* while not finished ... */
-+
-+			}
-+		}
-+
-+#if ( PRNTlevel>=1 )
-+		t = SuperLU_timer_() - t;
-+		stat->utime[SOL_L] = t;
-+		if ( !iam ) {
-+			printf(".. L-solve time\t%8.4f\n", t);
-+			fflush(stdout);
-+		}
-+
-+
-+		MPI_Reduce (&t, &tmax, 1, MPI_DOUBLE,
-+				MPI_MAX, 0, grid->comm);
-+		if ( !iam ) {
-+			printf(".. L-solve time (MAX) \t%8.4f\n", tmax);	
-+			fflush(stdout);
-+		}	
-+
-+
-+		t = SuperLU_timer_();
- #endif
- 
-+
- #if ( DEBUGlevel==2 )
--    {
--      printf("(%d) .. After L-solve: y =\n", iam);
--      for (i = 0, k = 0; k < nsupers; ++k) {
--	  krow = PROW( k, grid );
--	  kcol = PCOL( k, grid );
--	  if ( myrow == krow && mycol == kcol ) { /* Diagonal process */
--	      knsupc = SuperSize( k );
--	      lk = LBi( k, grid );
--	      ii = X_BLK( lk );
--	      for (j = 0; j < knsupc; ++j)
--		printf("\t(%d)\t%4d\t%.10f\n", iam, xsup[k]+j, x[ii+j]);
--	      fflush(stdout);
--	  }
--	  MPI_Barrier( grid->comm );
--      }
--    }
-+		{
-+			printf("(%d) .. After L-solve: y =\n", iam);
-+			for (i = 0, k = 0; k < nsupers; ++k) {
-+				krow = PROW( k, grid );
-+				kcol = PCOL( k, grid );
-+				if ( myrow == krow && mycol == kcol ) { /* Diagonal process */
-+					knsupc = SuperSize( k );
-+					lk = LBi( k, grid );
-+					ii = X_BLK( lk );
-+					for (j = 0; j < knsupc; ++j)
-+						printf("\t(%d)\t%4d\t%.10f\n", iam, xsup[k]+j, x[ii+j]);
-+					fflush(stdout);
-+				}
-+				MPI_Barrier( grid->comm );
-+			}
-+		}
- #endif
- 
--    SUPERLU_FREE(fmod);
--    SUPERLU_FREE(frecv);
--    SUPERLU_FREE(rtemp);
-+		SUPERLU_FREE(fmod);
-+		SUPERLU_FREE(frecv);
-+		SUPERLU_FREE(leaf_send);
-+		SUPERLU_FREE(leafsups);
-+		SUPERLU_FREE(recvbuf_BC_fwd);
- 
--    /*for (i = 0; i < Llu->SolveMsgSent; ++i) MPI_Request_free(&send_req[i]);*/
-+		// for (ljb = 0; ljb < nsupers_j; ++ljb) 
-+			// if(Llu->Lrowind_bc_2_lsum[ljb]!=NULL)
-+				// SUPERLU_FREE(Llu->Lrowind_bc_2_lsum[ljb]);
-+		// SUPERLU_FREE(Llu->Lrowind_bc_2_lsum);
-+	
- 
--    for (i = 0; i < Llu->SolveMsgSent; ++i) MPI_Wait(&send_req[i], &status);
--    Llu->SolveMsgSent = 0;
-+		for (lk=0;lk<nsupers_j;++lk){
-+			if(LBtree_ptr[lk]!=NULL){
-+				// if(BcTree_IsRoot(LBtree_ptr[lk])==YES){			
-+				BcTree_waitSendRequest(LBtree_ptr[lk]);		
-+				// }
-+				// deallocate requests here
-+			}
-+		}
- 
--    MPI_Barrier( grid->comm );
-+		for (lk=0;lk<nsupers_i;++lk){
-+			if(LRtree_ptr[lk]!=NULL){		
-+				RdTree_waitSendRequest(LRtree_ptr[lk]);		
-+				// deallocate requests here
-+			}
-+		}		
-+		MPI_Barrier( grid->comm );
- 
- 
--    /*---------------------------------------------------
--     * Back solve Ux = y.
--     *
--     * The Y components from the forward solve is already
--     * on the diagonal processes.
--     *---------------------------------------------------*/
-+#if ( VAMPIR>=1 )	
-+		VT_traceoff();	
-+		VT_finalize(); 
-+#endif
- 
--    /* Save the count to be altered so it can be used by
--       subsequent call to PDGSTRS. */
--    if ( !(bmod = intMalloc_dist(nlb)) )
--	ABORT("Calloc fails for bmod[].");
--    for (i = 0; i < nlb; ++i) bmod[i] = Llu->bmod[i];
--    if ( !(brecv = intMalloc_dist(nlb)) )
--	ABORT("Malloc fails for brecv[].");
--    Llu->brecv = brecv;
- 
--    /*
--     * Compute brecv[] and nbrecvmod counts on the diagonal processes.
--     */
--    {
--	superlu_scope_t *scp = &grid->rscp;
-+		/*---------------------------------------------------
-+		 * Back solve Ux = y.
-+		 *
-+		 * The Y components from the forward solve is already
-+		 * on the diagonal processes.
-+		 *---------------------------------------------------*/
-+		 
-+		 
-+		/* Save the count to be altered so it can be used by
-+		   subsequent call to PDGSTRS. */
-+		if ( !(bmod = intMalloc_dist(nlb)) )
-+			ABORT("Calloc fails for bmod[].");
-+		for (i = 0; i < nlb; ++i) bmod[i] = Llu->bmod[i];
-+		if ( !(brecv = intCalloc_dist(nlb)) )
-+			ABORT("Malloc fails for brecv[].");
-+		Llu->brecv = brecv;
-+
-+		k = SUPERLU_MAX( Llu->nfsendx, Llu->nbsendx ) + nlb;
-+		// if ( !(send_req = (MPI_Request*) SUPERLU_MALLOC(k*sizeof(MPI_Request))) )
-+			// ABORT("Malloc fails for send_req[].");
-+		
-+		/* Re-initialize lsum to zero. Each block header is already in place. */
-+		
-+#ifdef _OPENMP
-+
-+	#pragma omp parallel default(shared) private(thread_id,k,krow,knsupc,lk,il,dest,j,i)
-+	{
-+		thread_id = omp_get_thread_num ();
-+		for (k = 0; k < nsupers; ++k) {
-+			krow = PROW( k, grid );
-+			if ( myrow == krow ) {
-+				knsupc = SuperSize( k );
-+				lk = LBi( k, grid );
-+				il = LSUM_BLK( lk );
-+				dest = &lsum[il];
-+					
-+				RHS_ITERATE(j) {
-+					for (i = 0; i < knsupc; ++i) dest[i + j*knsupc + thread_id*sizelsum] = zero;
-+				}	
-+			}
-+		}	
-+	}	
- 
--#if 1
--	for (k = 0; k < nlb; ++k) mod_bit[k] = 0;
-+#else	
- 	for (k = 0; k < nsupers; ++k) {
--	    krow = PROW( k, grid );
--	    if ( myrow == krow ) {
--		lk = LBi( k, grid );    /* local block number */
--		kcol = PCOL( k, grid ); /* root process in this row scope */
--		if ( mycol != kcol && bmod[lk] )
--		    mod_bit[lk] = 1;  /* Contribution from off-diagonal */
--	    }
-+		krow = PROW( k, grid );
-+		if ( myrow == krow ) {
-+			knsupc = SuperSize( k );
-+			lk = LBi( k, grid );
-+			il = LSUM_BLK( lk );
-+			dest = &lsum[il];
-+			
-+			for (jj = 0; jj < num_thread; ++jj) {						
-+				RHS_ITERATE(j) {
-+					for (i = 0; i < knsupc; ++i) dest[i + j*knsupc + jj*sizelsum] = zero;
-+				}	
-+			}	
-+		}
- 	}
-+#endif		
-+		
- 
--	/* Every process receives the count, but it is only useful on the
--	   diagonal processes.  */
--	MPI_Allreduce( mod_bit, brecv, nlb, mpi_int_t, MPI_SUM, scp->comm );
- 
--	for (k = 0; k < nsupers; ++k) {
--	    krow = PROW( k, grid );
--	    if ( myrow == krow ) {
--		lk = LBi( k, grid );    /* local block number */
--		kcol = PCOL( k, grid ); /* root process in this row scope. */
--		if ( mycol == kcol ) { /* diagonal process */
--		    nbrecvmod += brecv[lk];
--		    if ( !brecv[lk] && !bmod[lk] ) ++nroot;
-+		// /* Set up additional pointers for the index and value arrays of U.
-+		   // nub is the number of local block columns. */
-+		// nub = CEILING( nsupers, Pc ); /* Number of local block columns. */
-+		// if ( !(Urbs = (int_t *) intCalloc_dist(3*nub)) )
-+			// ABORT("Malloc fails for Urbs[]"); /* Record number of nonzero
-+							     // blocks in a block column. */
-+		// Urbs1 = Urbs + nub;
-+		// Urbs2 = Urbs + nub*2;
-+		// if ( !(Ucb_indptr = SUPERLU_MALLOC(nub * sizeof(Ucb_indptr_t *))) )
-+			// ABORT("Malloc fails for Ucb_indptr[]");
-+		// if ( !(Ucb_valptr = SUPERLU_MALLOC(nub * sizeof(int_t *))) )
-+			// ABORT("Malloc fails for Ucb_valptr[]");
-+
-+		// /* Count number of row blocks in a block column. 
-+		   // One pass of the skeleton graph of U. */
-+		// for (lk = 0; lk < nlb; ++lk) {
-+			// usub = Ufstnz_br_ptr[lk];
-+			// if ( usub ) { /* Not an empty block row. */
-+				// /* usub[0] -- number of column blocks in this block row. */
-+// #if ( DEBUGlevel>=2 )
-+				// Ublocks += usub[0];
-+// #endif
-+				// i = BR_HEADER; /* Pointer in index array. */
-+				// for (lb = 0; lb < usub[0]; ++lb) { /* For all column blocks. */
-+					// k = usub[i];            /* Global block number */
-+					// ++Urbs[LBj(k,grid)];
-+					// i += UB_DESCRIPTOR + SuperSize( k );
-+				// }
-+			// }
-+		// }
-+
-+		// /* Set up the vertical linked lists for the row blocks.
-+		   // One pass of the skeleton graph of U. */
-+		// for (lb = 0; lb < nub; ++lb) {
-+			// if ( Urbs[lb] ) { /* Not an empty block column. */
-+				// if ( !(Ucb_indptr[lb]
-+							// = SUPERLU_MALLOC(Urbs[lb] * sizeof(Ucb_indptr_t))) )
-+					// ABORT("Malloc fails for Ucb_indptr[lb][]");
-+				// if ( !(Ucb_valptr[lb] = (int_t *) intMalloc_dist(Urbs[lb])) )
-+					// ABORT("Malloc fails for Ucb_valptr[lb][]");
-+			// }
-+		// }
-+		// for (lk = 0; lk < nlb; ++lk) { /* For each block row. */
-+			// usub = Ufstnz_br_ptr[lk];
-+			// if ( usub ) { /* Not an empty block row. */
-+				// i = BR_HEADER; /* Pointer in index array. */
-+				// j = 0;         /* Pointer in nzval array. */
-+				
-+				// // gik = lk * grid->nprow + myrow;/* Global block number, row-wise. */
-+				// // iklrow = FstBlockC( gik+1 );
-+				
-+				// for (lb = 0; lb < usub[0]; ++lb) { /* For all column blocks. */
-+					// k = usub[i];          /* Global block number, column-wise. */
-+					// ljb = LBj( k, grid ); /* Local block number, column-wise. */
-+					// Ucb_indptr[ljb][Urbs1[ljb]].lbnum = lk;
-+					
-+					
-+					
-+					
-+					// Ucb_indptr[ljb][Urbs1[ljb]].indpos = i;
-+					// Ucb_valptr[ljb][Urbs1[ljb]] = j;
-+					
-+					// // knsupc = SuperSize( k );
-+					// // nbrow = 0;
-+					// // for (jj = 0; jj < knsupc; ++jj) {
-+						// // fnz = usub[i +UB_DESCRIPTOR+ jj];
-+						// // if ( fnz < iklrow ) {
-+						// // if(nbrow<iklrow-fnz)nbrow=iklrow-fnz;
-+						// // }
-+					// // }
-+					
-+					// // Urbs2[ljb] += nbrow;
-+					// // // if(ljb==53253){
-+						// // // printf("Urbs2[ljb] %5d nbrow %5d \n",Urbs2[ljb],nbrow);
-+						// // // fflush(stdout);
-+					// // // }
-+					
-+					// ++Urbs1[ljb];
-+					// j += usub[i+1];
-+					// i += UB_DESCRIPTOR + SuperSize( k );
-+				// }
-+			// }
-+		// }
-+
- #if ( DEBUGlevel>=2 )
--		    printf("(%2d) brecv[%4d]  %2d\n", iam, k, brecv[lk]);
--		    assert( brecv[lk] < Pc );
--#endif
-+		for (p = 0; p < Pr*Pc; ++p) {
-+			if (iam == p) {
-+				printf("(%2d) .. Ublocks %d\n", iam, Ublocks);
-+				for (lb = 0; lb < nub; ++lb) {
-+					printf("(%2d) Local col %2d: # row blocks %2d\n",
-+							iam, lb, Urbs[lb]);
-+					if ( Urbs[lb] ) {
-+						for (i = 0; i < Urbs[lb]; ++i)
-+							printf("(%2d) .. row blk %2d:\
-+									lbnum %d, indpos %d, valpos %d\n",
-+									iam, i, 
-+									Ucb_indptr[lb][i].lbnum,
-+									Ucb_indptr[lb][i].indpos,
-+									Ucb_valptr[lb][i]);
-+					}
-+				}
-+			}
-+			MPI_Barrier( grid->comm );
- 		}
--	    }
--	}
-+		for (p = 0; p < Pr*Pc; ++p) {
-+			if ( iam == p ) {
-+				printf("\n(%d) bsendx_plist[][]", iam);
-+				for (lb = 0; lb < nub; ++lb) {
-+					printf("\n(%d) .. local col %2d: ", iam, lb);
-+					for (i = 0; i < Pr; ++i)
-+						printf("%4d", bsendx_plist[lb][i]);
-+				}
-+				printf("\n");
-+			}
-+			MPI_Barrier( grid->comm );
-+		}
-+#endif /* DEBUGlevel */
- 
--#else /* old */
- 
--	for (k = 0; k < nsupers; ++k) {
--	    krow = PROW( k, grid );
--	    if ( myrow == krow ) {
--		lk = LBi( k, grid );    /* Local block number. */
--		kcol = PCOL( k, grid ); /* Root process in this row scope. */
--		if ( mycol != kcol && bmod[lk] )
--		    i = 1;  /* Contribution from non-diagonal process. */
--		else i = 0;
--		MPI_Reduce( &i, &brecv[lk], 1, mpi_int_t,
--			   MPI_SUM, kcol, scp->comm );
--		if ( mycol == kcol ) { /* Diagonal process. */
--		    nbrecvmod += brecv[lk];
--		    if ( !brecv[lk] && !bmod[lk] ) ++nroot;
--#if ( DEBUGlevel>=2 )
--		    printf("(%2d) brecv[%4d]  %2d\n", iam, k, brecv[lk]);
--		    assert( brecv[lk] < Pc );
--#endif
-+
-+
-+	/* ---------------------------------------------------------
-+	   Initialize the async Bcast trees on all processes.
-+	   --------------------------------------------------------- */		
-+	nsupers_j = CEILING( nsupers, grid->npcol ); /* Number of local block columns */
-+
-+	nbtree = 0;
-+	for (lk=0;lk<nsupers_j;++lk){
-+		if(UBtree_ptr[lk]!=NULL){
-+			// printf("UBtree_ptr lk %5d\n",lk); 
-+			if(BcTree_IsRoot(UBtree_ptr[lk])==NO){			
-+				nbtree++;
-+				if(BcTree_getDestCount(UBtree_ptr[lk])>0)nbrecvx_buf++;				  
-+			}
-+			BcTree_allocateRequest(UBtree_ptr[lk]);
- 		}
--	    }
- 	}
--#endif
--    }
--
--    /* Re-initialize lsum to zero. Each block header is already in place. */
--    for (k = 0; k < nsupers; ++k) {
--	krow = PROW( k, grid );
--	if ( myrow == krow ) {
--	    knsupc = SuperSize( k );
--	    lk = LBi( k, grid );
--	    il = LSUM_BLK( lk );
--	    dest = &lsum[il];
--	    RHS_ITERATE(j) {
--		for (i = 0; i < knsupc; ++i) dest[i + j*knsupc] = zero;
--	    }
--	}
--    }
--
--    /* Set up additional pointers for the index and value arrays of U.
--       nub is the number of local block columns. */
--    nub = CEILING( nsupers, Pc ); /* Number of local block columns. */
--    if ( !(Urbs = (int_t *) intCalloc_dist(2*nub)) )
--	ABORT("Malloc fails for Urbs[]"); /* Record number of nonzero
--					     blocks in a block column. */
--    Urbs1 = Urbs + nub;
--    if ( !(Ucb_indptr = SUPERLU_MALLOC(nub * sizeof(Ucb_indptr_t *))) )
--        ABORT("Malloc fails for Ucb_indptr[]");
--    if ( !(Ucb_valptr = SUPERLU_MALLOC(nub * sizeof(int_t *))) )
--        ABORT("Malloc fails for Ucb_valptr[]");
--
--    /* Count number of row blocks in a block column. 
--       One pass of the skeleton graph of U. */
--    for (lk = 0; lk < nlb; ++lk) {
--	usub = Ufstnz_br_ptr[lk];
--	if ( usub ) { /* Not an empty block row. */
--	    /* usub[0] -- number of column blocks in this block row. */
--#if ( DEBUGlevel>=2 )
--	    Ublocks += usub[0];
--#endif
--	    i = BR_HEADER; /* Pointer in index array. */
--	    for (lb = 0; lb < usub[0]; ++lb) { /* For all column blocks. */
--		k = usub[i];            /* Global block number */
--		++Urbs[LBj(k,grid)];
--		i += UB_DESCRIPTOR + SuperSize( k );
--	    }
--	}
--    }
--
--    /* Set up the vertical linked lists for the row blocks.
--       One pass of the skeleton graph of U. */
--    for (lb = 0; lb < nub; ++lb) {
--	if ( Urbs[lb] ) { /* Not an empty block column. */
--	    if ( !(Ucb_indptr[lb]
--		   = SUPERLU_MALLOC(Urbs[lb] * sizeof(Ucb_indptr_t))) )
--		ABORT("Malloc fails for Ucb_indptr[lb][]");
--	    if ( !(Ucb_valptr[lb] = (int_t *) intMalloc_dist(Urbs[lb])) )
--		ABORT("Malloc fails for Ucb_valptr[lb][]");
--	}
--    }
--    for (lk = 0; lk < nlb; ++lk) { /* For each block row. */
--	usub = Ufstnz_br_ptr[lk];
--	if ( usub ) { /* Not an empty block row. */
--	    i = BR_HEADER; /* Pointer in index array. */
--	    j = 0;         /* Pointer in nzval array. */
--	    for (lb = 0; lb < usub[0]; ++lb) { /* For all column blocks. */
--		k = usub[i];          /* Global block number, column-wise. */
--		ljb = LBj( k, grid ); /* Local block number, column-wise. */
--		Ucb_indptr[ljb][Urbs1[ljb]].lbnum = lk;
--		Ucb_indptr[ljb][Urbs1[ljb]].indpos = i;
--		Ucb_valptr[ljb][Urbs1[ljb]] = j;
--		++Urbs1[ljb];
--		j += usub[i+1];
--		i += UB_DESCRIPTOR + SuperSize( k );
--	    }
--	}
--    }
- 
--#if ( DEBUGlevel>=2 )
--    for (p = 0; p < Pr*Pc; ++p) {
--	if (iam == p) {
--	    printf("(%2d) .. Ublocks %d\n", iam, Ublocks);
--	    for (lb = 0; lb < nub; ++lb) {
--		printf("(%2d) Local col %2d: # row blocks %2d\n",
--		       iam, lb, Urbs[lb]);
--		if ( Urbs[lb] ) {
--		    for (i = 0; i < Urbs[lb]; ++i)
--			printf("(%2d) .. row blk %2d:\
--                               lbnum %d, indpos %d, valpos %d\n",
--			       iam, i, 
--			       Ucb_indptr[lb][i].lbnum,
--			       Ucb_indptr[lb][i].indpos,
--			       Ucb_valptr[lb][i]);
-+	nsupers_i = CEILING( nsupers, grid->nprow ); /* Number of local block rows */
-+	if ( !(	rootsups = (int_t*)intCalloc_dist(nsupers_i)) )
-+		ABORT("Calloc fails for rootsups.");
-+
-+	nrtree = 0;
-+	nroot=0;
-+	for (lk=0;lk<nsupers_i;++lk){
-+		if(URtree_ptr[lk]!=NULL){
-+			// printf("here lk %5d myid %5d\n",lk,iam);
-+			// fflush(stdout);
-+			nrtree++;
-+			RdTree_allocateRequest(URtree_ptr[lk]);			
-+			brecv[lk] = RdTree_GetDestCount(URtree_ptr[lk]);
-+			nbrecvmod += brecv[lk];
-+		}else{
-+			gb = myrow+lk*grid->nprow;  /* not sure */
-+			if(gb<nsupers){
-+				kcol = PCOL( gb, grid );
-+				if(mycol==kcol) { /* Diagonal process */
-+					if (bmod[lk]==0){
-+						rootsups[nroot]=gb;				
-+						++nroot;
-+					}
-+				}
-+			}
- 		}
--	    }
--	}
--	MPI_Barrier( grid->comm );
--    }
--    for (p = 0; p < Pr*Pc; ++p) {
--	if ( iam == p ) {
--	    printf("\n(%d) bsendx_plist[][]", iam);
--	    for (lb = 0; lb < nub; ++lb) {
--		printf("\n(%d) .. local col %2d: ", iam, lb);
--		for (i = 0; i < Pr; ++i)
--		    printf("%4d", bsendx_plist[lb][i]);
--	    }
--	    printf("\n");
--	}
--	MPI_Barrier( grid->comm );
--    }
--#endif /* DEBUGlevel */
-+	}	
- 
- 
--#if ( PRNTlevel>=3 )
--    t = SuperLU_timer_() - t;
--    if ( !iam) printf(".. Setup U-solve time\t%8.2f\n", t);
--    t = SuperLU_timer_();
--#endif
-+	for (i = 0; i < nlb; ++i) bmod[i] += brecv[i];
-+	// for (i = 0; i < nlb; ++i)printf("bmod[i]: %5d\n",bmod[i]);
-+	
-+	
-+	if ( !(recvbuf_BC_fwd = doubleMalloc_dist(maxrecvsz*(nbrecvx+1))) )   // this needs to be optimized for 1D row mapping
-+		ABORT("Malloc fails for recvbuf_BC_fwd[].");	
-+	nbrecvx_buf=0;			
- 
--    /*
--     * Solve the roots first by all the diagonal processes.
--     */
- #if ( DEBUGlevel>=2 )
--    printf("(%2d) nroot %4d\n", iam, nroot);
-+	printf("(%2d) nbrecvx %4d,  nbrecvmod %4d,  nroot %4d\n,  nbtree %4d\n,  nrtree %4d\n",
-+			iam, nbrecvx, nbrecvmod, nroot, nbtree, nrtree);
-+	fflush(stdout);
- #endif
--    for (k = nsupers-1; k >= 0 && nroot; --k) {
--	krow = PROW( k, grid );
--	kcol = PCOL( k, grid );
--	if ( myrow == krow && mycol == kcol ) { /* Diagonal process. */
--	    knsupc = SuperSize( k );
--	    lk = LBi( k, grid ); /* Local block number, row-wise. */
--	    if ( brecv[lk]==0 && bmod[lk]==0 ) {
--		bmod[lk] = -1;       /* Do not solve X[k] in the future. */
--		ii = X_BLK( lk );
--		lk = LBj( k, grid ); /* Local block number, column-wise */
--		lsub = Lrowind_bc_ptr[lk];
--		lusup = Lnzval_bc_ptr[lk];
--		nsupr = lsub[1];
--#ifdef _CRAY
--		STRSM(ftcs1, ftcs3, ftcs2, ftcs2, &knsupc, &nrhs, &alpha,
--		      lusup, &nsupr, &x[ii], &knsupc);
--#elif defined (USE_VENDOR_BLAS)
--		dtrsm_("L", "U", "N", "N", &knsupc, &nrhs, &alpha, 
--		       lusup, &nsupr, &x[ii], &knsupc, 1, 1, 1, 1);
--#else
--		dtrsm_("L", "U", "N", "N", &knsupc, &nrhs, &alpha, 
--		       lusup, &nsupr, &x[ii], &knsupc);
--#endif
--		stat->ops[SOLVE] += knsupc * (knsupc + 1) * nrhs;
--		--nroot;
--#if ( DEBUGlevel>=2 )
--		printf("(%2d) Solve X[%2d]\n", iam, k);
-+
-+
-+#if ( PRNTlevel>=1 )
-+	t = SuperLU_timer_() - t;
-+	if ( !iam) printf(".. Setup U-solve time\t%8.4f\n", t);
-+	fflush(stdout);
-+	MPI_Barrier( grid->comm );	
-+	t = SuperLU_timer_();
- #endif
-+
- 		/*
--		 * Send Xk to process column Pc[k].
-+		 * Solve the roots first by all the diagonal processes.
- 		 */
--		for (p = 0; p < Pr; ++p) {
--		    if ( bsendx_plist[lk][p] != EMPTY ) {
--			pi = PNUM( p, kcol, grid );
-+#if ( DEBUGlevel>=2 )
-+		printf("(%2d) nroot %4d\n", iam, nroot);
-+		fflush(stdout);				
-+#endif
-+		
-+		
- 
--			MPI_Isend( &x[ii - XK_H], knsupc * nrhs + XK_H,
--                                   MPI_DOUBLE, pi, Xk, grid->comm,
--                                   &send_req[Llu->SolveMsgSent++]);
--#if 0
--			MPI_Send( &x[ii - XK_H], knsupc * nrhs + XK_H,
--                                  MPI_DOUBLE, pi, Xk,
--                                  grid->comm );
-+#ifdef _OPENMP
-+#pragma omp parallel default (shared) 
- #endif
--#if ( DEBUGlevel>=2 )
--			printf("(%2d) Sent X[%2.0f] to P %2d\n",
--			       iam, x[ii-XK_H], pi);
-+	{	
-+#ifdef _OPENMP
-+#pragma omp master
- #endif
--		    }
--		}
--		/*
--		 * Perform local block modifications: lsum[i] -= U_i,k * X[k]
--		 */
--		if ( Urbs[lk] ) 
--		    dlsum_bmod(lsum, x, &x[ii], nrhs, k, bmod, Urbs,
--			       Ucb_indptr, Ucb_valptr, xsup, grid, Llu,
--			       send_req, stat);
--	    } /* if root ... */
--	} /* if diagonal process ... */
--    } /* for k ... */
-+		{
-+#ifdef _OPENMP
-+#pragma	omp	taskloop firstprivate (nrhs,beta,alpha,x,rtemp,ldalsum) private (ii,jj,k,knsupc,lk,luptr,lsub,nsupr,lusup,thread_id,t1,t2,Uinv,i,lib,rtemp_loc,nroot_send_tmp) nogroup	
-+#endif		
-+		for (jj=0;jj<nroot;jj++){
-+			k=rootsups[jj];	
-+
-+#if ( PROFlevel>=1 )
-+			TIC(t1);
-+#endif	
-+#ifdef _OPENMP
-+			thread_id = omp_get_thread_num ();
-+#else
-+			thread_id = 0;
-+#endif
-+			rtemp_loc = &rtemp[sizertemp* thread_id];
-+
-+
-+			
-+			knsupc = SuperSize( k );
-+			lk = LBi( k, grid ); /* Local block number, row-wise. */		
- 
-+			// bmod[lk] = -1;       /* Do not solve X[k] in the future. */
-+			ii = X_BLK( lk );
-+			lk = LBj( k, grid ); /* Local block number, column-wise */
-+			lsub = Lrowind_bc_ptr[lk];
-+			lusup = Lnzval_bc_ptr[lk];
-+			nsupr = lsub[1];
- 
--    /*
--     * Compute the internal nodes asynchronously by all processes.
--     */
--    while ( nbrecvx || nbrecvmod ) { /* While not finished. */
- 
--	/* Receive a message. */
--	MPI_Recv( recvbuf, maxrecvsz, MPI_DOUBLE,
--                  MPI_ANY_SOURCE, MPI_ANY_TAG, grid->comm, &status );
--	k = *recvbuf;
-+			if(Llu->inv == 1){
-+
-+				Uinv = Uinv_bc_ptr[lk];
-+#ifdef _CRAY
-+				SGEMM( ftcs2, ftcs2, &knsupc, &nrhs, &knsupc,
-+						&alpha, Uinv, &knsupc, &x[ii],
-+						&knsupc, &beta, rtemp_loc, &knsupc );
-+#elif defined (USE_VENDOR_BLAS)
-+				dgemm_( "N", "N", &knsupc, &nrhs, &knsupc,
-+						&alpha, Uinv, &knsupc, &x[ii],
-+						&knsupc, &beta, rtemp_loc, &knsupc, 1, 1 );
-+#else
-+				dgemm_( "N", "N", &knsupc, &nrhs, &knsupc,
-+						&alpha, Uinv, &knsupc, &x[ii],
-+						&knsupc, &beta, rtemp_loc, &knsupc );
-+#endif			   
-+
-+				for (i=0 ; i<knsupc*nrhs ; i++){
-+					x[ii+i] = rtemp_loc[i];
-+				}		
-+			}else{
-+#ifdef _CRAY
-+				STRSM(ftcs1, ftcs3, ftcs2, ftcs2, &knsupc, &nrhs, &alpha,
-+						lusup, &nsupr, &x[ii], &knsupc);
-+#elif defined (USE_VENDOR_BLAS)
-+				dtrsm_("L", "U", "N", "N", &knsupc, &nrhs, &alpha, 
-+						lusup, &nsupr, &x[ii], &knsupc, 1, 1, 1, 1);	
-+#else
-+				dtrsm_("L", "U", "N", "N", &knsupc, &nrhs, &alpha, 
-+						lusup, &nsupr, &x[ii], &knsupc);
-+#endif
-+			}		
-+
-+			// for (i=0 ; i<knsupc*nrhs ; i++){
-+				// printf("x: %f\n",x[ii+i]);
-+				// fflush(stdout);
-+			// }
-+
-+#if ( PROFlevel>=1 )
-+			TOC(t2, t1);
-+			stat_loc[thread_id]->utime[SOL_TRSM] += t2;
-+#endif	
-+			stat_loc[thread_id]->ops[SOLVE] += knsupc * (knsupc + 1) * nrhs;
- 
- #if ( DEBUGlevel>=2 )
--	printf("(%2d) Recv'd block %d, tag %2d\n", iam, k, status.MPI_TAG);
-+			printf("(%2d) Solve X[%2d]\n", iam, k);
- #endif
- 
--	switch ( status.MPI_TAG ) {
--	    case Xk:
--	        --nbrecvx;
--		lk = LBj( k, grid ); /* Local block number, column-wise. */
-+			/*
-+			 * Send Xk to process column Pc[k].
-+			 */
-+
-+			if(UBtree_ptr[lk]!=NULL){ 
-+#ifdef _OPENMP
-+#pragma omp atomic capture
-+#endif
-+				nroot_send_tmp = ++nroot_send;
-+				root_send[nroot_send_tmp-1] = lk;
-+				
-+				// lib = LBi( k, grid ); /* Local block number, row-wise. */
-+				// ii = X_BLK( lib );				
-+				// BcTree_forwardMessageSimple(UBtree_ptr[lk],&x[ii - XK_H]);
-+			}
-+
-+			/*
-+			 * Perform local block modifications: lsum[i] -= U_i,k * X[k]
-+			 */
-+			if ( Urbs[lk] ) 
-+				dlsum_bmod_inv(lsum, x, &x[ii], rtemp, nrhs, k, bmod, Urbs,Urbs2, 
-+						Ucb_indptr, Ucb_valptr, xsup, grid, Llu,
-+						send_req, stat_loc, root_send, &nroot_send, sizelsum,sizertemp);
-+									
-+		} /* for k ... */
-+	}
-+}
-+
-+
-+for (i=0;i<nroot_send;i++){
-+	lk = root_send[i];
-+	if(lk>=0){ // this is a bcast forwarding
-+		gb = mycol+lk*grid->npcol;  /* not sure */
-+		lib = LBi( gb, grid ); /* Local block number, row-wise. */
-+		ii = X_BLK( lib );			
-+		BcTree_forwardMessageSimple(UBtree_ptr[lk],&x[ii - XK_H]);
-+	}else{ // this is a reduce forwarding
-+		lk = -lk - 1;
-+		il = LSUM_BLK( lk );
-+		RdTree_forwardMessageSimple(URtree_ptr[lk],&lsum[il - LSUM_H ]);
-+	}
-+}
-+
-+
- 		/*
--		 * Perform local block modifications:
--		 *         lsum[i] -= U_i,k * X[k]
-+		 * Compute the internal nodes asychronously by all processes.
- 		 */
--		dlsum_bmod(lsum, x, &recvbuf[XK_H], nrhs, k, bmod, Urbs,
--			   Ucb_indptr, Ucb_valptr, xsup, grid, Llu, 
--			   send_req, stat);
- 
--	        break;
-+#ifdef _OPENMP
-+#pragma omp parallel default (shared) 
-+#endif
-+	{	
-+#ifdef _OPENMP
-+#pragma omp master 
-+#endif		 
-+		for ( nbrecv =0; nbrecv<nbrecvx+nbrecvmod;nbrecv++) { /* While not finished. */
-+
-+			// printf("iam %4d nbrecv %4d nbrecvx %4d nbrecvmod %4d\n", iam, nbrecv, nbrecvxnbrecvmod);
-+			// fflush(stdout);			
-+			
-+			
-+			
-+			thread_id = 0;
-+#if ( PROFlevel>=1 )
-+			TIC(t1);
-+#endif	
-+
-+			recvbuf0 = &recvbuf_BC_fwd[nbrecvx_buf*maxrecvsz];
-+
-+			/* Receive a message. */
-+			MPI_Recv( recvbuf0, maxrecvsz, MPI_DOUBLE,
-+					MPI_ANY_SOURCE, MPI_ANY_TAG, grid->comm, &status );	 	
-+
-+#if ( PROFlevel>=1 )		 
-+			TOC(t2, t1);
-+			stat_loc[thread_id]->utime[SOL_COMM] += t2;
-+
-+			msg_cnt += 1;
-+			msg_vol += maxrecvsz * dword;			
-+#endif	
-+		 
-+			k = *recvbuf0;
-+#if ( DEBUGlevel>=2 )
-+			printf("(%2d) Recv'd block %d, tag %2d\n", iam, k, status.MPI_TAG);
-+			fflush(stdout);
-+#endif
-+
-+			if(status.MPI_TAG==BC_U){
-+				// --nfrecvx;
-+				nbrecvx_buf++;
-+				
-+				lk = LBj( k, grid );    /* local block number */
-+
-+				if(BcTree_getDestCount(UBtree_ptr[lk])>0){
-+
-+					BcTree_forwardMessageSimple(UBtree_ptr[lk],recvbuf0);	
-+					// nfrecvx_buf++;
-+				}
-+
-+				/*
-+				 * Perform local block modifications: lsum[i] -= L_i,k * X[k]
-+				 */	  
-+
-+				lk = LBj( k, grid ); /* Local block number, column-wise. */
-+				dlsum_bmod_inv_master(lsum, x, &recvbuf0[XK_H], rtemp, nrhs, k, bmod, Urbs,Urbs2,
-+						Ucb_indptr, Ucb_valptr, xsup, grid, Llu, 
-+						send_req, stat_loc, sizelsum,sizertemp);
-+			}else if(status.MPI_TAG==RD_U){
-+
-+				lk = LBi( k, grid ); /* Local block number, row-wise. */
-+				
-+				knsupc = SuperSize( k );
-+				tempv = &recvbuf0[LSUM_H];
-+				il = LSUM_BLK( lk );		  
-+				RHS_ITERATE(j) {
-+					for (i = 0; i < knsupc; ++i)
-+						lsum[i + il + j*knsupc + thread_id*sizelsum] += tempv[i + j*knsupc];
-+				}					
-+			// #ifdef _OPENMP
-+			// #pragma omp atomic capture
-+			// #endif
-+				bmod_tmp=--bmod[lk];
-+				thread_id = 0;									
-+				rtemp_loc = &rtemp[sizertemp* thread_id];
-+				if ( bmod_tmp==0 ) {
-+					if(RdTree_IsRoot(URtree_ptr[lk])==YES){							
-+						
-+						knsupc = SuperSize( k );
-+						for (ii=1;ii<num_thread;ii++)
-+							for (jj=0;jj<knsupc*nrhs;jj++)
-+								lsum[il+ jj ] += lsum[il + jj + ii*sizelsum];						
-+						
-+						ii = X_BLK( lk );
-+						RHS_ITERATE(j)
-+							for (i = 0; i < knsupc; ++i)	
-+								x[i + ii + j*knsupc] += lsum[i + il + j*knsupc ];						
-+					
-+						lk = LBj( k, grid ); /* Local block number, column-wise. */
-+						lsub = Lrowind_bc_ptr[lk];
-+						lusup = Lnzval_bc_ptr[lk];
-+						nsupr = lsub[1];
-+
-+						if(Llu->inv == 1){
-+
-+							Uinv = Uinv_bc_ptr[lk];
- 
--	    case LSUM: /* Receiver must be a diagonal process */
--		--nbrecvmod;
--		lk = LBi( k, grid ); /* Local block number, row-wise. */
--		ii = X_BLK( lk );
--		knsupc = SuperSize( k );
--		tempv = &recvbuf[LSUM_H];
--		RHS_ITERATE(j) {
--		    for (i = 0; i < knsupc; ++i)
--			x[i + ii + j*knsupc] += tempv[i + j*knsupc];
--		}
-+#ifdef _CRAY
-+							SGEMM( ftcs2, ftcs2, &knsupc, &nrhs, &knsupc,
-+									&alpha, Uinv, &knsupc, &x[ii],
-+									&knsupc, &beta, rtemp_loc, &knsupc );
-+#elif defined (USE_VENDOR_BLAS)
-+							dgemm_( "N", "N", &knsupc, &nrhs, &knsupc,
-+									&alpha, Uinv, &knsupc, &x[ii],
-+									&knsupc, &beta, rtemp_loc, &knsupc, 1, 1 );
-+#else
-+							dgemm_( "N", "N", &knsupc, &nrhs, &knsupc,
-+									&alpha, Uinv, &knsupc, &x[ii],
-+									&knsupc, &beta, rtemp_loc, &knsupc );
-+#endif		
- 
--		if ( (--brecv[lk])==0 && bmod[lk]==0 ) {
--		    bmod[lk] = -1; /* Do not solve X[k] in the future. */
--		    lk = LBj( k, grid ); /* Local block number, column-wise. */
--		    lsub = Lrowind_bc_ptr[lk];
--		    lusup = Lnzval_bc_ptr[lk];
--		    nsupr = lsub[1];
-+							for (i=0 ; i<knsupc*nrhs ; i++){
-+								x[ii+i] = rtemp_loc[i];
-+							}		
-+						}else{
- #ifdef _CRAY
--		    STRSM(ftcs1, ftcs3, ftcs2, ftcs2, &knsupc, &nrhs, &alpha,
--			  lusup, &nsupr, &x[ii], &knsupc);
-+							STRSM(ftcs1, ftcs3, ftcs2, ftcs2, &knsupc, &nrhs, &alpha,
-+									lusup, &nsupr, &x[ii], &knsupc);
- #elif defined (USE_VENDOR_BLAS)
--		    dtrsm_("L", "U", "N", "N", &knsupc, &nrhs, &alpha, 
--			   lusup, &nsupr, &x[ii], &knsupc, 1, 1, 1, 1);
-+							dtrsm_("L", "U", "N", "N", &knsupc, &nrhs, &alpha, 
-+									lusup, &nsupr, &x[ii], &knsupc, 1, 1, 1, 1);		
- #else
--		    dtrsm_("L", "U", "N", "N", &knsupc, &nrhs, &alpha, 
--			   lusup, &nsupr, &x[ii], &knsupc);
-+							dtrsm_("L", "U", "N", "N", &knsupc, &nrhs, &alpha, 
-+									lusup, &nsupr, &x[ii], &knsupc);
- #endif
--		    stat->ops[SOLVE] += knsupc * (knsupc + 1) * nrhs;
-+						}
-+
-+#if ( PROFlevel>=1 )
-+							TOC(t2, t1);
-+							stat_loc[thread_id]->utime[SOL_TRSM] += t2;
-+#endif	
-+							stat_loc[thread_id]->ops[SOLVE] += knsupc * (knsupc + 1) * nrhs;
- #if ( DEBUGlevel>=2 )
--		    printf("(%2d) Solve X[%2d]\n", iam, k);
-+						printf("(%2d) Solve X[%2d]\n", iam, k);
- #endif
--		    /*
--		     * Send Xk to process column Pc[k].
--		     */
--		    kcol = PCOL( k, grid );
--		    for (p = 0; p < Pr; ++p) {
--			if ( bsendx_plist[lk][p] != EMPTY ) {
--			    pi = PNUM( p, kcol, grid );
--
--			    MPI_Isend( &x[ii - XK_H], knsupc * nrhs + XK_H,
--                                       MPI_DOUBLE, pi, Xk, grid->comm,
--                                       &send_req[Llu->SolveMsgSent++] );
--#if 0
--			    MPI_Send( &x[ii - XK_H], knsupc * nrhs + XK_H,
--                                      MPI_DOUBLE, pi, Xk,
--                                      grid->comm );
-+
-+						/*
-+						 * Send Xk to process column Pc[k].
-+						 */						
-+						if(UBtree_ptr[lk]!=NULL){ 
-+							BcTree_forwardMessageSimple(UBtree_ptr[lk],&x[ii - XK_H]);
-+						}							
-+						
-+
-+						/*
-+						 * Perform local block modifications: 
-+						 *         lsum[i] -= U_i,k * X[k]
-+						 */
-+						if ( Urbs[lk] )
-+							dlsum_bmod_inv_master(lsum, x, &x[ii], rtemp, nrhs, k, bmod, Urbs,Urbs2,
-+									Ucb_indptr, Ucb_valptr, xsup, grid, Llu,
-+									send_req, stat_loc, sizelsum,sizertemp);
-+
-+					}else{
-+						il = LSUM_BLK( lk );		  
-+						knsupc = SuperSize( k );
-+
-+						for (ii=1;ii<num_thread;ii++)
-+							for (jj=0;jj<knsupc*nrhs;jj++)
-+								lsum[il + jj] += lsum[il + jj + ii*sizelsum];
-+												
-+						RdTree_forwardMessageSimple(URtree_ptr[lk],&lsum[il-LSUM_H]); 
-+					}						
-+				
-+				}
-+			}
-+		} /* while not finished ... */
-+	}
-+#if ( PRNTlevel>=1 )
-+		t = SuperLU_timer_() - t;
-+		if ( !iam ) printf(".. U-solve time\t%8.4f\n", t);
-+		MPI_Reduce (&t, &tmax, 1, MPI_DOUBLE,
-+				MPI_MAX, 0, grid->comm);
-+		if ( !iam ) {
-+			printf(".. U-solve time (MAX) \t%8.4f\n", tmax);	
-+			fflush(stdout);
-+		}			
-+		t = SuperLU_timer_();			
- #endif
-+
-+
-+
-+
- #if ( DEBUGlevel>=2 )
--			    printf("(%2d) Sent X[%2.0f] to P %2d\n",
--				   iam, x[ii - XK_H], pi);
-+		{
-+			double *x_col;
-+			int diag;
-+			printf("\n(%d) .. After U-solve: x (ON DIAG PROCS) = \n", iam);
-+			ii = 0;
-+			for (k = 0; k < nsupers; ++k) {
-+				knsupc = SuperSize( k );
-+				krow = PROW( k, grid );
-+				kcol = PCOL( k, grid );
-+				diag = PNUM( krow, kcol, grid);
-+				if ( iam == diag ) { /* Diagonal process. */
-+					lk = LBi( k, grid );
-+					jj = X_BLK( lk );
-+					x_col = &x[jj];
-+					RHS_ITERATE(j) {
-+						for (i = 0; i < knsupc; ++i) { /* X stored in blocks */
-+							printf("\t(%d)\t%4d\t%.10f\n",
-+									iam, xsup[k]+i, x_col[i]);
-+						}
-+						x_col += knsupc;
-+					}
-+				}
-+				ii += knsupc;
-+			} /* for k ... */
-+		}
- #endif
--			}
--		    }
--		    /*
--		     * Perform local block modifications: 
--		     *         lsum[i] -= U_i,k * X[k]
--		     */
--		    if ( Urbs[lk] )
--			dlsum_bmod(lsum, x, &x[ii], nrhs, k, bmod, Urbs,
--				   Ucb_indptr, Ucb_valptr, xsup, grid, Llu,
--				   send_req, stat);
--		} /* if becomes solvable */
--		
--		break;
- 
--#if ( DEBUGlevel>=2 )
--	      default:
--		printf("(%2d) Recv'd wrong message tag %4d\n", iam, status.MPI_TAG);
--		break;
--#endif		
-+		pdReDistribute_X_to_B(n, B, m_loc, ldb, fst_row, nrhs, x, ilsum,
-+				ScalePermstruct, Glu_persist, grid, SOLVEstruct);
- 
--	} /* switch */
- 
--    } /* while not finished ... */
-+#if ( PRNTlevel>=1 )
-+		t = SuperLU_timer_() - t;
-+		if ( !iam) printf(".. X to B redistribute time\t%8.4f\n", t);
-+		t = SuperLU_timer_();
-+#endif	
- 
--#if ( PRNTlevel>=3 )
--    t = SuperLU_timer_() - t;
--    if ( !iam ) printf(".. U-solve time\t%8.2f\n", t);
--#endif
- 
--#if ( DEBUGlevel>=2 )
--    {
--	double *x_col;
--	int diag;
--	printf("\n(%d) .. After U-solve: x (ON DIAG PROCS) = \n", iam);
--	ii = 0;
--	for (k = 0; k < nsupers; ++k) {
--	    knsupc = SuperSize( k );
--	    krow = PROW( k, grid );
--	    kcol = PCOL( k, grid );
--	    diag = PNUM( krow, kcol, grid);
--	    if ( iam == diag ) { /* Diagonal process. */
--		lk = LBi( k, grid );
--		jj = X_BLK( lk );
--		x_col = &x[jj];
--		RHS_ITERATE(j) {
--		    for (i = 0; i < knsupc; ++i) { /* X stored in blocks */
--			printf("\t(%d)\t%4d\t%.10f\n",
--			       iam, xsup[k]+i, x_col[i]);
--		    }
--		    x_col += knsupc;
-+		double tmp1=0; 
-+		double tmp2=0;
-+		double tmp3=0;
-+		double tmp4=0;
-+		for(i=0;i<num_thread;i++){
-+			tmp1 = MAX(tmp1,stat_loc[i]->utime[SOL_TRSM]);
-+			tmp2 = MAX(tmp2,stat_loc[i]->utime[SOL_GEMM]);
-+			tmp3 = MAX(tmp3,stat_loc[i]->utime[SOL_COMM]);
-+			tmp4 += stat_loc[i]->ops[SOLVE];
-+#if ( PRNTlevel>=2 )
-+			if(iam==0)printf("thread %5d gemm %9.5f\n",i,stat_loc[i]->utime[SOL_GEMM]);
-+#endif	
- 		}
--	    }
--	    ii += knsupc;
--	} /* for k ... */
--    }
--#endif
- 
--    pdReDistribute_X_to_B(n, B, m_loc, ldb, fst_row, nrhs, x, ilsum,
--			  ScalePermstruct, Glu_persist, grid, SOLVEstruct);
- 
-+		stat->utime[SOL_TRSM] += tmp1;
-+		stat->utime[SOL_GEMM] += tmp2;
-+		stat->utime[SOL_COMM] += tmp3;
-+		stat->ops[SOLVE]+= tmp4;	  
- 
--    /* Deallocate storage. */
--    SUPERLU_FREE(lsum);
--    SUPERLU_FREE(x);
--    SUPERLU_FREE(recvbuf);
--    for (i = 0; i < nub; ++i) {
--	if ( Urbs[i] ) {
--	    SUPERLU_FREE(Ucb_indptr[i]);
--	    SUPERLU_FREE(Ucb_valptr[i]);
--	}
--    }
--    SUPERLU_FREE(Ucb_indptr);
--    SUPERLU_FREE(Ucb_valptr);
--    SUPERLU_FREE(Urbs);
--    SUPERLU_FREE(bmod);
--    SUPERLU_FREE(brecv);
- 
--    /*for (i = 0; i < Llu->SolveMsgSent; ++i) MPI_Request_free(&send_req[i]);*/
-+		/* Deallocate storage. */
-+		SUPERLU_FREE(stat_loc);
-+		SUPERLU_FREE(rtemp);
-+		SUPERLU_FREE(lsum);
-+		SUPERLU_FREE(x);
-+		// SUPERLU_FREE(recvbuf);
-+		
-+		
-+		// for (i = 0; i < nub; ++i) {
-+			// if ( Urbs[i] ) {
-+				// SUPERLU_FREE(Ucb_indptr[i]);
-+				// SUPERLU_FREE(Ucb_valptr[i]);
-+			// }
-+		// }
-+		// SUPERLU_FREE(Ucb_indptr);
-+		// SUPERLU_FREE(Ucb_valptr);
-+		// SUPERLU_FREE(Urbs);
-+		
-+		
-+		SUPERLU_FREE(bmod);
-+		SUPERLU_FREE(brecv);
-+		SUPERLU_FREE(root_send);
-+		
-+		SUPERLU_FREE(rootsups);
-+		SUPERLU_FREE(recvbuf_BC_fwd);		
-+		
-+		for (lk=0;lk<nsupers_j;++lk){
-+			if(UBtree_ptr[lk]!=NULL){
-+				// if(BcTree_IsRoot(LBtree_ptr[lk])==YES){			
-+				BcTree_waitSendRequest(UBtree_ptr[lk]);		
-+				// }
-+				// deallocate requests here
-+			}
-+		}
- 
--    for (i = 0; i < Llu->SolveMsgSent; ++i) MPI_Wait(&send_req[i], &status);
--    SUPERLU_FREE(send_req);
-+		for (lk=0;lk<nsupers_i;++lk){
-+			if(URtree_ptr[lk]!=NULL){		
-+				RdTree_waitSendRequest(URtree_ptr[lk]);		
-+				// deallocate requests here
-+			}
-+		}		
-+		MPI_Barrier( grid->comm );
- 
--    MPI_Barrier( grid->comm );
-+		
-+		
-+		
-+		
-+		/*for (i = 0; i < Llu->SolveMsgSent; ++i) MPI_Request_free(&send_req[i]);*/
-+
-+		// for (i = 0; i < Llu->SolveMsgSent; ++i) MPI_Wait(&send_req[i], &status);
-+		// SUPERLU_FREE(send_req);
-+
-+		// MPI_Barrier( grid->comm );
-+
-+
-+#if ( PROFlevel>=2 )
-+		{
-+			float msg_vol_max, msg_vol_sum, msg_cnt_max, msg_cnt_sum;
-+
-+			MPI_Reduce (&msg_cnt, &msg_cnt_sum,
-+					1, MPI_FLOAT, MPI_SUM, 0, grid->comm);
-+			MPI_Reduce (&msg_cnt, &msg_cnt_max,
-+					1, MPI_FLOAT, MPI_MAX, 0, grid->comm);
-+			MPI_Reduce (&msg_vol, &msg_vol_sum,
-+					1, MPI_FLOAT, MPI_SUM, 0, grid->comm);
-+			MPI_Reduce (&msg_vol, &msg_vol_max,
-+					1, MPI_FLOAT, MPI_MAX, 0, grid->comm);
-+			if (!iam) {
-+				printf ("\tPDGSTRS comm stat:"
-+						"\tAvg\tMax\t\tAvg\tMax\n"
-+						"\t\t\tCount:\t%.0f\t%.0f\tVol(MB)\t%.2f\t%.2f\n",
-+						msg_cnt_sum / Pr / Pc, msg_cnt_max,
-+						msg_vol_sum / Pr / Pc * 1e-6, msg_vol_max * 1e-6);
-+			}
-+		}
-+#endif	
- 
--    stat->utime[SOLVE] = SuperLU_timer_() - t;
-+		TOC(t2_sol,t1_sol);
-+		stat->utime[SOLVE] = t2_sol;
- 
- #if ( DEBUGlevel>=1 )
--    CHECK_MALLOC(iam, "Exit pdgstrs()");
-+		CHECK_MALLOC(iam, "Exit pdgstrs()");
- #endif
- 
--    return;
--} /* PDGSTRS */
-+		return;
-+	} /* PDGSTRS */
- 
-diff --git a/SRC/pdgstrs_lsum.c b/SRC/pdgstrs_lsum.c
-index 8d3da84..b9abffe 100644
---- a/SRC/pdgstrs_lsum.c
-+++ b/SRC/pdgstrs_lsum.c
-@@ -1,13 +1,13 @@
- /*! \file
--Copyright (c) 2003, The Regents of the University of California, through
--Lawrence Berkeley National Laboratory (subject to receipt of any required 
--approvals from U.S. Dept. of Energy) 
-+  Copyright (c) 2003, The Regents of the University of California, through
-+  Lawrence Berkeley National Laboratory (subject to receipt of any required 
-+  approvals from U.S. Dept. of Energy) 
- 
--All rights reserved. 
-+  All rights reserved. 
- 
--The source code is distributed under BSD license, see the file License.txt
--at the top-level directory.
--*/
-+  The source code is distributed under BSD license, see the file License.txt
-+  at the top-level directory.
-+ */
- 
- 
- /*! @file 
-@@ -25,6 +25,7 @@ at the top-level directory.
-  */
- 
- #include "superlu_ddefs.h"
-+#include "superlu_defs.h"
- 
- #define ISEND_IRECV
- 
-@@ -33,14 +34,20 @@ at the top-level directory.
-  */
- #ifdef _CRAY
- fortran void STRSM(_fcd, _fcd, _fcd, _fcd, int*, int*, double*,
--		   double*, int*, double*, int*);
-+		double*, int*, double*, int*);
- fortran void SGEMM(_fcd, _fcd, int*, int*, int*, double*, double*, 
--		   int*, double*, int*, double*, double*, int*);
-+		int*, double*, int*, double*, double*, int*);
- _fcd ftcs1;
- _fcd ftcs2;
- _fcd ftcs3;
- #endif
- 
-+
-+// #ifndef CACHELINE
-+// #define CACHELINE 64  /* bytes, Xeon Phi KNL, Cori haswell, Edision */
-+// #endif
-+
-+
- /************************************************************************/
- /*! \brief
-  *
-@@ -69,146 +76,181 @@ void dlsum_fmod
-  LocalLU_t *Llu,
-  MPI_Request send_req[], /* input/output */
-  SuperLUStat_t *stat
--)
-+ )
- {
--    double alpha = 1.0, beta = 0.0;
--    double *lusup, *lusup1;
--    double *dest;
--    int    iam, iknsupc, myrow, nbrow, nsupr, nsupr1, p, pi;
--    int_t  i, ii, ik, il, ikcol, irow, j, lb, lk, rel;
--    int_t  *lsub, *lsub1, nlb1, lptr1, luptr1;
--    int_t  *ilsum = Llu->ilsum; /* Starting position of each supernode in lsum.   */
--    int_t  *frecv = Llu->frecv;
--    int_t  **fsendx_plist = Llu->fsendx_plist;
--    MPI_Status status;
--    int test_flag;
--
--    iam = grid->iam;
--    myrow = MYROW( iam, grid );
--    lk = LBj( k, grid ); /* Local block number, column-wise. */
--    lsub = Llu->Lrowind_bc_ptr[lk];
--    lusup = Llu->Lnzval_bc_ptr[lk];
--    nsupr = lsub[1];
--
--    for (lb = 0; lb < nlb; ++lb) {
--	ik = lsub[lptr]; /* Global block number, row-wise. */
--	nbrow = lsub[lptr+1];
-+	double alpha = 1.0, beta = 0.0;
-+	double *lusup, *lusup1;
-+	double *dest;
-+	int    iam, iknsupc, myrow, nbrow, nsupr, nsupr1, p, pi;
-+	int_t  i, ii, ik, il, ikcol, irow, j, lb, lk, lib, rel;
-+	int_t  *lsub, *lsub1, nlb1, lptr1, luptr1;
-+	int_t  *ilsum = Llu->ilsum; /* Starting position of each supernode in lsum.   */
-+	int_t  *frecv = Llu->frecv;
-+	int_t  **fsendx_plist = Llu->fsendx_plist;
-+	MPI_Status status;
-+	int test_flag;
-+
-+#if ( PROFlevel>=1 )
-+	double t1, t2;
-+	float msg_vol = 0, msg_cnt = 0;
-+#endif 
-+
-+
-+#if ( PROFlevel>=1 )
-+	TIC(t1);
-+#endif	
-+
-+	iam = grid->iam;
-+	myrow = MYROW( iam, grid );
-+	lk = LBj( k, grid ); /* Local block number, column-wise. */
-+	lsub = Llu->Lrowind_bc_ptr[lk];
-+	lusup = Llu->Lnzval_bc_ptr[lk];
-+	nsupr = lsub[1];
-+
-+	for (lb = 0; lb < nlb; ++lb) {
-+		ik = lsub[lptr]; /* Global block number, row-wise. */
-+		nbrow = lsub[lptr+1];
- #ifdef _CRAY
--	SGEMM( ftcs2, ftcs2, &nbrow, &nrhs, &knsupc,
--	      &alpha, &lusup[luptr], &nsupr, xk,
--	      &knsupc, &beta, rtemp, &nbrow );
-+		SGEMM( ftcs2, ftcs2, &nbrow, &nrhs, &knsupc,
-+				&alpha, &lusup[luptr], &nsupr, xk,
-+				&knsupc, &beta, rtemp, &nbrow );
- #elif defined (USE_VENDOR_BLAS)
--	dgemm_( "N", "N", &nbrow, &nrhs, &knsupc,
--	       &alpha, &lusup[luptr], &nsupr, xk,
--	       &knsupc, &beta, rtemp, &nbrow, 1, 1 );
--#else
--	dgemm_( "N", "N", &nbrow, &nrhs, &knsupc,
--	       &alpha, &lusup[luptr], &nsupr, xk,
--	       &knsupc, &beta, rtemp, &nbrow );
--#endif
--	stat->ops[SOLVE] += 2 * nbrow * nrhs * knsupc + nbrow * nrhs;
--   
--	lk = LBi( ik, grid ); /* Local block number, row-wise. */
--	iknsupc = SuperSize( ik );
--	il = LSUM_BLK( lk );
--	dest = &lsum[il];
--	lptr += LB_DESCRIPTOR;
--	rel = xsup[ik]; /* Global row index of block ik. */
--	for (i = 0; i < nbrow; ++i) {
--	    irow = lsub[lptr++] - rel; /* Relative row. */
--	    RHS_ITERATE(j)
--		dest[irow + j*iknsupc] -= rtemp[i + j*nbrow];
--	}
--	luptr += nbrow;
--		    
--	if ( (--fmod[lk])==0 ) { /* Local accumulation done. */
--	    ikcol = PCOL( ik, grid );
--	    p = PNUM( myrow, ikcol, grid );
--	    if ( iam != p ) {
-+		dgemm_( "N", "N", &nbrow, &nrhs, &knsupc,
-+				&alpha, &lusup[luptr], &nsupr, xk,
-+				&knsupc, &beta, rtemp, &nbrow, 1, 1 );
-+#else
-+		dgemm_( "N", "N", &nbrow, &nrhs, &knsupc,
-+				&alpha, &lusup[luptr], &nsupr, xk,
-+				&knsupc, &beta, rtemp, &nbrow );
-+#endif
-+		stat->ops[SOLVE] += 2 * nbrow * nrhs * knsupc + nbrow * nrhs;
-+
-+		lk = LBi( ik, grid ); /* Local block number, row-wise. */
-+		iknsupc = SuperSize( ik );
-+		il = LSUM_BLK( lk );
-+		dest = &lsum[il];
-+		lptr += LB_DESCRIPTOR;
-+		rel = xsup[ik]; /* Global row index of block ik. */
-+		for (i = 0; i < nbrow; ++i) {
-+			irow = lsub[lptr++] - rel; /* Relative row. */
-+			RHS_ITERATE(j)
-+				dest[irow + j*iknsupc] -= rtemp[i + j*nbrow];
-+		}
-+		luptr += nbrow;
-+
-+
-+
-+#if ( PROFlevel>=1 )
-+		TOC(t2, t1);
-+		stat->utime[SOL_GEMM] += t2;
-+
-+#endif	
-+
-+
-+
-+
-+		if ( (--fmod[lk])==0 ) { /* Local accumulation done. */
-+			ikcol = PCOL( ik, grid );
-+			p = PNUM( myrow, ikcol, grid );
-+			if ( iam != p ) {
- #ifdef ISEND_IRECV
--		MPI_Isend( &lsum[il - LSUM_H], iknsupc * nrhs + LSUM_H,
--			   MPI_DOUBLE, p, LSUM, grid->comm,
--                           &send_req[Llu->SolveMsgSent++] );
-+				MPI_Isend( &lsum[il - LSUM_H], iknsupc * nrhs + LSUM_H,
-+						MPI_DOUBLE, p, LSUM, grid->comm,
-+						&send_req[Llu->SolveMsgSent++] );
- #else
- #ifdef BSEND
--		MPI_Bsend( &lsum[il - LSUM_H], iknsupc * nrhs + LSUM_H,
--			   MPI_DOUBLE, p, LSUM, grid->comm );
-+				MPI_Bsend( &lsum[il - LSUM_H], iknsupc * nrhs + LSUM_H,
-+						MPI_DOUBLE, p, LSUM, grid->comm );
- #else
--		MPI_Send( &lsum[il - LSUM_H], iknsupc * nrhs + LSUM_H,
--			 MPI_DOUBLE, p, LSUM, grid->comm );
-+				MPI_Send( &lsum[il - LSUM_H], iknsupc * nrhs + LSUM_H,
-+						MPI_DOUBLE, p, LSUM, grid->comm );
- #endif
- #endif
- #if ( DEBUGlevel>=2 )
--		printf("(%2d) Sent LSUM[%2.0f], size %2d, to P %2d\n",
--		       iam, lsum[il-LSUM_H], iknsupc*nrhs+LSUM_H, p);
--#endif
--	    } else { /* Diagonal process: X[i] += lsum[i]. */
--		ii = X_BLK( lk );
--		RHS_ITERATE(j)
--		    for (i = 0; i < iknsupc; ++i)
--			x[i + ii + j*iknsupc] += lsum[i + il + j*iknsupc];
--		if ( frecv[lk]==0 ) { /* Becomes a leaf node. */
--		    fmod[lk] = -1; /* Do not solve X[k] in the future. */
--		    lk = LBj( ik, grid );/* Local block number, column-wise. */
--		    lsub1 = Llu->Lrowind_bc_ptr[lk];
--		    lusup1 = Llu->Lnzval_bc_ptr[lk];
--		    nsupr1 = lsub1[1];
-+				printf("(%2d) Sent LSUM[%2.0f], size %2d, to P %2d\n",
-+						iam, lsum[il-LSUM_H], iknsupc*nrhs+LSUM_H, p);
-+#endif
-+			} else { /* Diagonal process: X[i] += lsum[i]. */
-+				ii = X_BLK( lk );
-+				RHS_ITERATE(j)
-+					for (i = 0; i < iknsupc; ++i)
-+						x[i + ii + j*iknsupc] += lsum[i + il + j*iknsupc];
-+				if ( frecv[lk]==0 ) { /* Becomes a leaf node. */
-+					fmod[lk] = -1; /* Do not solve X[k] in the future. */
-+					lk = LBj( ik, grid );/* Local block number, column-wise. */
-+					lsub1 = Llu->Lrowind_bc_ptr[lk];
-+					lusup1 = Llu->Lnzval_bc_ptr[lk];
-+					nsupr1 = lsub1[1];
-+
-+
-+#if ( PROFlevel>=1 )
-+					TIC(t1);
-+#endif				
-+
- #ifdef _CRAY
--		    STRSM(ftcs1, ftcs1, ftcs2, ftcs3, &iknsupc, &nrhs, &alpha,
--			  lusup1, &nsupr1, &x[ii], &iknsupc);
-+					STRSM(ftcs1, ftcs1, ftcs2, ftcs3, &iknsupc, &nrhs, &alpha,
-+							lusup1, &nsupr1, &x[ii], &iknsupc);
- #elif defined (USE_VENDOR_BLAS)
--		    dtrsm_("L", "L", "N", "U", &iknsupc, &nrhs, &alpha, 
--			   lusup1, &nsupr1, &x[ii], &iknsupc, 1, 1, 1, 1);
-+					dtrsm_("L", "L", "N", "U", &iknsupc, &nrhs, &alpha, 
-+							lusup1, &nsupr1, &x[ii], &iknsupc, 1, 1, 1, 1);		   
- #else
--		    dtrsm_("L", "L", "N", "U", &iknsupc, &nrhs, &alpha, 
--			   lusup1, &nsupr1, &x[ii], &iknsupc);
-+					dtrsm_("L", "L", "N", "U", &iknsupc, &nrhs, &alpha, 
-+							lusup1, &nsupr1, &x[ii], &iknsupc);
- #endif
--		    stat->ops[SOLVE] += iknsupc * (iknsupc - 1) * nrhs;
-+
-+
-+#if ( PROFlevel>=1 )
-+					TOC(t2, t1);
-+					stat->utime[SOL_TRSM] += t2;
-+
-+#endif	
-+
-+
-+					stat->ops[SOLVE] += iknsupc * (iknsupc - 1) * nrhs;
- #if ( DEBUGlevel>=2 )
--		    printf("(%2d) Solve X[%2d]\n", iam, ik);
-+					printf("(%2d) Solve X[%2d]\n", iam, ik);
- #endif
--		
--		    /*
--		     * Send Xk to process column Pc[k].
--		     */
--		    for (p = 0; p < grid->nprow; ++p) {
--			if ( fsendx_plist[lk][p] != EMPTY ) {
--			    pi = PNUM( p, ikcol, grid );
-+
-+					/*
-+					 * Send Xk to process column Pc[k].
-+					 */			 
-+					for (p = 0; p < grid->nprow; ++p) {
-+						if ( fsendx_plist[lk][p] != EMPTY ) {
-+							pi = PNUM( p, ikcol, grid );
- #ifdef ISEND_IRECV
--			    MPI_Isend( &x[ii - XK_H], iknsupc * nrhs + XK_H,
--				       MPI_DOUBLE, pi, Xk, grid->comm,
--				       &send_req[Llu->SolveMsgSent++] );
-+							MPI_Isend( &x[ii - XK_H], iknsupc * nrhs + XK_H,
-+									MPI_DOUBLE, pi, Xk, grid->comm,
-+									&send_req[Llu->SolveMsgSent++] );
- #else
- #ifdef BSEND
--			    MPI_Bsend( &x[ii - XK_H], iknsupc * nrhs + XK_H,
--				       MPI_DOUBLE, pi, Xk, grid->comm );
-+							MPI_Bsend( &x[ii - XK_H], iknsupc * nrhs + XK_H,
-+									MPI_DOUBLE, pi, Xk, grid->comm );
- #else
--			    MPI_Send( &x[ii - XK_H], iknsupc * nrhs + XK_H,
--				     MPI_DOUBLE, pi, Xk, grid->comm );
-+							MPI_Send( &x[ii - XK_H], iknsupc * nrhs + XK_H,
-+									MPI_DOUBLE, pi, Xk, grid->comm );
- #endif
- #endif
- #if ( DEBUGlevel>=2 )
--			    printf("(%2d) Sent X[%2.0f] to P %2d\n",
--				   iam, x[ii-XK_H], pi);
-+							printf("(%2d) Sent X[%2.0f] to P %2d\n",
-+									iam, x[ii-XK_H], pi);
- #endif
--			}
--                    }
--		    /*
--		     * Perform local block modifications.
--		     */
--		    nlb1 = lsub1[0] - 1;
--		    lptr1 = BC_HEADER + LB_DESCRIPTOR + iknsupc;
--		    luptr1 = iknsupc; /* Skip diagonal block L(I,I). */
--
--		    dlsum_fmod(lsum, x, &x[ii], rtemp, nrhs, iknsupc, ik,
--			       fmod, nlb1, lptr1, luptr1, xsup,
--			       grid, Llu, send_req, stat);
--		} /* if frecv[lk] == 0 */
--	    } /* if iam == p */
--	} /* if fmod[lk] == 0 */
--
--    } /* for lb ... */
-+						}
-+					}
-+					/*
-+					 * Perform local block modifications.
-+					 */
-+					nlb1 = lsub1[0] - 1;
-+					lptr1 = BC_HEADER + LB_DESCRIPTOR + iknsupc;
-+					luptr1 = iknsupc; /* Skip diagonal block L(I,I). */
-+
-+					dlsum_fmod(lsum, x, &x[ii], rtemp, nrhs, iknsupc, ik,
-+							fmod, nlb1, lptr1, luptr1, xsup,
-+							grid, Llu, send_req, stat);
-+				} /* if frecv[lk] == 0 */
-+			} /* if iam == p */
-+		} /* if fmod[lk] == 0 */
- 
-+	} /* for lb ... */
- } /* dLSUM_FMOD */
- 
- 
-@@ -232,143 +274,1871 @@ void dlsum_bmod
-  SuperLUStat_t *stat
-  )
- {
--/*
-- * Purpose
-- * =======
-- *   Perform local block modifications: lsum[i] -= U_i,k * X[k].
-- */
--    double alpha = 1.0;
--    int    iam, iknsupc, knsupc, myrow, nsupr, p, pi;
--    int_t  fnz, gik, gikcol, i, ii, ik, ikfrow, iklrow, il, irow,
--           j, jj, lk, lk1, nub, ub, uptr;
--    int_t  *usub;
--    double *uval, *dest, *y;
--    int_t  *lsub;
--    double *lusup;
--    int_t  *ilsum = Llu->ilsum; /* Starting position of each supernode in lsum.   */
--    int_t  *brecv = Llu->brecv;
--    int_t  **bsendx_plist = Llu->bsendx_plist;
--    MPI_Status status;
--    int test_flag;
--
--    iam = grid->iam;
--    myrow = MYROW( iam, grid );
--    knsupc = SuperSize( k );
--    lk = LBj( k, grid ); /* Local block number, column-wise. */
--    nub = Urbs[lk];      /* Number of U blocks in block column lk */
--
--    for (ub = 0; ub < nub; ++ub) {
--	ik = Ucb_indptr[lk][ub].lbnum; /* Local block number, row-wise. */
--	usub = Llu->Ufstnz_br_ptr[ik];
--	uval = Llu->Unzval_br_ptr[ik];
--	i = Ucb_indptr[lk][ub].indpos; /* Start of the block in usub[]. */
--	i += UB_DESCRIPTOR;
--	il = LSUM_BLK( ik );
--	gik = ik * grid->nprow + myrow;/* Global block number, row-wise. */
--	iknsupc = SuperSize( gik );
--	ikfrow = FstBlockC( gik );
--	iklrow = FstBlockC( gik+1 );
--
--	RHS_ITERATE(j) {
--	    dest = &lsum[il + j*iknsupc];
--	    y = &xk[j*knsupc];
--	    uptr = Ucb_valptr[lk][ub]; /* Start of the block in uval[]. */
--	    for (jj = 0; jj < knsupc; ++jj) {
--		fnz = usub[i + jj];
--		if ( fnz < iklrow ) { /* Nonzero segment. */
--		    /* AXPY */
--		    for (irow = fnz; irow < iklrow; ++irow)
--			dest[irow - ikfrow] -= uval[uptr++] * y[jj];
--		    stat->ops[SOLVE] += 2 * (iklrow - fnz);
-+	/*
-+	 * Purpose
-+	 * =======
-+	 *   Perform local block modifications: lsum[i] -= U_i,k * X[k].
-+	 */
-+	double alpha = 1.0, beta = 0.0;
-+	int    iam, iknsupc, knsupc, myrow, nsupr, p, pi;
-+	int_t  fnz, gik, gikcol, i, ii, ik, ikfrow, iklrow, il, irow,
-+	       j, jj, lk, lk1, nub, ub, uptr;
-+	int_t  *usub;
-+	double *uval, *dest, *y;
-+	int_t  *lsub;
-+	double *lusup;
-+	int_t  *ilsum = Llu->ilsum; /* Starting position of each supernode in lsum.   */
-+	int_t  *brecv = Llu->brecv;
-+	int_t  **bsendx_plist = Llu->bsendx_plist;
-+	MPI_Status status;
-+	int test_flag;
-+
-+	iam = grid->iam;
-+	myrow = MYROW( iam, grid );
-+	knsupc = SuperSize( k );
-+	lk = LBj( k, grid ); /* Local block number, column-wise. */
-+	nub = Urbs[lk];      /* Number of U blocks in block column lk */
-+
-+	for (ub = 0; ub < nub; ++ub) {
-+		ik = Ucb_indptr[lk][ub].lbnum; /* Local block number, row-wise. */
-+		usub = Llu->Ufstnz_br_ptr[ik];
-+		uval = Llu->Unzval_br_ptr[ik];
-+		i = Ucb_indptr[lk][ub].indpos; /* Start of the block in usub[]. */
-+		i += UB_DESCRIPTOR;
-+		il = LSUM_BLK( ik );
-+		gik = ik * grid->nprow + myrow;/* Global block number, row-wise. */
-+		iknsupc = SuperSize( gik );
-+		ikfrow = FstBlockC( gik );
-+		iklrow = FstBlockC( gik+1 );
-+
-+		RHS_ITERATE(j) {
-+			dest = &lsum[il + j*iknsupc];
-+			y = &xk[j*knsupc];
-+			uptr = Ucb_valptr[lk][ub]; /* Start of the block in uval[]. */
-+			for (jj = 0; jj < knsupc; ++jj) {
-+				fnz = usub[i + jj];
-+				if ( fnz < iklrow ) { /* Nonzero segment. */
-+					/* AXPY */
-+					for (irow = fnz; irow < iklrow; ++irow)
-+						dest[irow - ikfrow] -= uval[uptr++] * y[jj];
-+					stat->ops[SOLVE] += 2 * (iklrow - fnz);
-+				}
-+			} /* for jj ... */
- 		}
--	    } /* for jj ... */
--	}
- 
--	if ( (--bmod[ik]) == 0 ) { /* Local accumulation done. */
--	    gikcol = PCOL( gik, grid );
--	    p = PNUM( myrow, gikcol, grid );
--	    if ( iam != p ) {
-+		if ( (--bmod[ik]) == 0 ) { /* Local accumulation done. */
-+			gikcol = PCOL( gik, grid );
-+			p = PNUM( myrow, gikcol, grid );
-+			if ( iam != p ) {
- #ifdef ISEND_IRECV
--		MPI_Isend( &lsum[il - LSUM_H], iknsupc * nrhs + LSUM_H,
--			   MPI_DOUBLE, p, LSUM, grid->comm,
--                           &send_req[Llu->SolveMsgSent++] );
-+				MPI_Isend( &lsum[il - LSUM_H], iknsupc * nrhs + LSUM_H,
-+						MPI_DOUBLE, p, LSUM, grid->comm,
-+						&send_req[Llu->SolveMsgSent++] );
- #else
- #ifdef BSEND
--		MPI_Bsend( &lsum[il - LSUM_H], iknsupc * nrhs + LSUM_H,
--			   MPI_DOUBLE, p, LSUM, grid->comm );
-+				MPI_Bsend( &lsum[il - LSUM_H], iknsupc * nrhs + LSUM_H,
-+						MPI_DOUBLE, p, LSUM, grid->comm );
- #else
--		MPI_Send( &lsum[il - LSUM_H], iknsupc * nrhs + LSUM_H,
--			  MPI_DOUBLE, p, LSUM, grid->comm );
-+				MPI_Send( &lsum[il - LSUM_H], iknsupc * nrhs + LSUM_H,
-+						MPI_DOUBLE, p, LSUM, grid->comm );
- #endif
- #endif
- #if ( DEBUGlevel>=2 )
--		printf("(%2d) Sent LSUM[%2.0f], size %2d, to P %2d\n",
--		       iam, lsum[il-LSUM_H], iknsupc*nrhs+LSUM_H, p);
--#endif
--	    } else { /* Diagonal process: X[i] += lsum[i]. */
--		ii = X_BLK( ik );
--		dest = &x[ii];
--		RHS_ITERATE(j)
--		    for (i = 0; i < iknsupc; ++i)
--			dest[i + j*iknsupc] += lsum[i + il + j*iknsupc];
--		if ( !brecv[ik] ) { /* Becomes a leaf node. */
--		    bmod[ik] = -1; /* Do not solve X[k] in the future. */
--		    lk1 = LBj( gik, grid ); /* Local block number. */
--		    lsub = Llu->Lrowind_bc_ptr[lk1];
--		    lusup = Llu->Lnzval_bc_ptr[lk1];
--		    nsupr = lsub[1];
-+				printf("(%2d) Sent LSUM[%2.0f], size %2d, to P %2d\n",
-+						iam, lsum[il-LSUM_H], iknsupc*nrhs+LSUM_H, p);
-+#endif
-+			} else { /* Diagonal process: X[i] += lsum[i]. */
-+				ii = X_BLK( ik );
-+				dest = &x[ii];
-+				RHS_ITERATE(j)
-+					for (i = 0; i < iknsupc; ++i)
-+						dest[i + j*iknsupc] += lsum[i + il + j*iknsupc];
-+				if ( !brecv[ik] ) { /* Becomes a leaf node. */
-+					bmod[ik] = -1; /* Do not solve X[k] in the future. */
-+					lk1 = LBj( gik, grid ); /* Local block number. */
-+					lsub = Llu->Lrowind_bc_ptr[lk1];
-+					lusup = Llu->Lnzval_bc_ptr[lk1];
-+					nsupr = lsub[1];
- #ifdef _CRAY
--		    STRSM(ftcs1, ftcs3, ftcs2, ftcs2, &iknsupc, &nrhs, &alpha,
--			  lusup, &nsupr, &x[ii], &iknsupc);
-+					STRSM(ftcs1, ftcs3, ftcs2, ftcs2, &iknsupc, &nrhs, &alpha,
-+							lusup, &nsupr, &x[ii], &iknsupc);
- #elif defined (USE_VENDOR_BLAS)
--		    dtrsm_("L", "U", "N", "N", &iknsupc, &nrhs, &alpha, 
--			   lusup, &nsupr, &x[ii], &iknsupc, 1, 1, 1, 1);
-+					dtrsm_("L", "U", "N", "N", &iknsupc, &nrhs, &alpha, 
-+							lusup, &nsupr, &x[ii], &iknsupc, 1, 1, 1, 1);	
- #else
--		    dtrsm_("L", "U", "N", "N", &iknsupc, &nrhs, &alpha, 
--			   lusup, &nsupr, &x[ii], &iknsupc);
-+					dtrsm_("L", "U", "N", "N", &iknsupc, &nrhs, &alpha, 
-+							lusup, &nsupr, &x[ii], &iknsupc);
- #endif
--		    stat->ops[SOLVE] += iknsupc * (iknsupc + 1) * nrhs;
-+					stat->ops[SOLVE] += iknsupc * (iknsupc + 1) * nrhs;
- #if ( DEBUGlevel>=2 )
--		    printf("(%2d) Solve X[%2d]\n", iam, gik);
-+					printf("(%2d) Solve X[%2d]\n", iam, gik);
- #endif
- 
--		    /*
--		     * Send Xk to process column Pc[k].
--		     */
--		    for (p = 0; p < grid->nprow; ++p) {
--			if ( bsendx_plist[lk1][p] != EMPTY ) {
--			    pi = PNUM( p, gikcol, grid );
-+					/*
-+					 * Send Xk to process column Pc[k].
-+					 */
-+					for (p = 0; p < grid->nprow; ++p) {
-+						if ( bsendx_plist[lk1][p] != EMPTY ) {
-+							pi = PNUM( p, gikcol, grid );
- #ifdef ISEND_IRECV
--			    MPI_Isend( &x[ii - XK_H], iknsupc * nrhs + XK_H,
--				       MPI_DOUBLE, pi, Xk, grid->comm,
--				       &send_req[Llu->SolveMsgSent++] );
-+							MPI_Isend( &x[ii - XK_H], iknsupc * nrhs + XK_H,
-+									MPI_DOUBLE, pi, Xk, grid->comm,
-+									&send_req[Llu->SolveMsgSent++] );
- #else
- #ifdef BSEND
--			    MPI_Bsend( &x[ii - XK_H], iknsupc * nrhs + XK_H,
--				       MPI_DOUBLE, pi, Xk, grid->comm );
-+							MPI_Bsend( &x[ii - XK_H], iknsupc * nrhs + XK_H,
-+									MPI_DOUBLE, pi, Xk, grid->comm );
- #else
--			    MPI_Send( &x[ii - XK_H], iknsupc * nrhs + XK_H,
--				     MPI_DOUBLE, pi, Xk, grid->comm );
-+							MPI_Send( &x[ii - XK_H], iknsupc * nrhs + XK_H,
-+									MPI_DOUBLE, pi, Xk, grid->comm );
- #endif
- #endif
- #if ( DEBUGlevel>=2 )
--			    printf("(%2d) Sent X[%2.0f] to P %2d\n",
--				   iam, x[ii-XK_H], pi);
-+							printf("(%2d) Sent X[%2.0f] to P %2d\n",
-+									iam, x[ii-XK_H], pi);
- #endif
-+						}
-+					}
-+					/*
-+					 * Perform local block modifications.
-+					 */
-+					if ( Urbs[lk1] )
-+						dlsum_bmod(lsum, x, &x[ii], nrhs, gik, bmod, Urbs,
-+								Ucb_indptr, Ucb_valptr, xsup, grid, Llu,
-+								send_req, stat);
-+				} /* if brecv[ik] == 0 */
- 			}
--                     }
--		    /*
--		     * Perform local block modifications.
--		     */
--		    if ( Urbs[lk1] )
--			dlsum_bmod(lsum, x, &x[ii], nrhs, gik, bmod, Urbs,
--				   Ucb_indptr, Ucb_valptr, xsup, grid, Llu,
--				   send_req, stat);
--		} /* if brecv[ik] == 0 */
--	    }
--	} /* if bmod[ik] == 0 */
--
--    } /* for ub ... */
-+		} /* if bmod[ik] == 0 */
-+
-+	} /* for ub ... */
- 
- } /* dlSUM_BMOD */
- 
-+
-+/************************************************************************/
-+/*! \brief
-+ *
-+ * <pre>
-+ * Purpose
-+ * =======
-+ *   Perform local block modifications: lsum[i] -= L_i,k * X[k].
-+ * </pre>
-+ */
-+void dlsum_fmod_inv
-+/************************************************************************/
-+(
-+ double *lsum,    /* Sum of local modifications.                        */
-+ double *x,       /* X array (local)                                    */
-+ double *xk,      /* X[k].                                              */
-+ double *rtemp,   /* Result of full matrix-vector multiply.             */
-+ int   nrhs,      /* Number of right-hand sides.                        */
-+ int   knsupc,    /* Size of supernode k.                               */
-+ int_t k,         /* The k-th component of X.                           */
-+ int_t *fmod,     /* Modification count for L-solve.                    */
-+ int_t nlb,       /* Number of L blocks.                                */
-+ int_t *xsup,
-+ gridinfo_t *grid,
-+ LocalLU_t *Llu,
-+ SuperLUStat_t **stat,
-+ int_t *leaf_send,
-+ int_t *nleaf_send,
-+ int_t sizelsum,
-+ int_t sizertemp,
-+ int_t recurlevel
-+ )
-+{
-+	double alpha = 1.0, beta = 0.0,malpha=-1.0;
-+	double *lusup, *lusup1;
-+	double *dest;
-+	double *Linv;/* Inverse of diagonal block */    
-+	int    iam, iknsupc, myrow, krow, nbrow, nbrow1, nbrow_ref, nsupr, nsupr1, p, pi, idx_r,m;
-+	int_t  i, ii,jj, ik, il, ikcol, irow, j, lb, lk, rel, lib,lready;
-+	int_t  *lsub, *lsub1, nlb1, lptr1, luptr1,*lloc;
-+	int_t  *ilsum = Llu->ilsum; /* Starting position of each supernode in lsum.   */
-+	int_t  *frecv = Llu->frecv;
-+	int_t  **fsendx_plist = Llu->fsendx_plist;
-+	int_t  luptr_tmp,luptr_tmp1,lptr1_tmp,maxrecvsz, idx_i, idx_v,idx_n,  idx_l, fmod_tmp, lbstart,lbend,nn,Nchunk,nlb_loc,remainder;
-+	int thread_id,thread_id1,num_thread;
-+	flops_t ops_loc=0.0;         
-+	MPI_Status status;
-+	int test_flag;
-+	yes_no_t done;
-+	BcTree  *LBtree_ptr = Llu->LBtree_ptr;
-+	RdTree  *LRtree_ptr = Llu->LRtree_ptr;
-+	int_t* idx_lsum,idx_lsum1;
-+	double *rtemp_loc;
-+	int_t ldalsum,maxsuper,aln_d;
-+	int dword = sizeof (double);	
-+	int_t nleaf_send_tmp;
-+	int_t lptr;      /* Starting position in lsub[*].                      */
-+	int_t luptr;     /* Starting position in lusup[*].                     */
-+
-+	maxsuper = sp_ienv_dist(3);
-+
-+#ifdef _OPENMP
-+	thread_id = omp_get_thread_num ();
-+	num_thread = omp_get_num_threads ();
-+#else
-+	thread_id = 0;
-+	num_thread = 1;
-+#endif
-+	ldalsum=Llu->ldalsum;
-+
-+	rtemp_loc = &rtemp[sizertemp* thread_id];
-+
-+	// #if ( PROFlevel>=1 )
-+	double t1, t2, t3, t4;
-+	float msg_vol = 0, msg_cnt = 0;
-+	// #endif 
-+
-+
-+	if(nlb>0){
-+		maxrecvsz = sp_ienv_dist(3) * nrhs + SUPERLU_MAX( XK_H, LSUM_H );
-+
-+		iam = grid->iam;
-+		myrow = MYROW( iam, grid );
-+		lk = LBj( k, grid ); /* Local block number, column-wise. */
-+
-+		// printf("ya1 %5d k %5d lk %5d\n",thread_id,k,lk);
-+		// fflush(stdout);	
-+
-+		lsub = Llu->Lrowind_bc_ptr[lk];
-+
-+		// printf("ya2 %5d k %5d lk %5d\n",thread_id,k,lk);
-+		// fflush(stdout);	
-+
-+		lusup = Llu->Lnzval_bc_ptr[lk];
-+		lloc = Llu->Lindval_loc_bc_ptr[lk];
-+		// idx_lsum = Llu->Lrowind_bc_2_lsum[lk];
-+
-+		nsupr = lsub[1];
-+
-+		// printf("nlb: %5d lk: %5d\n",nlb,lk);
-+		// fflush(stdout);
-+
-+		krow = PROW( k, grid );
-+		if(myrow==krow){
-+			idx_n = 1;
-+			idx_i = nlb+2;
-+			idx_v = 2*nlb+3;
-+			luptr_tmp = lloc[idx_v];
-+			m = nsupr-knsupc;
-+		}else{
-+			idx_n = 0;
-+			idx_i = nlb;
-+			idx_v = 2*nlb;
-+			luptr_tmp = lloc[idx_v];
-+			m = nsupr;
-+		}
-+
-+		assert(m>0);
-+				
-+		if(m>8*maxsuper){ 
-+			// if(m<1){
-+			// TIC(t1);
-+			Nchunk=num_thread;
-+			nlb_loc = floor(((double)nlb)/Nchunk);
-+			remainder = nlb % Nchunk;
-+
-+#ifdef _OPENMP
-+#pragma	omp	taskloop private (lptr1,luptr1,nlb1,thread_id1,lsub1,lusup1,nsupr1,Linv,nn,lbstart,lbend,luptr_tmp1,nbrow,lb,lptr1_tmp,rtemp_loc,nbrow_ref,lptr,nbrow1,ik,rel,lk,iknsupc,il,i,irow,fmod_tmp,ikcol,p,ii,jj,t1,t2,j,nleaf_send_tmp) untied nogroup	
-+#endif	
-+			for (nn=0;nn<Nchunk;++nn){
-+
-+#ifdef _OPENMP				 
-+				thread_id1 = omp_get_thread_num ();
-+#else
-+				thread_id1 = 0;
-+#endif		
-+				rtemp_loc = &rtemp[sizertemp* thread_id1];
-+
-+				if(nn<remainder){
-+					lbstart = nn*(nlb_loc+1);
-+					lbend = (nn+1)*(nlb_loc+1);
-+				}else{
-+					lbstart = remainder+nn*nlb_loc;
-+					lbend = remainder + (nn+1)*nlb_loc;
-+				}
-+
-+				if(lbstart<lbend){
-+
-+#if ( PROFlevel>=1 )
-+					TIC(t1);
-+#endif				
-+					luptr_tmp1 = lloc[lbstart+idx_v];
-+					nbrow=0;
-+					for (lb = lbstart; lb < lbend; ++lb){ 		
-+						lptr1_tmp = lloc[lb+idx_i];		
-+						nbrow += lsub[lptr1_tmp+1];
-+					}
-+
-+#ifdef _CRAY
-+					SGEMM( ftcs2, ftcs2, &nbrow, &nrhs, &knsupc,
-+							&alpha, &lusup[luptr_tmp1], &nsupr, xk,
-+							&knsupc, &beta, rtemp_loc, &nbrow );
-+#elif defined (USE_VENDOR_BLAS)
-+					dgemm_( "N", "N", &nbrow, &nrhs, &knsupc,
-+							&alpha, &lusup[luptr_tmp1], &nsupr, xk,
-+							&knsupc, &beta, rtemp_loc, &nbrow, 1, 1 );
-+#else
-+					dgemm_( "N", "N", &nbrow, &nrhs, &knsupc,
-+							&alpha, &lusup[luptr_tmp1], &nsupr, xk,
-+							&knsupc, &beta, rtemp_loc, &nbrow );
-+#endif  			
-+
-+					nbrow_ref=0;
-+					for (lb = lbstart; lb < lbend; ++lb){ 		
-+						lptr1_tmp = lloc[lb+idx_i];	
-+						lptr= lptr1_tmp+2;	
-+						nbrow1 = lsub[lptr1_tmp+1];
-+						ik = lsub[lptr1_tmp]; /* Global block number, row-wise. */
-+						rel = xsup[ik]; /* Global row index of block ik. */
-+
-+
-+						lk = LBi( ik, grid ); /* Local block number, row-wise. */	
-+
-+						iknsupc = SuperSize( ik );
-+						il = LSUM_BLK( lk );
-+
-+						RHS_ITERATE(j)					
-+							for (i = 0; i < nbrow1; ++i) {
-+								irow = lsub[lptr+i] - rel; /* Relative row. */
-+
-+								lsum[il+irow + j*iknsupc+sizelsum*thread_id1] -= rtemp_loc[nbrow_ref+i + j*nbrow];
-+							}
-+						nbrow_ref+=nbrow1;
-+					}
-+
-+#if ( PROFlevel>=1 )
-+					TOC(t2, t1);
-+					stat[thread_id1]->utime[SOL_GEMM] += t2;
-+#endif	
-+
-+					for (lb=lbstart;lb<lbend;lb++){
-+						lk = lloc[lb+idx_n];
-+
-+#ifdef _OPENMP
-+#pragma omp atomic capture
-+#endif
-+						fmod_tmp=--fmod[lk];
-+
-+						if ( fmod_tmp==0 ) { /* Local accumulation done. */
-+
-+							lptr1_tmp = lloc[lb+idx_i];	
-+
-+							ik = lsub[lptr1_tmp]; /* Global block number, row-wise. */
-+							lk = LBi( ik, grid ); /* Local block number, row-wise. */	
-+
-+							iknsupc = SuperSize( ik );
-+							il = LSUM_BLK( lk );
-+
-+							ikcol = PCOL( ik, grid );
-+							p = PNUM( myrow, ikcol, grid );
-+							if ( iam != p ) {
-+
-+								for (ii=1;ii<num_thread;ii++)
-+									for (jj=0;jj<iknsupc*nrhs;jj++)
-+										lsum[il + jj ] += lsum[il + jj + ii*sizelsum];
-+
-+#ifdef _OPENMP
-+#pragma omp atomic capture
-+#endif
-+								nleaf_send_tmp = ++nleaf_send[0];
-+								leaf_send[nleaf_send_tmp-1] = -lk-1;		
-+								// RdTree_forwardMessageSimple(LRtree_ptr[lk],&lsum[il - LSUM_H ]);
-+
-+							} else { /* Diagonal process: X[i] += lsum[i]. */
-+
-+#if ( PROFlevel>=1 )
-+								TIC(t1);
-+#endif		
-+								for (ii=1;ii<num_thread;ii++)
-+									// if(ii!=thread_id1)
-+									for (jj=0;jj<iknsupc*nrhs;jj++)
-+										lsum[il+ jj] += lsum[il + jj + ii*sizelsum];
-+
-+								ii = X_BLK( lk );
-+								// for (jj=0;jj<num_thread;jj++)
-+								RHS_ITERATE(j)
-+									for (i = 0; i < iknsupc; ++i)	
-+										x[i + ii + j*iknsupc] += lsum[i + il + j*iknsupc ] ;
-+
-+
-+								// fmod[lk] = -1; /* Do not solve X[k] in the future. */
-+								lk = LBj( ik, grid );/* Local block number, column-wise. */
-+								lsub1 = Llu->Lrowind_bc_ptr[lk];
-+								lusup1 = Llu->Lnzval_bc_ptr[lk];
-+								nsupr1 = lsub1[1];
-+
-+								if(Llu->inv == 1){
-+									Linv = Llu->Linv_bc_ptr[lk];
-+#ifdef _CRAY
-+									SGEMM( ftcs2, ftcs2, &iknsupc, &nrhs, &iknsupc,
-+											&alpha, Linv, &iknsupc, &x[ii],
-+											&iknsupc, &beta, rtemp_loc, &iknsupc );
-+#elif defined (USE_VENDOR_BLAS)
-+									dgemm_( "N", "N", &iknsupc, &nrhs, &iknsupc,
-+											&alpha, Linv, &iknsupc, &x[ii],
-+											&iknsupc, &beta, rtemp_loc, &iknsupc, 1, 1 );
-+#else
-+									dgemm_( "N", "N", &iknsupc, &nrhs, &iknsupc,
-+											&alpha, Linv, &iknsupc, &x[ii],
-+											&iknsupc, &beta, rtemp_loc, &iknsupc );
-+#endif   
-+									for (i=0 ; i<iknsupc*nrhs ; i++){
-+										x[ii+i] = rtemp_loc[i];
-+									}		
-+								}else{
-+#ifdef _CRAY
-+									STRSM(ftcs1, ftcs1, ftcs2, ftcs3, &iknsupc, &nrhs, &alpha,
-+											lusup1, &nsupr1, &x[ii], &iknsupc);
-+#elif defined (USE_VENDOR_BLAS)
-+									dtrsm_("L", "L", "N", "U", &iknsupc, &nrhs, &alpha, 
-+											lusup1, &nsupr1, &x[ii], &iknsupc, 1, 1, 1, 1);		   
-+#else
-+									dtrsm_("L", "L", "N", "U", &iknsupc, &nrhs, &alpha, 
-+											lusup1, &nsupr1, &x[ii], &iknsupc);
-+#endif
-+								}
-+
-+#if ( PROFlevel>=1 )
-+								TOC(t2, t1);
-+								stat[thread_id1]->utime[SOL_TRSM] += t2;
-+
-+#endif	
-+								stat[thread_id1]->ops[SOLVE] += iknsupc * (iknsupc - 1) * nrhs;
-+#if ( DEBUGlevel>=2 )
-+								printf("(%2d) Solve X[%2d]\n", iam, ik);
-+#endif
-+
-+								/*
-+								 * Send Xk to process column Pc[k].
-+								 */
-+
-+								if(LBtree_ptr[lk]!=NULL){
-+#ifdef _OPENMP
-+#pragma omp atomic capture
-+#endif
-+									nleaf_send_tmp = ++nleaf_send[0];
-+									leaf_send[nleaf_send_tmp-1] = lk;
-+									// BcTree_forwardMessageSimple(LBtree_ptr[lk],&x[ii - XK_H]);
-+								}
-+
-+								/*
-+								 * Perform local block modifications.
-+								 */
-+
-+								// #ifdef _OPENMP
-+								// #pragma	omp	task firstprivate (Llu,sizelsum,iknsupc,ii,ik,lsub1,x,rtemp,fmod,lsum,stat,nrhs,grid,xsup,recurlevel) private(lptr1,luptr1,nlb1,thread_id1) untied priority(1) 	
-+								// #endif
-+								{
-+
-+									nlb1 = lsub1[0] - 1;						
-+									dlsum_fmod_inv(lsum, x, &x[ii], rtemp, nrhs, iknsupc, ik,
-+											fmod, nlb1, xsup,
-+											grid, Llu, stat, leaf_send, nleaf_send ,sizelsum,sizertemp,1+recurlevel);
-+								}		   
-+
-+								// } /* if frecv[lk] == 0 */
-+						} /* if iam == p */
-+					} /* if fmod[lk] == 0 */				
-+				}
-+
-+			}
-+		}	
-+
-+		}else{ 
-+
-+#if ( PROFlevel>=1 )
-+			TIC(t1);
-+#endif	
-+
-+#ifdef _CRAY
-+			SGEMM( ftcs2, ftcs2, &m, &nrhs, &knsupc,
-+					&alpha, &lusup[luptr_tmp], &nsupr, xk,
-+					&knsupc, &beta, rtemp_loc, &m );
-+#elif defined (USE_VENDOR_BLAS)
-+			dgemm_( "N", "N", &m, &nrhs, &knsupc,
-+					&alpha, &lusup[luptr_tmp], &nsupr, xk,
-+					&knsupc, &beta, rtemp_loc, &m, 1, 1 );
-+#else
-+			dgemm_( "N", "N", &m, &nrhs, &knsupc,
-+					&alpha, &lusup[luptr_tmp], &nsupr, xk,
-+					&knsupc, &beta, rtemp_loc, &m );
-+#endif   	
-+
-+			// for (i = 0; i < m*nrhs; ++i) {
-+				// lsum[idx_lsum[i]+sizelsum*thread_id] -=rtemp_loc[i];
-+			// }
-+			
-+			nbrow=0;
-+			for (lb = 0; lb < nlb; ++lb){ 		
-+				lptr1_tmp = lloc[lb+idx_i];		
-+				nbrow += lsub[lptr1_tmp+1];
-+			}			
-+			nbrow_ref=0;
-+			for (lb = 0; lb < nlb; ++lb){ 		
-+				lptr1_tmp = lloc[lb+idx_i];	
-+				lptr= lptr1_tmp+2;	
-+				nbrow1 = lsub[lptr1_tmp+1];
-+				ik = lsub[lptr1_tmp]; /* Global block number, row-wise. */
-+				rel = xsup[ik]; /* Global row index of block ik. */
-+
-+				lk = LBi( ik, grid ); /* Local block number, row-wise. */	
-+
-+				iknsupc = SuperSize( ik );
-+				il = LSUM_BLK( lk );
-+
-+				RHS_ITERATE(j)					
-+					for (i = 0; i < nbrow1; ++i) {
-+						irow = lsub[lptr+i] - rel; /* Relative row. */
-+
-+						lsum[il+irow + j*iknsupc+sizelsum*thread_id] -= rtemp_loc[nbrow_ref+i + j*nbrow];
-+					}
-+				nbrow_ref+=nbrow1;
-+			}			
-+			
-+
-+
-+			// TOC(t3, t1);
-+
-+#if ( PROFlevel>=1 )
-+			TOC(t2, t1);
-+			stat[thread_id]->utime[SOL_GEMM] += t2;
-+
-+#endif		
-+
-+			thread_id1 = omp_get_thread_num ();
-+			rtemp_loc = &rtemp[sizertemp* thread_id1];
-+			for (lb=0;lb<nlb;lb++){
-+				lk = lloc[lb+idx_n];
-+
-+#ifdef _OPENMP
-+#pragma omp atomic capture
-+#endif
-+				fmod_tmp=--fmod[lk];
-+
-+
-+				if ( fmod_tmp==0 ) { /* Local accumulation done. */
-+
-+					lptr1_tmp = lloc[lb+idx_i];	
-+
-+					ik = lsub[lptr1_tmp]; /* Global block number, row-wise. */
-+					lk = LBi( ik, grid ); /* Local block number, row-wise. */	
-+
-+					iknsupc = SuperSize( ik );
-+					il = LSUM_BLK( lk );
-+					ikcol = PCOL( ik, grid );
-+					p = PNUM( myrow, ikcol, grid );
-+					if ( iam != p ) {
-+						for (ii=1;ii<num_thread;ii++)
-+							// if(ii!=thread_id1)
-+							for (jj=0;jj<iknsupc*nrhs;jj++)
-+								lsum[il + jj] += lsum[il + jj + ii*sizelsum];
-+
-+#ifdef _OPENMP
-+#pragma omp atomic capture
-+#endif
-+						nleaf_send_tmp = ++nleaf_send[0];
-+						leaf_send[nleaf_send_tmp-1] = -lk-1;						
-+
-+						// RdTree_forwardMessageSimple(LRtree_ptr[lk],&lsum[il - LSUM_H ]);
-+					} else { /* Diagonal process: X[i] += lsum[i]. */
-+
-+#if ( PROFlevel>=1 )
-+						TIC(t1);
-+#endif		
-+						for (ii=1;ii<num_thread;ii++)
-+							// if(ii!=thread_id1)
-+							for (jj=0;jj<iknsupc*nrhs;jj++)
-+								lsum[il+ jj] += lsum[il + jj + ii*sizelsum];
-+
-+						ii = X_BLK( lk );
-+						// for (jj=0;jj<num_thread;jj++)
-+						RHS_ITERATE(j)
-+							for (i = 0; i < iknsupc; ++i)	
-+								x[i + ii + j*iknsupc] += lsum[i + il + j*iknsupc] ;
-+
-+
-+						// fmod[lk] = -1; /* Do not solve X[k] in the future. */
-+						lk = LBj( ik, grid );/* Local block number, column-wise. */
-+						lsub1 = Llu->Lrowind_bc_ptr[lk];
-+						lusup1 = Llu->Lnzval_bc_ptr[lk];
-+						nsupr1 = lsub1[1];
-+
-+						if(Llu->inv == 1){
-+							Linv = Llu->Linv_bc_ptr[lk];
-+#ifdef _CRAY
-+							SGEMM( ftcs2, ftcs2, &iknsupc, &nrhs, &iknsupc,
-+									&alpha, Linv, &iknsupc, &x[ii],
-+									&iknsupc, &beta, rtemp_loc, &iknsupc );
-+#elif defined (USE_VENDOR_BLAS)
-+							dgemm_( "N", "N", &iknsupc, &nrhs, &iknsupc,
-+									&alpha, Linv, &iknsupc, &x[ii],
-+									&iknsupc, &beta, rtemp_loc, &iknsupc, 1, 1 );
-+#else
-+							dgemm_( "N", "N", &iknsupc, &nrhs, &iknsupc,
-+									&alpha, Linv, &iknsupc, &x[ii],
-+									&iknsupc, &beta, rtemp_loc, &iknsupc );
-+#endif   
-+							for (i=0 ; i<iknsupc*nrhs ; i++){
-+								x[ii+i] = rtemp_loc[i];
-+							}		
-+						}else{
-+#ifdef _CRAY
-+							STRSM(ftcs1, ftcs1, ftcs2, ftcs3, &iknsupc, &nrhs, &alpha,
-+									lusup1, &nsupr1, &x[ii], &iknsupc);
-+#elif defined (USE_VENDOR_BLAS)
-+							dtrsm_("L", "L", "N", "U", &iknsupc, &nrhs, &alpha, 
-+									lusup1, &nsupr1, &x[ii], &iknsupc, 1, 1, 1, 1);		   
-+#else
-+							dtrsm_("L", "L", "N", "U", &iknsupc, &nrhs, &alpha, 
-+									lusup1, &nsupr1, &x[ii], &iknsupc);
-+#endif
-+						}
-+
-+#if ( PROFlevel>=1 )
-+						TOC(t2, t1);
-+						stat[thread_id1]->utime[SOL_TRSM] += t2;
-+
-+#endif	
-+
-+
-+						stat[thread_id1]->ops[SOLVE] += iknsupc * (iknsupc - 1) * nrhs;
-+#if ( DEBUGlevel>=2 )
-+						printf("(%2d) Solve X[%2d]\n", iam, ik);
-+#endif
-+
-+						/*
-+						 * Send Xk to process column Pc[k].
-+						 */
-+
-+						if(LBtree_ptr[lk]!=NULL){
-+
-+#ifdef _OPENMP
-+#pragma omp atomic capture
-+#endif
-+							nleaf_send_tmp = ++nleaf_send[0];
-+							// printf("nleaf_send_tmp %5d lk %5d\n",nleaf_send_tmp);
-+							leaf_send[nleaf_send_tmp-1] = lk;
-+							// BcTree_forwardMessageSimple(LBtree_ptr[lk],&x[ii - XK_H]);
-+						}
-+
-+						/*
-+						 * Perform local block modifications.
-+						 */
-+
-+						// #ifdef _OPENMP
-+						// #pragma	omp	task firstprivate (Llu,sizelsum,iknsupc,ii,ik,lsub1,x,rtemp,fmod,lsum,send_req,stat,nrhs,grid,xsup,recurlevel) private(lptr1,luptr1,nlb1,thread_id1) untied priority(1) 	
-+						// #endif
-+
-+						{
-+							nlb1 = lsub1[0] - 1;
-+							dlsum_fmod_inv(lsum, x, &x[ii], rtemp, nrhs, iknsupc, ik,
-+									fmod, nlb1, xsup,
-+									grid, Llu, stat, leaf_send, nleaf_send ,sizelsum,sizertemp,1+recurlevel);
-+						}		   
-+
-+						// } /* if frecv[lk] == 0 */
-+				} /* if iam == p */
-+			} /* if fmod[lk] == 0 */				
-+		}
-+		// }
-+
-+}
-+
-+
-+stat[thread_id]->ops[SOLVE] += 2 * m * nrhs * knsupc;	
-+
-+} /* if nlb>0*/
-+} /* dLSUM_FMOD_inv */
-+
-+
-+
-+
-+
-+/************************************************************************/
-+/*! \brief
-+ *
-+ * <pre>
-+ * Purpose
-+ * =======
-+ *   Perform local block modifications: lsum[i] -= L_i,k * X[k].
-+ * </pre>
-+ */
-+void dlsum_fmod_inv_master
-+/************************************************************************/
-+(
-+ double *lsum,    /* Sum of local modifications.                        */
-+ double *x,       /* X array (local)                                    */
-+ double *xk,      /* X[k].                                              */
-+ double *rtemp,   /* Result of full matrix-vector multiply.             */
-+ int   nrhs,      /* Number of right-hand sides.                        */
-+ int   knsupc,    /* Size of supernode k.                               */
-+ int_t k,         /* The k-th component of X.                           */
-+ int_t *fmod,     /* Modification count for L-solve.                    */
-+ int_t nlb,       /* Number of L blocks.                                */
-+ int_t *xsup,
-+ gridinfo_t *grid,
-+ LocalLU_t *Llu,
-+ SuperLUStat_t **stat,
-+ int_t sizelsum,
-+ int_t sizertemp,
-+ int_t recurlevel
-+ )
-+{
-+	double alpha = 1.0, beta = 0.0,malpha=-1.0;
-+	double *lusup, *lusup1;
-+	double *dest;
-+	double *Linv;/* Inverse of diagonal block */    
-+	int    iam, iknsupc, myrow, krow, nbrow, nbrow1, nbrow_ref, nsupr, nsupr1, p, pi, idx_r;
-+	int_t  i, ii,jj, ik, il, ikcol, irow, j, lb, lk, rel, lib,lready;
-+	int_t  *lsub, *lsub1, nlb1, lptr1, luptr1,*lloc;
-+	int_t  *ilsum = Llu->ilsum; /* Starting position of each supernode in lsum.   */
-+	int_t  *frecv = Llu->frecv;
-+	int_t  **fsendx_plist = Llu->fsendx_plist;
-+	int_t  luptr_tmp,luptr_tmp1,lptr1_tmp,maxrecvsz, idx_i, idx_v,idx_n, idx_l, fmod_tmp, lbstart,lbend,nn,Nchunk,nlb_loc,remainder;
-+	int thread_id,thread_id1,num_thread;
-+	int m;
-+	flops_t ops_loc=0.0;         
-+	MPI_Status status;
-+	int test_flag;
-+	yes_no_t done;
-+	BcTree  *LBtree_ptr = Llu->LBtree_ptr;
-+	RdTree  *LRtree_ptr = Llu->LRtree_ptr;
-+	int_t* idx_lsum,idx_lsum1;
-+	double *rtemp_loc;
-+	int_t ldalsum,maxsuper,aln_d;
-+	int dword = sizeof (double);	
-+	int_t lptr;      /* Starting position in lsub[*].                      */
-+	int_t luptr;     /* Starting position in lusup[*].                     */
-+
-+	maxsuper = sp_ienv_dist(3);
-+
-+
-+#ifdef _OPENMP
-+	thread_id = omp_get_thread_num ();
-+	num_thread = omp_get_num_threads ();
-+#else
-+	thread_id = 0;
-+	num_thread = 1;
-+#endif
-+	ldalsum=Llu->ldalsum;
-+
-+	rtemp_loc = &rtemp[sizertemp* thread_id];
-+
-+
-+	// #if ( PROFlevel>=1 )
-+	double t1, t2, t3, t4;
-+	float msg_vol = 0, msg_cnt = 0;
-+	// #endif 
-+
-+
-+	if(nlb>0){
-+
-+		maxrecvsz = sp_ienv_dist(3) * nrhs + SUPERLU_MAX( XK_H, LSUM_H );
-+
-+		iam = grid->iam;
-+		myrow = MYROW( iam, grid );
-+		lk = LBj( k, grid ); /* Local block number, column-wise. */
-+
-+		lsub = Llu->Lrowind_bc_ptr[lk];
-+
-+		lusup = Llu->Lnzval_bc_ptr[lk];
-+		lloc = Llu->Lindval_loc_bc_ptr[lk];
-+		// idx_lsum = Llu->Lrowind_bc_2_lsum[lk];
-+
-+		nsupr = lsub[1];
-+
-+
-+		krow = PROW( k, grid );
-+		if(myrow==krow){
-+			idx_n = 1;
-+			idx_i = nlb+2;
-+			idx_v = 2*nlb+3;
-+			luptr_tmp = lloc[idx_v];
-+			m = nsupr-knsupc;
-+		}else{
-+			idx_n = 0;
-+			idx_i = nlb;
-+			idx_v = 2*nlb;
-+			luptr_tmp = lloc[idx_v];
-+			m = nsupr;
-+		}
-+
-+		assert(m>0);
-+		
-+		if(m>4*maxsuper || nrhs>10){ 
-+			// if(m<1){
-+
-+
-+
-+			// TIC(t1);
-+			Nchunk=num_thread;
-+			nlb_loc = floor(((double)nlb)/Nchunk);
-+			remainder = nlb % Nchunk;
-+
-+
-+
-+#ifdef _OPENMP
-+#pragma	omp	taskloop private (lptr1,luptr1,nlb1,thread_id1,lsub1,lusup1,nsupr1,Linv,nn,lbstart,lbend,luptr_tmp1,nbrow,lb,lptr1_tmp,rtemp_loc,nbrow_ref,lptr,nbrow1,ik,rel,lk,iknsupc,il,i,irow,fmod_tmp,ikcol,p,ii,jj,t1,t2,j) untied	
-+#endif	
-+			for (nn=0;nn<Nchunk;++nn){
-+
-+#ifdef _OPENMP				 
-+				thread_id1 = omp_get_thread_num ();
-+#else
-+				thread_id1 = 0;
-+#endif		
-+
-+
-+
-+				rtemp_loc = &rtemp[sizertemp* thread_id1];
-+
-+				if(nn<remainder){
-+					lbstart = nn*(nlb_loc+1);
-+					lbend = (nn+1)*(nlb_loc+1);
-+				}else{
-+					lbstart = remainder+nn*nlb_loc;
-+					lbend = remainder + (nn+1)*nlb_loc;
-+				}
-+
-+				if(lbstart<lbend){
-+
-+#if ( PROFlevel>=1 )
-+					TIC(t1);
-+#endif				
-+
-+					luptr_tmp1 = lloc[lbstart+idx_v];
-+					nbrow=0;
-+					for (lb = lbstart; lb < lbend; ++lb){ 		
-+						lptr1_tmp = lloc[lb+idx_i];		
-+						nbrow += lsub[lptr1_tmp+1];
-+					}
-+
-+
-+#ifdef _CRAY
-+					SGEMM( ftcs2, ftcs2, &nbrow, &nrhs, &knsupc,
-+							&alpha, &lusup[luptr_tmp1], &nsupr, xk,
-+							&knsupc, &beta, rtemp_loc, &nbrow );
-+#elif defined (USE_VENDOR_BLAS)
-+					dgemm_( "N", "N", &nbrow, &nrhs, &knsupc,
-+							&alpha, &lusup[luptr_tmp1], &nsupr, xk,
-+							&knsupc, &beta, rtemp_loc, &nbrow, 1, 1 );
-+#else
-+					dgemm_( "N", "N", &nbrow, &nrhs, &knsupc,
-+							&alpha, &lusup[luptr_tmp1], &nsupr, xk,
-+							&knsupc, &beta, rtemp_loc, &nbrow );
-+#endif  			
-+
-+
-+
-+					nbrow_ref=0;
-+					for (lb = lbstart; lb < lbend; ++lb){ 		
-+						lptr1_tmp = lloc[lb+idx_i];	
-+						lptr= lptr1_tmp+2;	
-+						nbrow1 = lsub[lptr1_tmp+1];
-+						ik = lsub[lptr1_tmp]; /* Global block number, row-wise. */
-+						rel = xsup[ik]; /* Global row index of block ik. */
-+
-+
-+						lk = LBi( ik, grid ); /* Local block number, row-wise. */	
-+
-+						iknsupc = SuperSize( ik );
-+						il = LSUM_BLK( lk );
-+
-+						RHS_ITERATE(j)					
-+							for (i = 0; i < nbrow1; ++i) {
-+								irow = lsub[lptr+i] - rel; /* Relative row. */
-+
-+								lsum[il+irow + j*iknsupc] -= rtemp_loc[nbrow_ref+i + j*nbrow];
-+							}
-+						nbrow_ref+=nbrow1;
-+					}
-+
-+#if ( PROFlevel>=1 )
-+					TOC(t2, t1);
-+					stat[thread_id1]->utime[SOL_GEMM] += t2;
-+#endif	
-+				}
-+			}	
-+		}else{ 
-+
-+#if ( PROFlevel>=1 )
-+			TIC(t1);
-+#endif	
-+
-+#ifdef _CRAY
-+			SGEMM( ftcs2, ftcs2, &m, &nrhs, &knsupc,
-+					&alpha, &lusup[luptr_tmp], &nsupr, xk,
-+					&knsupc, &beta, rtemp_loc, &m );
-+#elif defined (USE_VENDOR_BLAS)
-+			dgemm_( "N", "N", &m, &nrhs, &knsupc,
-+					&alpha, &lusup[luptr_tmp], &nsupr, xk,
-+					&knsupc, &beta, rtemp_loc, &m, 1, 1 );
-+#else
-+			dgemm_( "N", "N", &m, &nrhs, &knsupc,
-+					&alpha, &lusup[luptr_tmp], &nsupr, xk,
-+					&knsupc, &beta, rtemp_loc, &m );
-+#endif   	
-+
-+			// for (i = 0; i < m*nrhs; ++i) {
-+				// lsum[idx_lsum[i]] -=rtemp_loc[i];
-+			// }
-+			
-+			nbrow=0;
-+			for (lb = 0; lb < nlb; ++lb){ 		
-+				lptr1_tmp = lloc[lb+idx_i];		
-+				nbrow += lsub[lptr1_tmp+1];
-+			}
-+			nbrow_ref=0;
-+			for (lb = 0; lb < nlb; ++lb){ 		
-+				lptr1_tmp = lloc[lb+idx_i];	
-+				lptr= lptr1_tmp+2;	
-+				nbrow1 = lsub[lptr1_tmp+1];
-+				ik = lsub[lptr1_tmp]; /* Global block number, row-wise. */
-+				rel = xsup[ik]; /* Global row index of block ik. */
-+
-+				lk = LBi( ik, grid ); /* Local block number, row-wise. */	
-+
-+				iknsupc = SuperSize( ik );
-+				il = LSUM_BLK( lk );
-+
-+				RHS_ITERATE(j)					
-+					for (i = 0; i < nbrow1; ++i) {
-+						irow = lsub[lptr+i] - rel; /* Relative row. */
-+
-+						lsum[il+irow + j*iknsupc+sizelsum*thread_id] -= rtemp_loc[nbrow_ref+i + j*nbrow];
-+					}
-+				nbrow_ref+=nbrow1;
-+			}					
-+			
-+#if ( PROFlevel>=1 )
-+			TOC(t2, t1);
-+			stat[thread_id]->utime[SOL_GEMM] += t2;
-+
-+#endif	
-+
-+		}
-+
-+		// TOC(t3, t1);
-+
-+
-+
-+		thread_id1 = omp_get_thread_num ();
-+
-+
-+
-+
-+		rtemp_loc = &rtemp[sizertemp* thread_id1];
-+
-+
-+		for (lb=0;lb<nlb;lb++){
-+			lk = lloc[lb+idx_n];
-+
-+			// #ifdef _OPENMP
-+			// #pragma omp atomic capture
-+			// #endif
-+			fmod_tmp=--fmod[lk];
-+
-+
-+			if ( fmod_tmp==0 ) { /* Local accumulation done. */
-+				// --fmod[lk];
-+
-+
-+				lptr1_tmp = lloc[lb+idx_i];	
-+				// luptr_tmp = lloc[lb+idx_v];
-+
-+				ik = lsub[lptr1_tmp]; /* Global block number, row-wise. */
-+				lk = LBi( ik, grid ); /* Local block number, row-wise. */	
-+
-+				iknsupc = SuperSize( ik );
-+				il = LSUM_BLK( lk );
-+
-+				// nbrow = lsub[lptr1_tmp+1];
-+
-+				ikcol = PCOL( ik, grid );
-+				p = PNUM( myrow, ikcol, grid );
-+				if ( iam != p ) {
-+					// if(frecv[lk]==0){
-+					// fmod[lk] = -1;
-+
-+					for (ii=1;ii<num_thread;ii++)
-+						// if(ii!=thread_id1)
-+						for (jj=0;jj<iknsupc*nrhs;jj++)
-+							lsum[il + jj] += lsum[il + jj + ii*sizelsum];
-+
-+
-+					RdTree_forwardMessageSimple(LRtree_ptr[lk],&lsum[il - LSUM_H ]);
-+					// }
-+
-+
-+				} else { /* Diagonal process: X[i] += lsum[i]. */
-+
-+
-+
-+					// if ( frecv[lk]==0 ) { /* Becomes a leaf node. */
-+
-+#if ( PROFlevel>=1 )
-+					TIC(t1);
-+#endif		
-+					for (ii=1;ii<num_thread;ii++)
-+						// if(ii!=thread_id1)
-+						for (jj=0;jj<iknsupc*nrhs;jj++)
-+							lsum[il+ jj] += lsum[il + jj + ii*sizelsum];
-+
-+					ii = X_BLK( lk );
-+					// for (jj=0;jj<num_thread;jj++)
-+					RHS_ITERATE(j)
-+						for (i = 0; i < iknsupc; ++i)	
-+							x[i + ii + j*iknsupc] += lsum[i + il + j*iknsupc] ;
-+
-+
-+					// fmod[lk] = -1; /* Do not solve X[k] in the future. */
-+					lk = LBj( ik, grid );/* Local block number, column-wise. */
-+					lsub1 = Llu->Lrowind_bc_ptr[lk];
-+					lusup1 = Llu->Lnzval_bc_ptr[lk];
-+					nsupr1 = lsub1[1];
-+
-+
-+
-+
-+					if(Llu->inv == 1){
-+						Linv = Llu->Linv_bc_ptr[lk];
-+#ifdef _CRAY
-+						SGEMM( ftcs2, ftcs2, &iknsupc, &nrhs, &iknsupc,
-+								&alpha, Linv, &iknsupc, &x[ii],
-+								&iknsupc, &beta, rtemp_loc, &iknsupc );
-+#elif defined (USE_VENDOR_BLAS)
-+						dgemm_( "N", "N", &iknsupc, &nrhs, &iknsupc,
-+								&alpha, Linv, &iknsupc, &x[ii],
-+								&iknsupc, &beta, rtemp_loc, &iknsupc, 1, 1 );
-+#else
-+						dgemm_( "N", "N", &iknsupc, &nrhs, &iknsupc,
-+								&alpha, Linv, &iknsupc, &x[ii],
-+								&iknsupc, &beta, rtemp_loc, &iknsupc );
-+#endif   
-+						for (i=0 ; i<iknsupc*nrhs ; i++){
-+							x[ii+i] = rtemp_loc[i];
-+						}		
-+					}else{
-+#ifdef _CRAY
-+						STRSM(ftcs1, ftcs1, ftcs2, ftcs3, &iknsupc, &nrhs, &alpha,
-+								lusup1, &nsupr1, &x[ii], &iknsupc);
-+#elif defined (USE_VENDOR_BLAS)
-+						dtrsm_("L", "L", "N", "U", &iknsupc, &nrhs, &alpha, 
-+								lusup1, &nsupr1, &x[ii], &iknsupc, 1, 1, 1, 1);		   
-+#else
-+						dtrsm_("L", "L", "N", "U", &iknsupc, &nrhs, &alpha, 
-+								lusup1, &nsupr1, &x[ii], &iknsupc);
-+#endif
-+					}
-+
-+#if ( PROFlevel>=1 )
-+					TOC(t2, t1);
-+					stat[thread_id1]->utime[SOL_TRSM] += t2;
-+
-+#endif	
-+
-+
-+					stat[thread_id1]->ops[SOLVE] += iknsupc * (iknsupc - 1) * nrhs;
-+#if ( DEBUGlevel>=2 )
-+					printf("(%2d) Solve X[%2d]\n", iam, ik);
-+#endif
-+
-+					/*
-+					 * Send Xk to process column Pc[k].
-+					 */
-+
-+					if(LBtree_ptr[lk]!=NULL)
-+						BcTree_forwardMessageSimple(LBtree_ptr[lk],&x[ii - XK_H]);
-+
-+					/*
-+					 * Perform local block modifications.
-+					 */
-+
-+					// #ifdef _OPENMP
-+					// #pragma	omp	task firstprivate (Llu,sizelsum,iknsupc,ii,ik,lsub1,x,rtemp,fmod,lsum,send_req,stat,nrhs,grid,xsup,recurlevel) private(lptr1,luptr1,nlb1,thread_id1) untied priority(1) 	
-+					// #endif
-+					{
-+						nlb1 = lsub1[0] - 1;
-+
-+
-+						dlsum_fmod_inv_master(lsum, x, &x[ii], rtemp, nrhs, iknsupc, ik,
-+								fmod, nlb1, xsup,
-+								grid, Llu, stat,sizelsum,sizertemp,1+recurlevel);
-+					}		   
-+
-+					// } /* if frecv[lk] == 0 */
-+				} /* if iam == p */
-+			} /* if fmod[lk] == 0 */				
-+		}
-+		// }
-+
-+
-+		stat[thread_id]->ops[SOLVE] += 2 * m * nrhs * knsupc;	
-+
-+	} /* if nlb>0*/
-+} /* dlsum_fmod_inv_master */
-+
-+
-+
-+
-+/************************************************************************/
-+void dlsum_bmod_inv
-+/************************************************************************/
-+(
-+ double *lsum,        /* Sum of local modifications.                    */
-+ double *x,           /* X array (local).                               */
-+ double *xk,          /* X[k].                                          */
-+ double *rtemp,   /* Result of full matrix-vector multiply.             */
-+ int    nrhs,	      /* Number of right-hand sides.                    */
-+ int_t  k,            /* The k-th component of X.                       */
-+ int_t  *bmod,        /* Modification count for L-solve.                */
-+ int_t  *Urbs,        /* Number of row blocks in each block column of U.*/
-+ int_t  *Urbs2,
-+ Ucb_indptr_t **Ucb_indptr,/* Vertical linked list pointing to Uindex[].*/
-+ int_t  **Ucb_valptr, /* Vertical linked list pointing to Unzval[].     */
-+ int_t  *xsup,
-+ gridinfo_t *grid,
-+ LocalLU_t *Llu,
-+ MPI_Request send_req[], /* input/output */
-+ SuperLUStat_t **stat,
-+ int_t* root_send, 
-+ int_t* nroot_send, 
-+ int_t sizelsum,
-+ int_t sizertemp
-+ )
-+{
-+	/*
-+	 * Purpose
-+	 * =======
-+	 *   Perform local block modifications: lsum[i] -= U_i,k * X[k].
-+	 */
-+	double alpha = 1.0, beta = 0.0;
-+	int    iam, iknsupc, knsupc, myrow, nsupr, p, pi;
-+	int_t  fnz, gik, gikcol, i, ii, ik, ikfrow, iklrow, il, irow,
-+	       j, jj, lk, lk1, nub, ub, uptr;
-+	int_t  *usub;
-+	double *uval, *dest, *y;
-+	int_t  *lsub;
-+	double *lusup;
-+	int_t  *ilsum = Llu->ilsum; /* Starting position of each supernode in lsum.   */
-+	int_t  *brecv = Llu->brecv;
-+	int_t  **bsendx_plist = Llu->bsendx_plist;
-+	BcTree  *UBtree_ptr = Llu->UBtree_ptr;
-+	RdTree  *URtree_ptr = Llu->URtree_ptr;	
-+	MPI_Status status;
-+	int test_flag;
-+	int_t bmod_tmp;
-+	int thread_id,thread_id1,num_thread;
-+	double *rtemp_loc;
-+	int_t nroot_send_tmp;	
-+	double *Uinv;/* Inverse of diagonal block */    
-+
-+	double t1, t2;
-+	float msg_vol = 0, msg_cnt = 0;
-+	int_t Nchunk, nub_loc,remainder,nn,lbstart,lbend;  
-+	
-+#ifdef _OPENMP
-+	thread_id = omp_get_thread_num ();
-+	num_thread = omp_get_num_threads ();
-+#else
-+	thread_id = 0;
-+	num_thread = 1;
-+#endif	
-+	rtemp_loc = &rtemp[sizertemp* thread_id];
-+	
-+	
-+	iam = grid->iam;
-+	myrow = MYROW( iam, grid );
-+	knsupc = SuperSize( k );
-+	lk = LBj( k, grid ); /* Local block number, column-wise. */
-+	nub = Urbs[lk];      /* Number of U blocks in block column lk */	
-+
-+	
-+	 
-+	// printf("Urbs2[lk] %5d lk %5d nub %5d\n",Urbs2[lk],lk,nub);
-+	// fflush(stdout);
-+	
-+	if(nub>num_thread){
-+	// // // // if(Urbs2[lk]>num_thread){
-+	// if(Urbs2[lk]>0){
-+		Nchunk=num_thread;
-+		nub_loc = floor(((double)nub)/Nchunk);
-+		remainder = nub % Nchunk;
-+
-+#ifdef _OPENMP
-+#pragma	omp	taskloop firstprivate (send_req,stat) private (thread_id1,Uinv,nn,lbstart,lbend,ub,rtemp_loc,ik,lk1,gik,gikcol,usub,uval,lsub,lusup,iknsupc,il,i,irow,bmod_tmp,p,ii,jj,t1,t2,j,ikfrow,iklrow,dest,y,uptr,fnz,nsupr) untied nogroup	
-+#endif	
-+		for (nn=0;nn<Nchunk;++nn){
-+
-+#ifdef _OPENMP				 
-+			thread_id1 = omp_get_thread_num ();
-+#else
-+			thread_id1 = 0;
-+#endif		
-+			rtemp_loc = &rtemp[sizertemp* thread_id1];
-+
-+			if(nn<remainder){
-+				lbstart = nn*(nub_loc+1);
-+				lbend = (nn+1)*(nub_loc+1);
-+			}else{
-+				lbstart = remainder+nn*nub_loc;
-+				lbend = remainder + (nn+1)*nub_loc;
-+			}			
-+			for (ub = lbstart; ub < lbend; ++ub){
-+				ik = Ucb_indptr[lk][ub].lbnum; /* Local block number, row-wise. */
-+				usub = Llu->Ufstnz_br_ptr[ik];
-+				uval = Llu->Unzval_br_ptr[ik];
-+				i = Ucb_indptr[lk][ub].indpos; /* Start of the block in usub[]. */
-+				i += UB_DESCRIPTOR;
-+				il = LSUM_BLK( ik );
-+				gik = ik * grid->nprow + myrow;/* Global block number, row-wise. */
-+				iknsupc = SuperSize( gik );
-+				ikfrow = FstBlockC( gik );
-+				iklrow = FstBlockC( gik+1 );
-+
-+#if ( PROFlevel>=1 )
-+				TIC(t1);
-+#endif					
-+				
-+				RHS_ITERATE(j) {
-+					dest = &lsum[il + j*iknsupc+sizelsum*thread_id1];
-+					y = &xk[j*knsupc];
-+					uptr = Ucb_valptr[lk][ub]; /* Start of the block in uval[]. */
-+					for (jj = 0; jj < knsupc; ++jj) {
-+						fnz = usub[i + jj];
-+						if ( fnz < iklrow ) { /* Nonzero segment. */
-+							/* AXPY */
-+							for (irow = fnz; irow < iklrow; ++irow)
-+								dest[irow - ikfrow] -= uval[uptr++] * y[jj];
-+							stat[thread_id1]->ops[SOLVE] += 2 * (iklrow - fnz);
-+						}
-+					} /* for jj ... */
-+				}
-+				
-+#if ( PROFlevel>=1 )
-+				TOC(t2, t1);
-+				stat[thread_id1]->utime[SOL_GEMM] += t2;
-+#endif					
-+				
-+
-+		#ifdef _OPENMP
-+		#pragma omp atomic capture
-+		#endif		
-+				bmod_tmp=--bmod[ik];
-+				
-+				if ( bmod_tmp == 0 ) { /* Local accumulation done. */
-+					gikcol = PCOL( gik, grid );
-+					p = PNUM( myrow, gikcol, grid );
-+					if ( iam != p ) {
-+						for (ii=1;ii<num_thread;ii++)
-+							// if(ii!=thread_id1)
-+							for (jj=0;jj<iknsupc*nrhs;jj++)
-+								lsum[il + jj] += lsum[il + jj + ii*sizelsum];			
-+						
-+#ifdef _OPENMP
-+#pragma omp atomic capture
-+#endif
-+						nroot_send_tmp = ++nroot_send[0];
-+						root_send[nroot_send_tmp-1] = -ik-1;							
-+						// RdTree_forwardMessageSimple(URtree_ptr[ik],&lsum[il - LSUM_H ]);
-+
-+		#if ( DEBUGlevel>=2 )
-+						printf("(%2d) Sent LSUM[%2.0f], size %2d, to P %2d\n",
-+								iam, lsum[il-LSUM_H], iknsupc*nrhs+LSUM_H, p);
-+		#endif
-+					} else { /* Diagonal process: X[i] += lsum[i]. */
-+						
-+#if ( PROFlevel>=1 )
-+						TIC(t1);
-+#endif								
-+						
-+						for (ii=1;ii<num_thread;ii++)
-+							// if(ii!=thread_id1)
-+							for (jj=0;jj<iknsupc*nrhs;jj++)
-+								lsum[il + jj] += lsum[il + jj + ii*sizelsum];					
-+
-+						ii = X_BLK( ik );
-+						dest = &x[ii];
-+								
-+						RHS_ITERATE(j)
-+							for (i = 0; i < iknsupc; ++i)
-+								dest[i + j*iknsupc] += lsum[i + il + j*iknsupc];
-+						// if ( !brecv[ik] ) { /* Becomes a leaf node. */
-+							// bmod[ik] = -1; /* Do not solve X[k] in the future. */
-+							lk1 = LBj( gik, grid ); /* Local block number. */
-+							lsub = Llu->Lrowind_bc_ptr[lk1];
-+							lusup = Llu->Lnzval_bc_ptr[lk1];
-+							nsupr = lsub[1];
-+
-+							if(Llu->inv == 1){
-+								Uinv = Llu->Uinv_bc_ptr[lk1];  
-+		#ifdef _CRAY
-+								SGEMM( ftcs2, ftcs2, &iknsupc, &nrhs, &iknsupc,
-+										&alpha, Uinv, &iknsupc, &x[ii],
-+										&iknsupc, &beta, rtemp_loc, &iknsupc );
-+		#elif defined (USE_VENDOR_BLAS)
-+								dgemm_( "N", "N", &iknsupc, &nrhs, &iknsupc,
-+										&alpha, Uinv, &iknsupc, &x[ii],
-+										&iknsupc, &beta, rtemp_loc, &iknsupc, 1, 1 );
-+		#else
-+								dgemm_( "N", "N", &iknsupc, &nrhs, &iknsupc,
-+										&alpha, Uinv, &iknsupc, &x[ii],
-+										&iknsupc, &beta, rtemp_loc, &iknsupc );
-+		#endif	   
-+								for (i=0 ; i<iknsupc*nrhs ; i++){
-+									x[ii+i] = rtemp_loc[i];
-+								}		
-+							}else{
-+		#ifdef _CRAY
-+								STRSM(ftcs1, ftcs3, ftcs2, ftcs2, &iknsupc, &nrhs, &alpha,
-+										lusup, &nsupr, &x[ii], &iknsupc);
-+		#elif defined (USE_VENDOR_BLAS)
-+								dtrsm_("L", "U", "N", "N", &iknsupc, &nrhs, &alpha, 
-+										lusup, &nsupr, &x[ii], &iknsupc, 1, 1, 1, 1);	
-+		#else
-+								dtrsm_("L", "U", "N", "N", &iknsupc, &nrhs, &alpha, 
-+										lusup, &nsupr, &x[ii], &iknsupc);
-+		#endif
-+							}
-+					
-+		#if ( PROFlevel>=1 )
-+							TOC(t2, t1);
-+							stat[thread_id1]->utime[SOL_TRSM] += t2;
-+		#endif	
-+							stat[thread_id1]->ops[SOLVE] += iknsupc * (iknsupc + 1) * nrhs;					
-+							
-+		#if ( DEBUGlevel>=2 )
-+							printf("(%2d) Solve X[%2d]\n", iam, gik);
-+		#endif
-+
-+							/*
-+							 * Send Xk to process column Pc[k].
-+							 */
-+
-+							 // for (i=0 ; i<iknsupc*nrhs ; i++){
-+								// printf("xre: %f\n",x[ii+i]);
-+								// fflush(stdout);
-+							// }
-+							if(UBtree_ptr[lk1]!=NULL){							
-+#ifdef _OPENMP
-+#pragma omp atomic capture
-+#endif
-+							nroot_send_tmp = ++nroot_send[0];
-+							root_send[nroot_send_tmp-1] = lk1;							
-+							// BcTree_forwardMessageSimple(UBtree_ptr[lk1],&x[ii - XK_H]); 
-+							} 
-+
-+							/*
-+							 * Perform local block modifications.
-+							 */
-+							if ( Urbs[lk1] ){
-+								// #ifdef _OPENMP
-+								// #pragma	omp	task firstprivate (Ucb_indptr,Ucb_valptr,Llu,sizelsum,ii,gik,x,rtemp,bmod,Urbs,Urbs2,lsum,stat,nrhs,grid,xsup) untied 
-+								// #endif
-+								{
-+								dlsum_bmod_inv(lsum, x, &x[ii], rtemp, nrhs, gik, bmod, Urbs,Urbs2,
-+										Ucb_indptr, Ucb_valptr, xsup, grid, Llu,
-+										send_req, stat, root_send, nroot_send, sizelsum,sizertemp);
-+								}
-+							}
-+						// } /* if brecv[ik] == 0 */
-+					}
-+				} /* if bmod[ik] == 0 */				
-+			}				
-+		}
-+
-+	}else{ 
-+	
-+		thread_id1 = omp_get_thread_num ();
-+		rtemp_loc = &rtemp[sizertemp* thread_id1];
-+
-+		for (ub = 0; ub < nub; ++ub) {
-+			ik = Ucb_indptr[lk][ub].lbnum; /* Local block number, row-wise. */
-+			usub = Llu->Ufstnz_br_ptr[ik];
-+			uval = Llu->Unzval_br_ptr[ik];
-+			i = Ucb_indptr[lk][ub].indpos; /* Start of the block in usub[]. */
-+			i += UB_DESCRIPTOR;
-+			il = LSUM_BLK( ik );
-+			gik = ik * grid->nprow + myrow;/* Global block number, row-wise. */
-+			iknsupc = SuperSize( gik );
-+			ikfrow = FstBlockC( gik );
-+			iklrow = FstBlockC( gik+1 );
-+
-+#if ( PROFlevel>=1 )
-+		TIC(t1);
-+#endif					
-+			RHS_ITERATE(j) {
-+				dest = &lsum[il + j*iknsupc+sizelsum*thread_id1];
-+				y = &xk[j*knsupc];
-+				uptr = Ucb_valptr[lk][ub]; /* Start of the block in uval[]. */
-+				for (jj = 0; jj < knsupc; ++jj) {
-+					fnz = usub[i + jj];
-+					if ( fnz < iklrow ) { /* Nonzero segment. */
-+						/* AXPY */
-+						for (irow = fnz; irow < iklrow; ++irow)
-+							dest[irow - ikfrow] -= uval[uptr++] * y[jj];
-+						stat[thread_id1]->ops[SOLVE] += 2 * (iklrow - fnz);
-+					}
-+				} /* for jj ... */
-+			}
-+
-+#if ( PROFlevel>=1 )
-+		TOC(t2, t1);
-+		stat[thread_id1]->utime[SOL_GEMM] += t2;
-+#endif				
-+			
-+	#ifdef _OPENMP
-+	#pragma omp atomic capture
-+	#endif		
-+			bmod_tmp=--bmod[ik];
-+
-+			if ( bmod_tmp == 0 ) { /* Local accumulation done. */
-+				gikcol = PCOL( gik, grid );
-+				p = PNUM( myrow, gikcol, grid );
-+				if ( iam != p ) {
-+					for (ii=1;ii<num_thread;ii++)
-+						// if(ii!=thread_id1)
-+						for (jj=0;jj<iknsupc*nrhs;jj++)
-+							lsum[il + jj] += lsum[il + jj + ii*sizelsum];			
-+					
-+#ifdef _OPENMP
-+#pragma omp atomic capture
-+#endif
-+					nroot_send_tmp = ++nroot_send[0];
-+					root_send[nroot_send_tmp-1] = -ik-1;					
-+					// RdTree_forwardMessageSimple(URtree_ptr[ik],&lsum[il - LSUM_H ]);
-+
-+	#if ( DEBUGlevel>=2 )
-+					printf("(%2d) Sent LSUM[%2.0f], size %2d, to P %2d\n",
-+							iam, lsum[il-LSUM_H], iknsupc*nrhs+LSUM_H, p);
-+	#endif
-+				} else { /* Diagonal process: X[i] += lsum[i]. */
-+					
-+#if ( PROFlevel>=1 )
-+					TIC(t1);
-+#endif							
-+					
-+					for (ii=1;ii<num_thread;ii++)
-+						// if(ii!=thread_id1)
-+						for (jj=0;jj<iknsupc*nrhs;jj++)
-+							lsum[il + jj] += lsum[il + jj + ii*sizelsum];					
-+
-+					ii = X_BLK( ik );
-+					dest = &x[ii];
-+							
-+					RHS_ITERATE(j)
-+						for (i = 0; i < iknsupc; ++i)
-+							dest[i + j*iknsupc] += lsum[i + il + j*iknsupc];
-+					// if ( !brecv[ik] ) { /* Becomes a leaf node. */
-+						// bmod[ik] = -1; /* Do not solve X[k] in the future. */
-+						lk1 = LBj( gik, grid ); /* Local block number. */
-+						lsub = Llu->Lrowind_bc_ptr[lk1];
-+						lusup = Llu->Lnzval_bc_ptr[lk1];
-+						nsupr = lsub[1];
-+
-+						if(Llu->inv == 1){
-+							Uinv = Llu->Uinv_bc_ptr[lk1];  
-+	#ifdef _CRAY
-+							SGEMM( ftcs2, ftcs2, &iknsupc, &nrhs, &iknsupc,
-+									&alpha, Uinv, &iknsupc, &x[ii],
-+									&iknsupc, &beta, rtemp_loc, &iknsupc );
-+	#elif defined (USE_VENDOR_BLAS)
-+							dgemm_( "N", "N", &iknsupc, &nrhs, &iknsupc,
-+									&alpha, Uinv, &iknsupc, &x[ii],
-+									&iknsupc, &beta, rtemp_loc, &iknsupc, 1, 1 );
-+	#else
-+							dgemm_( "N", "N", &iknsupc, &nrhs, &iknsupc,
-+									&alpha, Uinv, &iknsupc, &x[ii],
-+									&iknsupc, &beta, rtemp_loc, &iknsupc );
-+	#endif	   
-+							for (i=0 ; i<iknsupc*nrhs ; i++){
-+								x[ii+i] = rtemp_loc[i];
-+							}		
-+						}else{
-+	#ifdef _CRAY
-+							STRSM(ftcs1, ftcs3, ftcs2, ftcs2, &iknsupc, &nrhs, &alpha,
-+									lusup, &nsupr, &x[ii], &iknsupc);
-+	#elif defined (USE_VENDOR_BLAS)
-+							dtrsm_("L", "U", "N", "N", &iknsupc, &nrhs, &alpha, 
-+									lusup, &nsupr, &x[ii], &iknsupc, 1, 1, 1, 1);	
-+	#else
-+							dtrsm_("L", "U", "N", "N", &iknsupc, &nrhs, &alpha, 
-+									lusup, &nsupr, &x[ii], &iknsupc);
-+	#endif
-+						}
-+				
-+	#if ( PROFlevel>=1 )
-+						TOC(t2, t1);
-+						stat[thread_id1]->utime[SOL_TRSM] += t2;
-+	#endif	
-+						stat[thread_id1]->ops[SOLVE] += iknsupc * (iknsupc + 1) * nrhs;					
-+						
-+	#if ( DEBUGlevel>=2 )
-+						printf("(%2d) Solve X[%2d]\n", iam, gik);
-+	#endif
-+
-+						/*
-+						 * Send Xk to process column Pc[k].
-+						 */
-+
-+						 // for (i=0 ; i<iknsupc*nrhs ; i++){
-+							// printf("xre: %f\n",x[ii+i]);
-+							// fflush(stdout);
-+						// }
-+						if(UBtree_ptr[lk1]!=NULL){
-+#ifdef _OPENMP
-+#pragma omp atomic capture
-+#endif
-+						nroot_send_tmp = ++nroot_send[0];
-+						root_send[nroot_send_tmp-1] = lk1;							
-+						// BcTree_forwardMessageSimple(UBtree_ptr[lk1],&x[ii - XK_H]); 
-+						} 
-+
-+						/*
-+						 * Perform local block modifications.
-+						 */
-+						if ( Urbs[lk1] )
-+						
-+							if(Urbs[lk1]>num_thread){
-+							#ifdef _OPENMP
-+							#pragma	omp	task firstprivate (Ucb_indptr,Ucb_valptr,Llu,sizelsum,ii,gik,x,rtemp,bmod,Urbs,Urbs2,lsum,stat,nrhs,grid,xsup) untied 
-+							#endif						
-+								dlsum_bmod_inv(lsum, x, &x[ii], rtemp, nrhs, gik, bmod, Urbs,Urbs2,
-+										Ucb_indptr, Ucb_valptr, xsup, grid, Llu,
-+										send_req, stat, root_send, nroot_send, sizelsum,sizertemp);
-+							}else{
-+								dlsum_bmod_inv(lsum, x, &x[ii], rtemp, nrhs, gik, bmod, Urbs,Urbs2,
-+										Ucb_indptr, Ucb_valptr, xsup, grid, Llu,
-+										send_req, stat, root_send, nroot_send, sizelsum,sizertemp);							
-+							}		
-+									
-+					// } /* if brecv[ik] == 0 */
-+				}
-+			} /* if bmod[ik] == 0 */
-+
-+		} /* for ub ... */
-+	}
-+
-+} /* dlSUM_BMOD_inv */
-+
-+
-+
-+
-+
-+
-+
-+/************************************************************************/
-+void dlsum_bmod_inv_master
-+/************************************************************************/
-+(
-+ double *lsum,        /* Sum of local modifications.                    */
-+ double *x,           /* X array (local).                               */
-+ double *xk,          /* X[k].                                          */
-+ double *rtemp,   /* Result of full matrix-vector multiply.             */
-+ int    nrhs,	      /* Number of right-hand sides.                    */
-+ int_t  k,            /* The k-th component of X.                       */
-+ int_t  *bmod,        /* Modification count for L-solve.                */
-+ int_t  *Urbs,        /* Number of row blocks in each block column of U.*/
-+ int_t  *Urbs2,
-+ Ucb_indptr_t **Ucb_indptr,/* Vertical linked list pointing to Uindex[].*/
-+ int_t  **Ucb_valptr, /* Vertical linked list pointing to Unzval[].     */
-+ int_t  *xsup,
-+ gridinfo_t *grid,
-+ LocalLU_t *Llu,
-+ MPI_Request send_req[], /* input/output */
-+ SuperLUStat_t **stat,
-+ int_t sizelsum,
-+ int_t sizertemp
-+ )
-+{
-+	/*
-+	 * Purpose
-+	 * =======
-+	 *   Perform local block modifications: lsum[i] -= U_i,k * X[k].
-+	 */
-+	double alpha = 1.0, beta = 0.0;
-+	int    iam, iknsupc, knsupc, myrow, nsupr, p, pi;
-+	int_t  fnz, gik, gikcol, i, ii, ik, ikfrow, iklrow, il, irow,
-+	       j, jj, lk, lk1, nub, ub, uptr;
-+	int_t  *usub;
-+	double *uval, *dest, *y;
-+	int_t  *lsub;
-+	double *lusup;
-+	int_t  *ilsum = Llu->ilsum; /* Starting position of each supernode in lsum.   */
-+	int_t  *brecv = Llu->brecv;
-+	int_t  **bsendx_plist = Llu->bsendx_plist;
-+	BcTree  *UBtree_ptr = Llu->UBtree_ptr;
-+	RdTree  *URtree_ptr = Llu->URtree_ptr;	
-+	MPI_Status status;
-+	int test_flag;
-+	int_t bmod_tmp;
-+	int thread_id,thread_id1,num_thread;
-+	double *rtemp_loc;
-+		
-+	double *Uinv;/* Inverse of diagonal block */    
-+
-+	double t1, t2;
-+	float msg_vol = 0, msg_cnt = 0;
-+	int_t Nchunk, nub_loc,remainder,nn,lbstart,lbend;  
-+	
-+#ifdef _OPENMP
-+	thread_id = omp_get_thread_num ();
-+	num_thread = omp_get_num_threads ();
-+#else
-+	thread_id = 0;
-+	num_thread = 1;
-+#endif	
-+	rtemp_loc = &rtemp[sizertemp* thread_id];
-+	
-+	
-+	iam = grid->iam;
-+	myrow = MYROW( iam, grid );
-+	knsupc = SuperSize( k );
-+	lk = LBj( k, grid ); /* Local block number, column-wise. */
-+	nub = Urbs[lk];      /* Number of U blocks in block column lk */	
-+
-+	
-+	 
-+	// printf("Urbs2[lk] %5d lk %5d nub %5d\n",Urbs2[lk],lk,nub);
-+	// fflush(stdout);
-+	
-+	if(nub>num_thread){
-+	// if(nub>0){
-+		Nchunk=num_thread;
-+		nub_loc = floor(((double)nub)/Nchunk);
-+		remainder = nub % Nchunk;
-+
-+#ifdef _OPENMP
-+#pragma	omp	taskloop firstprivate (send_req,stat) private (thread_id1,nn,lbstart,lbend,ub,rtemp_loc,ik,gik,usub,uval,iknsupc,il,i,irow,jj,t1,t2,j,ikfrow,iklrow,dest,y,uptr,fnz) untied	
-+#endif	
-+		for (nn=0;nn<Nchunk;++nn){
-+
-+#ifdef _OPENMP				 
-+			thread_id1 = omp_get_thread_num ();
-+#else
-+			thread_id1 = 0;
-+#endif		
-+			rtemp_loc = &rtemp[sizertemp* thread_id1];
-+
-+#if ( PROFlevel>=1 )
-+			TIC(t1);
-+#endif				
-+			
-+			if(nn<remainder){
-+				lbstart = nn*(nub_loc+1);
-+				lbend = (nn+1)*(nub_loc+1);
-+			}else{
-+				lbstart = remainder+nn*nub_loc;
-+				lbend = remainder + (nn+1)*nub_loc;
-+			}			
-+			for (ub = lbstart; ub < lbend; ++ub){
-+				ik = Ucb_indptr[lk][ub].lbnum; /* Local block number, row-wise. */
-+				usub = Llu->Ufstnz_br_ptr[ik];
-+				uval = Llu->Unzval_br_ptr[ik];
-+				i = Ucb_indptr[lk][ub].indpos; /* Start of the block in usub[]. */
-+				i += UB_DESCRIPTOR;
-+				il = LSUM_BLK( ik );
-+				gik = ik * grid->nprow + myrow;/* Global block number, row-wise. */
-+				iknsupc = SuperSize( gik );
-+				ikfrow = FstBlockC( gik );
-+				iklrow = FstBlockC( gik+1 );				
-+				
-+				RHS_ITERATE(j) {
-+					dest = &lsum[il + j*iknsupc+sizelsum*thread_id1];
-+					y = &xk[j*knsupc];
-+					uptr = Ucb_valptr[lk][ub]; /* Start of the block in uval[]. */
-+					for (jj = 0; jj < knsupc; ++jj) {
-+						fnz = usub[i + jj];
-+						if ( fnz < iklrow ) { /* Nonzero segment. */
-+							/* AXPY */
-+							for (irow = fnz; irow < iklrow; ++irow)
-+								dest[irow - ikfrow] -= uval[uptr++] * y[jj];
-+							stat[thread_id1]->ops[SOLVE] += 2 * (iklrow - fnz);
-+						}
-+					} /* for jj ... */
-+				}
-+			}
-+#if ( PROFlevel>=1 )
-+			TOC(t2, t1);
-+			stat[thread_id1]->utime[SOL_GEMM] += t2;
-+#endif	
-+		}
-+				
-+	}else{
-+#ifdef _OPENMP				 
-+		thread_id1 = omp_get_thread_num ();
-+#else
-+		thread_id1 = 0;
-+#endif	
-+		rtemp_loc = &rtemp[sizertemp* thread_id1];
-+#if ( PROFlevel>=1 )
-+		TIC(t1);
-+#endif	
-+		for (ub = 0; ub < nub; ++ub) {
-+			ik = Ucb_indptr[lk][ub].lbnum; /* Local block number, row-wise. */
-+			usub = Llu->Ufstnz_br_ptr[ik];
-+			uval = Llu->Unzval_br_ptr[ik];
-+			i = Ucb_indptr[lk][ub].indpos; /* Start of the block in usub[]. */
-+			i += UB_DESCRIPTOR;
-+			il = LSUM_BLK( ik );
-+			gik = ik * grid->nprow + myrow;/* Global block number, row-wise. */
-+			iknsupc = SuperSize( gik );
-+			ikfrow = FstBlockC( gik );
-+			iklrow = FstBlockC( gik+1 );
-+				
-+			RHS_ITERATE(j) {
-+				dest = &lsum[il + j*iknsupc+sizelsum*thread_id1];
-+				y = &xk[j*knsupc];
-+				uptr = Ucb_valptr[lk][ub]; /* Start of the block in uval[]. */
-+				for (jj = 0; jj < knsupc; ++jj) {
-+					fnz = usub[i + jj];
-+					if ( fnz < iklrow ) { /* Nonzero segment. */
-+						/* AXPY */
-+						for (irow = fnz; irow < iklrow; ++irow)
-+							dest[irow - ikfrow] -= uval[uptr++] * y[jj];
-+						stat[thread_id1]->ops[SOLVE] += 2 * (iklrow - fnz);
-+					}
-+				} /* for jj ... */
-+			}			
-+		}	
-+#if ( PROFlevel>=1 )
-+		TOC(t2, t1);
-+		stat[thread_id1]->utime[SOL_GEMM] += t2;
-+#endif				
-+	}
-+
-+	
-+	
-+#ifdef _OPENMP				 
-+	thread_id1 = omp_get_thread_num ();
-+#else
-+	thread_id1 = 0;
-+#endif	
-+	rtemp_loc = &rtemp[sizertemp* thread_id1];	
-+	for (ub = 0; ub < nub; ++ub){
-+		ik = Ucb_indptr[lk][ub].lbnum; /* Local block number, row-wise. */
-+		il = LSUM_BLK( ik );
-+		gik = ik * grid->nprow + myrow;/* Global block number, row-wise. */
-+		iknsupc = SuperSize( gik );
-+
-+	// #ifdef _OPENMP
-+	// #pragma omp atomic capture
-+	// #endif		
-+		bmod_tmp=--bmod[ik];
-+		
-+		if ( bmod_tmp == 0 ) { /* Local accumulation done. */
-+			gikcol = PCOL( gik, grid );
-+			p = PNUM( myrow, gikcol, grid );
-+			if ( iam != p ) {
-+				for (ii=1;ii<num_thread;ii++)
-+					// if(ii!=thread_id1)
-+					for (jj=0;jj<iknsupc*nrhs;jj++)
-+						lsum[il + jj] += lsum[il + jj + ii*sizelsum];			
-+				RdTree_forwardMessageSimple(URtree_ptr[ik],&lsum[il - LSUM_H ]);
-+
-+#if ( DEBUGlevel>=2 )
-+				printf("(%2d) Sent LSUM[%2.0f], size %2d, to P %2d\n",
-+						iam, lsum[il-LSUM_H], iknsupc*nrhs+LSUM_H, p);
-+#endif
-+			} else { /* Diagonal process: X[i] += lsum[i]. */
-+				
-+#if ( PROFlevel>=1 )
-+				TIC(t1);
-+#endif								
-+				for (ii=1;ii<num_thread;ii++)
-+					// if(ii!=thread_id1)
-+					for (jj=0;jj<iknsupc*nrhs;jj++)
-+						lsum[il + jj] += lsum[il + jj + ii*sizelsum];					
-+
-+				ii = X_BLK( ik );
-+				dest = &x[ii];
-+						
-+				RHS_ITERATE(j)
-+					for (i = 0; i < iknsupc; ++i)
-+						dest[i + j*iknsupc] += lsum[i + il + j*iknsupc];
-+				// if ( !brecv[ik] ) { /* Becomes a leaf node. */
-+					// bmod[ik] = -1; /* Do not solve X[k] in the future. */
-+					lk1 = LBj( gik, grid ); /* Local block number. */
-+					lsub = Llu->Lrowind_bc_ptr[lk1];
-+					lusup = Llu->Lnzval_bc_ptr[lk1];
-+					nsupr = lsub[1];
-+
-+					if(Llu->inv == 1){
-+						Uinv = Llu->Uinv_bc_ptr[lk1];  
-+#ifdef _CRAY
-+						SGEMM( ftcs2, ftcs2, &iknsupc, &nrhs, &iknsupc,
-+								&alpha, Uinv, &iknsupc, &x[ii],
-+								&iknsupc, &beta, rtemp_loc, &iknsupc );
-+#elif defined (USE_VENDOR_BLAS)
-+						dgemm_( "N", "N", &iknsupc, &nrhs, &iknsupc,
-+								&alpha, Uinv, &iknsupc, &x[ii],
-+								&iknsupc, &beta, rtemp_loc, &iknsupc, 1, 1 );
-+#else
-+						dgemm_( "N", "N", &iknsupc, &nrhs, &iknsupc,
-+								&alpha, Uinv, &iknsupc, &x[ii],
-+								&iknsupc, &beta, rtemp_loc, &iknsupc );
-+#endif	   
-+						for (i=0 ; i<iknsupc*nrhs ; i++){
-+							x[ii+i] = rtemp_loc[i];
-+						}		
-+					}else{
-+#ifdef _CRAY
-+						STRSM(ftcs1, ftcs3, ftcs2, ftcs2, &iknsupc, &nrhs, &alpha,
-+								lusup, &nsupr, &x[ii], &iknsupc);
-+#elif defined (USE_VENDOR_BLAS)
-+						dtrsm_("L", "U", "N", "N", &iknsupc, &nrhs, &alpha, 
-+								lusup, &nsupr, &x[ii], &iknsupc, 1, 1, 1, 1);	
-+#else
-+						dtrsm_("L", "U", "N", "N", &iknsupc, &nrhs, &alpha, 
-+								lusup, &nsupr, &x[ii], &iknsupc);
-+#endif
-+					}
-+			
-+#if ( PROFlevel>=1 )
-+					TOC(t2, t1);
-+					stat[thread_id1]->utime[SOL_TRSM] += t2;
-+#endif	
-+					stat[thread_id1]->ops[SOLVE] += iknsupc * (iknsupc + 1) * nrhs;					
-+					
-+#if ( DEBUGlevel>=2 )
-+					printf("(%2d) Solve X[%2d]\n", iam, gik);
-+#endif
-+
-+					/*
-+					 * Send Xk to process column Pc[k].
-+					 */
-+
-+					 // for (i=0 ; i<iknsupc*nrhs ; i++){
-+						// printf("xre: %f\n",x[ii+i]);
-+						// fflush(stdout);
-+					// }
-+					if(UBtree_ptr[lk1]!=NULL){
-+					BcTree_forwardMessageSimple(UBtree_ptr[lk1],&x[ii - XK_H]); 
-+					} 
-+
-+					/*
-+					 * Perform local block modifications.
-+					 */
-+					if ( Urbs[lk1] ){
-+						// #ifdef _OPENMP
-+						// #pragma	omp	task firstprivate (Ucb_indptr,Ucb_valptr,Llu,sizelsum,ii,gik,x,rtemp,bmod,Urbs,Urbs2,lsum,stat,nrhs,grid,xsup) untied 
-+						// #endif
-+						{
-+						dlsum_bmod_inv_master(lsum, x, &x[ii], rtemp, nrhs, gik, bmod, Urbs,Urbs2,
-+								Ucb_indptr, Ucb_valptr, xsup, grid, Llu,
-+								send_req, stat, sizelsum,sizertemp);
-+						}
-+					}
-+				// } /* if brecv[ik] == 0 */
-+			}
-+		} /* if bmod[ik] == 0 */		
-+	}	
-+	
-+} /* dlsum_bmod_inv_master */
-+
-+
-+
-+
-diff --git a/SRC/pdsymbfact_distdata.c b/SRC/pdsymbfact_distdata.c
-index c301c81..a66189b 100644
---- a/SRC/pdsymbfact_distdata.c
-+++ b/SRC/pdsymbfact_distdata.c
-@@ -32,6 +32,10 @@ at the top-level directory.
- #include "superlu_ddefs.h"
- #include "psymbfact.h"
- 
-+#ifndef CACHELINE
-+#define CACHELINE 64  /* bytes, Xeon Phi KNL, Cori haswell, Edision */
-+#endif
-+
- /*! \brief
-  *
-  * <pre>
-@@ -1187,19 +1191,21 @@ float
- ddist_psymbtonum(fact_t fact, int_t n, SuperMatrix *A,
- 		ScalePermstruct_t *ScalePermstruct,
- 		Pslu_freeable_t *Pslu_freeable, 
--		LUstruct_t *LUstruct, gridinfo_t *grid)
-+		LUstruct_t *LUstruct, gridinfo_t *grid, int_t nrhs)
- {
-   Glu_persist_t *Glu_persist = LUstruct->Glu_persist;
-   Glu_freeable_t Glu_freeable_n;
-   LocalLU_t *Llu = LUstruct->Llu;
--  int_t bnnz, fsupc, i, irow, istart, j, jb, jj, k, 
-+  int_t bnnz, fsupc, i, irow, istart, j, jb,ib, jj, k, k1,  
-     len, len1, nsupc, nsupc_gb, ii, nprocs;
-+  int_t lib;  /* local block row number */
-+  int_t nlb;  /* local block rows*/  
-   int_t ljb;  /* local block column number */
-   int_t nrbl; /* number of L blocks in current block column */
-   int_t nrbu; /* number of U blocks in current block column */
-   int_t gb;   /* global block number; 0 < gb <= nsuper */
-   int_t lb;   /* local block number; 0 < lb <= ceil(NSUPERS/Pr) */
--  int iam, jbrow, jbcol, jcol, kcol, mycol, myrow, pc, pr, ljb_i, ljb_j, p;
-+  int iam, jbrow, jbcol, jcol, kcol, krow, mycol, myrow, pc, pr, ljb_i, ljb_j, p;
-   int_t mybufmax[NBUFFERS];
-   NRformat_loc *Astore;
-   double *a;
-@@ -1207,7 +1213,7 @@ ddist_psymbtonum(fact_t fact, int_t n, SuperMatrix *A,
-   int_t *ainf_colptr, *ainf_rowind, *asup_rowptr, *asup_colind;
-   double *asup_val, *ainf_val;
-   int_t *xsup, *supno;    /* supernode and column mapping */
--  int_t *lsub, *xlsub, *usub, *xusub;
-+  int_t *lsub, *xlsub, *usub, *usub1, *xusub;
-   int_t nsupers, nsupers_i, nsupers_j, nsupers_ij;
-   int_t next_ind;      /* next available position in index[*] */
-   int_t next_val;      /* next available position in nzval[*] */
-@@ -1217,10 +1223,25 @@ ddist_psymbtonum(fact_t fact, int_t n, SuperMatrix *A,
-   int_t *recvBuf;
-   int *ptrToRecv, *nnzToRecv, *ptrToSend, *nnzToSend;
-   double **Lnzval_bc_ptr;  /* size ceil(NSUPERS/Pc) */
-+  double **Linv_bc_ptr;  /* size ceil(NSUPERS/Pc) */
-+  double **Uinv_bc_ptr;  /* size ceil(NSUPERS/Pc) */
-   int_t  **Lrowind_bc_ptr; /* size ceil(NSUPERS/Pc) */
-+  int_t   **Lindval_loc_bc_ptr; /* size ceil(NSUPERS/Pc)                 */	 
-+  int_t *index_srt;         /* indices consist of headers and row subscripts */	
-+  double *lusup_srt; /* nonzero values in L and U */  
-   double **Unzval_br_ptr;  /* size ceil(NSUPERS/Pr) */
-   int_t  **Ufstnz_br_ptr;  /* size ceil(NSUPERS/Pr) */
--  
-+
-+  BcTree  *LBtree_ptr;       /* size ceil(NSUPERS/Pc)                */
-+  RdTree  *LRtree_ptr;		  /* size ceil(NSUPERS/Pr)                */
-+  BcTree  *UBtree_ptr;       /* size ceil(NSUPERS/Pc)                */
-+  RdTree  *URtree_ptr;		  /* size ceil(NSUPERS/Pr)                */	
-+  int msgsize;
-+
-+  int_t  *Urbs,*Urbs1; /* Number of row blocks in each block column of U. */
-+  Ucb_indptr_t **Ucb_indptr;/* Vertical linked list pointing to Uindex[] */
-+  int_t  **Ucb_valptr;      /* Vertical linked list pointing to Unzval[] */  
-+
-   /*-- Counts to be used in factorization. --*/
-   int  *ToRecv, *ToSendD, **ToSendR;
-   
-@@ -1247,10 +1268,8 @@ ddist_psymbtonum(fact_t fact, int_t n, SuperMatrix *A,
-   int_t *LUb_number; /* global block number; size nsupers_ij */
-   int_t *LUb_valptr; /* pointers to U nzval[]; size ceil(NSUPERS/Pc)      */
-   int_t *Lrb_marker;  /* block hit marker; size ceil(NSUPERS/Pr)           */
--  double *dense, *dense_col; /* SPA */
--  double zero = 0.0;
--  int_t ldaspa;     /* LDA of SPA */
--  int_t iword, dword;
-+
-+
-   float memStrLU, memA,
-         memDist = 0.; /* memory used for redistributing the data, which does
- 		         not include the memory for the numerical values
-@@ -1258,10 +1277,35 @@ ddist_psymbtonum(fact_t fact, int_t n, SuperMatrix *A,
-   float  memNLU = 0.; /* memory allocated for storing the numerical values of 
- 		         L and U, that will be used in the numeric
-                          factorization (positive number) */
--
-+  int_t *ActiveFlag;
-+  int_t *ActiveFlagAll;
-+  int_t Iactive;
-+  int *ranks;
-+  int_t *idxs;
-+  int_t **nzrows;
-+  double rseed;
-+  int rank_cnt,rank_cnt_ref,Root;
-+  double *dense, *dense_col; /* SPA */
-+  double zero = 0.0;
-+  int_t ldaspa;     /* LDA of SPA */
-+  int_t iword, dword;
-+  float mem_use = 0.0;
-+  int_t *mod_bit;
-+  int_t *frecv, *brecv, *lloc; 
-+  double *SeedSTD_BC,*SeedSTD_RD;				 
-+  int_t idx_indx,idx_lusup;
-+  int_t nbrow;
-+  int_t  ik, il, lk, rel, knsupc, idx_r;
-+  int_t  lptr1_tmp, idx_i, idx_v,m, uu, aln_i;	
-+  int_t	nub;
-+	
- #if ( PRNTlevel>=1 )
-   int_t nLblocks = 0, nUblocks = 0;
- #endif
-+#if ( PROFlevel>=1 ) 
-+	double t, t_u, t_l;
-+	int_t u_blks;
-+#endif
-   
-   /* Initialization. */
-   iam = grid->iam;
-@@ -1277,6 +1321,8 @@ ddist_psymbtonum(fact_t fact, int_t n, SuperMatrix *A,
-   iword = sizeof(int_t);
-   dword = sizeof(double);
- 
-+  aln_i = ceil(CACHELINE/(double)iword); 
-+  
-   if (fact == SamePattern_SameRowPerm) {
-     ABORT ("ERROR: call of dist_psymbtonum with fact equals SamePattern_SameRowPerm.");  
-   }
-@@ -1441,13 +1487,33 @@ ddist_psymbtonum(fact_t fact, int_t n, SuperMatrix *A,
-     fprintf(stderr, "Malloc fails for Lnzval_bc_ptr[].");
-     return (memDist + memNLU);
-   }
-+  if ( !(Linv_bc_ptr = 
-+	 (double**)SUPERLU_MALLOC(nsupers_j * sizeof(double*))) ) {
-+    fprintf(stderr, "Malloc fails for Linv_bc_ptr[].");
-+    return (memDist + memNLU);
-+  }  
-+  if ( !(Uinv_bc_ptr = 
-+	 (double**)SUPERLU_MALLOC(nsupers_j * sizeof(double*))) ) {
-+    fprintf(stderr, "Malloc fails for Uinv_bc_ptr[].");
-+    return (memDist + memNLU);
-+  }  
-+  
-   if ( !(Lrowind_bc_ptr = (int_t**)SUPERLU_MALLOC(nsupers_j * sizeof(int_t*))) ) {
-     fprintf(stderr, "Malloc fails for Lrowind_bc_ptr[].");
-     return (memDist + memNLU);
-   }
--  memNLU += nsupers_j * sizeof(double*) + nsupers_j * sizeof(int_t*);
-+  
-+  if ( !(Lindval_loc_bc_ptr = (int_t**)SUPERLU_MALLOC(nsupers_j * sizeof(int_t*))) ){
-+    fprintf(stderr, "Malloc fails for Lindval_loc_bc_ptr[].");
-+    return (memDist + memNLU);
-+  }
-+  
-+  memNLU += nsupers_j * sizeof(double*) + nsupers_j * sizeof(int_t*)+ nsupers_j * sizeof(int_t*);
-   Lnzval_bc_ptr[nsupers_j-1] = NULL;
-+  Linv_bc_ptr[nsupers_j-1] = NULL;
-+  Uinv_bc_ptr[nsupers_j-1] = NULL;
-   Lrowind_bc_ptr[nsupers_j-1] = NULL;
-+  Lindval_loc_bc_ptr[nsupers_j-1] = NULL;  
-   
-   /* These lists of processes will be used for triangular solves. */
-   if ( !(fsendx_plist = (int_t **) SUPERLU_MALLOC(nsupers_j*sizeof(int_t*))) ) {
-@@ -1735,7 +1801,24 @@ ddist_psymbtonum(fact_t fact, int_t n, SuperMatrix *A,
- 	  fprintf(stderr, "Malloc fails for Lnzval_bc_ptr[*][] col block " IFMT, jb);
- 	  return (memDist + memNLU);
- 	}
-+
-+	if (!(Linv_bc_ptr[ljb_j] = 
-+	      doubleCalloc_dist(nsupc*nsupc))) {
-+	  fprintf(stderr, "Malloc fails for Linv_bc_ptr[*][] col block " IFMT, jb);
-+	  return (memDist + memNLU);
-+	}
-+	if (!(Uinv_bc_ptr[ljb_j] = 
-+	      doubleCalloc_dist(nsupc*nsupc))) {
-+	  fprintf(stderr, "Malloc fails for Uinv_bc_ptr[*][] col block " IFMT, jb);
-+	  return (memDist + memNLU);
-+	}
-+
- 	memNLU += len1*iword + len*nsupc*dword;
-+
-+	if ( !(Lindval_loc_bc_ptr[ljb_j] = intCalloc_dist(((nrbl*3 + (aln_i - 1)) / aln_i) * aln_i)) ) 
-+		ABORT("Malloc fails for Lindval_loc_bc_ptr[ljb_j][]");
-+
-+
- 	
- 	lusup = Lnzval_bc_ptr[ljb_j];
- 	mybufmax[0] = SUPERLU_MAX( mybufmax[0], len1 );
-@@ -1749,6 +1832,11 @@ ddist_psymbtonum(fact_t fact, int_t n, SuperMatrix *A,
- 	  gb = LUb_number[k];
- 	  lb = LBi( gb, grid );
- 	  len = LUb_length[lb];
-+	  
-+	  Lindval_loc_bc_ptr[ljb_j][k] = lb;
-+	  Lindval_loc_bc_ptr[ljb_j][k+nrbl] = next_ind;
-+	  Lindval_loc_bc_ptr[ljb_j][k+nrbl*2] = next_val;			  
-+	  
- 	  LUb_length[lb] = 0;
- 	  index[next_ind++] = gb; /* Descriptor */
- 	  index[next_ind++] = len; 
-@@ -1777,9 +1865,65 @@ ddist_psymbtonum(fact_t fact, int_t n, SuperMatrix *A,
- 	      }
- 	    }
- 	  } /* for i ... */
-+	  
-+
-+		/* sort Lindval_loc_bc_ptr[ljb_j], Lrowind_bc_ptr[ljb_j] and Lnzval_bc_ptr[ljb_j] here*/
-+		if(nrbl>1){
-+			krow = PROW( jb, grid );
-+			if(myrow==krow){ /* skip the diagonal block */
-+				uu=nrbl-2;
-+				lloc = &Lindval_loc_bc_ptr[ljb_j][1];
-+			}else{
-+				uu=nrbl-1;	
-+				lloc = Lindval_loc_bc_ptr[ljb_j];
-+			}	
-+			quickSortM(lloc,0,uu,nrbl,0,3);	
-+		}
-+
-+
-+		if ( !(index_srt = intMalloc_dist(len1)) ) 
-+			ABORT("Malloc fails for index_srt[]");				
-+		if (!(lusup_srt = doubleMalloc_dist(len*nsupc))) 
-+			ABORT("Malloc fails for lusup_srt[]");
-+
-+		idx_indx = BC_HEADER;
-+		idx_lusup = 0;
-+		for (jj=0;jj<BC_HEADER;jj++)
-+			index_srt[jj] = index[jj];
-+
-+		for(i=0;i<nrbl;i++){
-+			nbrow = index[Lindval_loc_bc_ptr[ljb_j][i+nrbl]+1];
-+			for (jj=0;jj<LB_DESCRIPTOR+nbrow;jj++){
-+				index_srt[idx_indx++] = index[Lindval_loc_bc_ptr[ljb_j][i+nrbl]+jj];
-+			}
-+
-+			Lindval_loc_bc_ptr[ljb_j][i+nrbl] = idx_indx - LB_DESCRIPTOR - nbrow; 
-+
-+			for (jj=0;jj<nbrow;jj++){
-+				k=idx_lusup;
-+				k1=Lindval_loc_bc_ptr[ljb_j][i+nrbl*2]+jj;
-+				for (j = 0; j < nsupc; ++j) {				
-+					lusup_srt[k] = lusup[k1];
-+					k += len;
-+					k1 += len;
-+				}	
-+				idx_lusup++;
-+			}				
-+			Lindval_loc_bc_ptr[ljb_j][i+nrbl*2] = idx_lusup - nbrow;	
-+		}
-+
-+		SUPERLU_FREE(lusup);
-+		SUPERLU_FREE(index);
-+
-+		Lrowind_bc_ptr[ljb_j] = index_srt;
-+		Lnzval_bc_ptr[ljb_j] = lusup_srt; 			
-+	  
- 	} else {
- 	  Lrowind_bc_ptr[ljb_j] = NULL;
- 	  Lnzval_bc_ptr[ljb_j] = NULL;
-+	  Linv_bc_ptr[ljb_j] = NULL;
-+	  Uinv_bc_ptr[ljb_j] = NULL;
-+	  Lindval_loc_bc_ptr[ljb_j] = NULL;
- 	} /* if nrbl ... */		  
-       } /* if mycol == pc */
-   } /* for jb ... */
-@@ -1792,14 +1936,7 @@ ddist_psymbtonum(fact_t fact, int_t n, SuperMatrix *A,
-   SUPERLU_FREE(LUb_valptr);
-   SUPERLU_FREE(Lrb_marker);
-   SUPERLU_FREE(dense);
--  
--  /* Free the memory used for storing L and U */
--  SUPERLU_FREE(xlsub); SUPERLU_FREE(xusub);
--  if (lsub != NULL)
--    SUPERLU_FREE(lsub);  
--  if (usub != NULL)
--    SUPERLU_FREE(usub);
--  
-+
-   /* Free the memory used for storing A */
-   SUPERLU_FREE(ainf_colptr);
-   if (ainf_rowind != NULL) {
-@@ -1924,6 +2061,699 @@ ddist_psymbtonum(fact_t fact, int_t n, SuperMatrix *A,
-     }
-   }
-   
-+
-+
-+
-+			/////////////////////////////////////////////////////////////////
-+			
-+			/* Set up additional pointers for the index and value arrays of U.
-+			   nub is the number of local block columns. */
-+			nub = CEILING( nsupers, grid->npcol); /* Number of local block columns. */
-+			if ( !(Urbs = (int_t *) intCalloc_dist(2*nub)) )
-+				ABORT("Malloc fails for Urbs[]"); /* Record number of nonzero
-+									 blocks in a block column. */
-+			Urbs1 = Urbs + nub;
-+			if ( !(Ucb_indptr = SUPERLU_MALLOC(nub * sizeof(Ucb_indptr_t *))) )
-+				ABORT("Malloc fails for Ucb_indptr[]");
-+			if ( !(Ucb_valptr = SUPERLU_MALLOC(nub * sizeof(int_t *))) )
-+				ABORT("Malloc fails for Ucb_valptr[]");
-+			nlb = CEILING( nsupers, grid->nprow ); /* Number of local block rows. */
-+
-+			/* Count number of row blocks in a block column. 
-+			   One pass of the skeleton graph of U. */
-+			for (lk = 0; lk < nlb; ++lk) {
-+				usub1 = Ufstnz_br_ptr[lk];
-+				if ( usub1 ) { /* Not an empty block row. */
-+					/* usub1[0] -- number of column blocks in this block row. */
-+					i = BR_HEADER; /* Pointer in index array. */
-+					for (lb = 0; lb < usub1[0]; ++lb) { /* For all column blocks. */
-+						k = usub1[i];            /* Global block number */
-+						++Urbs[LBj(k,grid)];
-+						i += UB_DESCRIPTOR + SuperSize( k );
-+					}
-+				}
-+			}
-+
-+			/* Set up the vertical linked lists for the row blocks.
-+			   One pass of the skeleton graph of U. */
-+			for (lb = 0; lb < nub; ++lb) {
-+				if ( Urbs[lb] ) { /* Not an empty block column. */
-+					if ( !(Ucb_indptr[lb]
-+								= SUPERLU_MALLOC(Urbs[lb] * sizeof(Ucb_indptr_t))) )
-+						ABORT("Malloc fails for Ucb_indptr[lb][]");
-+					if ( !(Ucb_valptr[lb] = (int_t *) intMalloc_dist(Urbs[lb])) )
-+						ABORT("Malloc fails for Ucb_valptr[lb][]");
-+				}
-+			}
-+			for (lk = 0; lk < nlb; ++lk) { /* For each block row. */
-+				usub1 = Ufstnz_br_ptr[lk];
-+				if ( usub1 ) { /* Not an empty block row. */
-+					i = BR_HEADER; /* Pointer in index array. */
-+					j = 0;         /* Pointer in nzval array. */
-+
-+					for (lb = 0; lb < usub1[0]; ++lb) { /* For all column blocks. */
-+						k = usub1[i];          /* Global block number, column-wise. */
-+						ljb = LBj( k, grid ); /* Local block number, column-wise. */
-+						Ucb_indptr[ljb][Urbs1[ljb]].lbnum = lk;
-+
-+						Ucb_indptr[ljb][Urbs1[ljb]].indpos = i;
-+						Ucb_valptr[ljb][Urbs1[ljb]] = j;
-+						
-+						++Urbs1[ljb];
-+						j += usub1[i+1];
-+						i += UB_DESCRIPTOR + SuperSize( k );
-+					}
-+				}
-+			}			
-+			
-+			
-+			
-+			
-+			/////////////////////////////////////////////////////////////////
-+
-+			// if(LSUM<nsupers)ABORT("Need increase LSUM."); /* temporary*/
-+
-+#if ( PROFlevel>=1 )
-+				t = SuperLU_timer_();
-+#endif				
-+			/* construct the Bcast tree for L ... */
-+
-+			k = CEILING( nsupers, grid->npcol );/* Number of local block columns */
-+			if ( !(LBtree_ptr = (BcTree*)SUPERLU_MALLOC(k * sizeof(BcTree))) )
-+				ABORT("Malloc fails for LBtree_ptr[].");
-+			if ( !(ActiveFlag = intCalloc_dist(grid->nprow*2)) )
-+				ABORT("Calloc fails for ActiveFlag[].");	
-+			if ( !(ranks = (int*)SUPERLU_MALLOC(grid->nprow * sizeof(int))) )
-+				ABORT("Malloc fails for ranks[].");	
-+			if ( !(SeedSTD_BC = (double*)SUPERLU_MALLOC(k * sizeof(double))) )
-+				ABORT("Malloc fails for SeedSTD_BC[].");	
-+
-+			for (i=0;i<k;i++){
-+				SeedSTD_BC[i]=rand();		
-+			}
-+
-+			MPI_Allreduce(MPI_IN_PLACE,&SeedSTD_BC[0],k,MPI_DOUBLE,MPI_MAX,grid->cscp.comm);					  
-+
-+			for (ljb = 0; ljb <k ; ++ljb) {
-+				LBtree_ptr[ljb]=NULL;
-+			}			
-+			
-+
-+			if ( !(ActiveFlagAll = intMalloc_dist(grid->nprow*k)) )
-+				ABORT("Calloc fails for ActiveFlag[].");				
-+			for (j=0;j<grid->nprow*k;++j)ActiveFlagAll[j]=3*nsupers;	
-+			for (ljb = 0; ljb < k; ++ljb) { /* for each local block column ... */
-+				jb = mycol+ljb*grid->npcol;  /* not sure */
-+				if(jb<nsupers){
-+				pc = PCOL( jb, grid );
-+				
-+				istart = xlsub[ljb];
-+				for (i = istart; i < xlsub[ljb+1]; ++i) {
-+					irow = lsub[i];
-+					gb = BlockNum( irow );
-+					pr = PROW( gb, grid );
-+					ActiveFlagAll[pr+ljb*grid->nprow]=MIN(ActiveFlagAll[pr+ljb*grid->nprow],gb);
-+				} /* for j ... */
-+				}
-+			}			
-+
-+			
-+			MPI_Allreduce(MPI_IN_PLACE,ActiveFlagAll,grid->nprow*k,mpi_int_t,MPI_MIN,grid->cscp.comm);					  
-+			
-+			
-+			
-+			for (ljb = 0; ljb < k; ++ljb) { /* for each local block column ... */
-+				
-+				jb = mycol+ljb*grid->npcol;  /* not sure */
-+				if(jb<nsupers){
-+				pc = PCOL( jb, grid );
-+
-+				for (j=0;j<grid->nprow;++j)ActiveFlag[j]=ActiveFlagAll[j+ljb*grid->nprow];
-+				for (j=0;j<grid->nprow;++j)ActiveFlag[j+grid->nprow]=j;
-+				for (j=0;j<grid->nprow;++j)ranks[j]=-1;
-+
-+				Root=-1; 
-+				Iactive = 0;				
-+				for (j=0;j<grid->nprow;++j){
-+					if(ActiveFlag[j]!=3*nsupers){
-+					gb = ActiveFlag[j];
-+					pr = PROW( gb, grid );
-+					if(gb==jb)Root=pr;
-+					if(myrow==pr)Iactive=1;		
-+					}					
-+				}
-+				
-+
-+				quickSortM(ActiveFlag,0,grid->nprow-1,grid->nprow,0,2);	
-+
-+				if(Iactive==1){
-+					// printf("jb %5d damn\n",jb);
-+					// fflush(stdout);
-+					assert( Root>-1 );
-+					rank_cnt = 1;
-+					ranks[0]=Root;
-+					for (j = 0; j < grid->nprow; ++j){
-+						if(ActiveFlag[j]!=3*nsupers && ActiveFlag[j+grid->nprow]!=Root){
-+							ranks[rank_cnt]=ActiveFlag[j+grid->nprow];
-+							++rank_cnt;
-+						}
-+					}		
-+
-+					if(rank_cnt>1){
-+
-+						for (ii=0;ii<rank_cnt;ii++)   // use global ranks rather than local ranks
-+							ranks[ii] = PNUM( ranks[ii], pc, grid );
-+
-+						// rseed=rand();
-+						// rseed=1.0;
-+						msgsize = SuperSize( jb )*nrhs+XK_H;
-+						LBtree_ptr[ljb] = BcTree_Create(grid->comm, ranks, rank_cnt, msgsize,SeedSTD_BC[ljb]);  	
-+						BcTree_SetTag(LBtree_ptr[ljb],BC_L);
-+
-+						// printf("iam %5d btree rank_cnt %5d \n",iam,rank_cnt);
-+						// fflush(stdout);
-+
-+						// if(iam==15 || iam==3){
-+						// printf("iam %5d btree lk %5d tag %5d root %5d\n",iam, ljb,jb,BcTree_IsRoot(LBtree_ptr[ljb]));
-+						// fflush(stdout);
-+						// }
-+
-+						// #if ( PRNTlevel>=1 )		
-+						if(Root==myrow){
-+							rank_cnt_ref=1;
-+							for (j = 0; j < grid->nprow; ++j) {
-+								if ( fsendx_plist[ljb][j] != EMPTY ) {	
-+									++rank_cnt_ref;		
-+								}
-+							}
-+							assert(rank_cnt==rank_cnt_ref);		
-+
-+							// printf("Partial Bcast Procs: col%7d np%4d\n",jb,rank_cnt);
-+
-+							// // printf("Partial Bcast Procs: %4d %4d: ",iam, rank_cnt);
-+							// // for(j=0;j<rank_cnt;++j)printf("%4d",ranks[j]);
-+							// // printf("\n");
-+						}
-+						// #endif
-+					}	
-+				}
-+				}
-+			}
-+
-+			
-+			SUPERLU_FREE(ActiveFlag);
-+			SUPERLU_FREE(ActiveFlagAll);
-+			SUPERLU_FREE(ranks);
-+			SUPERLU_FREE(SeedSTD_BC);
-+			
-+			
-+#if ( PROFlevel>=1 )
-+		t = SuperLU_timer_() - t;
-+		if ( !iam) printf(".. Construct Bcast tree for L: %.2f\t\n", t);
-+#endif			
-+	
-+
-+#if ( PROFlevel>=1 )
-+				t = SuperLU_timer_();
-+#endif			
-+			/* construct the Reduce tree for L ... */
-+			/* the following is used as reference */
-+			nlb = CEILING( nsupers, grid->nprow );/* Number of local block rows */
-+			if ( !(mod_bit = intMalloc_dist(nlb)) )
-+				ABORT("Malloc fails for mod_bit[].");
-+			if ( !(frecv = intMalloc_dist(nlb)) )
-+				ABORT("Malloc fails for frecv[].");
-+
-+			for (k = 0; k < nlb; ++k) mod_bit[k] = 0;
-+			for (k = 0; k < nsupers; ++k) {
-+				pr = PROW( k, grid );
-+				if ( myrow == pr ) {
-+					lib = LBi( k, grid );    /* local block number */
-+					kcol = PCOL( k, grid );
-+					if (mycol == kcol || fmod[lib] )
-+						mod_bit[lib] = 1;  /* contribution from off-diagonal and diagonal*/
-+				}
-+			}
-+			/* Every process receives the count, but it is only useful on the
-+			   diagonal processes.  */
-+			MPI_Allreduce( mod_bit, frecv, nlb, mpi_int_t, MPI_SUM, grid->rscp.comm);
-+
-+
-+
-+			k = CEILING( nsupers, grid->nprow );/* Number of local block rows */
-+			if ( !(LRtree_ptr = (RdTree*)SUPERLU_MALLOC(k * sizeof(RdTree))) )
-+				ABORT("Malloc fails for LRtree_ptr[].");
-+			if ( !(ActiveFlag = intCalloc_dist(grid->npcol*2)) )
-+				ABORT("Calloc fails for ActiveFlag[].");	
-+			if ( !(ranks = (int*)SUPERLU_MALLOC(grid->npcol * sizeof(int))) )
-+				ABORT("Malloc fails for ranks[].");	
-+
-+			// if ( !(idxs = intCalloc_dist(nsupers)) )
-+				// ABORT("Calloc fails for idxs[].");	
-+
-+			// if ( !(nzrows = (int_t**)SUPERLU_MALLOC(nsupers * sizeof(int_t*))) )
-+				// ABORT("Malloc fails for nzrows[].");
-+
-+			if ( !(SeedSTD_RD = (double*)SUPERLU_MALLOC(k * sizeof(double))) )
-+				ABORT("Malloc fails for SeedSTD_RD[].");	
-+
-+			for (i=0;i<k;i++){
-+				SeedSTD_RD[i]=rand();		
-+			}
-+
-+			MPI_Allreduce(MPI_IN_PLACE,&SeedSTD_RD[0],k,MPI_DOUBLE,MPI_MAX,grid->rscp.comm);					  
-+
-+
-+			for (lib = 0; lib <k ; ++lib) {
-+				LRtree_ptr[lib]=NULL;
-+			}
-+
-+			
-+			if ( !(ActiveFlagAll = intMalloc_dist(grid->npcol*k)) )
-+				ABORT("Calloc fails for ActiveFlagAll[].");				
-+			for (j=0;j<grid->npcol*k;++j)ActiveFlagAll[j]=-3*nsupers;	
-+				
-+				
-+				
-+			for (ljb = 0; ljb < CEILING( nsupers, grid->npcol); ++ljb) { /* for each local block column ... */
-+				jb = mycol+ljb*grid->npcol;  /* not sure */
-+				if(jb<nsupers){
-+					pc = PCOL( jb, grid );
-+					for(i=xlsub[ljb];i<xlsub[ljb+1];++i){
-+						irow = lsub[i];
-+						ib = BlockNum( irow );
-+						pr = PROW( ib, grid );
-+						if ( myrow == pr ) { /* Block row ib in my process row */
-+							lib = LBi( ib, grid ); /* Local block number */
-+							ActiveFlagAll[pc+lib*grid->npcol]=MAX(ActiveFlagAll[pc+lib*grid->npcol],jb);
-+						}
-+					}
-+				}
-+			}
-+
-+			MPI_Allreduce(MPI_IN_PLACE,ActiveFlagAll,grid->npcol*k,mpi_int_t,MPI_MAX,grid->rscp.comm);
-+			
-+			for (lib=0;lib<k;++lib){
-+				ib = myrow+lib*grid->nprow;  /* not sure */
-+				if(ib<nsupers){
-+					pr = PROW( ib, grid );
-+					for (j=0;j<grid->npcol;++j)ActiveFlag[j]=ActiveFlagAll[j+lib*grid->npcol];;
-+					for (j=0;j<grid->npcol;++j)ActiveFlag[j+grid->npcol]=j;
-+					for (j=0;j<grid->npcol;++j)ranks[j]=-1;
-+					Root=-1; 
-+					Iactive = 0;				
-+
-+					for (j=0;j<grid->npcol;++j){
-+						if(ActiveFlag[j]!=-3*nsupers){
-+						jb = ActiveFlag[j];
-+						pc = PCOL( jb, grid );
-+						if(jb==ib)Root=pc;
-+						if(mycol==pc)Iactive=1;		
-+						}					
-+					}
-+				
-+				
-+					quickSortM(ActiveFlag,0,grid->npcol-1,grid->npcol,1,2);
-+
-+					if(Iactive==1){
-+						assert( Root>-1 );
-+						rank_cnt = 1;
-+						ranks[0]=Root;
-+						for (j = 0; j < grid->npcol; ++j){
-+							if(ActiveFlag[j]!=-3*nsupers && ActiveFlag[j+grid->npcol]!=Root){
-+								ranks[rank_cnt]=ActiveFlag[j+grid->npcol];
-+								++rank_cnt;
-+							}
-+						}
-+						if(rank_cnt>1){
-+
-+							for (ii=0;ii<rank_cnt;ii++)   // use global ranks rather than local ranks
-+								ranks[ii] = PNUM( pr, ranks[ii], grid );		
-+
-+							// rseed=rand();
-+							// rseed=1.0;
-+							msgsize = SuperSize( ib )*nrhs+LSUM_H;
-+
-+							// if(ib==0){
-+
-+							LRtree_ptr[lib] = RdTree_Create(grid->comm, ranks, rank_cnt, msgsize,SeedSTD_RD[lib]);  	
-+							RdTree_SetTag(LRtree_ptr[lib], RD_L);
-+							// }
-+
-+							// printf("iam %5d rtree rank_cnt %5d \n",iam,rank_cnt);
-+							// fflush(stdout);
-+
-+	
-+							#if ( PRNTlevel>=1 )
-+							if(Root==mycol){
-+							assert(rank_cnt==frecv[lib]);
-+							// printf("Partial Reduce Procs: row%7d np%4d\n",ib,rank_cnt);
-+							// printf("Partial Reduce Procs: %4d %4d: ",iam, rank_cnt);
-+							// // for(j=0;j<rank_cnt;++j)printf("%4d",ranks[j]);
-+							// printf("\n");
-+							}
-+							#endif		
-+						}
-+					}				
-+				}	
-+			}
-+
-+			SUPERLU_FREE(mod_bit);
-+			SUPERLU_FREE(frecv);
-+
-+
-+			SUPERLU_FREE(ActiveFlag);
-+			SUPERLU_FREE(ActiveFlagAll);
-+			SUPERLU_FREE(ranks);	
-+			// SUPERLU_FREE(idxs);	 
-+			SUPERLU_FREE(SeedSTD_RD);	
-+			// for(i=0;i<nsupers;++i){
-+				// if(nzrows[i])SUPERLU_FREE(nzrows[i]);
-+			// }
-+			// SUPERLU_FREE(nzrows);
-+
-+				////////////////////////////////////////////////////////
-+
-+#if ( PROFlevel>=1 )
-+		t = SuperLU_timer_() - t;
-+		if ( !iam) printf(".. Construct Reduce tree for L: %.2f\t\n", t);
-+#endif					
-+
-+#if ( PROFlevel>=1 )
-+			t = SuperLU_timer_();
-+#endif	
-+
-+			/* construct the Bcast tree for U ... */
-+
-+			k = CEILING( nsupers, grid->npcol );/* Number of local block columns */
-+			if ( !(UBtree_ptr = (BcTree*)SUPERLU_MALLOC(k * sizeof(BcTree))) )
-+				ABORT("Malloc fails for UBtree_ptr[].");
-+			if ( !(ActiveFlag = intCalloc_dist(grid->nprow*2)) )
-+				ABORT("Calloc fails for ActiveFlag[].");	
-+			if ( !(ranks = (int*)SUPERLU_MALLOC(grid->nprow * sizeof(int))) )
-+				ABORT("Malloc fails for ranks[].");	
-+			if ( !(SeedSTD_BC = (double*)SUPERLU_MALLOC(k * sizeof(double))) )
-+				ABORT("Malloc fails for SeedSTD_BC[].");	
-+
-+			for (i=0;i<k;i++){
-+				SeedSTD_BC[i]=rand();		
-+			}
-+
-+			MPI_Allreduce(MPI_IN_PLACE,&SeedSTD_BC[0],k,MPI_DOUBLE,MPI_MAX,grid->cscp.comm);					  
-+
-+
-+			for (ljb = 0; ljb <k ; ++ljb) {
-+				UBtree_ptr[ljb]=NULL;
-+			}	
-+
-+			if ( !(ActiveFlagAll = intMalloc_dist(grid->nprow*k)) )
-+				ABORT("Calloc fails for ActiveFlagAll[].");				
-+			for (j=0;j<grid->nprow*k;++j)ActiveFlagAll[j]=-3*nsupers;	
-+			
-+			
-+			
-+			for (lib = 0; lib < CEILING( nsupers, grid->nprow); ++lib) { /* for each local block row ... */
-+				ib = myrow+lib*grid->nprow;  /* not sure */
-+				
-+			// if(ib==0)printf("iam %5d ib %5d\n",iam,ib);
-+			// fflush(stdout);				
-+				
-+				if(ib<nsupers){
-+					for (i = xusub[lib]; i < xusub[lib+1]; i++) {
-+					  jcol = usub[i];
-+					  jb = BlockNum( jcol );
-+					  ljb = LBj( jb, grid );    /* local block number */
-+					  pc = PCOL( jb, grid );
-+					  pr = PROW( ib, grid );
-+					  if ( mycol == pc ) { /* Block column ib in my process column */		
-+						ActiveFlagAll[pr+ljb*grid->nprow]=MAX(ActiveFlagAll[pr+ljb*grid->nprow],ib);			  
-+					  }
-+					}  /* for i ... */
-+					pr = PROW( ib, grid ); // take care of diagonal node stored as L
-+					pc = PCOL( ib, grid );
-+					if ( mycol == pc ) { /* Block column ib in my process column */					
-+						ljb = LBj( ib, grid );    /* local block number */
-+						ActiveFlagAll[pr+ljb*grid->nprow]=MAX(ActiveFlagAll[pr+ljb*grid->nprow],ib);					
-+						// if(pr+ljb*grid->nprow==0)printf("iam %5d ib %5d ActiveFlagAll %5d pr %5d ljb %5d\n",iam,ib,ActiveFlagAll[pr+ljb*grid->nprow],pr,ljb);
-+						// fflush(stdout);	
-+					}					
-+				}	
-+			}
-+			
-+			// printf("iam %5d ActiveFlagAll %5d\n",iam,ActiveFlagAll[0]);
-+			// fflush(stdout);
-+			
-+			MPI_Allreduce(MPI_IN_PLACE,ActiveFlagAll,grid->nprow*k,mpi_int_t,MPI_MAX,grid->cscp.comm);					  
-+						
-+			for (ljb = 0; ljb < k; ++ljb) { /* for each block column ... */
-+				jb = mycol+ljb*grid->npcol;  /* not sure */
-+				if(jb<nsupers){
-+				pc = PCOL( jb, grid );
-+				// if ( mycol == pc ) { /* Block column jb in my process column */
-+
-+				for (j=0;j<grid->nprow;++j)ActiveFlag[j]=ActiveFlagAll[j+ljb*grid->nprow];
-+				for (j=0;j<grid->nprow;++j)ActiveFlag[j+grid->nprow]=j;
-+				for (j=0;j<grid->nprow;++j)ranks[j]=-1;
-+
-+				Root=-1; 
-+				Iactive = 0;				
-+				for (j=0;j<grid->nprow;++j){
-+					if(ActiveFlag[j]!=-3*nsupers){
-+					gb = ActiveFlag[j];
-+					pr = PROW( gb, grid );
-+					if(gb==jb)Root=pr;
-+					if(myrow==pr)Iactive=1;		
-+					}
-+				}						
-+				
-+				quickSortM(ActiveFlag,0,grid->nprow-1,grid->nprow,1,2);	
-+			// printf("jb: %5d Iactive %5d\n",jb,Iactive);
-+			// fflush(stdout);
-+				if(Iactive==1){
-+					// if(jb==0)printf("root:%5d jb: %5d ActiveFlag %5d \n",Root,jb,ActiveFlag[0]);
-+					fflush(stdout);
-+					assert( Root>-1 );
-+					rank_cnt = 1;
-+					ranks[0]=Root;
-+					for (j = 0; j < grid->nprow; ++j){
-+						if(ActiveFlag[j]!=-3*nsupers && ActiveFlag[j+grid->nprow]!=Root){
-+							ranks[rank_cnt]=ActiveFlag[j+grid->nprow];
-+							++rank_cnt;
-+						}
-+					}		
-+			// printf("jb: %5d rank_cnt %5d\n",jb,rank_cnt);
-+			// fflush(stdout);
-+					if(rank_cnt>1){
-+						for (ii=0;ii<rank_cnt;ii++)   // use global ranks rather than local ranks
-+							ranks[ii] = PNUM( ranks[ii], pc, grid );
-+
-+						// rseed=rand();
-+						// rseed=1.0;
-+						msgsize = SuperSize( jb )*nrhs+XK_H;
-+						UBtree_ptr[ljb] = BcTree_Create(grid->comm, ranks, rank_cnt, msgsize,SeedSTD_BC[ljb]);  	
-+						BcTree_SetTag(UBtree_ptr[ljb],BC_U);
-+
-+						// printf("iam %5d btree rank_cnt %5d \n",iam,rank_cnt);
-+						// fflush(stdout);
-+						
-+						if(Root==myrow){
-+						rank_cnt_ref=1;
-+						for (j = 0; j < grid->nprow; ++j) {
-+							// printf("ljb %5d j %5d nprow %5d\n",ljb,j,grid->nprow);
-+							// fflush(stdout);
-+							if ( bsendx_plist[ljb][j] != EMPTY ) {	
-+								++rank_cnt_ref;		
-+							}
-+						}
-+						// printf("ljb %5d rank_cnt %5d rank_cnt_ref %5d\n",ljb,rank_cnt,rank_cnt_ref);
-+						// fflush(stdout);								
-+						assert(rank_cnt==rank_cnt_ref);		
-+						}						
-+					}
-+				}
-+				}
-+			}	
-+			SUPERLU_FREE(ActiveFlag);
-+			SUPERLU_FREE(ActiveFlagAll);
-+			SUPERLU_FREE(ranks);				
-+			SUPERLU_FREE(SeedSTD_BC);				
-+				
-+#if ( PROFlevel>=1 )
-+		t = SuperLU_timer_() - t;
-+		if ( !iam) printf(".. Construct Bcast tree for U: %.2f\t\n", t);
-+#endif					
-+
-+#if ( PROFlevel>=1 )
-+				t = SuperLU_timer_();
-+#endif					
-+			/* construct the Reduce tree for U ... */
-+			/* the following is used as reference */
-+			nlb = CEILING( nsupers, grid->nprow );/* Number of local block rows */
-+			if ( !(mod_bit = intMalloc_dist(nlb)) )
-+				ABORT("Malloc fails for mod_bit[].");
-+			if ( !(brecv = intMalloc_dist(nlb)) )
-+				ABORT("Malloc fails for brecv[].");
-+
-+			for (k = 0; k < nlb; ++k) mod_bit[k] = 0;
-+			for (k = 0; k < nsupers; ++k) {
-+				pr = PROW( k, grid );
-+				if ( myrow == pr ) {
-+					lib = LBi( k, grid );    /* local block number */
-+					kcol = PCOL( k, grid );
-+					if (mycol == kcol || bmod[lib] )
-+						mod_bit[lib] = 1;  /* contribution from off-diagonal and diagonal*/
-+				}
-+			}
-+			/* Every process receives the count, but it is only useful on the
-+			   diagonal processes.  */
-+			MPI_Allreduce( mod_bit, brecv, nlb, mpi_int_t, MPI_SUM, grid->rscp.comm);
-+
-+
-+
-+			k = CEILING( nsupers, grid->nprow );/* Number of local block rows */
-+			if ( !(URtree_ptr = (RdTree*)SUPERLU_MALLOC(k * sizeof(RdTree))) )
-+				ABORT("Malloc fails for URtree_ptr[].");
-+			if ( !(ActiveFlag = intCalloc_dist(grid->npcol*2)) )
-+				ABORT("Calloc fails for ActiveFlag[].");	
-+			if ( !(ranks = (int*)SUPERLU_MALLOC(grid->npcol * sizeof(int))) )
-+				ABORT("Malloc fails for ranks[].");	
-+
-+			// if ( !(idxs = intCalloc_dist(nsupers)) )
-+				// ABORT("Calloc fails for idxs[].");	
-+
-+			// if ( !(nzrows = (int_t**)SUPERLU_MALLOC(nsupers * sizeof(int_t*))) )
-+				// ABORT("Malloc fails for nzrows[].");
-+
-+			if ( !(SeedSTD_RD = (double*)SUPERLU_MALLOC(k * sizeof(double))) )
-+				ABORT("Malloc fails for SeedSTD_RD[].");	
-+
-+			for (i=0;i<k;i++){
-+				SeedSTD_RD[i]=rand();		
-+			}
-+
-+			MPI_Allreduce(MPI_IN_PLACE,&SeedSTD_RD[0],k,MPI_DOUBLE,MPI_MAX,grid->rscp.comm);					  
-+
-+			for (lib = 0; lib <k ; ++lib) {
-+				URtree_ptr[lib]=NULL;
-+			}
-+
-+			
-+			if ( !(ActiveFlagAll = intMalloc_dist(grid->npcol*k)) )
-+				ABORT("Calloc fails for ActiveFlagAll[].");				
-+			for (j=0;j<grid->npcol*k;++j)ActiveFlagAll[j]=3*nsupers;	
-+							
-+			for (lib = 0; lib < CEILING( nsupers, grid->nprow); ++lib) { /* for each local block row ... */
-+				ib = myrow+lib*grid->nprow;  /* not sure */
-+				if(ib<nsupers){
-+					for (i = xusub[lib]; i < xusub[lib+1]; i++) {
-+					  jcol = usub[i];
-+					  jb = BlockNum( jcol );
-+					  pc = PCOL( jb, grid );
-+					  if ( mycol == pc ) { /* Block column ib in my process column */	
-+						ActiveFlagAll[pc+lib*grid->npcol]=MIN(ActiveFlagAll[pc+lib*grid->npcol],jb);			  
-+					  }	
-+					}  /* for i ... */
-+					pc = PCOL( ib, grid );
-+					if ( mycol == pc ) { /* Block column ib in my process column */						
-+						ActiveFlagAll[pc+lib*grid->npcol]=MIN(ActiveFlagAll[pc+lib*grid->npcol],ib);
-+					}						
-+				}	
-+			}
-+			
-+			MPI_Allreduce(MPI_IN_PLACE,ActiveFlagAll,grid->npcol*k,mpi_int_t,MPI_MIN,grid->rscp.comm);	
-+			
-+			for (lib=0;lib<k;++lib){
-+				ib = myrow+lib*grid->nprow;  /* not sure */
-+				if(ib<nsupers){
-+					pr = PROW( ib, grid );
-+					for (j=0;j<grid->npcol;++j)ActiveFlag[j]=ActiveFlagAll[j+lib*grid->npcol];;
-+					for (j=0;j<grid->npcol;++j)ActiveFlag[j+grid->npcol]=j;
-+					for (j=0;j<grid->npcol;++j)ranks[j]=-1;
-+					Root=-1; 
-+					Iactive = 0;				
-+
-+					for (j=0;j<grid->npcol;++j){
-+						if(ActiveFlag[j]!=3*nsupers){
-+						jb = ActiveFlag[j];
-+						pc = PCOL( jb, grid );
-+						if(jb==ib)Root=pc;
-+						if(mycol==pc)Iactive=1;		
-+						}					
-+					}
-+					
-+					quickSortM(ActiveFlag,0,grid->npcol-1,grid->npcol,0,2);
-+
-+					if(Iactive==1){
-+						assert( Root>-1 );
-+						rank_cnt = 1;
-+						ranks[0]=Root;
-+						for (j = 0; j < grid->npcol; ++j){
-+							if(ActiveFlag[j]!=3*nsupers && ActiveFlag[j+grid->npcol]!=Root){
-+								ranks[rank_cnt]=ActiveFlag[j+grid->npcol];
-+								++rank_cnt;
-+							}
-+						}
-+						if(rank_cnt>1){
-+
-+							for (ii=0;ii<rank_cnt;ii++)   // use global ranks rather than local ranks
-+								ranks[ii] = PNUM( pr, ranks[ii], grid );		
-+
-+							// rseed=rand();
-+							// rseed=1.0;
-+							msgsize = SuperSize( ib )*nrhs+LSUM_H;
-+
-+							// if(ib==0){
-+
-+							URtree_ptr[lib] = RdTree_Create(grid->comm, ranks, rank_cnt, msgsize,SeedSTD_RD[lib]);  	
-+							RdTree_SetTag(URtree_ptr[lib], RD_U);
-+							// }
-+	
-+							// #if ( PRNTlevel>=1 )
-+							if(Root==mycol){
-+							// printf("Partial Reduce Procs: %4d %4d %5d \n",iam, rank_cnt,brecv[lib]);
-+							// fflush(stdout);
-+							assert(rank_cnt==brecv[lib]);
-+							// printf("Partial Reduce Procs: row%7d np%4d\n",ib,rank_cnt);
-+							// printf("Partial Reduce Procs: %4d %4d: ",iam, rank_cnt);
-+							// // for(j=0;j<rank_cnt;++j)printf("%4d",ranks[j]);
-+							// printf("\n");
-+							}
-+							// #endif		
-+						}
-+					}
-+				}						
-+			}
-+
-+			SUPERLU_FREE(mod_bit);
-+			SUPERLU_FREE(brecv);
-+
-+
-+			SUPERLU_FREE(ActiveFlag);
-+			SUPERLU_FREE(ActiveFlagAll);
-+			SUPERLU_FREE(ranks);	
-+			// SUPERLU_FREE(idxs);	
-+			SUPERLU_FREE(SeedSTD_RD);	
-+			// for(i=0;i<nsupers;++i){
-+				// if(nzrows[i])SUPERLU_FREE(nzrows[i]);
-+			// }
-+			// SUPERLU_FREE(nzrows);				
-+				
-+#if ( PROFlevel>=1 )
-+		t = SuperLU_timer_() - t;
-+		if ( !iam) printf(".. Construct Reduce tree for U: %.2f\t\n", t);
-+#endif						
-+				
-+		////////////////////////////////////////////////////////
-+    
-+  
-+  
-+  /* Free the memory used for storing L and U */
-+  SUPERLU_FREE(xlsub); SUPERLU_FREE(xusub);
-+  if (lsub != NULL)
-+    SUPERLU_FREE(lsub);  
-+  if (usub != NULL)
-+    SUPERLU_FREE(usub);  
-+  
-   SUPERLU_FREE(nnzToRecv);
-   SUPERLU_FREE(ptrToRecv);
-   SUPERLU_FREE(nnzToSend);
-@@ -1931,7 +2761,10 @@ ddist_psymbtonum(fact_t fact, int_t n, SuperMatrix *A,
-   SUPERLU_FREE(recvBuf);
-   
-   Llu->Lrowind_bc_ptr = Lrowind_bc_ptr;
-+  Llu->Lindval_loc_bc_ptr = Lindval_loc_bc_ptr;
-   Llu->Lnzval_bc_ptr = Lnzval_bc_ptr;
-+  Llu->Linv_bc_ptr = Linv_bc_ptr;
-+  Llu->Uinv_bc_ptr = Uinv_bc_ptr;
-   Llu->Ufstnz_br_ptr = Ufstnz_br_ptr;
-   Llu->Unzval_br_ptr = Unzval_br_ptr;
-   Llu->ToRecv = ToRecv;
-@@ -1948,6 +2781,14 @@ ddist_psymbtonum(fact_t fact, int_t n, SuperMatrix *A,
-   Llu->ilsum = ilsum;
-   Llu->ldalsum = ldaspa;
-   LUstruct->Glu_persist = Glu_persist;	
-+  Llu->LRtree_ptr = LRtree_ptr;
-+  Llu->LBtree_ptr = LBtree_ptr;
-+  Llu->URtree_ptr = URtree_ptr;
-+  Llu->UBtree_ptr = UBtree_ptr;
-+  Llu->Urbs = Urbs; 
-+  Llu->Ucb_indptr = Ucb_indptr; 
-+  Llu->Ucb_valptr = Ucb_valptr; 
-+
- #if ( PRNTlevel>=1 )
-   if ( !iam ) printf(".. # L blocks " IFMT "\t# U blocks " IFMT "\n",
- 		     nLblocks, nUblocks);
-diff --git a/TEST/pztest.c b/SRC/pdtest.c
-similarity index 81%
-rename from TEST/pztest.c
-rename to SRC/pdtest.c
-index 3ba5439..f76a80e 100644
---- a/TEST/pztest.c
-+++ b/SRC/pdtest.c
-@@ -9,30 +9,26 @@ The source code is distributed under BSD license, see the file License.txt
- at the top-level directory.
- */
- 
-+
- /*! @file 
-- * \brief Driver program for testing PZGSSVX.
-+ * \brief Driver program for testing PDGSSVX.
-  *
-  * <pre>
-- * -- Distributed SuperLU routine (version 5.2) --
-+ * -- Distributed SuperLU routine (version 5.0) --
-  * Lawrence Berkeley National Lab, Univ. of California Berkeley.
-- * September 30, 2017
-+ * March 16, 2017
-  * </pre>
-  */
- /*
-- * File name:		pztest.c
-+ * File name:		pdtest.c
-  * Purpose:             MAIN test program
-  */
- #include <stdio.h>
- #include <stdlib.h>
--//#include <unistd.h>
--#ifdef _MSC_VER
--#include <wingetopt.h>
--#else
-+#include <unistd.h>
- #include <getopt.h>
--#endif
- #include <math.h>
--#include "superlu_dist_config.h"
--#include "superlu_zdefs.h"
-+#include "superlu_ddefs.h"
- 
- #define NTESTS 1 /*5*/      /* Number of test types */
- #define NTYPES 11     /* Number of matrix types */
-@@ -50,13 +46,13 @@ parse_command_line(int argc, char *argv[], int *nprow, int *npcol,
- 		   int *nrhs, FILE **fp);
- 
- extern int
--pzcompute_resid(int m, int n, int nrhs, SuperMatrix *A,
--		doublecomplex *x, int ldx, doublecomplex *b, int ldb,
-+pdcompute_resid(int m, int n, int nrhs, SuperMatrix *A,
-+		double *x, int ldx, double *b, int ldb,
- 		gridinfo_t *grid, SOLVEstruct_t *SOLVEstruct, double *resid);
- 
- /*! \brief Copy matrix A into matrix B, in distributed compressed row format. */
- void
--zCopy_CompRowLoc_Matrix_dist(SuperMatrix *A, SuperMatrix *B)
-+dCopy_CompRowLoc_Matrix_dist(SuperMatrix *A, SuperMatrix *B)
- {
-     NRformat_loc *Astore;
-     NRformat_loc *Bstore;
-@@ -74,7 +70,7 @@ zCopy_CompRowLoc_Matrix_dist(SuperMatrix *A, SuperMatrix *B)
-     Bstore->m_loc = Astore->m_loc;
-     m_loc = Astore->m_loc;
-     Bstore->fst_row = Astore->fst_row;
--    memcpy(Bstore->nzval, Astore->nzval, nnz_loc * sizeof(doublecomplex));
-+    memcpy(Bstore->nzval, Astore->nzval, nnz_loc * sizeof(double));
-     memcpy(Bstore->colind, Astore->colind, nnz_loc * sizeof(int_t));
-     memcpy(Bstore->rowptr, Astore->rowptr, (m_loc+1) * sizeof(int_t));
- }
-@@ -100,8 +96,8 @@ int main(int argc, char *argv[])
-  * Purpose
-  * =======
-  *
-- * PZTEST is the main test program for the DOUBLE COMPLEX linear 
-- * equation driver routines PZGSSVX.
-+ * PDTEST is the main test program for the DOUBLE linear 
-+ * equation driver routines PDGSSVX.
-  * 
-  * The program is invoked by a shell script file -- dtest.csh.
-  * The output from the tests are written into a file -- dtest.out.
-@@ -114,19 +110,17 @@ int main(int argc, char *argv[])
-     LUstruct_t LUstruct;
-     SOLVEstruct_t SOLVEstruct;
-     gridinfo_t grid;
--    doublecomplex   *nzval_save;
-+    double   *nzval_save;
-     int_t    *colind_save, *rowptr_save;
-     double   *berr, *R, *C;
--    doublecomplex   *b, *bsave, *xtrue, *solx;
-+    double   *b, *bsave, *xtrue, *solx;
-     int    i, j, m, n, izero = 0;
-     int    nprow, npcol;
-     int    iam, info, ldb, ldx, nrhs;
--    int_t  iinfo;
-     char     **cpp, c;
-     FILE *fp, *fopen();
-     char matrix_type[8], equed[1];
--    int  relax, maxsuper=sp_ienv_dist(3), fill_ratio=sp_ienv_dist(6),
--         min_gemm_gpu_offload=0;
-+    int  relax, maxsuper=0, fill_ratio=0, min_gemm_gpu_offload=0;
-     int    equil, ifact, nfact, iequil, iequed, prefact, notfactored;
-     int    nt, nrun=0, nfail=0, nerrs=0, imat, fimat=0, nimat=1;
-     fact_t fact;
-@@ -191,12 +185,12 @@ int main(int argc, char *argv[])
- 	/* ------------------------------------------------------------
- 	   GET THE MATRIX FROM FILE AND SETUP THE RIGHT HAND SIDE. 
- 	   ------------------------------------------------------------*/
--	zcreate_matrix(&A, nrhs, &b, &ldb, &xtrue, &ldx, fp, &grid);
-+	dcreate_matrix(&A, nrhs, &b, &ldb, &xtrue, &ldx, fp, &grid);
- 
- 	m = A.nrow;
- 	n = A.ncol;
- 
--	if ( !(bsave = doublecomplexMalloc_dist(ldb * nrhs)) )
-+	if ( !(bsave = doubleMalloc_dist(ldb * nrhs)) )
- 	    ABORT("Malloc fails for bsave[]");
- 	for (j = 0; j < nrhs; ++j)
- 	    for (i = 0; i < ldb; ++i) bsave[i+j*ldb] = b[i+j*ldb];
-@@ -205,13 +199,13 @@ int main(int argc, char *argv[])
- 	Astore = (NRformat_loc *) A.Store;
- 	int_t nnz_loc = Astore->nnz_loc;
- 	int_t m_loc = Astore->m_loc;
--	nzval_save = (doublecomplex *) doublecomplexMalloc_dist(nnz_loc);
-+	nzval_save = (double *) doubleMalloc_dist(nnz_loc);
- 	colind_save = (int_t *) intMalloc_dist(nnz_loc);
- 	rowptr_save = (int_t *) intMalloc_dist(m_loc + 1);
--	zCreate_CompRowLoc_Matrix_dist(&Asave, m, n, nnz_loc, m_loc, Astore->fst_row,
-+	dCreate_CompRowLoc_Matrix_dist(&Asave, m, n, nnz_loc, m_loc, Astore->fst_row,
- 				       nzval_save, colind_save, rowptr_save,
- 				       SLU_NR_loc, SLU_D, SLU_GE);
--	zCopy_CompRowLoc_Matrix_dist(&A, &Asave);
-+	dCopy_CompRowLoc_Matrix_dist(&A, &Asave);
- 
- 	for (iequed = 0; iequed < 4; ++iequed) {
- 	    int what_equil = equils[iequed];
-@@ -238,7 +232,7 @@ int main(int argc, char *argv[])
- 				  options.Fact == SamePattern_SameRowPerm );
- 
- 		    /* Restore the matrix A. */
--		    zCopy_CompRowLoc_Matrix_dist(&Asave, &A);
-+		    dCopy_CompRowLoc_Matrix_dist(&Asave, &A);
- 
- 		    /* Initialize ScalePermstruct and LUstruct. */
- 		    ScalePermstructInit(m, n, &ScalePermstruct);
-@@ -252,15 +246,15 @@ int main(int argc, char *argv[])
- 			R = (double *) SUPERLU_MALLOC(m*sizeof(double));
- 			C = (double *) SUPERLU_MALLOC(n*sizeof(double));
- 			
--			/* Later call to PZGSSVX only needs to solve. */
-+			/* Later call to PDGSSVX only needs to solve. */
-                         if ( equil || iequed ) {
- 			    /* Compute row and column scale factors to
- 			       equilibrate matrix A.    */
--			    pzgsequ(&A, R, C, &rowcnd, &colcnd, &amax, &iinfo,
-+			    pdgsequ(&A, R, C, &rowcnd, &colcnd, &amax, &info,
- 				    &grid);
- 
- 			    /* Force equilibration. */
--			    if ( iinfo==0 && n > 0 ) {
-+			    if ( info==0 && n > 0 ) {
- 				if ( what_equil == ROW ) {
- 				    rowcnd = 0.;
- 				    colcnd = 1.;
-@@ -281,7 +275,7 @@ int main(int argc, char *argv[])
- 			    }
- 			
- 			    /* Equilibrate the matrix. */
--			    pzlaqgs(&A, R, C, rowcnd, colcnd, amax, equed);
-+			    pdlaqgs(&A, R, C, rowcnd, colcnd, amax, equed);
- 			    // printf("after pdlaqgs: *equed %c\n", *equed);
- 
- 			    /* Not equilibrate anymore when calling PDGSSVX,.
-@@ -300,7 +294,7 @@ int main(int argc, char *argv[])
- 			PStatInit(&stat);
- 	
- 			int nrhs1 = 0; /* Only performs factorization */
--			pzgssvx(&options, &A, &ScalePermstruct, b, ldb, nrhs1,
-+			pdgssvx(&options, &A, &ScalePermstruct, b, ldb, nrhs1,
- 				&grid, &LUstruct, &SOLVEstruct,
- 				berr, &stat, &info);
- 
-@@ -323,41 +317,41 @@ int main(int argc, char *argv[])
- 		    } /* end if .. first time factor */
- 
- 		    /*----------------
--		     * Test pzgssvx
-+		     * Test pdgssvx
- 		     *----------------*/
- 
- 		    if ( options.Fact != FACTORED ) {
- 			/* Restore the matrix A. */
--			zCopy_CompRowLoc_Matrix_dist(&Asave, &A);
-+			dCopy_CompRowLoc_Matrix_dist(&Asave, &A);
- 		    } 
- 
- 		    /* Set the right-hand side. */
--		    zCopy_Dense_Matrix_dist(m_loc, nrhs, bsave, ldb, b, ldb);
-+		    dCopy_Dense_Matrix_dist(m_loc, nrhs, bsave, ldb, b, ldb);
- 
- 		    PStatInit(&stat);
- 
- 		    /*if ( !iam ) printf("\ttest pdgssvx: nrun %d, iequed %d, equil %d, fact %d\n", 
- 		      nrun, iequed, equil, options.Fact);*/
- 		    /* Testing PDGSSVX: solve and compute the error bounds. */
--		    pzgssvx(&options, &A, &ScalePermstruct, b, ldb, nrhs,
-+		    pdgssvx(&options, &A, &ScalePermstruct, b, ldb, nrhs,
- 			    &grid, &LUstruct, &SOLVEstruct,
- 			    berr, &stat, &info);
- 
- 		    PStatFree(&stat);
- #if 0
--		    pdinf_norm_error(iam, ((NRformat_loc *)A.Store)->m_loc,
-+c		    pdinf_norm_error(iam, ((NRformat_loc *)A.Store)->m_loc,
- 				     nrhs, b, ldb, xtrue, ldx, &grid);
- #endif
- 		    /*		    if ( info && info != izero ) {*/
- 		    if ( info ) {
--			printf(FMT3, "pzgssvx",info,izero,n,nrhs,imat,nfail);
-+			printf(FMT3, "pdgssvx",info,izero,n,nrhs,imat,nfail);
- 		    } else {
- 			/* Restore the matrix A. */
--			zCopy_CompRowLoc_Matrix_dist(&Asave, &A);
-+			dCopy_CompRowLoc_Matrix_dist(&Asave, &A);
- 
- 			/* Compute residual of the computed solution.*/
- 			solx = b;
--			pzcompute_resid(m, n, nrhs, &A, solx, ldx, bsave, ldb,
-+			pdcompute_resid(m, n, nrhs, &A, solx, ldx, bsave, ldb,
- 					&grid, &SOLVEstruct, &result[0]);
- 			
- #if 0  /* how to get RCOND? */
-@@ -369,11 +363,11 @@ int main(int argc, char *argv[])
- #endif
- 
- 			/* Print information about the tests that did
--			   not pass the threshold.    */
-+				   not pass the threshold.    */
- 			int k1 = 0;
- 			for (i = k1; i < NTESTS; ++i) {
- 			    if ( result[i] >= THRESH ) {
--				printf(FMT2, "pzgssvx", options.Fact, 
-+				printf(FMT2, "pdgssvx", options.Fact, 
- 				       ScalePermstruct.DiagScale,
- 				       n, imat, i, result[i], berr[0]);
- 				++nfail;
-@@ -394,7 +388,7 @@ int main(int argc, char *argv[])
- 		    Destroy_LU(n, &grid, &LUstruct);
- 		    LUstructFree(&LUstruct);
- 		    if ( options.SolveInitialized ) {
--			zSolveFinalize(&options, &SOLVEstruct);
-+			dSolveFinalize(&options, &SOLVEstruct);
- 		    }
- 
- 		} /* end for equil ... */
-@@ -449,8 +443,6 @@ parse_command_line(int argc, char *argv[], int *nprow, int *npcol,
-     int c;
-     extern char *optarg;
-     char  str[20];
--    char *xenvstr, *menvstr, *benvstr, *genvstr;
--    xenvstr = menvstr = benvstr = genvstr = 0;
- 
-     while ( (c = getopt(argc, argv, "hr:c:t:n:x:m:b:g:s:f:")) != EOF ) {
- 	switch (c) {
-@@ -469,48 +461,33 @@ parse_command_line(int argc, char *argv[], int *nprow, int *npcol,
- 	    break;
- 	  case 'r': *nprow = atoi(optarg);
- 	            break;
--	  case 'c': *npcol = atoi(optarg);
-+	  case 'c': *npcol = atoi(c);
- 	            break;
- 	  case 'n': *n = atoi(optarg);
- 	            break;
--// Use putenv as exists on Windows
--#ifdef _MSC_VER
--#define putenv _putenv
--#endif
--	  case 'x': // c = atoi(optarg); 
--	            // sprintf(str, "%d", c);
--	            // setenv("NREL", str, 1);
--		    xenvstr = (char*) malloc((6+strlen(optarg))*sizeof(char));
--		    strcpy(xenvstr, "NREL=");
--		    strcat(xenvstr, optarg);
--		    putenv(xenvstr);
-+	  case 'x': c = atoi(optarg); 
-+	            sprintf(str, "%d", c);
-+	            setenv("NREL", str, 1);
- 	            //printf("Reset relax env. variable to %d\n", c);
- 	            break;
--	  case 'm': // c = atoi(optarg); 
--	            // sprintf(str, "%d", c);
--		    // setenv("NSUP", str, 1);
--		    menvstr = (char*) malloc((6+strlen(optarg))*sizeof(char));
--		    strcpy(menvstr, "NSUP=");
--		    strcat(menvstr, optarg);
--		    putenv(menvstr);
-+	  case 'm': c = atoi(optarg); 
-+	            sprintf(str, "%d", c);
-+		    setenv("NSUP", str, 1);
- 		    //printf("Reset maxsuper env. variable to %d\n", c);
- 	            break;
--	  case 'b': // c = atoi(optarg); 
--	            // sprintf(str, "%d", c);
--		    // setenv("FILL", str, 1);
--		    benvstr = (char*) malloc((6+strlen(optarg))*sizeof(char));
--		    strcpy(benvstr, "FILL=");
--		    strcat(benvstr, optarg);
--		    putenv(benvstr);
-+	  case 'e': c = atoi(optarg); 
-+	            sprintf(str, "%d", c);
-+		    setenv("NPROBE", str, 1);
-+		    //printf("Reset maxsuper env. variable to %d\n", c);
-+	            break;				
-+	  case 'b': c = atoi(optarg); 
-+	            sprintf(str, "%d", c);
-+		    setenv("FILL", str, 1);
- 		    //printf("Reset fill_ratio env. variable to %d\n", c);
- 	            break;
--	  case 'g': // c = atoi(optarg); 
--	            // sprintf(str, "%d", c);
--		    // setenv("N_GEMM", str, 1);
--		    genvstr = (char*) malloc((8+strlen(optarg))*sizeof(char));
--		    strcpy(genvstr, "N_GEMM=");
--		    strcat(genvstr, optarg);
--		    putenv(genvstr);
-+	  case 'g': c = atoi(optarg); 
-+	            sprintf(str, "%d", c);
-+		    setenv("N_GEMM", str, 1);
- 		    //printf("Reset min_gemm_gpu_offload env. variable to %d\n", c);
- 	            break;
- 	  case 's': *nrhs = atoi(optarg); 
-diff --git a/SRC/pdutil.c b/SRC/pdutil.c
-index 05975b0..a27f223 100644
---- a/SRC/pdutil.c
-+++ b/SRC/pdutil.c
-@@ -533,6 +533,17 @@ void pdinf_norm_error(int iam, int_t n, int_t nrhs, double x[], int_t ldx,
- 
-       err = err / xnorm;
-       if ( !iam ) printf("\tSol %2d: ||X-Xtrue||/||X|| = %e\n", j, err);
-+	  fflush(stdout);
-+	  
-+	  // while(1);
-+	  
-+      // if(err>1e-5){
-+		// if( !iam ) printf("Wrong solution! \n");
-+		// fflush(stdout);
-+		// while(1);
-+
-+		// ABORT("Wrong solution! \n");
-+// }
-     }
- }
- 
-diff --git a/SRC/psymbfact.h b/SRC/psymbfact.h
-index 549e51e..3330695 100644
---- a/SRC/psymbfact.h
-+++ b/SRC/psymbfact.h
-@@ -299,3 +299,6 @@ typedef struct {
- 
- 
- #endif /* __SUPERLU_DIST_PSYMBFACT */
-+
-+
-+
-diff --git a/SRC/pzgssvx.c b/SRC/pzgssvx.c
-index b56147b..dc382bc 100644
---- a/SRC/pzgssvx.c
-+++ b/SRC/pzgssvx.c
-@@ -239,7 +239,7 @@ at the top-level directory.
-  *      The user must also supply 
-  *
-  *        o  A, the unfactored matrix, only in the case that iterative
-- *              refinement is to be done (specifically A must be the output
-+ *              refinment is to be done (specifically A must be the output
-  *              A from the previous call, so that it has been scaled and permuted)
-  *        o  all of ScalePermstruct
-  *        o  all of LUstruct, including the actual numerical values of
-@@ -342,7 +342,7 @@ at the top-level directory.
-  *           = SLU_DOUBLE: accumulate residual in double precision.
-  *           = SLU_EXTRA:  accumulate residual in extra precision.
-  *
-- *         NOTE: all options must be identical on all processes when
-+ *         NOTE: all options must be indentical on all processes when
-  *               calling this routine.
-  *
-  * A (input/output) SuperMatrix* (local)
-@@ -467,7 +467,7 @@ at the top-level directory.
-  * SOLVEstruct (input/output) SOLVEstruct_t*
-  *         The data structure to hold the communication pattern used
-  *         in the phases of triangular solution and iterative refinement.
-- *         This pattern should be initialized only once for repeated solutions.
-+ *         This pattern should be intialized only once for repeated solutions.
-  *         If options->SolveInitialized = YES, it is an input argument.
-  *         If options->SolveInitialized = NO and nrhs != 0, it is an output
-  *         argument. See superlu_zdefs.h for the definition of 'SOLVEstruct_t'.
-@@ -649,10 +649,8 @@ pzgssvx(superlu_dist_options_t *options, SuperMatrix *A,
-     }
- 
-     /* ------------------------------------------------------------
--     * Diagonal scaling to equilibrate the matrix. (simple scheme)
--     *   for row i = 1:n,  A(i,:) <- A(i,:) / max(abs(A(i,:));
--     *   for column j = 1:n,  A(:,j) <- A(:, j) / max(abs(A(:,j))
--     * ------------------------------------------------------------*/
-+       Diagonal scaling to equilibrate the matrix. (simple scheme)
-+       ------------------------------------------------------------*/
-     if ( Equil ) {
- #if ( DEBUGlevel>=1 )
- 	CHECK_MALLOC(iam, "Enter equil");
-diff --git a/SRC/pzgssvx_ABglobal.c b/SRC/pzgssvx_ABglobal.c
-index c42dbe3..247f9e8 100644
---- a/SRC/pzgssvx_ABglobal.c
-+++ b/SRC/pzgssvx_ABglobal.c
-@@ -209,7 +209,7 @@ at the top-level directory.
-  *
-  *      The user must also supply 
-  *
-- *      -  A, the unfactored matrix, only in the case that iterative refinement
-+ *      -  A, the unfactored matrix, only in the case that iterative refinment
-  *            is to be done (specifically A must be the output A from 
-  *            the previous call, so that it has been scaled and permuted)
-  *      -  all of ScalePermstruct
-@@ -312,7 +312,7 @@ at the top-level directory.
-  *           = SLU_DOUBLE: accumulate residual in double precision.
-  *           = SLU_EXTRA:  accumulate residual in extra precision.
-  *
-- *         NOTE: all options must be identical on all processes when
-+ *         NOTE: all options must be indentical on all processes when
-  *               calling this routine.
-  *
-  * A (input/output) SuperMatrix*
-diff --git a/SRC/pzgstrf2.c b/SRC/pzgstrf2.c
-index e95eed3..c14d5dc 100644
---- a/SRC/pzgstrf2.c
-+++ b/SRC/pzgstrf2.c
-@@ -13,13 +13,10 @@ at the top-level directory.
-  * \brief Performs panel LU factorization.
-  *
-  * <pre>
-- * -- Distributed SuperLU routine (version 5.2) --
-+ * -- Distributed SuperLU routine (version 4.0) --
-  * Lawrence Berkeley National Lab, Univ. of California Berkeley.
-  * August 15, 2014
-  *
-- * Modified:
-- *   September 30, 2017
-- *
-  * <pre>
-  * Purpose
-  * =======
-@@ -99,7 +96,6 @@ pzgstrf2_trsm
-     int_t Pr;
-     MPI_Status status;
-     MPI_Comm comm = (grid->cscp).comm;
--    double t1, t2;
- 
-     /* Initialization. */
-     iam = grid->iam;
-@@ -131,25 +127,16 @@ pzgstrf2_trsm
-     if ( U_diag_blk_send_req && 
- 	 U_diag_blk_send_req[myrow] != MPI_REQUEST_NULL ) {
-         /* There are pending sends - wait for all Isend to complete */
--#if ( PROFlevel>=1 )
--	TIC (t1);
--#endif
--        for (pr = 0; pr < Pr; ++pr) {
-+        for (pr = 0; pr < Pr; ++pr)
-             if (pr != myrow) {
-                 MPI_Wait (U_diag_blk_send_req + pr, &status);
-             }
--	}
--#if ( PROFlevel>=1 )
--	TOC (t2, t1);
--	stat->utime[COMM] += t2;
--	stat->utime[COMM_DIAG] += t2;
--#endif
-+
- 	/* flag no more outstanding send request. */
- 	U_diag_blk_send_req[myrow] = MPI_REQUEST_NULL;
-     }
- 
-     if (iam == pkk) {            /* diagonal process */
--	/* ++++ First step compute diagonal block ++++++++++ */
-         for (j = 0; j < jlst - jfst; ++j) {  /* for each column in panel */
-             /* Diagonal pivot */
-             i = luptr;
-@@ -210,16 +197,13 @@ pzgstrf2_trsm
- 
-         }                       /* for column j ...  first loop */
- 
--	/* ++++ Second step compute off-diagonal block with communication  ++*/
-+	/* ++++++++++second step ====== */
- 
-         ublk_ptr = ujrow = Llu->ujrow;
- 
--        if (U_diag_blk_send_req && iam == pkk)  { /* Send the U block downward */
-+        if (U_diag_blk_send_req && iam == pkk)  { /* Send the U block */
-             /** ALWAYS SEND TO ALL OTHERS - TO FIX **/
--#if ( PROFlevel>=1 )
--	    TIC (t1);
--#endif
--            for (pr = 0; pr < Pr; ++pr) {
-+            for (pr = 0; pr < Pr; ++pr)
-                 if (pr != krow) {
-                     /* tag = ((k0<<2)+2) % tag_ub;        */
-                     /* tag = (4*(nsupers+k0)+2) % tag_ub; */
-@@ -228,12 +212,6 @@ pzgstrf2_trsm
-                                comm, U_diag_blk_send_req + pr);
- 
-                 }
--            }
--#if ( PROFlevel>=1 )
--	    TOC (t2, t1);
--	    stat->utime[COMM] += t2;
--	    stat->utime[COMM_DIAG] += t2;
--#endif
- 
- 	    /* flag outstanding Isend */
-             U_diag_blk_send_req[krow] = (MPI_Request) TRUE; /* Sherry */
-@@ -241,6 +219,8 @@ pzgstrf2_trsm
- 
-         /* pragma below would be changed by an MKL call */
- 
-+        char uplo = 'u', side = 'r', transa = 'n', diag = 'n';
-+
-         l = nsupr - nsupc;
-         // n = nsupc;
- 	doublecomplex alpha = {1.0, 0.0};
-@@ -259,33 +239,27 @@ pzgstrf2_trsm
- #endif
- 	stat->ops[FACT] += 4.0 * ((flops_t) nsupc * (nsupc+1) * l);
-     } else {  /* non-diagonal process */
--        /* ================================================================== *
--         * Receive the diagonal block of U for panel factorization of L(:,k). * 
--         * Note: we block for panel factorization of L(:,k), but panel        *
--	 * factorization of U(:,k) do not block                               *
--         * ================================================================== */
-+        /* ================================================ *
-+         * Receive the diagonal block of U                  *
-+         * for panel factorization of L(:,k)                *
-+         * note: we block for panel factorization of L(:,k) *
-+         * but panel factorization of U(:,k) don't          *
-+         * ================================================ */
- 
-         /* tag = ((k0<<2)+2) % tag_ub;        */
-         /* tag = (4*(nsupers+k0)+2) % tag_ub; */
-         // printf("hello message receiving%d %d\n",(nsupc*(nsupc+1))>>1,SLU_MPI_TAG(4,k0));
--#if ( PROFlevel>=1 )
--	TIC (t1);
--#endif
-         MPI_Recv (ublk_ptr, (nsupc * nsupc), SuperLU_MPI_DOUBLE_COMPLEX, krow,
-                   SLU_MPI_TAG (4, k0) /* tag */ ,
-                   comm, &status);
--#if ( PROFlevel>=1 )
--	TOC (t2, t1);
--	stat->utime[COMM] += t2;
--	stat->utime[COMM_DIAG] += t2;
--#endif
-         if (nsupr > 0) {
-+            char uplo = 'u', side = 'r', transa = 'n', diag = 'n';
-             doublecomplex alpha = {1.0, 0.0};
- 
- #ifdef PI_DEBUG
-             printf ("ztrsm non diagonal param 11:  %d \n", nsupr);
-             if (!lusup)
--                printf (" Rank :%d \t Empty block column occurred :\n", iam);
-+                printf (" Rank :%d \t Empty block column occured :\n", iam);
- #endif
- #if defined (USE_VENDOR_BLAS)
-             ztrsm_ ("R", "U", "N", "N", &nsupr, &nsupc,
-@@ -324,10 +298,12 @@ void pzgstrs2_omp
-     int_t *usub;
-     doublecomplex *lusup, *uval;
- 
--#if 0
--    //#ifdef USE_VTUNE
--    __SSC_MARK(0x111);// start SDE tracing, note uses 2 underscores
--    __itt_resume(); // start VTune, again use 2 underscores
-+#ifdef _OPENMP
-+    int thread_id = omp_get_thread_num ();
-+    int num_thread = omp_get_num_threads ();
-+#else
-+    int thread_id = 0;
-+    int num_thread = 1;
- #endif
- 
-     /* Quick return. */
-@@ -337,12 +313,15 @@ void pzgstrs2_omp
-     /* Initialization. */
-     iam = grid->iam;
-     pkk = PNUM (PROW (k, grid), PCOL (k, grid), grid);
--    //int k_row_cycle = k / grid->nprow;  /* for which cycle k exist (to assign rowwise thread blocking) */
--    //int gb_col_cycle;  /* cycle through block columns  */
-+    int k_row_cycle = k / grid->nprow;  /* for which cycle k exist (to assign rowwise thread blocking) */
-+    int gb_col_cycle;  /* cycle through block columns  */
-     klst = FstBlockC (k + 1);
-     knsupc = SuperSize (k);
-     usub = Llu->Ufstnz_br_ptr[lk];  /* index[] of block row U(k,:) */
-     uval = Llu->Unzval_br_ptr[lk];
-+    nb = usub[0];
-+    iukp = BR_HEADER;
-+    rukp = 0;
-     if (iam == pkk) {
-         lk = LBj (k, grid);
-         nsupr = Llu->Lrowind_bc_ptr[lk][1]; /* LDA of lusup[] */
-@@ -352,45 +331,28 @@ void pzgstrs2_omp
-         lusup = Llu->Lval_buf_2[k0 % (1 + stat->num_look_aheads)];
-     }
- 
--    /////////////////////new-test//////////////////////////
--    /* !! Taken from Carl/SuperLU_DIST_5.1.0/EXAMPLE/pdgstrf2_v3.c !! */
--
--    /* Master thread: set up pointers to each block in the row */
--    nb = usub[0];
--    iukp = BR_HEADER;
--    rukp = 0;
--    
--    int* blocks_index_pointers = SUPERLU_MALLOC (3 * nb * sizeof(int));
--    int* blocks_value_pointers = blocks_index_pointers + nb;
--    int* nsupc_temp = blocks_value_pointers + nb;
--    for (b = 0; b < nb; b++) { /* set up pointers to each block */
--	blocks_index_pointers[b] = iukp + UB_DESCRIPTOR;
--	blocks_value_pointers[b] = rukp;
--	gb = usub[iukp];
--	rukp += usub[iukp+1];
--	nsupc = SuperSize( gb );
--	nsupc_temp[b] = nsupc;
--	iukp += (UB_DESCRIPTOR + nsupc);  /* move to the next block */
--    }
--
--    // Sherry: this version is more NUMA friendly compared to pdgstrf2_v2.c
--    // https://stackoverflow.com/questions/13065943/task-based-programming-pragma-omp-task-versus-pragma-omp-parallel-for
--#pragma omp parallel for schedule(static) default(shared) \
--    private(b,j,iukp,rukp,segsize)
--    /* Loop through all the blocks in the row. */
--    for (b = 0; b < nb; ++b) {
--	iukp = blocks_index_pointers[b];
--	rukp = blocks_value_pointers[b];
-+    /* Loop through all the row blocks. */
-+    for (b = 0; b < nb; ++b)  {
-+        /* assuming column cyclic distribution of data among threads */
-+        gb = usub[iukp];
-+        gb_col_cycle = gb / grid->npcol;
-+        nsupc = SuperSize (gb);
-+        iukp += UB_DESCRIPTOR;
- 
-         /* Loop through all the segments in the block. */
--        for (j = 0; j < nsupc_temp[b]; j++) {
-+        for (j = 0; j < nsupc; ++j) {
-+#ifdef PI_DEBUG
-+            printf("segsize %d klst %d usub[%d] : %d",segsize,klst ,iukp,usub[iukp]);
-+#endif 
-             segsize = klst - usub[iukp++];
--	    if (segsize) {
--#pragma omp task default(shared) firstprivate(segsize,rukp) if (segsize > 30)
--		{ /* Nonzero segment. */
--		    int_t luptr = (knsupc - segsize) * (nsupr + 1);
--		    //printf("[2] segsize %d, nsupr %d\n", segsize, nsupr);
-+            if (segsize) {    /* Nonzero segment. */
-+                luptr = (knsupc - segsize) * (nsupr + 1);
- 
-+		/* if gb belongs to present thread then do the factorize */
-+                if ((gb_col_cycle + k_row_cycle + 1) % num_thread == thread_id) {
-+#ifdef PI_DEBUG
-+                    printf ("dtrsv param 4 %d param 6 %d\n", segsize, nsupr);
-+#endif
- #if defined (USE_VENDOR_BLAS)
-                     ztrsv_ ("L", "N", "U", &segsize, &lusup[luptr], &nsupr,
-                             &uval[rukp], &incx, 1, 1, 1);
-@@ -398,22 +360,12 @@ void pzgstrs2_omp
-                     ztrsv_ ("L", "N", "U", &segsize, &lusup[luptr], &nsupr,
-                             &uval[rukp], &incx);
- #endif
--		} /* end task */
--		rukp += segsize;
--		stat->ops[FACT] += segsize * (segsize + 1);
--	    } /* end if segsize > 0 */
--	} /* end for j in parallel ... */
--/* #pragma omp taskwait */
--    }  /* end for b ... */
--
--    /* Deallocate memory */
--    SUPERLU_FREE(blocks_index_pointers);
--
--#if 0
--    //#ifdef USE_VTUNE
--    __itt_pause(); // stop VTune
--    __SSC_MARK(0x222); // stop SDE tracing
--#endif
-+                }
-+                rukp += segsize;
-+		stat->ops[FACT] += 4.0 * (flops_t) segsize * (segsize + 1);
-+            } /* end if segsize > 0 */
-+        } /* end for j ... */
-+    } /* end for b ... */
- 
- } /* PZGSTRS2_omp */
- 
-diff --git a/SRC/pzgstrs.c b/SRC/pzgstrs.c
-index a537073..5a11f84 100644
---- a/SRC/pzgstrs.c
-+++ b/SRC/pzgstrs.c
-@@ -1174,7 +1174,7 @@ pzgstrs(int_t n, LUstruct_t *LUstruct,
- 
- 
-     /*
--     * Compute the internal nodes asynchronously by all processes.
-+     * Compute the internal nodes asychronously by all processes.
-      */
-     while ( nbrecvx || nbrecvmod ) { /* While not finished. */
- 
-diff --git a/SRC/pzgstrs1.c b/SRC/pzgstrs1.c
-index f2621c9..8924de8 100644
---- a/SRC/pzgstrs1.c
-+++ b/SRC/pzgstrs1.c
-@@ -779,7 +779,7 @@ void pzgstrs1(int_t n, LUstruct_t *LUstruct, gridinfo_t *grid,
- 
- 
-     /*
--     * Compute the internal nodes asynchronously by all processes.
-+     * Compute the internal nodes asychronously by all processes.
-      */
-     while ( nbrecvx || nbrecvmod ) { /* While not finished. */
- 
-diff --git a/SRC/pzgstrs_Bglobal.c b/SRC/pzgstrs_Bglobal.c
-index 47ba50f..e769f35 100644
---- a/SRC/pzgstrs_Bglobal.c
-+++ b/SRC/pzgstrs_Bglobal.c
-@@ -826,7 +826,7 @@ pzgstrs_Bglobal(int_t n, LUstruct_t *LUstruct, gridinfo_t *grid,
- 
- 
-     /*
--     * Compute the internal nodes asynchronously by all processes.
-+     * Compute the internal nodes asychronously by all processes.
-      */
-     while ( nbrecvx || nbrecvmod ) { /* While not finished. */
- 
-diff --git a/SRC/superlu_ddefs.h b/SRC/superlu_ddefs.h
-index 27b3487..7e6ccf2 100644
---- a/SRC/superlu_ddefs.h
-+++ b/SRC/superlu_ddefs.h
-@@ -46,8 +46,16 @@ typedef struct {
- typedef struct {
-     int_t   **Lrowind_bc_ptr; /* size ceil(NSUPERS/Pc)                 */
-     double  **Lnzval_bc_ptr;  /* size ceil(NSUPERS/Pc)                 */
-+    double  **Linv_bc_ptr;  /* size ceil(NSUPERS/Pc)                 */
-+	int_t   **Lindval_loc_bc_ptr; /* size ceil(NSUPERS/Pc)  pointers to locations in Lrowind_bc_ptr and Lnzval_bc_ptr               */
-+	int_t 	**Lrowind_bc_2_lsum; /* size ceil(NSUPERS/Pc)  map indices of Lrowind_bc_ptr to indices of lsum  */  
-+    double  **Uinv_bc_ptr;  /* size ceil(NSUPERS/Pc)                 */
-     int_t   **Ufstnz_br_ptr;  /* size ceil(NSUPERS/Pr)                 */
-     double  **Unzval_br_ptr;  /* size ceil(NSUPERS/Pr)                 */
-+	BcTree  *LBtree_ptr;       /* size ceil(NSUPERS/Pc)                */
-+	RdTree  *LRtree_ptr;		  /* size ceil(NSUPERS/Pr)                */
-+	BcTree  *UBtree_ptr;       /* size ceil(NSUPERS/Pc)                */
-+	RdTree  *URtree_ptr;		  /* size ceil(NSUPERS/Pr)                */	
- #if 0
-     int_t   *Lsub_buf;        /* Buffer for the remote subscripts of L */
-     double  *Lval_buf;        /* Buffer for the remote nonzeros of L   */
-@@ -92,7 +100,6 @@ typedef struct {
-     int_t   SolveMsgSent;     /* Number of actual messages sent in LU-solve */
-     int_t   SolveMsgVol;      /* Volume of messages sent in the solve phase */
- 
--
-     /*********************/	
-     /* The following variables are used in the hybrid solver */
- 
-@@ -118,6 +125,7 @@ typedef struct {
-     int_t n;
-     int_t nleaf;
-     int_t nfrecvmod;
-+	int_t inv; /* whether the diagonal block is inverted*/	
- } LocalLU_t;
- 
- 
-@@ -212,6 +220,10 @@ extern int     dcreate_matrix_rb(SuperMatrix *, int, double **, int *,
- extern int     dcreate_matrix_dat(SuperMatrix *, int, double **, int *, 
- 			      double **, int *, FILE *, gridinfo_t *);
- 
-+extern int 	   dcreate_matrix_postfix(SuperMatrix *, int, double **, int *, 
-+				  double **, int *, FILE *, char *, gridinfo_t *);				  
-+				  
-+				  
- /* Driver related */
- extern void    dgsequ_dist (SuperMatrix *, double *, double *, double *,
- 			    double *, double *, int_t *);
-@@ -242,11 +254,14 @@ extern void  pdgssvx_ABglobal(superlu_dist_options_t *, SuperMatrix *,
- 			      SuperLUStat_t *, int *);
- extern float pddistribute(fact_t, int_t, SuperMatrix *, 
- 			 ScalePermstruct_t *, Glu_freeable_t *, 
--			 LUstruct_t *, gridinfo_t *);
-+			 LUstruct_t *, gridinfo_t *, int_t);
- extern void  pdgssvx(superlu_dist_options_t *, SuperMatrix *, 
- 		     ScalePermstruct_t *, double *,
- 		     int, int, gridinfo_t *, LUstruct_t *,
- 		     SOLVEstruct_t *, double *, SuperLUStat_t *, int *);
-+
-+extern void  pdCompute_Diag_Inv(int_t, LUstruct_t *,gridinfo_t *, SuperLUStat_t *, int *);
-+		 
- extern int  dSolveInit(superlu_dist_options_t *, SuperMatrix *, int_t [], int_t [],
- 		       int_t, LUstruct_t *, gridinfo_t *, SOLVEstruct_t *);
- extern void dSolveFinalize(superlu_dist_options_t *, SOLVEstruct_t *);
-@@ -281,6 +296,22 @@ extern void dlsum_bmod(double *, double *, double *,
-                        int, int_t, int_t *, int_t *, Ucb_indptr_t **,
-                        int_t **, int_t *, gridinfo_t *, LocalLU_t *,
- 		       MPI_Request [], SuperLUStat_t *);
-+extern void dlsum_fmod_inv(double *, double *, double *, double *,
-+		       int, int, int_t , int_t *, int_t,
-+		       int_t *, gridinfo_t *, LocalLU_t *, 
-+		       SuperLUStat_t **, int_t *, int_t *, int_t, int_t, int_t);
-+extern void dlsum_fmod_inv_master(double *, double *, double *, double *,
-+		       int, int, int_t , int_t *, int_t, 
-+		       int_t *, gridinfo_t *, LocalLU_t *, 
-+		       SuperLUStat_t **, int_t, int_t, int_t);
-+extern void dlsum_bmod_inv(double *, double *, double *, double *,
-+                       int, int_t, int_t *, int_t *, int_t *, Ucb_indptr_t **,
-+                       int_t **, int_t *, gridinfo_t *, LocalLU_t *,
-+		       MPI_Request [], SuperLUStat_t **, int_t *, int_t *, int_t, int_t);
-+extern void dlsum_bmod_inv_master(double *, double *, double *, double *,
-+                       int, int_t, int_t *, int_t *, int_t *, Ucb_indptr_t **,
-+                       int_t **, int_t *, gridinfo_t *, LocalLU_t *,
-+		       MPI_Request [], SuperLUStat_t **, int_t, int_t);			   
- extern void pdgsrfs(int_t, SuperMatrix *, double, LUstruct_t *,
- 		    ScalePermstruct_t *, gridinfo_t *,
- 		    double [], int_t, double [], int_t, int,
-@@ -323,11 +354,12 @@ extern void  dreadrb_dist(int, FILE *, int_t *, int_t *, int_t *,
- 		     double **, int_t **, int_t **);
- extern void  dreadMM_dist(FILE *, int_t *, int_t *, int_t *,
- 	                  double **, int_t **, int_t **);
--
-+extern int  dread_binary(FILE *, int_t *, int_t *, int_t *,
-+	                  double **, int_t **, int_t **);													 
- /* Distribute the data for numerical factorization */
- extern float ddist_psymbtonum(fact_t, int_t, SuperMatrix *,
-                                 ScalePermstruct_t *, Pslu_freeable_t *, 
--                                LUstruct_t *, gridinfo_t *);
-+                                LUstruct_t *, gridinfo_t *, int_t nrhs);
- extern void pdGetDiagU(int_t, LUstruct_t *, gridinfo_t *, double *);
- 
- 
-@@ -358,6 +390,13 @@ extern void dgemv_(char *, int *, int *, double *, double *a, int *,
- extern void dger_(int*, int*, double*, double*, int*,
-                  double*, int*, double*, int*);
- 
-+extern int daxpy_(int *, double *, double *, int *, double *, int *);				 
-+
-+				 
-+extern void dtrtri_(char*, char*, int*, double*, int*,int*);				 
-+
-+			 	
-+				 
- #else
- extern int dgemm_(const char*, const char*, const int*, const int*, const int*,
-                    const double*,  const double*,  const int*,  const double*,
-@@ -370,6 +409,8 @@ extern int dgemv_(char *, int *, int *, double *, double *a, int *,
-                   double *, int *, double *, double *, int *);
- extern void dger_(int*, int*, double*, double*, int*,
-                  double*, int*, double*, int*);
-+				 
-+extern int daxpy_(int *, double *, double *, int *, double *, int *);				 
- 
- #endif
- 
-diff --git a/SRC/superlu_defs.h b/SRC/superlu_defs.h
-index 1925acb..553cbb9 100644
---- a/SRC/superlu_defs.h
-+++ b/SRC/superlu_defs.h
-@@ -42,13 +42,17 @@ at the top-level directory.
- #include <stdio.h>
- #include <limits.h>
- #include <string.h>
--
--/* Following is for vtune */
--#if 0
-+#include <stdatomic.h>
-+#include <math.h>
-+
-+// /* Following is for vtune */
-+// #if 0
-+// #include <ittnotify.h>
-+// #define USE_VTUNE
-+// #endif
-+#if ( VTUNE>=1 )
- #include <ittnotify.h>
--#define USE_VTUNE
- #endif
--
- /*************************************************************************
-  * Constants
-  **************************************************************************/
-@@ -65,9 +69,9 @@ at the top-level directory.
- #define SUPERLU_DIST_MAJOR_VERSION     5
- #define SUPERLU_DIST_MINOR_VERSION     3
- #define SUPERLU_DIST_PATCH_VERSION     0
--#define SUPERLU_DIST_RELEASE_DATE      "January 28, 2018"
-+#define SUPERLU_DIST_RELEASE_DATE      "January 28, 2018"					  
- 
--#include "superlu_dist_config.h"
-+#include "superlu_dist_config.h"							   
- /* Define my integer size int_t */
- #ifdef _CRAY
-   typedef short int_t;
-@@ -83,6 +87,21 @@ at the top-level directory.
-   #define IFMT "%8d"
- #endif
- 
-+ 
-+
-+
-+// /* Define atomic int_t */
-+// #ifdef _CRAY
-+  // typedef atomic_short int_t_ato  ;
-+// #elif defined (_LONGINT)
-+  // typedef atomic_llong int_t_ato  ;
-+// #else /* Default */
-+  // typedef atomic_int int_t_ato  ;
-+// #endif
-+
-+
-+
-+
- #include "superlu_enum_consts.h"
- #include "Cnames.h"
- #include "supermatrix.h"
-@@ -173,7 +192,13 @@ at the top-level directory.
- #define GSUM     20 
- #define Xk       21
- #define Yk       22
--#define LSUM     23
-+#define LSUM     23    
-+
-+ 
-+static const int BC_L=1;	/* MPI tag for x in L-solve*/	
-+static const int RD_L=2;	/* MPI tag for lsum in L-solve*/	
-+static const int BC_U=3;	/* MPI tag for x in U-solve*/
-+static const int RD_U=4;	/* MPI tag for lsum in U-solve*/	
- 
- /* 
-  * Communication scopes
-@@ -221,6 +246,10 @@ at the top-level directory.
- #define SuperLU_timer_  SuperLU_timer_dist_
- #define LOG2(x)   (log10((double) x) / log10(2.0))
- 
-+#define MIN(a,b) ((a) <= (b) ? (a) : (b))
-+#define MAX(a,b) ((a) >= (b) ? (a) : (b))
-+
-+
- 
- #if ( VAMPIR>=1 ) 
- #define VT_TRACEON    VT_traceon()
-@@ -243,6 +272,20 @@ at the top-level directory.
- #endif /* MSVC */
- #endif /* SUPERLU_DIST_EXPORT */
- 
-+#ifdef __cplusplus
-+extern "C" {
-+#endif
-+
-+
-+#ifndef max
-+    #define cmax(a,b) ((a) > (b) ? (a) : (b))
-+#endif
-+
-+#ifdef __cplusplus
-+  }
-+#endif
-+
-+
- /***********************************************************************
-  * New data types
-  ***********************************************************************/
-@@ -579,6 +622,7 @@ typedef struct {
- typedef struct {
-     fact_t        Fact;
-     yes_no_t      Equil;
-+    yes_no_t      DiagInv;
-     colperm_t     ColPerm;
-     trans_t       Trans;
-     IterRefine_t  IterRefine;
-@@ -724,6 +768,10 @@ extern void  PStatPrint(superlu_dist_options_t *, SuperLUStat_t *, gridinfo_t *)
- extern void  log_memory(long long, SuperLUStat_t *);
- extern void  print_memorylog(SuperLUStat_t *, char *);
- extern int   superlu_dist_GetVersionNumber(int *, int *, int *);
-+extern void  quickSort( int_t*, int_t, int_t, int_t);
-+extern void  quickSortM( int_t*, int_t, int_t, int_t, int_t, int_t);
-+extern int_t partition( int_t*, int_t, int_t, int_t);
-+extern int_t partitionM( int_t*, int_t, int_t, int_t, int_t, int_t);
- 
- /* Prototypes for parallel symbolic factorization */
- extern float symbfact_dist
-@@ -778,6 +826,45 @@ extern int   file_PrintInt10(FILE *, char *, int_t, int_t *);
- extern int   file_PrintInt32(FILE *, char *, int, int *);
- extern int   file_PrintLong10(FILE *, char *, int_t, int_t *);
- 
-+
-+/* Routines for Async_tree communication*/
-+
-+#ifndef __SUPERLU_ASYNC_TREE /* allow multiple inclusions */
-+#define __SUPERLU_ASYNC_TREE
-+typedef void* BcTree;
-+typedef void* RdTree;
-+typedef void* StdList;
-+#endif
-+
-+// typedef enum {NO, YES}  yes_no_t;
-+extern RdTree   RdTree_Create(MPI_Comm comm, int* ranks, int rank_cnt, int msgSize, double rseed);  
-+extern void   	RdTree_Destroy(RdTree Tree);
-+extern void 	RdTree_SetTag(RdTree Tree, int tag);
-+extern yes_no_t RdTree_IsRoot(RdTree Tree);
-+extern void 	RdTree_forwardMessageSimple(RdTree Tree, void* localBuffer);
-+extern void 	RdTree_allocateRequest(RdTree Tree);
-+extern int  	RdTree_GetDestCount(RdTree Tree);
-+extern void 	RdTree_waitSendRequest(RdTree Tree);
-+
-+extern BcTree   BcTree_Create(MPI_Comm comm, int* ranks, int rank_cnt, int msgSize, double rseed);  
-+extern void   	BcTree_Destroy(BcTree Tree);
-+extern void 	BcTree_SetTag(BcTree Tree, int tag);
-+extern yes_no_t BcTree_IsRoot(BcTree Tree);
-+extern void 	BcTree_forwardMessageSimple(BcTree Tree, void* localBuffer);
-+extern void 	BcTree_allocateRequest(BcTree Tree);
-+extern int 		BcTree_getDestCount(BcTree Tree); 
-+extern void 	BcTree_waitSendRequest(BcTree Tree);
-+ 
-+extern StdList 	StdList_Init();
-+extern void 	StdList_Pushback(StdList lst, int_t dat);
-+extern void 	StdList_Pushfront(StdList lst, int_t dat);
-+extern int_t 		StdList_Popfront(StdList lst);
-+extern yes_no_t StdList_Find(StdList lst, int_t dat);
-+extern int_t 	   	StdList_Size(StdList lst);
-+yes_no_t 		StdList_Empty(StdList lst);
-+
-+
-+
- #ifdef __cplusplus
-   }
- #endif
-diff --git a/SRC/superlu_dist_config.h b/SRC/superlu_dist_config.h
-deleted file mode 100644
-index 74a872f..0000000
---- a/SRC/superlu_dist_config.h
-+++ /dev/null
-@@ -1,12 +0,0 @@
--/* superlu_dist_config.h.in */
--
--/* Enable parmetis */
--#define HAVE_PARMETIS TRUE
--
--/* enable 64bit index mode */
--/* #undef XSDK_INDEX_SIZE */
--
--#if (XSDK_INDEX_SIZE == 64)
--#define _LONGINT 1
--#endif
--
-diff --git a/SRC/superlu_dist_config.h.in b/SRC/superlu_dist_config.h.in
-deleted file mode 100644
-index c27b453..0000000
---- a/SRC/superlu_dist_config.h.in
-+++ /dev/null
-@@ -1,12 +0,0 @@
--/* superlu_dist_config.h.in */
--
--/* Enable parmetis */
--#cmakedefine HAVE_PARMETIS @HAVE_PARMETIS@
--
--/* enable 64bit index mode */
--#cmakedefine XSDK_INDEX_SIZE @XSDK_INDEX_SIZE@
--
--#if (XSDK_INDEX_SIZE == 64)
--#define _LONGINT 1
--#endif
--
-diff --git a/SRC/superlu_dist_version.c b/SRC/superlu_dist_version.c
-deleted file mode 100644
-index c6c8759..0000000
---- a/SRC/superlu_dist_version.c
-+++ /dev/null
-@@ -1,30 +0,0 @@
--/*! \file
--Copyright (c) 2003, The Regents of the University of California, through
--Lawrence Berkeley National Laboratory (subject to receipt of any required 
--approvals from U.S. Dept. of Energy) 
--
--All rights reserved. 
--
--The source code is distributed under BSD license, see the file License.txt
--at the top-level directory.
--*/
--/** @file superlu_dist_version.h
-- * \brief Gets the SuperLU_DIST's version information from the library.
-- *
-- * -- Distributed SuperLU routine (version 5.2) --
-- * Lawrence Berkeley National Lab, Univ. of California Berkeley, 
-- * October 13, 2017
-- *
-- */
--
--#include "superlu_defs.h"
--
--int superlu_dist_GetVersionNumber(int *major, int *minor, int *bugfix)
--{
--  if (major) *major = SUPERLU_DIST_MAJOR_VERSION;
--  if (minor) *minor = SUPERLU_DIST_MINOR_VERSION;
--  if (bugfix) *bugfix = SUPERLU_DIST_PATCH_VERSION;
--  return 0;
--}
--
--
-diff --git a/SRC/superlu_enum_consts.h b/SRC/superlu_enum_consts.h
-index 628eecf..ceac433 100644
---- a/SRC/superlu_enum_consts.h
-+++ b/SRC/superlu_enum_consts.h
-@@ -14,7 +14,7 @@ at the top-level directory.
-  * -- SuperLU routine (version 4.1) --
-  * Lawrence Berkeley National Lab, Univ. of California Berkeley, 
-  * October 1, 2010
-- * January 28, 2018
-+ * January 28, 2018				   
-  *
-  */
- 
-@@ -33,7 +33,7 @@ typedef enum {NOTRANS, TRANS, CONJ}                             trans_t;
- typedef enum {NOEQUIL, ROW, COL, BOTH}                          DiagScale_t;
- typedef enum {NOREFINE, SLU_SINGLE=1, SLU_DOUBLE, SLU_EXTRA}    IterRefine_t;
- //typedef enum {LUSUP, UCOL, LSUB, USUB, LLVL, ULVL, NO_MEMTYPE}  MemType;
--typedef enum {USUB, LSUB, UCOL, LUSUP, LLVL, ULVL, NO_MEMTYPE}  MemType;
-+typedef enum {USUB, LSUB, UCOL, LUSUP, LLVL, ULVL, NO_MEMTYPE}  MemType;																		
- typedef enum {HEAD, TAIL}                                       stack_end_t;
- typedef enum {SYSTEM, USER}                                     LU_space_t;
- typedef enum {ONE_NORM, TWO_NORM, INF_NORM}			norm_t;
-@@ -71,8 +71,11 @@ typedef enum {
-     COMM,    /* communication for factorization */
-     COMM_DIAG, /* Bcast diagonal block to process column */
-     COMM_RIGHT, /* communicate L panel */
--    COMM_DOWN, /* communicate U panel */
-+    COMM_DOWN, /* communicate U panel */										
-     SOL_COMM,/* communication for solve */
-+    SOL_GEMM,/* gemm for solve */
-+    SOL_TRSM,/* trsm for solve */
-+	SOL_L,	/* LU-solve time*/
-     RCOND,   /* estimate reciprocal condition number */
-     SOLVE,   /* forward and back solves */
-     REFINE,  /* perform iterative refinement */
-@@ -82,5 +85,4 @@ typedef enum {
-     NPHASES  /* total number of phases */
- } PhaseType;
- 
--
- #endif /* __SUPERLU_ENUM_CONSTS */
-diff --git a/SRC/util.c b/SRC/util.c
-index afceea0..10782b9 100644
---- a/SRC/util.c
-+++ b/SRC/util.c
-@@ -22,6 +22,7 @@ at the top-level directory.
-  */
- 
- #include <math.h>
-+#include <unistd.h>
- #include "superlu_ddefs.h"
- 
- /*! \brief Deallocate the structure pointing to the actual storage of the matrix. */
-@@ -144,6 +145,64 @@ Destroy_LU(int_t n, gridinfo_t *grid, LUstruct_t *LUstruct)
-     SUPERLU_FREE(Llu->bsendx_plist);
-     SUPERLU_FREE(Llu->mod_bit);
- 
-+	
-+	
-+    nb = CEILING(nsupers, grid->npcol);
-+    for (i = 0; i < nb; ++i) 
-+	if ( Llu->Lindval_loc_bc_ptr[i] ) {
-+	    SUPERLU_FREE (Llu->Lindval_loc_bc_ptr[i]);
-+	}	
-+	SUPERLU_FREE(Llu->Lindval_loc_bc_ptr);
-+	
-+ 
-+	nb = CEILING(nsupers, grid->npcol);
-+	for (i=0;i<nb;++i){
-+		if(Llu->LBtree_ptr[i]!=NULL){
-+			BcTree_Destroy(Llu->LBtree_ptr[i]);
-+		}
-+		if(Llu->UBtree_ptr[i]!=NULL){
-+			BcTree_Destroy(Llu->UBtree_ptr[i]);
-+		}		
-+	}
-+	SUPERLU_FREE(Llu->LBtree_ptr);
-+	SUPERLU_FREE(Llu->UBtree_ptr);
-+	
-+ 	nb = CEILING(nsupers, grid->nprow);
-+	for (i=0;i<nb;++i){
-+		if(Llu->LRtree_ptr[i]!=NULL){
-+			RdTree_Destroy(Llu->LRtree_ptr[i]);
-+		}
-+		if(Llu->URtree_ptr[i]!=NULL){
-+			RdTree_Destroy(Llu->URtree_ptr[i]);
-+		}		
-+	}
-+	SUPERLU_FREE(Llu->LRtree_ptr);
-+	SUPERLU_FREE(Llu->URtree_ptr);
-+
-+	nb = CEILING(nsupers, grid->npcol);
-+	for (i=0;i<nb;++i){
-+		if(Llu->Linv_bc_ptr[i]!=NULL){
-+			SUPERLU_FREE(Llu->Linv_bc_ptr[i]);
-+		}
-+		if(Llu->Uinv_bc_ptr[i]!=NULL){
-+			SUPERLU_FREE(Llu->Uinv_bc_ptr[i]);
-+		}	
-+	}
-+	SUPERLU_FREE(Llu->Linv_bc_ptr);
-+	SUPERLU_FREE(Llu->Uinv_bc_ptr);
-+	
-+	
-+	nb = CEILING(nsupers, grid->npcol);
-+    for (i = 0; i < nb; ++i)
-+	if ( Llu->Urbs[i] ) {
-+	    SUPERLU_FREE(Llu->Ucb_indptr[i]);
-+	    SUPERLU_FREE(Llu->Ucb_valptr[i]);
-+	}
-+    SUPERLU_FREE(Llu->Ucb_indptr);
-+    SUPERLU_FREE(Llu->Ucb_valptr);	
-+	SUPERLU_FREE(Llu->Urbs);
-+
-+	
-     SUPERLU_FREE(Glu_persist->xsup);
-     SUPERLU_FREE(Glu_persist->supno);
- 
-@@ -193,6 +252,7 @@ void LUstructInit(const int_t n, LUstruct_t *LUstruct)
-     if ( !(LUstruct->Llu = (LocalLU_t *)
- 	   SUPERLU_MALLOC(sizeof(LocalLU_t))) )
- 	ABORT("Malloc fails for LocalLU_t.");
-+	LUstruct->Llu->inv = 0;
- }
- 
- /*! \brief Deallocate LUstruct */
-@@ -327,7 +387,7 @@ void set_default_options_dist(superlu_dist_options_t *options)
-     options->ColPerm           = METIS_AT_PLUS_A;
- #else
-     options->ColPerm            = MMD_AT_PLUS_A;
--#endif
-+#endif	  
-     options->RowPerm           = LargeDiag;
-     options->ReplaceTinyPivot  = NO;
-     options->IterRefine        = SLU_DOUBLE;
-@@ -338,6 +398,7 @@ void set_default_options_dist(superlu_dist_options_t *options)
-     options->num_lookaheads    = 10;
-     options->lookahead_etree   = NO;
-     options->SymPattern        = NO;
-+    options->DiagInv           = NO;
- }
- 
- /*! \brief Print the options setting.
-@@ -620,60 +681,121 @@ PStatPrint(superlu_dist_options_t *options, SuperLUStat_t *stat, gridinfo_t *gri
- 	       0, grid->comm);
-     solveflop = flopcnt;
-     if ( !iam ) {
--	printf("\tSOLVE time         %8.2f\n", utime[SOLVE]);
-+	printf("\tSOLVE time         %8.3f\n", utime[SOLVE]);
- 	if ( utime[SOLVE] != 0.0 )
- 	    printf("\tSolve flops\t%e\tMflops \t%8.2f\n",
- 		   flopcnt,
- 		   flopcnt*1e-6/utime[SOLVE]);
- 	if ( options->IterRefine != NOREFINE ) {
--	    printf("\tREFINEMENT time    %8.2f\tSteps%8d\n\n",
-+	    printf("\tREFINEMENT time    %8.3f\tSteps%8d\n\n",
- 		   utime[REFINE], stat->RefineSteps);
- 	}
- 	printf("**************************************************\n");
-     }
- 
-+	double  *utime1,*utime2,*utime3,*utime4;
-+	flops_t  *ops1;
- #if ( PROFlevel>=1 )
--    fflush(stdout);
-+	
-     MPI_Barrier( grid->comm );
- 
-     {
- 	int_t i, P = grid->nprow*grid->npcol;
- 	flops_t b, maxflop;
-+	
-+		
-+	if ( !iam )utime1=doubleMalloc_dist(P);
-+	if ( !iam )utime2=doubleMalloc_dist(P);
-+	if ( !iam )utime3=doubleMalloc_dist(P);
-+	if ( !iam )utime4=doubleMalloc_dist(P);
-+	if ( !iam )ops1=(flops_t *) SUPERLU_MALLOC(P * sizeof(flops_t));
-+
-+	
-+	// fflush(stdout); 
-+	// if ( !iam ) printf("\n.. Tree max sizes:\tbtree\trtree\n");
-+	// fflush(stdout);
-+	// sleep(2.0); 	
-+	// MPI_Barrier( grid->comm );
-+	// for (i = 0; i < P; ++i) {
-+	    // if ( iam == i) {
-+		// printf("\t\t%d %5d %5d\n", iam, stat->MaxActiveBTrees,stat->MaxActiveRTrees);
-+		// fflush(stdout);
-+	    // }
-+	    // MPI_Barrier( grid->comm );
-+	// }	
-+	
-+	// sleep(2.0); 	
-+
-+	
-+	MPI_Barrier( grid->comm );	
-+	
- 	if ( !iam ) printf("\n.. FACT time breakdown:\tcomm\ttotal\n");
-+
-+    MPI_Gather(&utime[COMM], 1, MPI_DOUBLE,utime1, 1 , MPI_DOUBLE, 0, grid->comm);	
-+    MPI_Gather(&utime[FACT], 1, MPI_DOUBLE,utime2, 1 , MPI_DOUBLE, 0, grid->comm);	
-+	if ( !iam ) 
- 	for (i = 0; i < P; ++i) {
--	    if ( iam == i) {
--		printf("\t\t(%d)%8.2f%8.2f\n", iam, utime[COMM], utime[FACT]);
--		fflush(stdout);
--	    }
--	    MPI_Barrier( grid->comm );
-+		printf("\t\t(%d)%8.2f%8.2f\n", i, utime1[i], utime2[i]);
- 	}
-+	fflush(stdout);
-+	MPI_Barrier( grid->comm );	
-+	
- 	if ( !iam ) printf("\n.. FACT ops distribution:\n");
-+    MPI_Gather(&ops[FACT], 1, MPI_FLOAT,ops1, 1 , MPI_FLOAT, 0, grid->comm);
-+	
-+	if ( !iam ) 
- 	for (i = 0; i < P; ++i) {
--	    if ( iam == i ) {
--		printf("\t\t(%d)\t%e\n", iam, ops[FACT]);
--		fflush(stdout);
--	    }
--	    MPI_Barrier( grid->comm );
-+		printf("\t\t(%d)\t%e\n", i, ops1[i]);
- 	}
-+	fflush(stdout);
-+	MPI_Barrier( grid->comm );
-+	
- 	MPI_Reduce(&ops[FACT], &maxflop, 1, MPI_FLOAT, MPI_MAX, 0, grid->comm);
-+
- 	if ( !iam ) {
- 	    b = factflop/P/maxflop;
- 	    printf("\tFACT load balance: %.2f\n", b);
- 	}
--	if ( !iam ) printf("\n.. SOLVE ops distribution:\n");
-+	fflush(stdout);
-+	MPI_Barrier( grid->comm );
-+
-+	
-+	if ( !iam ) printf("\n.. SOLVE time breakdown:\tcommL \tgemmL\ttrsmL\ttotal\n");
-+
-+    MPI_Gather(&utime[SOL_COMM], 1, MPI_DOUBLE,utime1, 1 , MPI_DOUBLE, 0, grid->comm);	
-+    MPI_Gather(&utime[SOL_GEMM], 1, MPI_DOUBLE,utime2, 1 , MPI_DOUBLE, 0, grid->comm);		
-+    MPI_Gather(&utime[SOL_TRSM], 1, MPI_DOUBLE,utime3, 1 , MPI_DOUBLE, 0, grid->comm);		
-+    MPI_Gather(&utime[SOL_L], 1, MPI_DOUBLE,utime4, 1 , MPI_DOUBLE, 0, grid->comm);		
-+	if ( !iam ) 	
-+	for (i = 0; i < P; ++i) {
-+		printf("\t\t\t%d%10.5f%10.5f%10.5f%10.5f\n", i,utime1[i],utime2[i],utime3[i], utime4[i]);
-+	}
-+	fflush(stdout); 
-+	MPI_Barrier( grid->comm );	
-+	
-+	if ( !iam ) printf("\n.. SOLVE ops distribution:\n"); 
-+    MPI_Gather(&ops[SOLVE], 1, MPI_FLOAT,ops1, 1 , MPI_FLOAT, 0, grid->comm);	
-+	if ( !iam ) 
- 	for (i = 0; i < P; ++i) {
--	    if ( iam == i ) {
--		printf("\t\t%d\t%e\n", iam, ops[SOLVE]);
--		fflush(stdout);
--	    }
--	    MPI_Barrier( grid->comm );
-+		printf("\t\t%d\t%e\n", i, ops1[i]);
- 	}
- 	MPI_Reduce(&ops[SOLVE], &maxflop, 1, MPI_FLOAT, MPI_MAX, 0,grid->comm);
- 	if ( !iam ) {
- 	    b = solveflop/P/maxflop;
- 	    printf("\tSOLVE load balance: %.2f\n", b);
-+		fflush(stdout);
- 	}
-+	
-     }
-+	
-+	if ( !iam ){
-+	SUPERLU_FREE(utime1);
-+	SUPERLU_FREE(utime2);
-+	SUPERLU_FREE(utime3);
-+	SUPERLU_FREE(utime4);
-+	SUPERLU_FREE(ops1);
-+	}
-+	
- #endif
- 
- /*  if ( !iam ) fflush(stdout);  CRASH THE SYSTEM pierre.  */
-@@ -1085,13 +1207,13 @@ arrive_at_ublock (int_t j,      /* j-th block in a U panel */
-         /* Reinitilize the pointers to the beginning of the 
- 	 * k-th column/row of L/U factors.
- 	 * usub[] - index array for panel U(k,:)
--	 */
-         // printf("iukp %d \n",*iukp );
-         *jb = usub[*iukp];      /* Global block number of block U(k,j). */
-         // printf("jb %d \n",*jb );
-         *nsupc = SuperSize (*jb);
-         // printf("nsupc %d \n",*nsupc );
-         *iukp += UB_DESCRIPTOR; /* Start fstnz of block U(k,j). */
-+
-         *rukp += usub[*iukp - 1]; /* Jump # of nonzeros in block U(k,jj);
- 				     Move to block U(k,jj+1) in nzval[] */ 
-         *iukp += *nsupc;
-@@ -1184,8 +1306,113 @@ int_t estimate_bigu_size(int_t nsupers,
-     MPI_Allreduce(&ldu, &max_ldu, 1, mpi_int_t, MPI_MAX, grid->cscp.comm);
- 
- #if ( PRNTlevel>=1 )
-+	if(iam==0)
-     printf("max_ncols %d, max_ldu %d, ldt %d, bigu_size=%d\n",
- 	   max_ncols, max_ldu, ldt, max_ldu*max_ncols);
- #endif
-     return(max_ldu * max_ncols);
- }
-+
-+
-+
-+void quickSort( int_t* a, int_t l, int_t r, int_t dir)
-+{
-+   int_t j;
-+
-+   if( l < r ) 
-+   {
-+   	// divide and conquer
-+       j = partition( a, l, r, dir);
-+       quickSort( a, l, j-1, dir);
-+       quickSort( a, j+1, r, dir);
-+   }
-+	
-+}
-+
-+int_t partition( int_t* a, int_t l, int_t r, int_t dir) {
-+   int_t pivot, i, j, t;
-+   pivot = a[l];
-+   i = l; j = r+1;
-+   
-+   if(dir==0){		
-+	   while( 1)
-+	   {
-+		do ++i; while( a[i] <= pivot && i <= r );
-+		do --j; while( a[j] > pivot );
-+		if( i >= j ) break;
-+		t = a[i]; a[i] = a[j]; a[j] = t;
-+	   }
-+	   t = a[l]; a[l] = a[j]; a[j] = t;
-+	   return j;
-+   }else if(dir==1){
-+	   while( 1)
-+	   {
-+		do ++i; while( a[i] >= pivot && i <= r );
-+		do --j; while( a[j] < pivot );
-+		if( i >= j ) break;
-+		t = a[i]; a[i] = a[j]; a[j] = t;
-+	   }
-+	   t = a[l]; a[l] = a[j]; a[j] = t;
-+	   return j;	   
-+   }
-+}
-+
-+
-+
-+void quickSortM( int_t* a, int_t l, int_t r, int_t lda, int_t dir, int_t dims)
-+{
-+   int_t j;
-+
-+   if( l < r ) 
-+   {
-+	   	// printf("dims: %5d",dims);
-+		// fflush(stdout);
-+		
-+   	// divide and conquer
-+       j = partitionM( a, l, r,lda,dir, dims);
-+       quickSortM( a, l, j-1,lda,dir,dims);
-+       quickSortM( a, j+1, r,lda,dir,dims);
-+   }
-+	
-+}
-+
-+
-+int_t partitionM( int_t* a, int_t l, int_t r, int_t lda, int_t dir, int_t dims) {
-+   int_t pivot, i, j, t, dd;
-+   pivot = a[l];
-+   i = l; j = r+1;
-+
-+	if(dir==0){
-+	   while( 1)
-+	   {
-+		do ++i; while( a[i] <= pivot && i <= r );
-+		do --j; while( a[j] > pivot );
-+		if( i >= j ) break; 
-+		for(dd=0;dd<dims;dd++){	
-+			t = a[i+lda*dd]; a[i+lda*dd] = a[j+lda*dd]; a[j+lda*dd] = t;	
-+		}
-+	   }
-+	   for(dd=0;dd<dims;dd++){	
-+		t = a[l+lda*dd]; a[l+lda*dd] = a[j+lda*dd]; a[j+lda*dd] = t;
-+	   }	   
-+	   return j;		
-+	}else if(dir==1){
-+	   while( 1)
-+	   {
-+		do ++i; while( a[i] >= pivot && i <= r );
-+		do --j; while( a[j] < pivot );
-+		if( i >= j ) break;
-+		for(dd=0;dd<dims;dd++){	
-+			t = a[i+lda*dd]; a[i+lda*dd] = a[j+lda*dd]; a[j+lda*dd] = t;	
-+		}
-+	   }
-+	   for(dd=0;dd<dims;dd++){	
-+		t = a[l+lda*dd]; a[l+lda*dd] = a[j+lda*dd]; a[j+lda*dd] = t;
-+	   } 
-+	   return j;		
-+	}
-+}
-+
-+
-+
-+
-diff --git a/SRC/util_dist.h b/SRC/util_dist.h
-index f40b29c..06ae379 100644
---- a/SRC/util_dist.h
-+++ b/SRC/util_dist.h
-@@ -90,6 +90,8 @@ typedef struct {
-     float   current_buffer; /* bytes allocated for buffer in numerical factorization */
-     float   peak_buffer;    /* monitor the peak buffer size (bytes) */
-     float   gpu_buffer;     /* monitor the buffer allocated on GPU (bytes) */
-+	int_t MaxActiveBTrees;
-+	int_t MaxActiveRTrees;	
- } SuperLUStat_t;
- 
- /* Headers for 2 types of dynamatically managed memory */
-@@ -117,7 +119,7 @@ typedef struct {
- 			      4: llvl; level number in L for ILU(k)
- 			      5: ulvl; level number in U for ILU(k)
-                            */
--#endif
-+#endif	  
- 
- /* Macros to manipulate stack */
- #define StackFull(x)         ( x + stack.used >= stack.size )
-diff --git a/SRC/wingetopt.c b/SRC/wingetopt.c
-deleted file mode 100644
-index 2128234..0000000
---- a/SRC/wingetopt.c
-+++ /dev/null
-@@ -1,53 +0,0 @@
--/* *****************************************************************
--*
--* Copyright 2016 Microsoft
--*
--*
--* Licensed under the Apache License, Version 2.0 (the "License");
--* you may not use this file except in compliance with the License.
--* You may obtain a copy of the License at
--*
--*      http://www.apache.org/licenses/LICENSE-2.0
--*
--* Unless required by applicable law or agreed to in writing, software
--* distributed under the License is distributed on an "AS IS" BASIS,
--* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
--* See the License for the specific language governing permissions and
--* limitations under the License.
--*
--* Obtained from https://github.com/iotivity/iotivity/tree/master/resource/c_common/windows
--*
--******************************************************************/
--
--#include "wingetopt.h"
--#include <windows.h>
--
--char* optarg = NULL;
--int optind = 1;
--
--int getopt(int argc, char *const argv[], const char *optstring)
--{
--    if ((optind >= argc) || (argv[optind][0] != '-') || (argv[optind][0] == 0))
--    {
--        return -1;
--    }
--
--    int opt = argv[optind][1];
--    const char *p = strchr(optstring, opt);
--
--    if (p == NULL)
--    {
--        return '?';
--    }
--    if (p[1] == ':')
--    {
--        optind++;
--        if (optind >= argc)
--        {
--            return '?';
--        }
--        optarg = argv[optind];
--        optind++;
--    }
--    return opt;
--}
-diff --git a/SRC/wingetopt.h b/SRC/wingetopt.h
-deleted file mode 100644
-index b5100cc..0000000
---- a/SRC/wingetopt.h
-+++ /dev/null
-@@ -1,38 +0,0 @@
--/* *****************************************************************
--*
--* Copyright 2016 Microsoft
--*
--*
--* Licensed under the Apache License, Version 2.0 (the "License");
--* you may not use this file except in compliance with the License.
--* You may obtain a copy of the License at
--*
--*      http://www.apache.org/licenses/LICENSE-2.0
--*
--* Unless required by applicable law or agreed to in writing, software
--* distributed under the License is distributed on an "AS IS" BASIS,
--* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
--* See the License for the specific language governing permissions and
--* limitations under the License.
--*
--* Obtained from https://github.com/iotivity/iotivity/tree/master/resource/c_common/windows
--*
--******************************************************************/
--
--#ifndef WINGETOPT_H__
--#define WINGETOPT_H__
--
--#ifdef __cplusplus
--extern "C" {
--#endif
--
--extern char *optarg;
--extern int optind;
--
--int getopt(int argc, char *const argv[], const char *optstring);
--
--#ifdef __cplusplus
--}
--#endif
--
--#endif
-diff --git a/TEST/pzcompute_resid.c b/TEST/pzcompute_resid.c
-deleted file mode 100644
-index 0c29fac..0000000
---- a/TEST/pzcompute_resid.c
-+++ /dev/null
-@@ -1,154 +0,0 @@
--/*! \file
--Copyright (c) 2003, The Regents of the University of California, through
--Lawrence Berkeley National Laboratory (subject to receipt of any required 
--approvals from U.S. Dept. of Energy) 
--
--All rights reserved. 
--
--The source code is distributed under BSD license, see the file License.txt
--at the top-level directory.
--*/
--
--/*! @file
-- * \brief Test for small residual.
-- *
-- * -- Distributed SuperLU routine (version 5.2) --
-- * Lawrence Berkeley National Lab, Univ. of California Berkeley.
-- * September 30, 2017
-- *
-- */
--#include "superlu_zdefs.h"
--
--int pzcompute_resid(int m, int n, int nrhs, SuperMatrix *A,
--		    doublecomplex *x, int ldx, doublecomplex *b, int ldb,
--		    gridinfo_t *grid, SOLVEstruct_t *SOLVEstruct, double *resid)
--{
--/*  
--    Purpose   
--    =======   
--
--    PZCOMPUTE_RESID computes the residual for a solution of a system of linear   
--    equations  A*x = b  or  A'*x = b:   
--       RESID = norm(B - A*X) / ( norm(A) * norm(X) * EPS ),   
--    where EPS is the machine epsilon.   
--
--    Arguments   
--    =========   
--
--    M       (input) INTEGER   
--            The number of rows of the matrix A.  M >= 0.   
--
--    N       (input) INTEGER   
--            The number of columns of the matrix A.  N >= 0.   
--
--    NRHS    (input) INTEGER   
--            The number of columns of B, the matrix of right hand sides.   
--            NRHS >= 0.
--	    
--    A       (input/output) SuperMatrix*
--            The original M x N sparse matrix A.   
--	    On exit, the column indices are modified due to SPMV setup.
--
--    X       (input) DOUBLE COMPLEX PRECISION array, dimension (LDX,NRHS)   
--            The computed solution vectors for the system of linear   
--            equations.   
--
--    LDX     (input) INTEGER   
--            The leading dimension of the array X.  If TRANS = NOTRANS,   
--            LDX >= max(1,N); if TRANS = TRANS or CONJ, LDX >= max(1,M).   
--
--    B       (input/output) DOUBLE COMPLEX PRECISION array, dimension (LDB,NRHS)   
--            On entry, the right hand side vectors for the system of   
--            linear equations.   
--            On exit, B is overwritten with the difference B - A*X.   
--
--    LDB     (input) INTEGER   
--            The leading dimension of the array B.  IF TRANS = NOTRANS,
--            LDB >= max(1,M); if TRANS = TRANS or CONJ, LDB >= max(1,N).
--
--    SOLVEstruct (input) SOLVEstruct_t*
--
--    GRID    (input) gridinfo_t*
--	    
--    RESID   (output) double PRECISION   
--            The maximum over the number of right-hand sides of
--            norm(B - A*X) / ( norm(A) * norm(X) * EPS ).   
--
--    =====================================================================
--*/
--
--    /* Table of constant values */
--    int    inc  = 1;
--    
--    /* Local variables */
--    int i, j;
--    double anorm, rnorm, rnorm_g;
--    double xnorm, xnorm_g;
--    double eps;
--    char transc[1];
--    doublecomplex *ax, *R;
--    pzgsmv_comm_t gsmv_comm; 
--    int m_loc = ((NRformat_loc*) A->Store)->m_loc;
--
--    /* Function prototypes */
--    extern double dzasum_(int *, doublecomplex *, int *);
--    
--    /* Function Body */
--    if ( m <= 0 || n <= 0 || nrhs == 0) {
--	*resid = 0.;
--	return 0;
--    }
--
--    /* Exit with RESID = 1/EPS if ANORM = 0. */
--    eps = dmach_dist("Epsilon");
--    anorm = pzlangs("1", A, grid);
--    if (anorm <= 0.) {
--	*resid = 1. / eps;
--	return 0;
--    }
--
--    if ( !(ax = doublecomplexMalloc_dist(m_loc)) ) ABORT("Malloc fails for work[]");
--    R = ax;
--
--    /* A is modified with colind[] permuted to [internal, external]. */
--    pzgsmv_init(A, SOLVEstruct->row_to_proc, grid, &gsmv_comm);
--
--    /* Compute the maximum over the number of right-hand sides of   
--       norm(B - A*X) / ( norm(A) * norm(X) * EPS ) . */
--    *resid = 0.;
--    for (j = 0; j < nrhs; ++j) {
--	doublecomplex *B_col = &b[j*ldb];
--	doublecomplex *X_col = &x[j*ldx];
--
--	/* Compute residual R = B - op(A) * X,   
--	   where op(A) = A, A**T, or A**H, depending on TRANS. */
--	/* Matrix-vector multiply. */
--	pzgsmv(0, A, grid, &gsmv_comm, X_col, ax);
--	    
--	/* Compute residual, stored in R[]. */
--	for (i = 0; i < m_loc; ++i) z_sub(&R[i], &B_col[i], &ax[i]);
--
--	rnorm = dzasum_(&m_loc, R, &inc);
--	xnorm = dzasum_(&m_loc, X_col, &inc);
--
--	/* */
--	MPI_Allreduce( &rnorm, &rnorm_g, 1, MPI_DOUBLE, MPI_SUM, grid->comm );
--	MPI_Allreduce( &xnorm, &xnorm_g, 1, MPI_DOUBLE, MPI_SUM, grid->comm );
--		
--	if (xnorm_g <= 0.) {
--	    *resid = 1. / eps;
--	} else {
--	    /* Computing MAX */
--	    double d1, d2;
--	    d1 = *resid;
--	    d2 = rnorm_g / anorm / xnorm_g / eps;
--	    *resid = SUPERLU_MAX(d1, d2);
--	}
--    } /* end for j ... */
--
--    pzgsmv_finalize(&gsmv_comm);
--    SUPERLU_FREE(ax);
--
--    return 0;
--
--} /* pzcompute_redid */
-diff --git a/TEST/pztest.sh b/TEST/pztest.sh
-deleted file mode 100755
-index d7956aa..0000000
---- a/TEST/pztest.sh
-+++ /dev/null
-@@ -1,64 +0,0 @@
--#!/bin/bash
--
--# bash hint: == is for string comparisons, -eq is for numeric ones.
--
--ofile=pztest.out			# output file
--if [ -e $ofile ]; then
--    rm -f $ofile
--fi
--echo "Double-complex testing output" > $ofile
--
--MATRICES=(../EXAMPLE/cg20.cua)
--NPROWS="1 2"
--NPCOLS="1 3"
--NVAL="9 19"
--NRHS="1 3"
--FILLRATIO="2 6"
--# following are blocking parameters, see sp_ienv.c
--RELAX="4 8"
--SUPERSIZE="10 20"
--MINGEMM="10000"
--
--##
--# Loop through all matrices ...
--#
--for mat in $MATRICES; do
--
--  #--------------------------------------------
--  # Test matrix types generated in LAPACK-style
--  #--------------------------------------------
--  if  [ "$mat" == "LAPACK" ]; then
--      echo '== LAPACK test matrices' >> $ofile
--      for n in $NVAL ; do
--        for s in $NRHS ; do
--	    echo '' >> $ofile
--            echo 'n='$n 'nrhs='$s >> $ofile
--	      mpiexec -n 2 pztest -r 1 -c 2 -x 4 -m 10 -b 5 -s 1 >> $ofile
--        done
--      done
--  #--------------------------------------------
--  # Test a specified sparse matrix
--  #--------------------------------------------
--  else
--    echo '' >> $ofile
--    echo '== sparse matrix:' $m >> $ofile
--    for s in $NRHS; do
--      for r in $NPROWS; do
--	for c in $NPCOLS; do
--	  np=$(($r*$c))
--	  for b in $FILLRATIO; do
--	    for x in $RELAX; do
--	      for m in $SUPERSIZE; do
--		echo '' >> $ofile
--   	        echo "**-- nrhs = $s, process grid = $r X $c, fill $b, relax $x, max-super $m"
--   	        echo "**-- nrhs = $s, process grid = $r X $c, fill $b, relax $x, max-super $m" >> $ofile
--		mpiexec -n $np pztest -r $r -c $c -x $x -m $m -b $b -s 1 -f $mat >> $ofile
--	      done
--	    done
--	  done
--	done
--      done
--    done
--  fi
--done
--
-diff --git a/TEST/zcreate_matrix.c b/TEST/zcreate_matrix.c
-deleted file mode 100644
-index 8660143..0000000
---- a/TEST/zcreate_matrix.c
-+++ /dev/null
-@@ -1,234 +0,0 @@
--/*! \file
--Copyright (c) 2003, The Regents of the University of California, through
--Lawrence Berkeley National Laboratory (subject to receipt of any required 
--approvals from U.S. Dept. of Energy) 
--
--All rights reserved. 
--
--The source code is distributed under BSD license, see the file License.txt
--at the top-level directory.
--*/
--
--/*! @file 
-- * \brief Read the matrix from data file
-- *
-- * <pre>
-- * -- Distributed SuperLU routine (version 2.0) --
-- * Lawrence Berkeley National Lab, Univ. of California Berkeley.
-- * March 15, 2003
-- * </pre>
-- */
--#include <math.h>
--#include "superlu_zdefs.h"
--
--/* \brief
-- *
-- * <pre>
-- * Purpose
-- * =======
-- * 
-- * ZCREATE_MATRIX read the matrix from data file in Harwell-Boeing format,
-- * and distribute it to processors in a distributed compressed row format.
-- * It also generate the distributed true solution X and the right-hand
-- * side RHS.
-- *
-- *
-- * Arguments   
-- * =========      
-- *
-- * A     (output) SuperMatrix*
-- *       Local matrix A in NR_loc format. 
-- *
-- * NRHS  (input) int_t
-- *       Number of right-hand sides.
-- *
-- * RHS   (output) doublecomplex**
-- *       The right-hand side matrix.
-- *
-- * LDB   (output) int*
-- *       Leading dimension of the right-hand side matrix.
-- *
-- * X     (output) doublecomplex**
-- *       The true solution matrix.
-- *
-- * LDX   (output) int*
-- *       The leading dimension of the true solution matrix.
-- *
-- * FP    (input) FILE*
-- *       The matrix file pointer.
-- *
-- * GRID  (input) gridinof_t*
-- *       The 2D process mesh.
-- * </pre>
-- */
--
--int zcreate_matrix(SuperMatrix *A, int nrhs, doublecomplex **rhs,
--                   int *ldb, doublecomplex **x, int *ldx,
--                   FILE *fp, gridinfo_t *grid)
--{
--    SuperMatrix GA;              /* global A */
--    doublecomplex   *b_global, *xtrue_global;  /* replicated on all processes */
--    int_t    *rowind, *colptr;	 /* global */
--    doublecomplex   *nzval;             /* global */
--    doublecomplex   *nzval_loc;         /* local */
--    int_t    *colind, *rowptr;	 /* local */
--    int_t    m, n, nnz;
--    int_t    m_loc, fst_row, nnz_loc;
--    int_t    m_loc_fst; /* Record m_loc of the first p-1 processors,
--			   when mod(m, p) is not zero. */ 
--    int_t    row, col, i, j, relpos;
--    int      iam;
--    char     trans[1];
--    int_t      *marker;
--
--    iam = grid->iam;
--
--#if ( DEBUGlevel>=1 )
--    CHECK_MALLOC(iam, "Enter zcreate_matrix()");
--#endif
--
--    if ( !iam ) {
--        double t = SuperLU_timer_();
--
--        /* Read the matrix stored on disk in Harwell-Boeing format. */
--        zreadhb_dist(iam, fp, &m, &n, &nnz, &nzval, &rowind, &colptr);
--
--	printf("Time to read and distribute matrix %.2f\n", 
--	        SuperLU_timer_() - t);  fflush(stdout);
--
--	/* Broadcast matrix A to the other PEs. */
--	MPI_Bcast( &m,     1,   mpi_int_t,  0, grid->comm );
--	MPI_Bcast( &n,     1,   mpi_int_t,  0, grid->comm );
--	MPI_Bcast( &nnz,   1,   mpi_int_t,  0, grid->comm );
--	MPI_Bcast( nzval,  nnz, SuperLU_MPI_DOUBLE_COMPLEX, 0, grid->comm );
--	MPI_Bcast( rowind, nnz, mpi_int_t,  0, grid->comm );
--	MPI_Bcast( colptr, n+1, mpi_int_t,  0, grid->comm );
--    } else {
--	/* Receive matrix A from PE 0. */
--	MPI_Bcast( &m,   1,   mpi_int_t,  0, grid->comm );
--	MPI_Bcast( &n,   1,   mpi_int_t,  0, grid->comm );
--	MPI_Bcast( &nnz, 1,   mpi_int_t,  0, grid->comm );
--
--	/* Allocate storage for compressed column representation. */
--	zallocateA_dist(n, nnz, &nzval, &rowind, &colptr);
--
--	MPI_Bcast( nzval,   nnz, SuperLU_MPI_DOUBLE_COMPLEX, 0, grid->comm );
--	MPI_Bcast( rowind,  nnz, mpi_int_t,  0, grid->comm );
--	MPI_Bcast( colptr,  n+1, mpi_int_t,  0, grid->comm );
--    }
--
--#if 0
--    nzval[0].r = 0.1; nzval[0].i = 0.0;
--#endif
--
--    /* Compute the number of rows to be distributed to local process */
--    m_loc = m / (grid->nprow * grid->npcol); 
--    m_loc_fst = m_loc;
--    /* When m / procs is not an integer */
--    if ((m_loc * grid->nprow * grid->npcol) != m) {
--        /*m_loc = m_loc+1;
--          m_loc_fst = m_loc;*/
--      if (iam == (grid->nprow * grid->npcol - 1)) /* last proc. gets all*/
--	  m_loc = m - m_loc * (grid->nprow * grid->npcol - 1);
--    }
--
--    /* Create compressed column matrix for GA. */
--    zCreate_CompCol_Matrix_dist(&GA, m, n, nnz, nzval, rowind, colptr,
--				SLU_NC, SLU_Z, SLU_GE);
--
--    /* Generate the exact solution and compute the right-hand side. */
--    if ( !(b_global = doublecomplexMalloc_dist(m*nrhs)) )
--        ABORT("Malloc fails for b[]");
--    if ( !(xtrue_global = doublecomplexMalloc_dist(n*nrhs)) )
--        ABORT("Malloc fails for xtrue[]");
--    *trans = 'N';
--
--    zGenXtrue_dist(n, nrhs, xtrue_global, n);
--    zFillRHS_dist(trans, nrhs, xtrue_global, n, &GA, b_global, m);
--
--    /*************************************************
--     * Change GA to a local A with NR_loc format     *
--     *************************************************/
--
--    rowptr = (int_t *) intMalloc_dist(m_loc+1);
--    marker = (int_t *) intCalloc_dist(n);
--
--    /* Get counts of each row of GA */
--    for (i = 0; i < n; ++i)
--      for (j = colptr[i]; j < colptr[i+1]; ++j) ++marker[rowind[j]];
--    /* Set up row pointers */
--    rowptr[0] = 0;
--    fst_row = iam * m_loc_fst;
--    nnz_loc = 0;
--    for (j = 0; j < m_loc; ++j) {
--      row = fst_row + j;
--      rowptr[j+1] = rowptr[j] + marker[row];
--      marker[j] = rowptr[j];
--    }
--    nnz_loc = rowptr[m_loc];
--
--    nzval_loc = (doublecomplex *) doublecomplexMalloc_dist(nnz_loc);
--    colind = (int_t *) intMalloc_dist(nnz_loc);
--
--    /* Transfer the matrix into the compressed row storage */
--    for (i = 0; i < n; ++i) {
--      for (j = colptr[i]; j < colptr[i+1]; ++j) {
--	row = rowind[j];
--	if ( (row>=fst_row) && (row<fst_row+m_loc) ) {
--	  row = row - fst_row;
--	  relpos = marker[row];
--	  colind[relpos] = i;
--	  nzval_loc[relpos] = nzval[j];
--	  ++marker[row];
--	}
--      }
--    }
--
--#if ( DEBUGlevel>=2 )
--    if ( !iam ) zPrint_CompCol_Matrix_dist(&GA);
--#endif   
--
--    /* Destroy GA */
--    Destroy_CompCol_Matrix_dist(&GA);
--
--    /******************************************************/
--    /* Change GA to a local A with NR_loc format */
--    /******************************************************/
--
--    /* Set up the local A in NR_loc format */
--    zCreate_CompRowLoc_Matrix_dist(A, m, n, nnz_loc, m_loc, fst_row,
--				   nzval_loc, colind, rowptr,
--				   SLU_NR_loc, SLU_Z, SLU_GE);
--    
--    /* Get the local B */
--    if ( !((*rhs) = doublecomplexMalloc_dist(m_loc*nrhs)) )
--        ABORT("Malloc fails for rhs[]");
--    for (j =0; j < nrhs; ++j) {
--	for (i = 0; i < m_loc; ++i) {
--	    row = fst_row + i;
--	    (*rhs)[j*m_loc+i] = b_global[j*n+row];
--	}
--    }
--    *ldb = m_loc;
--
--    /* Set the true X */    
--    *ldx = m_loc;
--    if ( !((*x) = doublecomplexMalloc_dist(*ldx * nrhs)) )
--        ABORT("Malloc fails for x_loc[]");
--
--    /* Get the local part of xtrue_global */
--    for (j = 0; j < nrhs; ++j) {
--      for (i = 0; i < m_loc; ++i)
--	(*x)[i + j*(*ldx)] = xtrue_global[i + fst_row + j*n];
--    }
--
--    SUPERLU_FREE(b_global);
--    SUPERLU_FREE(xtrue_global);
--    SUPERLU_FREE(marker);
--
--#if ( DEBUGlevel>=1 )
--    printf("sizeof(NRforamt_loc) %lu\n", sizeof(NRformat_loc));
--    CHECK_MALLOC(iam, "Exit zcreate_matrix()");
--#endif
--    return 0;
--}
-diff --git a/build/batch_script_mpi.sh b/build/batch_script_mpi.sh
-new file mode 100644
-index 0000000..498386f
---- /dev/null
-+++ b/build/batch_script_mpi.sh
-@@ -0,0 +1,146 @@
-+#!/bin/bash
-+# Bash script to submit many files to Cori/Edison/Queue
-+
-+EXIT_SUCCESS=0
-+EXIT_HOST=1
-+EXIT_PARAM=2
-+
-+# MAX_PARAMS=1
-+# # ^^^ This should be fixed, as it should just loop through everything
-+# if [[ $# -eq 0 ]]; then
-+  # echo "Must have at least one parameter; exiting"
-+  # exit $EXIT_PARAM
-+# fi
-+# if [[ $# -gt $MAX_PARAMS ]]; then
-+  # echo "Too many parameters; exiting"
-+  # exit $EXIT_PARAM
-+# fi
-+
-+# INPUT_FILE=$1
-+# # ^^^ Get the input ile
-+
-+CUR_DIR=`pwd`
-+FILE_DIR=$CUR_DIR/EXAMPLE
-+INPUT_DIR=~/Edison/my_research/SuperLU/SuperLUDIST_Begin/build_bac/EXAMPLE
-+FILE_NAME=pddrive
-+FILE=$FILE_DIR/$FILE_NAME
-+
-+TMP_BATCH_FILE=tmp_batch_file.slurm
-+# ^^^ Should check that this is not taken,
-+#     but I would never use this, so ...
-+
-+> $TMP_BATCH_FILE
-+
-+if [[ $NERSC_HOST == edison ]]; then
-+  CORES_PER_NODE=24
-+elif [[ $NERSC_HOST == cori ]]; then
-+  CORES_PER_NODE=32
-+  # This does not take hyperthreading into account
-+else
-+  # Host unknown; exiting
-+  exit $EXIT_HOST
-+fi
-+
-+
-+#nprows=(6 12 24 48 1 1 1 1 36 144 576 2304)
-+#npcols=(6 12 24 48 36 144 576 2304 1 1 1 1)
-+
-+nprows=(6 12 24 48 )
-+npcols=(6 12 24 48 )
-+
-+#nprows=(6 12 24 1 1 1  36 144 576 )
-+#npcols=(6 12 24 36 144 576 1 1 1 )
-+
-+#nprows=(12 1 144)
-+#npcols=(12 144 1)
-+
-+
-+
-+for ((i = 0; i < ${#npcols[@]}; i++)); do
-+NROW=${nprows[i]}
-+NCOL=${npcols[i]}
-+
-+# NROW=36
-+CORE_VAL=`expr $NCOL \* $NROW`
-+NODE_VAL=`expr $CORE_VAL / $CORES_PER_NODE`
-+MOD_VAL=`expr $CORE_VAL % $CORES_PER_NODE`
-+if [[ $MOD_VAL -ne 0 ]]
-+then
-+  NODE_VAL=`expr $NODE_VAL + 1`
-+fi
-+PARTITION=debug
-+#PARTITION=regular
-+LICENSE=SCRATCH
-+TIME=00:20:00
-+
-+if [[ $NERSC_HOST == edison ]]
-+then
-+  CONSTRAINT=0
-+fi
-+if [[ $NERSC_HOST == cori ]]
-+then
-+  CONSTRAINT=haswell
-+fi
-+
-+OMP_NUM_THREADS=1
-+
-+
-+
-+#for NSUP in 128 64 32 16 8
-+#do
-+  # for MAT in atmosmodl.rb nlpkkt80.mtx torso3.mtx Ga19As19H42.mtx A22.mtx cage13.rb 
-+  for MAT in torso3.mtx
-+  # for MAT in matrix121.dat matrix211.dat tdr190k.dat tdr455k.dat nlpkkt80.mtx torso3.mtx helm2d03.mtx  
-+  # for MAT in tdr190k.dat Ga19As19H42.mtx
-+  # for MAT in nlpkkt80.mtx torso3.mtx Ga19As19H42.mtx A22.mtx matrix121.dat tdr190k.dat   
-+  # for MAT in Ga19As19H42.mtx   
-+  do
-+    # Start of looping stuff
-+    > $TMP_BATCH_FILE
-+    echo "#!/bin/bash -l" >> $TMP_BATCH_FILE
-+    echo " " >> $TMP_BATCH_FILE
-+    echo "#SBATCH -p $PARTITION" >> $TMP_BATCH_FILE
-+    echo "#SBATCH -N $NODE_VAL" >>  $TMP_BATCH_FILE
-+    echo "#SBATCH -t $TIME" >> $TMP_BATCH_FILE
-+    echo "#SBATCH -L $LICENSE" >> $TMP_BATCH_FILE
-+    echo "#SBATCH -J SLU_$MAT" >> $TMP_BATCH_FILE
-+    echo "#SBATCH -o ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_async_simple_over_single" >> $TMP_BATCH_FILE
-+    echo "#SBATCH -e ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_async_simple_over_single" >> $TMP_BATCH_FILE
-+    # echo "#SBATCH --mail-type=BEGIN" >> $TMP_BATCH_FILE
-+    # echo "#SBATCH --mail-type=END" >> $TMP_BATCH_FILE
-+    echo "#SBATCH --mail-user=liuyangzhuan@lbl.gov" >> $TMP_BATCH_FILE
-+    if [[ $NERSC_HOST == cori ]]
-+    then
-+      echo "#SBATCH -C $CONSTRAINT" >> $TMP_BATCH_FILE
-+    fi
-+    mkdir -p $MAT   
-+    echo "export OMP_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE
-+    echo "export KMP_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE
-+    echo "export MKL_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE
-+    echo "export MPICH_MAX_THREAD_SAFETY=multiple" >> $TMP_BATCH_FILE 
-+    echo "export NSUP=128" >> $TMP_BATCH_FILE
-+    echo "export NREL=20" >> $TMP_BATCH_FILE
-+    
-+    echo " " >> $TMP_BATCH_FILE
-+    echo "FILE=$FILE" >> $TMP_BATCH_FILE
-+    echo "FILEMAT=$INPUT_DIR/$MAT" >> $TMP_BATCH_FILE	
-+    echo " " >> $TMP_BATCH_FILE
-+    echo "CORE_VAL=$CORE_VAL" >> $TMP_BATCH_FILE
-+    echo "NCOL=$NCOL" >> $TMP_BATCH_FILE
-+    echo "NROW=$NROW" >> $TMP_BATCH_FILE
-+    # This should be computed individually for each script...
-+
-+    echo "srun -n $CORE_VAL $FILE -c $NCOL -r $NROW $INPUT_DIR/$MAT" >> $TMP_BATCH_FILE
-+    # Add final line (srun line) to temporary slurm script
-+
-+    #cat $TMP_BATCH_FILE
-+    #echo " "
-+    sbatch $TMP_BATCH_FILE
-+  done
-+#one
-+
-+done
-+
-+
-+exit $EXIT_SUCCESS
-+
-diff --git a/build/batch_script_mpi_runit.sh b/build/batch_script_mpi_runit.sh
-new file mode 100644
-index 0000000..8c42251
---- /dev/null
-+++ b/build/batch_script_mpi_runit.sh
-@@ -0,0 +1,194 @@
-+#!/bin/bash
-+# Bash script to submit many files to Cori/Edison/Queue
-+
-+EXIT_SUCCESS=0
-+EXIT_HOST=1
-+EXIT_PARAM=2
-+
-+# MAX_PARAMS=1
-+# # ^^^ This should be fixed, as it should just loop through everything
-+# if [[ $# -eq 0 ]]; then
-+  # echo "Must have at least one parameter; exiting"
-+  # exit $EXIT_PARAM
-+# fi
-+# if [[ $# -gt $MAX_PARAMS ]]; then
-+  # echo "Too many parameters; exiting"
-+  # exit $EXIT_PARAM
-+# fi
-+
-+# INPUT_FILE=$1
-+# # ^^^ Get the input ile
-+
-+CUR_DIR=`pwd`
-+FILE_DIR=$CUR_DIR/EXAMPLE
-+INPUT_DIR=/project/projectdirs/sparse/liuyangz/my_research/matrix
-+FILE_NAME=pddrive
-+FILE=$FILE_DIR/$FILE_NAME
-+
-+TMP_BATCH_FILE=tmp_batch_file.slurm
-+# ^^^ Should check that this is not taken,
-+#     but I would never use this, so ...
-+
-+> $TMP_BATCH_FILE
-+
-+if [[ $NERSC_HOST == edison ]]; then
-+  CORES_PER_NODE=24
-+elif [[ $NERSC_HOST == cori ]]; then
-+  CORES_PER_NODE=32
-+  # This does not take hyperthreading into account
-+else
-+  # Host unknown; exiting
-+  exit $EXIT_HOST
-+fi
-+
-+
-+#nprows=(6 12 24)
-+#npcols=(6 12 24)
-+
-+
-+#nprows=(2048 1 32)
-+#npcols=(1 2048 64)
-+
-+# nprows=(32 )
-+# npcols=(64 )
-+
-+
-+
-+#nprows=(24 48 1 1 576 2304)
-+#npcols=(24 48 576 2304 1 1)
-+
-+
-+#nprows=(48  1  2304)
-+#npcols=(48  2304 1)
-+
-+#nprows=(6 12 24 48 )
-+#npcols=(6 12 24 48 )
-+
-+#nprows=(6 12 24 48 1 1 1 1 36 144 576 2304)
-+#npcols=(6 12 24 48 36 144 576 2304 1 1 1 1)
-+
-+# nprows=(32 128 512 1 1 1 4 8 16)
-+# npcols=(1 1 1 32 128 512 8 16 32)
-+
-+#nprows=(2048 1 32)
-+#npcols=(1 2048 64)
-+
-+
-+
-+
-+#nprows=(12 1 144)
-+#npcols=(12 144 1)
-+ 
-+NREP=1	  
-+
-+#nprows=(4 8 16 32 45)
-+#npcols=(4 8 16 32 45)
-+#nprows=(32)
-+#npcols=(48)
-+
-+nprows=(16)
-+npcols=(16)
-+ 
-+for ((i = 0; i < ${#npcols[@]}; i++)); do
-+NROW=${nprows[i]}
-+NCOL=${npcols[i]}
-+
-+# NROW=36
-+CORE_VAL=`expr $NCOL \* $NROW`
-+NODE_VAL=`expr $CORE_VAL / $CORES_PER_NODE`
-+MOD_VAL=`expr $CORE_VAL % $CORES_PER_NODE`
-+
-+if [[ $MOD_VAL -ne 0 ]]
-+then
-+  NODE_VAL=`expr $NODE_VAL + 1`
-+fi
-+#PARTITION=debug
-+PARTITION=regular
-+ICENSE=SCRATCH
-+TIME=00:20:00
-+
-+if [[ $NERSC_HOST == edison ]]
-+then
-+  CONSTRAINT=0
-+fi
-+if [[ $NERSC_HOST == cori ]]
-+then
-+  CONSTRAINT=haswell
-+fi
-+
-+OMP_NUM_THREADS=1
-+
-+THREADS_PER_RANK=`expr 2 \* $OMP_NUM_THREADS`											 
-+
-+
-+#for NSUP in 128 64 32 16 8
-+#do
-+  # for MAT in atmosmodl.rb nlpkkt80.mtx torso3.mtx Ga19As19H42.mtx A22.mtx cage13.rb 
-+  #for MAT in torso3.mtx
-+  # for MAT in matrix121.dat matrix211.dat tdr190k.dat tdr455k.dat nlpkkt80.mtx torso3.mtx helm2d03.mtx  
-+  # for MAT in tdr190k.dat Ga19As19H42.mtx
-+# for MAT in big.rua
-+# for MAT in tdr455k.bin
-+  # for MAT in A22.bin tdr455k.bin DG_GrapheneDisorder_8192.bin DNA_715_64cell.bin LU_C_BN_C_4by2.bin Li4244.bin atmosmodj.bin nlpkkt80.bin Ga19As19H42.bin Geo_1438.bin StocF-1465.bin cage13.bin																																							   
-+#  for MAT in globalmat118_1536.bin								  
-+#  for MAT in DG_PNF_14000.bin DG_GrapheneDisorder_32768.bin
-+ #  for MAT in DNA_715_64cell.mtx
-+ # for MAT in Ga19As19H42.mtx cage13.rb Geo_1438.mtx nlpkkt80.mtx torso3.mtx helm2d03.mtx gsm_106857.mtx atmosmodj.mtx StocF-1465.mtx hvdc2.mtx  
-+ for MAT in Geo_1438.bin
-+ do
-+    # Start of looping stuff
-+    > $TMP_BATCH_FILE
-+    echo "#!/bin/bash -l" >> $TMP_BATCH_FILE
-+    echo " " >> $TMP_BATCH_FILE
-+    echo "#SBATCH -p $PARTITION" >> $TMP_BATCH_FILE
-+    echo "#SBATCH -N $NODE_VAL" >>  $TMP_BATCH_FILE
-+    echo "#SBATCH -t $TIME" >> $TMP_BATCH_FILE
-+    echo "#SBATCH -L $LICENSE" >> $TMP_BATCH_FILE
-+    echo "#SBATCH -J SLU_$MAT" >> $TMP_BATCH_FILE
-+    #echo "#SBATCH -o ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_async_simple_over_icollec_mrhs" >> $TMP_BATCH_FILE
-+    #echo "#SBATCH -e ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_async_simple_over_icollec_mrhs" >> $TMP_BATCH_FILE
-+    # echo "#SBATCH --mail-type=BEGIN" >> $TMP_BATCH_FILE
-+    # echo "#SBATCH --mail-type=END" >> $TMP_BATCH_FILE
-+    echo "#SBATCH --mail-user=liuyangzhuan@lbl.gov" >> $TMP_BATCH_FILE
-+    if [[ $NERSC_HOST == cori ]]
-+    then
-+      echo "#SBATCH -C $CONSTRAINT" >> $TMP_BATCH_FILE
-+    fi
-+    mkdir -p $MAT   
-+    export OMP_NUM_THREADS=$OMP_NUM_THREADS
-+    export KMP_NUM_THREADS=$OMP_NUM_THREADS
-+    export MKL_NUM_THREADS=$OMP_NUM_THREADS  																			
-+    export NSUP=128
-+    export NREL=20
-+
-+    export OMP_PLACES=threads
-+    export OMP_PROC_BIND=spread
-+    export MPICH_MAX_THREAD_iSAFETY=multiple
-+    
-+    echo " " >> $TMP_BATCH_FILE
-+    echo "FILE=$FILE" >> $TMP_BATCH_FILE
-+    echo "FILEMAT=$INPUT_DIR/$MAT" >> $TMP_BATCH_FILE	
-+    echo " " >> $TMP_BATCH_FILE
-+    echo "CORE_VAL=$CORE_VAL" >> $TMP_BATCH_FILE
-+    echo "NCOL=$NCOL" >> $TMP_BATCH_FILE
-+    echo "NROW=$NROW" >> $TMP_BATCH_FILE
-+    # This should be computed individually for each script...
-+OUTPUT=./$MAT/SLU.o_mpi_${NROW}x${NCOL}_asyncU_cori	
-+	rm -rf $OUTPUT
-+	for ii in `seq 1 $NREP`
-+    do
-+    srun -n $CORE_VAL -N $NODE_VAL -c $THREADS_PER_RANK --cpu_bind=cores $FILE -c $NCOL -r $NROW $INPUT_DIR/$MAT | tee -a $OUTPUT
-+	done	 
-+    # Add final line (srun line) to temporary slurm script
-+
-+    #cat $TMP_BATCH_FILE
-+    #echo " "
-+    # sbatch $TMP_BATCH_FILE
-+  done
-+#one
-+
-+done
-+
-+
-+exit $EXIT_SUCCESS
-+
-diff --git a/build/batch_script_mpi_runit10.sh b/build/batch_script_mpi_runit10.sh
-new file mode 100644
-index 0000000..3623c1d
---- /dev/null
-+++ b/build/batch_script_mpi_runit10.sh
-@@ -0,0 +1,18 @@
-+#!/bin/bash
-+# Bash script to submit many files to Cori/Edison/Queue
-+
-+
-+EXIT_SUCCESS=0
-+EXIT_HOST=1
-+EXIT_PARAM=2
-+
-+export MPICH_MAX_THREAD_SAFETY=multiple
-+
-+for i in `seq 1 200`;
-+do
-+	srun -n 2 ./EXAMPLE/pddrive -r 1 -c 2 ../EXAMPLE/big.rua | tee -a a.out
-+done
-+
-+
-+exit $EXIT_SUCCESS
-+
-diff --git a/build/batch_script_mpi_runit11.sh b/build/batch_script_mpi_runit11.sh
-new file mode 100644
-index 0000000..6b4f2dd
---- /dev/null
-+++ b/build/batch_script_mpi_runit11.sh
-@@ -0,0 +1,191 @@
-+#!/bin/bash
-+# Bash script to submit many files to Cori/Edison/Queue
-+
-+EXIT_SUCCESS=0
-+EXIT_HOST=1
-+EXIT_PARAM=2
-+
-+# MAX_PARAMS=1
-+# # ^^^ This should be fixed, as it should just loop through everything
-+# if [[ $# -eq 0 ]]; then
-+  # echo "Must have at least one parameter; exiting"
-+  # exit $EXIT_PARAM
-+# fi
-+# if [[ $# -gt $MAX_PARAMS ]]; then
-+  # echo "Too many parameters; exiting"
-+  # exit $EXIT_PARAM
-+# fi
-+
-+# INPUT_FILE=$1
-+# # ^^^ Get the input ile
-+
-+CUR_DIR=`pwd`
-+FILE_DIR=$CUR_DIR/EXAMPLE
-+INPUT_DIR=/project/projectdirs/sparse/liuyangz/my_research/matrix
-+FILE_NAME=pddrive
-+FILE=$FILE_DIR/$FILE_NAME
-+
-+TMP_BATCH_FILE=tmp_batch_file.slurm
-+# ^^^ Should check that this is not taken,
-+#     but I would never use this, so ...
-+
-+> $TMP_BATCH_FILE
-+
-+if [[ $NERSC_HOST == edison ]]; then
-+  CORES_PER_NODE=24
-+elif [[ $NERSC_HOST == cori ]]; then
-+  CORES_PER_NODE=32
-+  # This does not take hyperthreading into account
-+else
-+  # Host unknown; exiting
-+  exit $EXIT_HOST
-+fi
-+
-+
-+#nprows=(6 12 24)
-+#npcols=(6 12 24)
-+
-+
-+#nprows=(2048 1 32)
-+#npcols=(1 2048 64)
-+
-+# nprows=(32 )
-+# npcols=(64 )
-+
-+
-+
-+#nprows=(24 48 1 1 576 2304)
-+#npcols=(24 48 576 2304 1 1)
-+
-+
-+#nprows=(48  1  2304)
-+#npcols=(48  2304 1)
-+
-+#nprows=(6 12 24 48 )
-+#npcols=(6 12 24 48 )
-+
-+#nprows=(6 12 24 48 1 1 1 1 36 144 576 2304)
-+#npcols=(6 12 24 48 36 144 576 2304 1 1 1 1)
-+
-+# nprows=(32 128 512 1 1 1 4 8 16)
-+# npcols=(1 1 1 32 128 512 8 16 32)
-+
-+#nprows=(2048 1 32)
-+#npcols=(1 2048 64)
-+
-+
-+
-+
-+#nprows=(12 1 144)
-+#npcols=(12 144 1)
-+ 
-+NREP=5 
-+
-+nprows=(32 )
-+npcols=(64)
-+
-+# nprows=(4 8)
-+# npcols=(4 8)
-+ 
-+for ((i = 0; i < ${#npcols[@]}; i++)); do
-+NROW=${nprows[i]}
-+NCOL=${npcols[i]}
-+
-+# NROW=36
-+CORE_VAL=`expr $NCOL \* $NROW`
-+NODE_VAL=`expr $CORE_VAL / $CORES_PER_NODE`
-+MOD_VAL=`expr $CORE_VAL % $CORES_PER_NODE`
-+
-+if [[ $MOD_VAL -ne 0 ]]
-+then
-+  NODE_VAL=`expr $NODE_VAL + 1`
-+fi
-+#PARTITION=debug
-+PARTITION=regular
-+ICENSE=SCRATCH
-+TIME=00:20:00
-+
-+if [[ $NERSC_HOST == edison ]]
-+then
-+  CONSTRAINT=0
-+fi
-+if [[ $NERSC_HOST == cori ]]
-+then
-+  CONSTRAINT=haswell
-+fi
-+
-+OMP_NUM_THREADS=1
-+
-+THREADS_PER_RANK=`expr 2 \* $OMP_NUM_THREADS`											 
-+
-+
-+#for NSUP in 128 64 32 16 8
-+#do
-+  # for MAT in atmosmodl.rb nlpkkt80.mtx torso3.mtx Ga19As19H42.mtx A22.mtx cage13.rb 
-+  #for MAT in torso3.mtx
-+  # for MAT in matrix121.dat matrix211.dat tdr190k.dat tdr455k.dat nlpkkt80.mtx torso3.mtx helm2d03.mtx  
-+  # for MAT in tdr190k.dat Ga19As19H42.mtx
-+# for MAT in big.rua
-+# for MAT in tdr455k.bin
-+  for MAT in torso3.mtx
-+#  for MAT in A22.bin tdr455k.bin DG_GrapheneDisorder_8192.bin DNA_715_64cell.bin LU_C_BN_C_4by2.bin Li4244.bin atmosmodj.bin nlpkkt80.bin Ga19As19H42.bin Geo_1438.bin StocF-1465.bin cage13.bin																																							   
-+#  for MAT in DG_PNF_14000.bin DG_GrapheneDisorder_32768.bin
-+ #  for MAT in DNA_715_64cell.mtx
-+ # for MAT in Ga19As19H42.mtx cage13.rb Geo_1438.mtx nlpkkt80.mtx torso3.mtx helm2d03.mtx gsm_106857.mtx atmosmodj.mtx StocF-1465.mtx hvdc2.mtx  
-+ do
-+    # Start of looping stuff
-+    > $TMP_BATCH_FILE
-+    echo "#!/bin/bash -l" >> $TMP_BATCH_FILE
-+    echo " " >> $TMP_BATCH_FILE
-+    echo "#SBATCH -p $PARTITION" >> $TMP_BATCH_FILE
-+    echo "#SBATCH -N $NODE_VAL" >>  $TMP_BATCH_FILE
-+    echo "#SBATCH -t $TIME" >> $TMP_BATCH_FILE
-+    echo "#SBATCH -L $LICENSE" >> $TMP_BATCH_FILE
-+    echo "#SBATCH -J SLU_$MAT" >> $TMP_BATCH_FILE
-+    #echo "#SBATCH -o ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_async_simple_over_icollec_mrhs" >> $TMP_BATCH_FILE
-+    #echo "#SBATCH -e ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_async_simple_over_icollec_mrhs" >> $TMP_BATCH_FILE
-+    # echo "#SBATCH --mail-type=BEGIN" >> $TMP_BATCH_FILE
-+    # echo "#SBATCH --mail-type=END" >> $TMP_BATCH_FILE
-+    echo "#SBATCH --mail-user=liuyangzhuan@lbl.gov" >> $TMP_BATCH_FILE
-+    if [[ $NERSC_HOST == cori ]]
-+    then
-+      echo "#SBATCH -C $CONSTRAINT" >> $TMP_BATCH_FILE
-+    fi
-+    mkdir -p $MAT   
-+    export OMP_NUM_THREADS=$OMP_NUM_THREADS
-+    export KMP_NUM_THREADS=$OMP_NUM_THREADS
-+    export MKL_NUM_THREADS=$OMP_NUM_THREADS  																			
-+    echo "export NSUP=128" >> $TMP_BATCH_FILE
-+    echo "export NREL=20" >> $TMP_BATCH_FILE
-+
-+    export OMP_PLACES=threads
-+    export OMP_PROC_BIND=spread
-+    export MPICH_MAX_THREAD_SAFETY=multiple
-+    
-+    echo " " >> $TMP_BATCH_FILE
-+    echo "FILE=$FILE" >> $TMP_BATCH_FILE
-+    echo "FILEMAT=$INPUT_DIR/$MAT" >> $TMP_BATCH_FILE	
-+    echo " " >> $TMP_BATCH_FILE
-+    echo "CORE_VAL=$CORE_VAL" >> $TMP_BATCH_FILE
-+    echo "NCOL=$NCOL" >> $TMP_BATCH_FILE
-+    echo "NROW=$NROW" >> $TMP_BATCH_FILE
-+    # This should be computed individually for each script...
-+OUTPUT=./$MAT/SLU.o_mpi_${NROW}x${NCOL}_groupgemm_cori	
-+	rm -rf $OUTPUT
-+	for ii in `seq 1 $NREP`
-+    do
-+    srun -n $CORE_VAL -N $NODE_VAL -c $THREADS_PER_RANK --cpu_bind=cores $FILE -c $NCOL -r $NROW $INPUT_DIR/$MAT | tee -a $OUTPUT
-+	done	 
-+    # Add final line (srun line) to temporary slurm script
-+
-+    #cat $TMP_BATCH_FILE
-+    #echo " "
-+    # sbatch $TMP_BATCH_FILE
-+  done
-+#one
-+
-+done
-+
-+
-+exit $EXIT_SUCCESS
-+
-diff --git a/build/batch_script_mpi_runit4.sh b/build/batch_script_mpi_runit4.sh
-new file mode 100644
-index 0000000..f2b7104
---- /dev/null
-+++ b/build/batch_script_mpi_runit4.sh
-@@ -0,0 +1,172 @@
-+#!/bin/bash
-+# Bash script to submit many files to Cori/Edison/Queue
-+
-+EXIT_SUCCESS=0
-+EXIT_HOST=1
-+EXIT_PARAM=2
-+
-+# MAX_PARAMS=1
-+# # ^^^ This should be fixed, as it should just loop through everything
-+# if [[ $# -eq 0 ]]; then
-+  # echo "Must have at least one parameter; exiting"
-+  # exit $EXIT_PARAM
-+# fi
-+# if [[ $# -gt $MAX_PARAMS ]]; then
-+  # echo "Too many parameters; exiting"
-+  # exit $EXIT_PARAM
-+# fi
-+
-+# INPUT_FILE=$1
-+# # ^^^ Get the input ile
-+
-+CUR_DIR=`pwd`
-+FILE_DIR=$CUR_DIR/EXAMPLE
-+INPUT_DIR=~/Edison/my_research/SuperLU/SuperLUDIST_Begin/build_bac/EXAMPLE
-+FILE_NAME=pddrive
-+FILE=$FILE_DIR/$FILE_NAME
-+
-+TMP_BATCH_FILE=tmp_batch_file.slurm
-+# ^^^ Should check that this is not taken,
-+#     but I would never use this, so ...
-+
-+> $TMP_BATCH_FILE
-+
-+if [[ $NERSC_HOST == edison ]]; then
-+  CORES_PER_NODE=24
-+elif [[ $NERSC_HOST == cori ]]; then
-+  CORES_PER_NODE=32
-+  # This does not take hyperthreading into account
-+else
-+  # Host unknown; exiting
-+  exit $EXIT_HOST
-+fi
-+
-+
-+#nprows=(6 12 24)
-+#npcols=(6 12 24)
-+
-+
-+#nprows=(2048 1 32)
-+#npcols=(1 2048 64)
-+
-+# nprows=(32 )
-+# npcols=(64 )
-+
-+
-+
-+#nprows=(24 48 1 1 576 2304)
-+#npcols=(24 48 576 2304 1 1)
-+
-+
-+#nprows=(48  1  2304)
-+#npcols=(48  2304 1)
-+
-+#nprows=(6 12 24 48 )
-+#npcols=(6 12 24 48 )
-+
-+#nprows=(6 12 24 48 1 1 1 1 36 144 576 2304)
-+#npcols=(6 12 24 48 36 144 576 2304 1 1 1 1)
-+
-+#nprows=(32 128 512 1 1 1 4 8 16)
-+#npcols=(1 1 1 32 128 512 8 16 32)
-+
-+nprows=(2048 1 1 32)
-+npcols=(1 512 2048 64)
-+
-+
-+
-+
-+#nprows=(12 1 144)
-+#npcols=(12 144 1)
-+
-+
-+#nprows=(48)
-+#npcols=(48)
-+ 
-+for ((i = 0; i < ${#npcols[@]}; i++)); do
-+NROW=${nprows[i]}
-+NCOL=${npcols[i]}
-+
-+# NROW=36
-+CORE_VAL=`expr $NCOL \* $NROW`
-+NODE_VAL=`expr $CORE_VAL / $CORES_PER_NODE`
-+MOD_VAL=`expr $CORE_VAL % $CORES_PER_NODE`
-+if [[ $MOD_VAL -ne 0 ]]
-+then
-+  NODE_VAL=`expr $NODE_VAL + 1`
-+fi
-+#PARTITION=debug
-+PARTITION=regular
-+LICENSE=SCRATCH
-+TIME=00:20:00
-+
-+if [[ $NERSC_HOST == edison ]]
-+then
-+  CONSTRAINT=0
-+fi
-+if [[ $NERSC_HOST == cori ]]
-+then
-+  CONSTRAINT=haswell
-+fi
-+
-+OMP_NUM_THREADS=1
-+
-+
-+
-+#for NSUP in 128 64 32 16 8
-+#do
-+  # for MAT in atmosmodl.rb nlpkkt80.mtx torso3.mtx Ga19As19H42.mtx A22.mtx cage13.rb 
-+  # for MAT in torso3.mtx
-+  # for MAT in matrix121.dat matrix211.dat tdr190k.dat tdr455k.dat nlpkkt80.mtx torso3.mtx helm2d03.mtx  
-+  # for MAT in tdr190k.dat Ga19As19H42.mtx
-+ for MAT in torso3.mtx 
-+ # for MAT in Ga19As19H42.mtx   
-+  do
-+    # Start of looping stuff
-+    > $TMP_BATCH_FILE
-+    echo "#!/bin/bash -l" >> $TMP_BATCH_FILE
-+    echo " " >> $TMP_BATCH_FILE
-+    echo "#SBATCH -p $PARTITION" >> $TMP_BATCH_FILE
-+    echo "#SBATCH -N $NODE_VAL" >>  $TMP_BATCH_FILE
-+    echo "#SBATCH -t $TIME" >> $TMP_BATCH_FILE
-+    echo "#SBATCH -L $LICENSE" >> $TMP_BATCH_FILE
-+    echo "#SBATCH -J SLU_$MAT" >> $TMP_BATCH_FILE
-+    #echo "#SBATCH -o ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_async_simple_over_icollec_flat_mrhs" >> $TMP_BATCH_FILE
-+    #echo "#SBATCH -e ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_async_simple_over_icollec_flat_mrhs" >> $TMP_BATCH_FILE
-+    # echo "#SBATCH --mail-type=BEGIN" >> $TMP_BATCH_FILE
-+    # echo "#SBATCH --mail-type=END" >> $TMP_BATCH_FILE
-+    echo "#SBATCH --mail-user=liuyangzhuan@lbl.gov" >> $TMP_BATCH_FILE
-+    if [[ $NERSC_HOST == cori ]]
-+    then
-+      echo "#SBATCH -C $CONSTRAINT" >> $TMP_BATCH_FILE
-+    fi
-+    mkdir -p $MAT   
-+    echo "export OMP_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE
-+    echo "export KMP_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE
-+    echo "export MKL_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE  																			
-+    echo "export NSUP=128" >> $TMP_BATCH_FILE
-+    echo "export NREL=20" >> $TMP_BATCH_FILE
-+    
-+    echo " " >> $TMP_BATCH_FILE
-+    echo "FILE=$FILE" >> $TMP_BATCH_FILE
-+    echo "FILEMAT=$INPUT_DIR/$MAT" >> $TMP_BATCH_FILE	
-+    echo " " >> $TMP_BATCH_FILE
-+    echo "CORE_VAL=$CORE_VAL" >> $TMP_BATCH_FILE
-+    echo "NCOL=$NCOL" >> $TMP_BATCH_FILE
-+    echo "NROW=$NROW" >> $TMP_BATCH_FILE
-+    # This should be computed individually for each script...
-+
-+    srun -n $CORE_VAL $FILE -c $NCOL -r $NROW $INPUT_DIR/$MAT | tee ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_async_simple_over_icollec_groupgemm_bettertree_cori_mrhs
-+    # Add final line (srun line) to temporary slurm script
-+
-+    #cat $TMP_BATCH_FILE
-+    #echo " "
-+    # sbatch $TMP_BATCH_FILE
-+  done
-+#one
-+
-+done
-+
-+
-+exit $EXIT_SUCCESS
-+
-diff --git a/build/batch_script_mpi_runit5.sh b/build/batch_script_mpi_runit5.sh
-new file mode 100644
-index 0000000..eb50ec2
---- /dev/null
-+++ b/build/batch_script_mpi_runit5.sh
-@@ -0,0 +1,168 @@
-+#!/bin/bash
-+# Bash script to submit many files to Cori/Edison/Queue
-+
-+EXIT_SUCCESS=0
-+EXIT_HOST=1
-+EXIT_PARAM=2
-+
-+# MAX_PARAMS=1
-+# # ^^^ This should be fixed, as it should just loop through everything
-+# if [[ $# -eq 0 ]]; then
-+  # echo "Must have at least one parameter; exiting"
-+  # exit $EXIT_PARAM
-+# fi
-+# if [[ $# -gt $MAX_PARAMS ]]; then
-+  # echo "Too many parameters; exiting"
-+  # exit $EXIT_PARAM
-+# fi
-+
-+# INPUT_FILE=$1
-+# # ^^^ Get the input ile
-+
-+CUR_DIR=`pwd`
-+FILE_DIR=$CUR_DIR/EXAMPLE
-+INPUT_DIR=~/Edison/my_research/SuperLU/SuperLUDIST_Begin/build_bac/EXAMPLE
-+FILE_NAME=pddrive
-+FILE=$FILE_DIR/$FILE_NAME
-+
-+TMP_BATCH_FILE=tmp_batch_file.slurm
-+# ^^^ Should check that this is not taken,
-+#     but I would never use this, so ...
-+
-+> $TMP_BATCH_FILE
-+
-+if [[ $NERSC_HOST == edison ]]; then
-+  CORES_PER_NODE=24
-+elif [[ $NERSC_HOST == cori ]]; then
-+  CORES_PER_NODE=32
-+  # This does not take hyperthreading into account
-+else
-+  # Host unknown; exiting
-+  exit $EXIT_HOST
-+fi
-+
-+
-+#nprows=(6 12 24)
-+#npcols=(6 12 24)
-+
-+
-+#nprows=(2048 1 32)
-+#npcols=(1 2048 64)
-+
-+# nprows=(32 )
-+# npcols=(64 )
-+
-+
-+
-+#nprows=(24 48 1 1 576 2304)
-+#npcols=(24 48 576 2304 1 1)
-+
-+
-+#nprows=(48  1  2304)
-+#npcols=(48  2304 1)
-+
-+#nprows=(6 12 24 48 )
-+#npcols=(6 12 24 48 )
-+
-+nprows=(6 12 24 48 1 1 1 1 36 144 576 2304)
-+npcols=(6 12 24 48 36 144 576 2304 1 1 1 1)
-+
-+# nprows=( 576 2304)
-+# npcols=( 1 1)
-+
-+
-+#nprows=(12 1 144)
-+#npcols=(12 144 1)
-+
-+
-+#nprows=(48)
-+#npcols=(48)
-+ 
-+for ((i = 0; i < ${#npcols[@]}; i++)); do
-+NROW=${nprows[i]}
-+NCOL=${npcols[i]}
-+
-+# NROW=36
-+CORE_VAL=`expr $NCOL \* $NROW`
-+NODE_VAL=`expr $CORE_VAL / $CORES_PER_NODE`
-+MOD_VAL=`expr $CORE_VAL % $CORES_PER_NODE`
-+if [[ $MOD_VAL -ne 0 ]]
-+then
-+  NODE_VAL=`expr $NODE_VAL + 1`
-+fi
-+#PARTITION=debug
-+PARTITION=regular
-+LICENSE=SCRATCH
-+TIME=00:20:00
-+
-+if [[ $NERSC_HOST == edison ]]
-+then
-+  CONSTRAINT=0
-+fi
-+if [[ $NERSC_HOST == cori ]]
-+then
-+  CONSTRAINT=haswell
-+fi
-+
-+OMP_NUM_THREADS=1
-+
-+
-+
-+#for NSUP in 128 64 32 16 8
-+#do
-+  # for MAT in atmosmodl.rb nlpkkt80.mtx torso3.mtx Ga19As19H42.mtx A22.mtx cage13.rb 
-+  # for MAT in torso3.mtx
-+  # for MAT in matrix121.dat matrix211.dat tdr190k.dat tdr455k.dat nlpkkt80.mtx torso3.mtx helm2d03.mtx  
-+  # for MAT in tdr190k.dat Ga19As19H42.mtx
-+ for MAT in helm2d03.mtx 
-+ # for MAT in torso3.mtx 
-+ # for MAT in Ga19As19H42.mtx   
-+  do
-+    # Start of looping stuff
-+    > $TMP_BATCH_FILE
-+    echo "#!/bin/bash -l" >> $TMP_BATCH_FILE
-+    echo " " >> $TMP_BATCH_FILE
-+    echo "#SBATCH -p $PARTITION" >> $TMP_BATCH_FILE
-+    echo "#SBATCH -N $NODE_VAL" >>  $TMP_BATCH_FILE
-+    echo "#SBATCH -t $TIME" >> $TMP_BATCH_FILE
-+    echo "#SBATCH -L $LICENSE" >> $TMP_BATCH_FILE
-+    echo "#SBATCH -J SLU_$MAT" >> $TMP_BATCH_FILE
-+    #echo "#SBATCH -o ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_async_simple_over_icollec_flat_mrhs" >> $TMP_BATCH_FILE
-+    #echo "#SBATCH -e ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_async_simple_over_icollec_flat_mrhs" >> $TMP_BATCH_FILE
-+    # echo "#SBATCH --mail-type=BEGIN" >> $TMP_BATCH_FILE
-+    # echo "#SBATCH --mail-type=END" >> $TMP_BATCH_FILE
-+    echo "#SBATCH --mail-user=liuyangzhuan@lbl.gov" >> $TMP_BATCH_FILE
-+    if [[ $NERSC_HOST == cori ]]
-+    then
-+      echo "#SBATCH -C $CONSTRAINT" >> $TMP_BATCH_FILE
-+    fi
-+    mkdir -p $MAT   
-+    echo "export OMP_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE
-+    echo "export KMP_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE
-+    echo "export MKL_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE  																			
-+    echo "export NSUP=128" >> $TMP_BATCH_FILE
-+    echo "export NREL=20" >> $TMP_BATCH_FILE
-+    
-+    echo " " >> $TMP_BATCH_FILE
-+    echo "FILE=$FILE" >> $TMP_BATCH_FILE
-+    echo "FILEMAT=$INPUT_DIR/$MAT" >> $TMP_BATCH_FILE	
-+    echo " " >> $TMP_BATCH_FILE
-+    echo "CORE_VAL=$CORE_VAL" >> $TMP_BATCH_FILE
-+    echo "NCOL=$NCOL" >> $TMP_BATCH_FILE
-+    echo "NROW=$NROW" >> $TMP_BATCH_FILE
-+    # This should be computed individually for each script...
-+
-+    srun -n $CORE_VAL $FILE -c $NCOL -r $NROW $INPUT_DIR/$MAT | tee ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_async_simple_over_icollec_groupgemm_bettertree_mrhs
-+    # Add final line (srun line) to temporary slurm script
-+
-+    #cat $TMP_BATCH_FILE
-+    #echo " "
-+    # sbatch $TMP_BATCH_FILE
-+  done
-+#one
-+
-+done
-+
-+
-+exit $EXIT_SUCCESS
-+
-diff --git a/build/batch_script_mpi_runit6.sh b/build/batch_script_mpi_runit6.sh
-new file mode 100644
-index 0000000..3105033
---- /dev/null
-+++ b/build/batch_script_mpi_runit6.sh
-@@ -0,0 +1,172 @@
-+#!/bin/bash
-+# Bash script to submit many files to Cori/Edison/Queue
-+
-+EXIT_SUCCESS=0
-+EXIT_HOST=1
-+EXIT_PARAM=2
-+
-+# MAX_PARAMS=1
-+# # ^^^ This should be fixed, as it should just loop through everything
-+# if [[ $# -eq 0 ]]; then
-+  # echo "Must have at least one parameter; exiting"
-+  # exit $EXIT_PARAM
-+# fi
-+# if [[ $# -gt $MAX_PARAMS ]]; then
-+  # echo "Too many parameters; exiting"
-+  # exit $EXIT_PARAM
-+# fi
-+
-+# INPUT_FILE=$1
-+# # ^^^ Get the input ile
-+
-+CUR_DIR=`pwd`
-+FILE_DIR=$CUR_DIR/EXAMPLE
-+INPUT_DIR=~/Edison/my_research/SuperLU/SuperLUDIST_Begin/build_bac/EXAMPLE
-+FILE_NAME=pddrive
-+FILE=$FILE_DIR/$FILE_NAME
-+
-+TMP_BATCH_FILE=tmp_batch_file.slurm
-+# ^^^ Should check that this is not taken,
-+#     but I would never use this, so ...
-+
-+> $TMP_BATCH_FILE
-+
-+if [[ $NERSC_HOST == edison ]]; then
-+  CORES_PER_NODE=24
-+elif [[ $NERSC_HOST == cori ]]; then
-+  CORES_PER_NODE=32
-+  # This does not take hyperthreading into account
-+else
-+  # Host unknown; exiting
-+  exit $EXIT_HOST
-+fi
-+
-+
-+#nprows=(6 12 24)
-+#npcols=(6 12 24)
-+
-+
-+#nprows=(2048 1 32)
-+#npcols=(1 2048 64)
-+
-+# nprows=(32 )
-+# npcols=(64 )
-+
-+
-+
-+#nprows=(24 48 1 1 576 2304)
-+#npcols=(24 48 576 2304 1 1)
-+
-+
-+#nprows=(48  1  2304)
-+#npcols=(48  2304 1)
-+
-+#nprows=(6 12 24 48 )
-+#npcols=(6 12 24 48 )
-+
-+#nprows=(6 12 24 48 1 1 1 1 36 144 576 2304)
-+#npcols=(6 12 24 48 36 144 576 2304 1 1 1 1)
-+
-+nprows=(1 2 2 4 6 )
-+npcols=(1 1 2 4 6 )
-+
-+#nprows=(2048 1 32)
-+#npcols=(1 2048 64)
-+
-+
-+
-+
-+#nprows=(12 1 144)
-+#npcols=(12 144 1)
-+
-+
-+#nprows=(48)
-+#npcols=(48)
-+ 
-+for ((i = 0; i < ${#npcols[@]}; i++)); do
-+NROW=${nprows[i]}
-+NCOL=${npcols[i]}
-+
-+# NROW=36
-+CORE_VAL=`expr $NCOL \* $NROW`
-+NODE_VAL=`expr $CORE_VAL / $CORES_PER_NODE`
-+MOD_VAL=`expr $CORE_VAL % $CORES_PER_NODE`
-+if [[ $MOD_VAL -ne 0 ]]
-+then
-+  NODE_VAL=`expr $NODE_VAL + 1`
-+fi
-+#PARTITION=debug
-+PARTITION=regular
-+LICENSE=SCRATCH
-+TIME=00:20:00
-+
-+if [[ $NERSC_HOST == edison ]]
-+then
-+  CONSTRAINT=0
-+fi
-+if [[ $NERSC_HOST == cori ]]
-+then
-+  CONSTRAINT=haswell
-+fi
-+
-+OMP_NUM_THREADS=1
-+
-+
-+
-+#for NSUP in 128 64 32 16 8
-+#do
-+  # for MAT in atmosmodl.rb nlpkkt80.mtx torso3.mtx Ga19As19H42.mtx A22.mtx cage13.rb 
-+  # for MAT in torso3.mtx
-+  # for MAT in matrix121.dat matrix211.dat tdr190k.dat tdr455k.dat nlpkkt80.mtx torso3.mtx helm2d03.mtx  
-+  # for MAT in tdr190k.dat Ga19As19H42.mtx
-+ for MAT in torso3.mtx 
-+ # for MAT in Ga19As19H42.mtx   
-+  do
-+    # Start of looping stuff
-+    > $TMP_BATCH_FILE
-+    echo "#!/bin/bash -l" >> $TMP_BATCH_FILE
-+    echo " " >> $TMP_BATCH_FILE
-+    echo "#SBATCH -p $PARTITION" >> $TMP_BATCH_FILE
-+    echo "#SBATCH -N $NODE_VAL" >>  $TMP_BATCH_FILE
-+    echo "#SBATCH -t $TIME" >> $TMP_BATCH_FILE
-+    echo "#SBATCH -L $LICENSE" >> $TMP_BATCH_FILE
-+    echo "#SBATCH -J SLU_$MAT" >> $TMP_BATCH_FILE
-+    #echo "#SBATCH -o ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_async_simple_over_icollec_flat_mrhs" >> $TMP_BATCH_FILE
-+    #echo "#SBATCH -e ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_async_simple_over_icollec_flat_mrhs" >> $TMP_BATCH_FILE
-+    # echo "#SBATCH --mail-type=BEGIN" >> $TMP_BATCH_FILE
-+    # echo "#SBATCH --mail-type=END" >> $TMP_BATCH_FILE
-+    echo "#SBATCH --mail-user=liuyangzhuan@lbl.gov" >> $TMP_BATCH_FILE
-+    if [[ $NERSC_HOST == cori ]]
-+    then
-+      echo "#SBATCH -C $CONSTRAINT" >> $TMP_BATCH_FILE
-+    fi
-+    mkdir -p $MAT   
-+    echo "export OMP_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE
-+    echo "export KMP_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE
-+    echo "export MKL_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE  																			
-+    echo "export NSUP=128" >> $TMP_BATCH_FILE
-+    echo "export NREL=20" >> $TMP_BATCH_FILE
-+    
-+    echo " " >> $TMP_BATCH_FILE
-+    echo "FILE=$FILE" >> $TMP_BATCH_FILE
-+    echo "FILEMAT=$INPUT_DIR/$MAT" >> $TMP_BATCH_FILE	
-+    echo " " >> $TMP_BATCH_FILE
-+    echo "CORE_VAL=$CORE_VAL" >> $TMP_BATCH_FILE
-+    echo "NCOL=$NCOL" >> $TMP_BATCH_FILE
-+    echo "NROW=$NROW" >> $TMP_BATCH_FILE
-+    # This should be computed individually for each script...
-+
-+    srun -n $CORE_VAL $FILE -c $NCOL -r $NROW $INPUT_DIR/$MAT | tee ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_async_simple_over_icollec_groupgemm_bettertree_norecur
-+    # Add final line (srun line) to temporary slurm script
-+
-+    #cat $TMP_BATCH_FILE
-+    #echo " "
-+    # sbatch $TMP_BATCH_FILE
-+  done
-+#one
-+
-+done
-+
-+
-+exit $EXIT_SUCCESS
-+
-diff --git a/build/batch_script_mpi_runit7.sh b/build/batch_script_mpi_runit7.sh
-new file mode 100644
-index 0000000..5b579a4
---- /dev/null
-+++ b/build/batch_script_mpi_runit7.sh
-@@ -0,0 +1,16 @@
-+#!/bin/bash
-+# Bash script to submit many files to Cori/Edison/Queue
-+
-+EXIT_SUCCESS=0
-+EXIT_HOST=1
-+EXIT_PARAM=2
-+
-+
-+for i in `seq 1 200`;
-+do
-+	srun -n 1 ./EXAMPLE/pddrive -r 1 -c 1 ./EXAMPLE/torso3.mtx | tee -a a.out
-+done
-+
-+
-+exit $EXIT_SUCCESS
-+
-diff --git a/build/batch_script_mpi_runit8.sh b/build/batch_script_mpi_runit8.sh
-new file mode 100644
-index 0000000..4496147
---- /dev/null
-+++ b/build/batch_script_mpi_runit8.sh
-@@ -0,0 +1,18 @@
-+#!/bin/bash
-+# Bash script to submit many files to Cori/Edison/Queue
-+
-+
-+EXIT_SUCCESS=0
-+EXIT_HOST=1
-+EXIT_PARAM=2
-+
-+export MPICH_MAX_THREAD_SAFETY=multiple
-+
-+for i in `seq 1 200`;
-+do
-+	srun -n 2 ./EXAMPLE/pddrive -r 2 -c 1 ../EXAMPLE/big.rua | tee -a a.out
-+done
-+
-+
-+exit $EXIT_SUCCESS
-+
-diff --git a/build/batch_script_mpi_runit9.sh b/build/batch_script_mpi_runit9.sh
-new file mode 100644
-index 0000000..5d4f5f9
---- /dev/null
-+++ b/build/batch_script_mpi_runit9.sh
-@@ -0,0 +1,18 @@
-+#!/bin/bash
-+# Bash script to submit many files to Cori/Edison/Queue
-+
-+
-+EXIT_SUCCESS=0
-+EXIT_HOST=1
-+EXIT_PARAM=2
-+
-+export MPICH_MAX_THREAD_SAFETY=multiple
-+
-+for i in `seq 1 200`;
-+do
-+	srun -n 2 ./EXAMPLE/pddrive -r 2 -c 1 ./EXAMPLE/torso3.mtx | tee -a a.out
-+done
-+
-+
-+exit $EXIT_SUCCESS
-+
-diff --git a/build/batch_script_mpi_runit_hybridOMPMPI.sh b/build/batch_script_mpi_runit_hybridOMPMPI.sh
-new file mode 100644
-index 0000000..64abef1
---- /dev/null
-+++ b/build/batch_script_mpi_runit_hybridOMPMPI.sh
-@@ -0,0 +1,187 @@
-+#!/bin/bash
-+# Bash script to submit many files to Cori/Edison/Queue
-+
-+EXIT_SUCCESS=0
-+EXIT_HOST=1
-+EXIT_PARAM=2
-+
-+# MAX_PARAMS=1
-+# # ^^^ This should be fixed, as it should just loop through everything
-+# if [[ $# -eq 0 ]]; then
-+  # echo "Must have at least one parameter; exiting"
-+  # exit $EXIT_PARAM
-+# fi
-+# if [[ $# -gt $MAX_PARAMS ]]; then
-+  # echo "Too many parameters; exiting"
-+  # exit $EXIT_PARAM
-+# fi
-+
-+# INPUT_FILE=$1
-+# # ^^^ Get the input ile
-+
-+CUR_DIR=`pwd`
-+FILE_DIR=$CUR_DIR/EXAMPLE
-+INPUT_DIR=/project/projectdirs/sparse/liuyangz/my_research/matrix
-+FILE_NAME=pddrive
-+FILE=$FILE_DIR/$FILE_NAME
-+
-+TMP_BATCH_FILE=tmp_batch_file.slurm
-+# ^^^ Should check that this is not taken,
-+#     but I would never use this, so ...
-+
-+> $TMP_BATCH_FILE
-+
-+if [[ $NERSC_HOST == edison ]]; then
-+  CORES_PER_NODE=24
-+  THREADS_PER_NODE=48
-+elif [[ $NERSC_HOST == cori ]]; then
-+  CORES_PER_NODE=32
-+  THREADS_PER_NODE=64
-+  # This does not take hyperthreading into account
-+else
-+  # Host unknown; exiting
-+  exit $EXIT_HOST
-+fi
-+
-+
-+#nprows=(6 12 24)
-+#npcols=(6 12 24)
-+
-+
-+#nprows=(2048 1 32)
-+#npcols=(1 2048 64)
-+
-+# nprows=(32 )
-+# npcols=(64 )
-+
-+
-+
-+#nprows=(24 48 1 1 576 2304)
-+#npcols=(24 48 576 2304 1 1)
-+
-+
-+#nprows=(48  1  2304)
-+#npcols=(48  2304 1)
-+
-+#nprows=(6 12 24 48 )
-+#npcols=(6 12 24 48 )
-+
-+#nprows=(6 12 24 48 1 1 1 1 36 144 576 2304)
-+#npcols=(6 12 24 48 36 144 576 2304 1 1 1 1)
-+
-+#nprows=(32 128 512 1 1 1 4 8 16)
-+#npcols=(1 1 1 32 128 512 8 16 32)
-+
-+#nprows=(2048 1 32)
-+#npcols=(1 2048 64)
-+
-+
-+
-+
-+#nprows=(12 1 144)
-+#npcols=(12 144 1)
-+
-+
-+nprows=(1 2 2 4)
-+npcols=(1 1 2 4)
-+ 
-+for ((i = 0; i < ${#npcols[@]}; i++)); do
-+NROW=${nprows[i]}
-+NCOL=${npcols[i]}
-+
-+# NROW=36
-+CORE_VAL=`expr $NCOL \* $NROW`
-+NODE_VAL=`expr $CORE_VAL / $CORES_PER_NODE`
-+MOD_VAL=`expr $CORE_VAL % $CORES_PER_NODE`
-+if [[ $MOD_VAL -ne 0 ]]
-+then
-+  NODE_VAL=`expr $NODE_VAL + 1`
-+fi
-+#PARTITION=debug
-+PARTITION=regular
-+LICENSE=SCRATCH
-+TIME=00:20:00
-+
-+if [[ $NERSC_HOST == edison ]]
-+then
-+  CONSTRAINT=0
-+fi
-+if [[ $NERSC_HOST == cori ]]
-+then
-+  CONSTRAINT=haswell
-+fi
-+
-+
-+#for NTH in 8 
-+for NTH in 1 2 4 8 
-+do
-+OMP_NUM_THREADS=$NTH
-+TH_PER_RANK=`expr $NTH \* 2`
-+
-+
-+#for NSUP in 128 64 32 16 8
-+#do
-+  # for MAT in atmosmodl.rb nlpkkt80.mtx torso3.mtx Ga19As19H42.mtx A22.mtx cage13.rb 
-+  for MAT in torso3.bin
-+  # for MAT in matrix121.dat matrix211.dat tdr190k.dat tdr455k.dat nlpkkt80.mtx torso3.mtx helm2d03.mtx  
-+  # for MAT in tdr190k.dat Ga19As19H42.mtx
-+ # for MAT in torso3.mtx hvdc2.mtx matrix121.dat nlpkkt80.mtx helm2d03.mtx
-+# for MAT in A22.bin DG_GrapheneDisorder_8192.bin DNA_715_64cell.bin LU_C_BN_C_4by2.bin Li4244.bin atmosmodj.bin Ga19As19H42.bin Geo_1438.bin StocF-1465.bin    
-+  # for MAT in  A22.bin DNA_715_64cell.bin LU_C_BN_C_4by2.bin
-+ # for MAT in Ga19As19H42.mtx   
-+  do
-+    # Start of looping stuff
-+    > $TMP_BATCH_FILE
-+    echo "#!/bin/bash -l" >> $TMP_BATCH_FILE
-+    echo " " >> $TMP_BATCH_FILE
-+    echo "#SBATCH -p $PARTITION" >> $TMP_BATCH_FILE
-+    echo "#SBATCH -N $NODE_VAL" >>  $TMP_BATCH_FILE
-+    echo "#SBATCH -t $TIME" >> $TMP_BATCH_FILE
-+    echo "#SBATCH -L $LICENSE" >> $TMP_BATCH_FILE
-+    echo "#SBATCH -J SLU_$MAT" >> $TMP_BATCH_FILE
-+    #echo "#SBATCH -o ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_async_simple_over_icollec_flat_mrhs" >> $TMP_BATCH_FILE
-+    #echo "#SBATCH -e ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_async_simple_over_icollec_flat_mrhs" >> $TMP_BATCH_FILE
-+    # echo "#SBATCH --mail-type=BEGIN" >> $TMP_BATCH_FILE
-+    # echo "#SBATCH --mail-type=END" >> $TMP_BATCH_FILE
-+    echo "#SBATCH --mail-user=liuyangzhuan@lbl.gov" >> $TMP_BATCH_FILE
-+    if [[ $NERSC_HOST == cori ]]
-+    then
-+      echo "#SBATCH -C $CONSTRAINT" >> $TMP_BATCH_FILE
-+    fi
-+    mkdir -p $MAT   
-+    echo "export OMP_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE
-+    echo "export KMP_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE
-+    echo "export MKL_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE  																			
-+    echo "export NSUP=128" >> $TMP_BATCH_FILE
-+    echo "export NREL=20" >> $TMP_BATCH_FILE
-+    echo "export OMP_PLACES=threads" >> $TMP_BATCH_FILE
-+    echo "export OMP_PROC_BIND=spread" >> $TMP_BATCH_FILE
-+    echo "export MPICH_MAX_THREAD_SAFETY=multiple" >> $TMP_BATCH_FILE
-+
-+    echo " " >> $TMP_BATCH_FILE
-+    echo "FILE=$FILE" >> $TMP_BATCH_FILE
-+    echo "FILEMAT=$INPUT_DIR/$MAT" >> $TMP_BATCH_FILE	
-+    echo " " >> $TMP_BATCH_FILE
-+    echo "CORE_VAL=$CORE_VAL" >> $TMP_BATCH_FILE
-+    echo "NCOL=$NCOL" >> $TMP_BATCH_FILE
-+    echo "NROW=$NROW" >> $TMP_BATCH_FILE
-+    # This should be computed individually for each script...
-+
-+    export OMP_NUM_THREADS=$OMP_NUM_THREADS
-+    export OMP_PLACES=threads
-+    export OMP_PROC_BIND=spread
-+    export MPICH_MAX_THREAD_SAFETY=multiple
-+    srun -n $CORE_VAL -N 4 -c $TH_PER_RANK --cpu_bind=cores $FILE -c $NCOL -r $NROW $INPUT_DIR/$MAT | tee ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_${OMP_NUM_THREADS}_mrhs
-+    # Add final line (srun line) to temporary slurm script
-+
-+    #cat $TMP_BATCH_FILE
-+    #echo " "
-+    # sbatch $TMP_BATCH_FILE
-+  done
-+#one
-+
-+done
-+done
-+
-+exit $EXIT_SUCCESS
-+
-diff --git a/build/batch_script_mpi_runit_hybridOMPMPI5.sh b/build/batch_script_mpi_runit_hybridOMPMPI5.sh
-new file mode 100644
-index 0000000..b13017e
---- /dev/null
-+++ b/build/batch_script_mpi_runit_hybridOMPMPI5.sh
-@@ -0,0 +1,187 @@
-+#!/bin/bash
-+# Bash script to submit many files to Cori/Edison/Queue
-+
-+EXIT_SUCCESS=0
-+EXIT_HOST=1
-+EXIT_PARAM=2
-+
-+# MAX_PARAMS=1
-+# # ^^^ This should be fixed, as it should just loop through everything
-+# if [[ $# -eq 0 ]]; then
-+  # echo "Must have at least one parameter; exiting"
-+  # exit $EXIT_PARAM
-+# fi
-+# if [[ $# -gt $MAX_PARAMS ]]; then
-+  # echo "Too many parameters; exiting"
-+  # exit $EXIT_PARAM
-+# fi
-+
-+# INPUT_FILE=$1
-+# # ^^^ Get the input ile
-+
-+CUR_DIR=`pwd`
-+FILE_DIR=$CUR_DIR/EXAMPLE
-+INPUT_DIR=/project/projectdirs/sparse/liuyangz/my_research/matrix
-+FILE_NAME=pddrive
-+FILE=$FILE_DIR/$FILE_NAME
-+
-+TMP_BATCH_FILE=tmp_batch_file.slurm
-+# ^^^ Should check that this is not taken,
-+#     but I would never use this, so ...
-+
-+> $TMP_BATCH_FILE
-+
-+if [[ $NERSC_HOST == edison ]]; then
-+  CORES_PER_NODE=24
-+  THREADS_PER_NODE=48
-+elif [[ $NERSC_HOST == cori ]]; then
-+  CORES_PER_NODE=32
-+  THREADS_PER_NODE=64
-+  # This does not take hyperthreading into account
-+else
-+  # Host unknown; exiting
-+  exit $EXIT_HOST
-+fi
-+
-+
-+#nprows=(6 12 24)
-+#npcols=(6 12 24)
-+
-+
-+#nprows=(2048 1 32)
-+#npcols=(1 2048 64)
-+
-+# nprows=(32 )
-+# npcols=(64 )
-+
-+
-+
-+#nprows=(24 48 1 1 576 2304)
-+#npcols=(24 48 576 2304 1 1)
-+
-+
-+#nprows=(48  1  2304)
-+#npcols=(48  2304 1)
-+
-+#nprows=(6 12 24 48 )
-+#npcols=(6 12 24 48 )
-+
-+#nprows=(6 12 24 48 1 1 1 1 36 144 576 2304)
-+#npcols=(6 12 24 48 36 144 576 2304 1 1 1 1)
-+
-+#nprows=(32 128 512 1 1 1 4 8 16)
-+#npcols=(1 1 1 32 128 512 8 16 32)
-+
-+#nprows=(2048 1 32)
-+#npcols=(1 2048 64)
-+
-+
-+
-+
-+#nprows=(12 1 144)
-+#npcols=(12 144 1)
-+
-+
-+nprows=(4)
-+npcols=(8)
-+ 
-+for ((i = 0; i < ${#npcols[@]}; i++)); do
-+NROW=${nprows[i]}
-+NCOL=${npcols[i]}
-+
-+# NROW=36
-+CORE_VAL=`expr $NCOL \* $NROW`
-+NODE_VAL=`expr $CORE_VAL / $CORES_PER_NODE`
-+MOD_VAL=`expr $CORE_VAL % $CORES_PER_NODE`
-+if [[ $MOD_VAL -ne 0 ]]
-+then
-+  NODE_VAL=`expr $NODE_VAL + 1`
-+fi
-+#PARTITION=debug
-+PARTITION=regular
-+LICENSE=SCRATCH
-+TIME=00:20:00
-+
-+if [[ $NERSC_HOST == edison ]]
-+then
-+  CONSTRAINT=0
-+fi
-+if [[ $NERSC_HOST == cori ]]
-+then
-+  CONSTRAINT=haswell
-+fi
-+
-+
-+for NTH in 1  
-+do
-+
-+OMP_NUM_THREADS=$NTH
-+TH_PER_RANK=`expr $NTH \* 2`
-+
-+
-+#for NSUP in 128 64 32 16 8
-+#do
-+  # for MAT in atmosmodl.rb nlpkkt80.mtx torso3.mtx Ga19As19H42.mtx A22.mtx cage13.rb 
-+  for MAT in torso3.bin
-+  # for MAT in matrix121.dat matrix211.dat tdr190k.dat tdr455k.dat nlpkkt80.mtx torso3.mtx helm2d03.mtx  
-+  # for MAT in tdr190k.dat Ga19As19H42.mtx
-+ # for MAT in torso3.mtx hvdc2.mtx matrix121.dat nlpkkt80.mtx helm2d03.mtx
-+# for MAT in A22.bin DG_GrapheneDisorder_8192.bin DNA_715_64cell.bin LU_C_BN_C_4by2.bin Li4244.bin atmosmodj.bin Ga19As19H42.bin Geo_1438.bin StocF-1465.bin    
-+  # for MAT in  A22.bin DNA_715_64cell.bin LU_C_BN_C_4by2.bin
-+ # for MAT in Ga19As19H42.mtx   
-+  do
-+    # Start of looping stuff
-+    > $TMP_BATCH_FILE
-+    echo "#!/bin/bash -l" >> $TMP_BATCH_FILE
-+    echo " " >> $TMP_BATCH_FILE
-+    echo "#SBATCH -p $PARTITION" >> $TMP_BATCH_FILE
-+    echo "#SBATCH -N $NODE_VAL" >>  $TMP_BATCH_FILE
-+    echo "#SBATCH -t $TIME" >> $TMP_BATCH_FILE
-+    echo "#SBATCH -L $LICENSE" >> $TMP_BATCH_FILE
-+    echo "#SBATCH -J SLU_$MAT" >> $TMP_BATCH_FILE
-+    #echo "#SBATCH -o ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_async_simple_over_icollec_flat_mrhs" >> $TMP_BATCH_FILE
-+    #echo "#SBATCH -e ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_async_simple_over_icollec_flat_mrhs" >> $TMP_BATCH_FILE
-+    # echo "#SBATCH --mail-type=BEGIN" >> $TMP_BATCH_FILE
-+    # echo "#SBATCH --mail-type=END" >> $TMP_BATCH_FILE
-+    echo "#SBATCH --mail-user=liuyangzhuan@lbl.gov" >> $TMP_BATCH_FILE
-+    if [[ $NERSC_HOST == cori ]]
-+    then
-+      echo "#SBATCH -C $CONSTRAINT" >> $TMP_BATCH_FILE
-+    fi
-+    mkdir -p $MAT   
-+    echo "export OMP_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE
-+    echo "export KMP_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE
-+    echo "export MKL_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE  																			
-+    echo "export NSUP=128" >> $TMP_BATCH_FILE
-+    echo "export NREL=20" >> $TMP_BATCH_FILE
-+    echo "export OMP_PLACES=threads" >> $TMP_BATCH_FILE
-+    echo "export OMP_PROC_BIND=spread" >> $TMP_BATCH_FILE
-+    echo "export MPICH_MAX_THREAD_SAFETY=multiple" >> $TMP_BATCH_FILE
-+
-+    echo " " >> $TMP_BATCH_FILE
-+    echo "FILE=$FILE" >> $TMP_BATCH_FILE
-+    echo "FILEMAT=$INPUT_DIR/$MAT" >> $TMP_BATCH_FILE	
-+    echo " " >> $TMP_BATCH_FILE
-+    echo "CORE_VAL=$CORE_VAL" >> $TMP_BATCH_FILE
-+    echo "NCOL=$NCOL" >> $TMP_BATCH_FILE
-+    echo "NROW=$NROW" >> $TMP_BATCH_FILE
-+    # This should be computed individually for each script...
-+
-+    export OMP_NUM_THREADS=$OMP_NUM_THREADS
-+    export OMP_PLACES=threads
-+    export OMP_PROC_BIND=spread
-+    export MPICH_MAX_THREAD_SAFETY=multiple
-+    srun -n $CORE_VAL -N 2 -c $TH_PER_RANK --cpu_bind=cores $FILE -c $NCOL -r $NROW $INPUT_DIR/$MAT | tee ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_${OMP_NUM_THREADS}_mrhs
-+    # Add final line (srun line) to temporary slurm script
-+
-+    #cat $TMP_BATCH_FILE
-+    #echo " "
-+    # sbatch $TMP_BATCH_FILE
-+  done
-+#one
-+
-+done
-+done
-+
-+exit $EXIT_SUCCESS
-+
-diff --git a/build/batch_script_mpi_runit_hybridOMPMPI_2.sh b/build/batch_script_mpi_runit_hybridOMPMPI_2.sh
-new file mode 100644
-index 0000000..cde2b97
---- /dev/null
-+++ b/build/batch_script_mpi_runit_hybridOMPMPI_2.sh
-@@ -0,0 +1,186 @@
-+#!/bin/bash
-+# Bash script to submit many files to Cori/Edison/Queue
-+
-+EXIT_SUCCESS=0
-+EXIT_HOST=1
-+EXIT_PARAM=2
-+
-+# MAX_PARAMS=1
-+# # ^^^ This should be fixed, as it should just loop through everything
-+# if [[ $# -eq 0 ]]; then
-+  # echo "Must have at least one parameter; exiting"
-+  # exit $EXIT_PARAM
-+# fi
-+# if [[ $# -gt $MAX_PARAMS ]]; then
-+  # echo "Too many parameters; exiting"
-+  # exit $EXIT_PARAM
-+# fi
-+
-+# INPUT_FILE=$1
-+# # ^^^ Get the input ile
-+
-+CUR_DIR=`pwd`
-+FILE_DIR=$CUR_DIR/EXAMPLE
-+INPUT_DIR=/project/projectdirs/sparse/liuyangz/my_research/matrix
-+FILE_NAME=pddrive
-+FILE=$FILE_DIR/$FILE_NAME
-+
-+TMP_BATCH_FILE=tmp_batch_file.slurm
-+# ^^^ Should check that this is not taken,
-+#     but I would never use this, so ...
-+
-+> $TMP_BATCH_FILE
-+
-+if [[ $NERSC_HOST == edison ]]; then
-+  CORES_PER_NODE=24
-+  THREADS_PER_NODE=48
-+elif [[ $NERSC_HOST == cori ]]; then
-+  CORES_PER_NODE=32
-+  THREADS_PER_NODE=64
-+  # This does not take hyperthreading into account
-+else
-+  # Host unknown; exiting
-+  exit $EXIT_HOST
-+fi
-+
-+
-+#nprows=(6 12 24)
-+#npcols=(6 12 24)
-+
-+
-+#nprows=(2048 1 32)
-+#npcols=(1 2048 64)
-+
-+# nprows=(32 )
-+# npcols=(64 )
-+
-+
-+
-+#nprows=(24 48 1 1 576 2304)
-+#npcols=(24 48 576 2304 1 1)
-+
-+
-+#nprows=(48  1  2304)
-+#npcols=(48  2304 1)
-+
-+#nprows=(6 12 24 48 )
-+#npcols=(6 12 24 48 )
-+
-+#nprows=(6 12 24 48 1 1 1 1 36 144 576 2304)
-+#npcols=(6 12 24 48 36 144 576 2304 1 1 1 1)
-+
-+#nprows=(32 128 512 1 1 1 4 8 16)
-+#npcols=(1 1 1 32 128 512 8 16 32)
-+
-+#nprows=(2048 1 32)
-+#npcols=(1 2048 64)
-+
-+
-+
-+
-+#nprows=(12 1 144)
-+#npcols=(12 144 1)
-+
-+
-+nprows=(4)
-+npcols=(4)
-+ 
-+for ((i = 0; i < ${#npcols[@]}; i++)); do
-+NROW=${nprows[i]}
-+NCOL=${npcols[i]}
-+
-+# NROW=36
-+CORE_VAL=`expr $NCOL \* $NROW`
-+NODE_VAL=`expr $CORE_VAL / $CORES_PER_NODE`
-+MOD_VAL=`expr $CORE_VAL % $CORES_PER_NODE`
-+if [[ $MOD_VAL -ne 0 ]]
-+then
-+  NODE_VAL=`expr $NODE_VAL + 1`
-+fi
-+#PARTITION=debug
-+PARTITION=regular
-+LICENSE=SCRATCH
-+TIME=00:20:00
-+
-+if [[ $NERSC_HOST == edison ]]
-+then
-+  CONSTRAINT=0
-+fi
-+if [[ $NERSC_HOST == cori ]]
-+then
-+  CONSTRAINT=haswell
-+fi
-+
-+
-+for NTH in 1 2 4 
-+do
-+
-+OMP_NUM_THREADS=$NTH
-+TH_PER_RANK=`expr $NTH \* 2`
-+
-+
-+#for NSUP in 128 64 32 16 8
-+#do
-+  # for MAT in atmosmodl.rb nlpkkt80.mtx torso3.mtx Ga19As19H42.mtx A22.mtx cage13.rb 
-+  # for MAT in torso3.mtx
-+  # for MAT in matrix121.dat matrix211.dat tdr190k.dat tdr455k.dat nlpkkt80.mtx torso3.mtx helm2d03.mtx  
-+  # for MAT in tdr190k.dat Ga19As19H42.mtx
-+ # for MAT in torso3.mtx hvdc2.mtx matrix121.dat nlpkkt80.mtx helm2d03.mtx
-+   for MAT in DG_GrapheneDisorder_8192.bin Li4244.bin torso3.bin
-+ # for MAT in Ga19As19H42.mtx   
-+  do
-+    # Start of looping stuff
-+    > $TMP_BATCH_FILE
-+    echo "#!/bin/bash -l" >> $TMP_BATCH_FILE
-+    echo " " >> $TMP_BATCH_FILE
-+    echo "#SBATCH -p $PARTITION" >> $TMP_BATCH_FILE
-+    echo "#SBATCH -N $NODE_VAL" >>  $TMP_BATCH_FILE
-+    echo "#SBATCH -t $TIME" >> $TMP_BATCH_FILE
-+    echo "#SBATCH -L $LICENSE" >> $TMP_BATCH_FILE
-+    echo "#SBATCH -J SLU_$MAT" >> $TMP_BATCH_FILE
-+    #echo "#SBATCH -o ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_async_simple_over_icollec_flat_mrhs" >> $TMP_BATCH_FILE
-+    #echo "#SBATCH -e ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_async_simple_over_icollec_flat_mrhs" >> $TMP_BATCH_FILE
-+    # echo "#SBATCH --mail-type=BEGIN" >> $TMP_BATCH_FILE
-+    # echo "#SBATCH --mail-type=END" >> $TMP_BATCH_FILE
-+    echo "#SBATCH --mail-user=liuyangzhuan@lbl.gov" >> $TMP_BATCH_FILE
-+    if [[ $NERSC_HOST == cori ]]
-+    then
-+      echo "#SBATCH -C $CONSTRAINT" >> $TMP_BATCH_FILE
-+    fi
-+    mkdir -p $MAT   
-+    echo "export OMP_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE
-+    echo "export KMP_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE
-+    echo "export MKL_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE  																			
-+    echo "export NSUP=128" >> $TMP_BATCH_FILE
-+    echo "export NREL=20" >> $TMP_BATCH_FILE
-+    echo "export OMP_PLACES=threads" >> $TMP_BATCH_FILE
-+    echo "export OMP_PROC_BIND=spread" >> $TMP_BATCH_FILE
-+    echo "export MPICH_MAX_THREAD_SAFETY=multiple" >> $TMP_BATCH_FILE
-+
-+    echo " " >> $TMP_BATCH_FILE
-+    echo "FILE=$FILE" >> $TMP_BATCH_FILE
-+    echo "FILEMAT=$INPUT_DIR/$MAT" >> $TMP_BATCH_FILE	
-+    echo " " >> $TMP_BATCH_FILE
-+    echo "CORE_VAL=$CORE_VAL" >> $TMP_BATCH_FILE
-+    echo "NCOL=$NCOL" >> $TMP_BATCH_FILE
-+    echo "NROW=$NROW" >> $TMP_BATCH_FILE
-+    # This should be computed individually for each script...
-+
-+    export OMP_NUM_THREADS=$OMP_NUM_THREADS
-+    export OMP_PLACES=threads
-+    export OMP_PROC_BIND=spread
-+    export MPICH_MAX_THREAD_SAFETY=multiple
-+    srun -n $CORE_VAL -N 64 -c $TH_PER_RANK --cpu_bind=cores $FILE -c $NCOL -r $NROW $INPUT_DIR/$MAT | tee ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_hybridOMPMPI_${OMP_NUM_THREADS}_cori
-+    # Add final line (srun line) to temporary slurm script
-+
-+    #cat $TMP_BATCH_FILE
-+    #echo " "
-+    # sbatch $TMP_BATCH_FILE
-+  done
-+#one
-+
-+done
-+done
-+
-+exit $EXIT_SUCCESS
-+
-diff --git a/build/batch_script_mpi_runit_hybridOMPMPI_3.sh b/build/batch_script_mpi_runit_hybridOMPMPI_3.sh
-new file mode 100644
-index 0000000..d3a087d
---- /dev/null
-+++ b/build/batch_script_mpi_runit_hybridOMPMPI_3.sh
-@@ -0,0 +1,187 @@
-+#!/bin/bash
-+# Bash script to submit many files to Cori/Edison/Queue
-+
-+EXIT_SUCCESS=0
-+EXIT_HOST=1
-+EXIT_PARAM=2
-+
-+# MAX_PARAMS=1
-+# # ^^^ This should be fixed, as it should just loop through everything
-+# if [[ $# -eq 0 ]]; then
-+  # echo "Must have at least one parameter; exiting"
-+  # exit $EXIT_PARAM
-+# fi
-+# if [[ $# -gt $MAX_PARAMS ]]; then
-+  # echo "Too many parameters; exiting"
-+  # exit $EXIT_PARAM
-+# fi
-+
-+# INPUT_FILE=$1
-+# # ^^^ Get the input ile
-+
-+CUR_DIR=`pwd`
-+FILE_DIR=$CUR_DIR/EXAMPLE
-+INPUT_DIR=/project/projectdirs/sparse/liuyangz/my_research/matrix
-+FILE_NAME=pddrive
-+FILE=$FILE_DIR/$FILE_NAME
-+
-+TMP_BATCH_FILE=tmp_batch_file.slurm
-+# ^^^ Should check that this is not taken,
-+#     but I would never use this, so ...
-+
-+> $TMP_BATCH_FILE
-+
-+if [[ $NERSC_HOST == edison ]]; then
-+  CORES_PER_NODE=24
-+  THREADS_PER_NODE=48
-+elif [[ $NERSC_HOST == cori ]]; then
-+  CORES_PER_NODE=32
-+  THREADS_PER_NODE=64
-+  # This does not take hyperthreading into account
-+else
-+  # Host unknown; exiting
-+  exit $EXIT_HOST
-+fi
-+
-+
-+#nprows=(6 12 24)
-+#npcols=(6 12 24)
-+
-+
-+#nprows=(2048 1 32)
-+#npcols=(1 2048 64)
-+
-+# nprows=(32 )
-+# npcols=(64 )
-+
-+
-+
-+#nprows=(24 48 1 1 576 2304)
-+#npcols=(24 48 576 2304 1 1)
-+
-+
-+#nprows=(48  1  2304)
-+#npcols=(48  2304 1)
-+
-+#nprows=(6 12 24 48 )
-+#npcols=(6 12 24 48 )
-+
-+#nprows=(6 12 24 48 1 1 1 1 36 144 576 2304)
-+#npcols=(6 12 24 48 36 144 576 2304 1 1 1 1)
-+
-+#nprows=(32 128 512 1 1 1 4 8 16)
-+#npcols=(1 1 1 32 128 512 8 16 32)
-+
-+#nprows=(2048 1 32)
-+#npcols=(1 2048 64)
-+
-+
-+
-+
-+#nprows=(12 1 144)
-+#npcols=(12 144 1)
-+
-+
-+nprows=(16)
-+npcols=(16)
-+ 
-+for ((i = 0; i < ${#npcols[@]}; i++)); do
-+NROW=${nprows[i]}
-+NCOL=${npcols[i]}
-+
-+# NROW=36
-+CORE_VAL=`expr $NCOL \* $NROW`
-+NODE_VAL=`expr $CORE_VAL / $CORES_PER_NODE`
-+MOD_VAL=`expr $CORE_VAL % $CORES_PER_NODE`
-+if [[ $MOD_VAL -ne 0 ]]
-+then
-+  NODE_VAL=`expr $NODE_VAL + 1`
-+fi
-+#PARTITION=debug
-+PARTITION=regular
-+LICENSE=SCRATCH
-+TIME=00:20:00
-+
-+if [[ $NERSC_HOST == edison ]]
-+then
-+  CONSTRAINT=0
-+fi
-+if [[ $NERSC_HOST == cori ]]
-+then
-+  CONSTRAINT=haswell
-+fi
-+
-+
-+for NTH in 1 2 4 
-+do
-+
-+OMP_NUM_THREADS=$NTH
-+TH_PER_RANK=`expr $NTH \* 2`
-+
-+
-+#for NSUP in 128 64 32 16 8
-+#do
-+  # for MAT in atmosmodl.rb nlpkkt80.mtx torso3.mtx Ga19As19H42.mtx A22.mtx cage13.rb 
-+  # for MAT in torso3.mtx
-+  # for MAT in matrix121.dat matrix211.dat tdr190k.dat tdr455k.dat nlpkkt80.mtx torso3.mtx helm2d03.mtx  
-+  # for MAT in tdr190k.dat Ga19As19H42.mtx
-+ for MAT in torso3.bin
-+# for MAT in A22.bin DG_GrapheneDisorder_8192.bin DNA_715_64cell.bin LU_C_BN_C_4by2.bin Li4244.bin atmosmodj.bin Ga19As19H42.bin Geo_1438.bin StocF-1465.bin    
-+  # for MAT in  A22.bin DNA_715_64cell.bin LU_C_BN_C_4by2.bin
-+ # for MAT in Ga19As19H42.mtx   
-+  do
-+    # Start of looping stuff
-+    > $TMP_BATCH_FILE
-+    echo "#!/bin/bash -l" >> $TMP_BATCH_FILE
-+    echo " " >> $TMP_BATCH_FILE
-+    echo "#SBATCH -p $PARTITION" >> $TMP_BATCH_FILE
-+    echo "#SBATCH -N $NODE_VAL" >>  $TMP_BATCH_FILE
-+    echo "#SBATCH -t $TIME" >> $TMP_BATCH_FILE
-+    echo "#SBATCH -L $LICENSE" >> $TMP_BATCH_FILE
-+    echo "#SBATCH -J SLU_$MAT" >> $TMP_BATCH_FILE
-+    #echo "#SBATCH -o ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_async_simple_over_icollec_flat_mrhs" >> $TMP_BATCH_FILE
-+    #echo "#SBATCH -e ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_async_simple_over_icollec_flat_mrhs" >> $TMP_BATCH_FILE
-+    # echo "#SBATCH --mail-type=BEGIN" >> $TMP_BATCH_FILE
-+    # echo "#SBATCH --mail-type=END" >> $TMP_BATCH_FILE
-+    echo "#SBATCH --mail-user=liuyangzhuan@lbl.gov" >> $TMP_BATCH_FILE
-+    if [[ $NERSC_HOST == cori ]]
-+    then
-+      echo "#SBATCH -C $CONSTRAINT" >> $TMP_BATCH_FILE
-+    fi
-+    mkdir -p $MAT   
-+    echo "export OMP_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE
-+    echo "export KMP_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE
-+    echo "export MKL_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE  																			
-+    echo "export NSUP=128" >> $TMP_BATCH_FILE
-+    echo "export NREL=20" >> $TMP_BATCH_FILE
-+    echo "export OMP_PLACES=threads" >> $TMP_BATCH_FILE
-+    echo "export OMP_PROC_BIND=spread" >> $TMP_BATCH_FILE
-+    echo "export MPICH_MAX_THREAD_SAFETY=multiple" >> $TMP_BATCH_FILE
-+
-+    echo " " >> $TMP_BATCH_FILE
-+    echo "FILE=$FILE" >> $TMP_BATCH_FILE
-+    echo "FILEMAT=$INPUT_DIR/$MAT" >> $TMP_BATCH_FILE	
-+    echo " " >> $TMP_BATCH_FILE
-+    echo "CORE_VAL=$CORE_VAL" >> $TMP_BATCH_FILE
-+    echo "NCOL=$NCOL" >> $TMP_BATCH_FILE
-+    echo "NROW=$NROW" >> $TMP_BATCH_FILE
-+    # This should be computed individually for each script...
-+
-+    export OMP_NUM_THREADS=$OMP_NUM_THREADS
-+    export OMP_PLACES=threads
-+    export OMP_PROC_BIND=spread
-+    export MPICH_MAX_THREAD_SAFETY=multiple
-+    srun -n $CORE_VAL -N 32 -c $TH_PER_RANK --cpu_bind=cores $FILE -c $NCOL -r $NROW $INPUT_DIR/$MAT | tee ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_${OMP_NUM_THREADS}
-+    # Add final line (srun line) to temporary slurm script
-+
-+    #cat $TMP_BATCH_FILE
-+    #echo " "
-+    # sbatch $TMP_BATCH_FILE
-+  done
-+#one
-+
-+done
-+done
-+
-+exit $EXIT_SUCCESS
-+
-diff --git a/build/batch_script_mpi_runit_hybridOMPMPI_4.sh b/build/batch_script_mpi_runit_hybridOMPMPI_4.sh
-new file mode 100644
-index 0000000..28109df
---- /dev/null
-+++ b/build/batch_script_mpi_runit_hybridOMPMPI_4.sh
-@@ -0,0 +1,187 @@
-+#!/bin/bash
-+# Bash script to submit many files to Cori/Edison/Queue
-+
-+EXIT_SUCCESS=0
-+EXIT_HOST=1
-+EXIT_PARAM=2
-+
-+# MAX_PARAMS=1
-+# # ^^^ This should be fixed, as it should just loop through everything
-+# if [[ $# -eq 0 ]]; then
-+  # echo "Must have at least one parameter; exiting"
-+  # exit $EXIT_PARAM
-+# fi
-+# if [[ $# -gt $MAX_PARAMS ]]; then
-+  # echo "Too many parameters; exiting"
-+  # exit $EXIT_PARAM
-+# fi
-+
-+# INPUT_FILE=$1
-+# # ^^^ Get the input ile
-+
-+CUR_DIR=`pwd`
-+FILE_DIR=$CUR_DIR/EXAMPLE
-+INPUT_DIR=/project/projectdirs/sparse/liuyangz/my_research/matrix
-+FILE_NAME=pddrive
-+FILE=$FILE_DIR/$FILE_NAME
-+
-+TMP_BATCH_FILE=tmp_batch_file.slurm
-+# ^^^ Should check that this is not taken,
-+#     but I would never use this, so ...
-+
-+> $TMP_BATCH_FILE
-+
-+if [[ $NERSC_HOST == edison ]]; then
-+  CORES_PER_NODE=24
-+  THREADS_PER_NODE=48
-+elif [[ $NERSC_HOST == cori ]]; then
-+  CORES_PER_NODE=32
-+  THREADS_PER_NODE=64
-+  # This does not take hyperthreading into account
-+else
-+  # Host unknown; exiting
-+  exit $EXIT_HOST
-+fi
-+
-+
-+#nprows=(6 12 24)
-+#npcols=(6 12 24)
-+
-+
-+#nprows=(2048 1 32)
-+#npcols=(1 2048 64)
-+
-+# nprows=(32 )
-+# npcols=(64 )
-+
-+
-+
-+#nprows=(24 48 1 1 576 2304)
-+#npcols=(24 48 576 2304 1 1)
-+
-+
-+#nprows=(48  1  2304)
-+#npcols=(48  2304 1)
-+
-+#nprows=(6 12 24 48 )
-+#npcols=(6 12 24 48 )
-+
-+#nprows=(6 12 24 48 1 1 1 1 36 144 576 2304)
-+#npcols=(6 12 24 48 36 144 576 2304 1 1 1 1)
-+
-+#nprows=(32 128 512 1 1 1 4 8 16)
-+#npcols=(1 1 1 32 128 512 8 16 32)
-+
-+#nprows=(2048 1 32)
-+#npcols=(1 2048 64)
-+
-+
-+
-+
-+#nprows=(12 1 144)
-+#npcols=(12 144 1)
-+
-+
-+nprows=(4)
-+npcols=(4)
-+ 
-+for ((i = 0; i < ${#npcols[@]}; i++)); do
-+NROW=${nprows[i]}
-+NCOL=${npcols[i]}
-+
-+# NROW=36
-+CORE_VAL=`expr $NCOL \* $NROW`
-+NODE_VAL=`expr $CORE_VAL / $CORES_PER_NODE`
-+MOD_VAL=`expr $CORE_VAL % $CORES_PER_NODE`
-+if [[ $MOD_VAL -ne 0 ]]
-+then
-+  NODE_VAL=`expr $NODE_VAL + 1`
-+fi
-+#PARTITION=debug
-+PARTITION=regular
-+LICENSE=SCRATCH
-+TIME=00:20:00
-+
-+if [[ $NERSC_HOST == edison ]]
-+then
-+  CONSTRAINT=0
-+fi
-+if [[ $NERSC_HOST == cori ]]
-+then
-+  CONSTRAINT=haswell
-+fi
-+
-+
-+for NTH in 1 2 4 
-+do
-+
-+OMP_NUM_THREADS=$NTH
-+TH_PER_RANK=`expr $NTH \* 2`
-+
-+
-+#for NSUP in 128 64 32 16 8
-+#do
-+  # for MAT in atmosmodl.rb nlpkkt80.mtx torso3.mtx Ga19As19H42.mtx A22.mtx cage13.rb 
-+  # for MAT in torso3.mtx
-+  # for MAT in matrix121.dat matrix211.dat tdr190k.dat tdr455k.dat nlpkkt80.mtx torso3.mtx helm2d03.mtx  
-+  # for MAT in tdr190k.dat Ga19As19H42.mtx
-+ for MAT in torso3.bin
-+# for MAT in A22.bin DG_GrapheneDisorder_8192.bin DNA_715_64cell.bin LU_C_BN_C_4by2.bin Li4244.bin atmosmodj.bin Ga19As19H42.bin Geo_1438.bin StocF-1465.bin    
-+  # for MAT in  A22.bin DNA_715_64cell.bin LU_C_BN_C_4by2.bin
-+ # for MAT in Ga19As19H42.mtx   
-+  do
-+    # Start of looping stuff
-+    > $TMP_BATCH_FILE
-+    echo "#!/bin/bash -l" >> $TMP_BATCH_FILE
-+    echo " " >> $TMP_BATCH_FILE
-+    echo "#SBATCH -p $PARTITION" >> $TMP_BATCH_FILE
-+    echo "#SBATCH -N $NODE_VAL" >>  $TMP_BATCH_FILE
-+    echo "#SBATCH -t $TIME" >> $TMP_BATCH_FILE
-+    echo "#SBATCH -L $LICENSE" >> $TMP_BATCH_FILE
-+    echo "#SBATCH -J SLU_$MAT" >> $TMP_BATCH_FILE
-+    #echo "#SBATCH -o ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_async_simple_over_icollec_flat_mrhs" >> $TMP_BATCH_FILE
-+    #echo "#SBATCH -e ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_async_simple_over_icollec_flat_mrhs" >> $TMP_BATCH_FILE
-+    # echo "#SBATCH --mail-type=BEGIN" >> $TMP_BATCH_FILE
-+    # echo "#SBATCH --mail-type=END" >> $TMP_BATCH_FILE
-+    echo "#SBATCH --mail-user=liuyangzhuan@lbl.gov" >> $TMP_BATCH_FILE
-+    if [[ $NERSC_HOST == cori ]]
-+    then
-+      echo "#SBATCH -C $CONSTRAINT" >> $TMP_BATCH_FILE
-+    fi
-+    mkdir -p $MAT   
-+    echo "export OMP_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE
-+    echo "export KMP_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE
-+    echo "export MKL_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE  																			
-+    echo "export NSUP=128" >> $TMP_BATCH_FILE
-+    echo "export NREL=20" >> $TMP_BATCH_FILE
-+    echo "export OMP_PLACES=threads" >> $TMP_BATCH_FILE
-+    echo "export OMP_PROC_BIND=spread" >> $TMP_BATCH_FILE
-+    echo "export MPICH_MAX_THREAD_SAFETY=multiple" >> $TMP_BATCH_FILE
-+
-+    echo " " >> $TMP_BATCH_FILE
-+    echo "FILE=$FILE" >> $TMP_BATCH_FILE
-+    echo "FILEMAT=$INPUT_DIR/$MAT" >> $TMP_BATCH_FILE	
-+    echo " " >> $TMP_BATCH_FILE
-+    echo "CORE_VAL=$CORE_VAL" >> $TMP_BATCH_FILE
-+    echo "NCOL=$NCOL" >> $TMP_BATCH_FILE
-+    echo "NROW=$NROW" >> $TMP_BATCH_FILE
-+    # This should be computed individually for each script...
-+
-+    export OMP_NUM_THREADS=$OMP_NUM_THREADS
-+    export OMP_PLACES=threads
-+    export OMP_PROC_BIND=spread
-+    export MPICH_MAX_THREAD_SAFETY=multiple
-+    srun -n $CORE_VAL -N 4 -c $TH_PER_RANK --cpu_bind=cores $FILE -c $NCOL -r $NROW $INPUT_DIR/$MAT | tee ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_${OMP_NUM_THREADS}
-+    # Add final line (srun line) to temporary slurm script
-+
-+    #cat $TMP_BATCH_FILE
-+    #echo " "
-+    # sbatch $TMP_BATCH_FILE
-+  done
-+#one
-+
-+done
-+done
-+
-+exit $EXIT_SUCCESS
-+
-diff --git a/build/batch_script_mpi_runit_hybridOMPMPI_6.sh b/build/batch_script_mpi_runit_hybridOMPMPI_6.sh
-new file mode 100644
-index 0000000..379a8e4
---- /dev/null
-+++ b/build/batch_script_mpi_runit_hybridOMPMPI_6.sh
-@@ -0,0 +1,189 @@
-+#!/bin/bash
-+# Bash script to submit many files to Cori/Edison/Queue
-+
-+EXIT_SUCCESS=0
-+EXIT_HOST=1
-+EXIT_PARAM=2
-+
-+# MAX_PARAMS=1
-+# # ^^^ This should be fixed, as it should just loop through everything
-+# if [[ $# -eq 0 ]]; then
-+  # echo "Must have at least one parameter; exiting"
-+  # exit $EXIT_PARAM
-+# fi
-+# if [[ $# -gt $MAX_PARAMS ]]; then
-+  # echo "Too many parameters; exiting"
-+  # exit $EXIT_PARAM
-+# fi
-+
-+# INPUT_FILE=$1
-+# # ^^^ Get the input ile
-+
-+CUR_DIR=`pwd`
-+FILE_DIR=$CUR_DIR/EXAMPLE
-+INPUT_DIR=/project/projectdirs/sparse/liuyangz/my_research/matrix
-+FILE_NAME=pddrive
-+FILE=$FILE_DIR/$FILE_NAME
-+
-+TMP_BATCH_FILE=tmp_batch_file.slurm
-+# ^^^ Should check that this is not taken,
-+#     but I would never use this, so ...
-+
-+> $TMP_BATCH_FILE
-+
-+if [[ $NERSC_HOST == edison ]]; then
-+  CORES_PER_NODE=24
-+  THREADS_PER_NODE=48
-+elif [[ $NERSC_HOST == cori ]]; then
-+  CORES_PER_NODE=32
-+  THREADS_PER_NODE=64
-+  # This does not take hyperthreading into account
-+else
-+  # Host unknown; exiting
-+  exit $EXIT_HOST
-+fi
-+
-+
-+#nprows=(6 12 24)
-+#npcols=(6 12 24)
-+
-+
-+#nprows=(2048 1 32)
-+#npcols=(1 2048 64)
-+
-+# nprows=(32 )
-+# npcols=(64 )
-+
-+
-+
-+#nprows=(24 48 1 1 576 2304)
-+#npcols=(24 48 576 2304 1 1)
-+
-+
-+#nprows=(48  1  2304)
-+#npcols=(48  2304 1)
-+
-+#nprows=(6 12 24 48 )
-+#npcols=(6 12 24 48 )
-+
-+#nprows=(6 12 24 48 1 1 1 1 36 144 576 2304)
-+#npcols=(6 12 24 48 36 144 576 2304 1 1 1 1)
-+
-+#nprows=(32 128 512 1 1 1 4 8 16)
-+#npcols=(1 1 1 32 128 512 8 16 32)
-+
-+#nprows=(2048 1 32)
-+#npcols=(1 2048 64)
-+
-+
-+
-+
-+#nprows=(12 1 144)
-+#npcols=(12 144 1)
-+
-+
-+nprows=(16 )
-+npcols=(16 )
-+ 
-+for ((i = 0; i < ${#npcols[@]}; i++)); do
-+NROW=${nprows[i]}
-+NCOL=${npcols[i]}
-+
-+# NROW=36
-+
-+#PARTITION=debug
-+PARTITION=regular
-+LICENSE=SCRATCH
-+TIME=00:20:00
-+
-+if [[ $NERSC_HOST == edison ]]
-+then
-+  CONSTRAINT=0
-+fi
-+if [[ $NERSC_HOST == cori ]]
-+then
-+  CONSTRAINT=haswell
-+fi
-+
-+
-+#for NTH in 8 
-+for NTH in 1 2 4 8 
-+do
-+OMP_NUM_THREADS=$NTH
-+TH_PER_RANK=`expr $NTH \* 2`
-+
-+CORE_VAL=`expr $NCOL \* $NROW`
-+NODE_VAL=`expr $CORE_VAL / $CORES_PER_NODE \* $NTH`
-+MOD_VAL=`expr $CORE_VAL % $CORES_PER_NODE`
-+if [[ $MOD_VAL -ne 0 ]]
-+then
-+  NODE_VAL=`expr $NODE_VAL + 1`
-+fi
-+
-+
-+#for NSUP in 128 64 32 16 8
-+#do
-+  # for MAT in atmosmodl.rb nlpkkt80.mtx torso3.mtx Ga19As19H42.mtx A22.mtx cage13.rb 
-+  for MAT in torso3.bin
-+  # for MAT in matrix121.dat matrix211.dat tdr190k.dat tdr455k.dat nlpkkt80.mtx torso3.mtx helm2d03.mtx  
-+  # for MAT in tdr190k.dat Ga19As19H42.mtx
-+ # for MAT in torso3.mtx hvdc2.mtx matrix121.dat nlpkkt80.mtx helm2d03.mtx
-+# for MAT in A22.bin DG_GrapheneDisorder_8192.bin DNA_715_64cell.bin LU_C_BN_C_4by2.bin Li4244.bin atmosmodj.bin Ga19As19H42.bin Geo_1438.bin StocF-1465.bin    
-+  # for MAT in  A22.bin DNA_715_64cell.bin LU_C_BN_C_4by2.bin
-+ # for MAT in Ga19As19H42.mtx   
-+  do
-+    # Start of looping stuff
-+    > $TMP_BATCH_FILE
-+    echo "#!/bin/bash -l" >> $TMP_BATCH_FILE
-+    echo " " >> $TMP_BATCH_FILE
-+    echo "#SBATCH -p $PARTITION" >> $TMP_BATCH_FILE
-+    echo "#SBATCH -N $NODE_VAL" >>  $TMP_BATCH_FILE
-+    echo "#SBATCH -t $TIME" >> $TMP_BATCH_FILE
-+    echo "#SBATCH -L $LICENSE" >> $TMP_BATCH_FILE
-+    echo "#SBATCH -J SLU_$MAT" >> $TMP_BATCH_FILE
-+    #echo "#SBATCH -o ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_async_simple_over_icollec_flat_mrhs" >> $TMP_BATCH_FILE
-+    #echo "#SBATCH -e ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_async_simple_over_icollec_flat_mrhs" >> $TMP_BATCH_FILE
-+    # echo "#SBATCH --mail-type=BEGIN" >> $TMP_BATCH_FILE
-+    # echo "#SBATCH --mail-type=END" >> $TMP_BATCH_FILE
-+    echo "#SBATCH --mail-user=liuyangzhuan@lbl.gov" >> $TMP_BATCH_FILE
-+    if [[ $NERSC_HOST == cori ]]
-+    then
-+      echo "#SBATCH -C $CONSTRAINT" >> $TMP_BATCH_FILE
-+    fi
-+    mkdir -p $MAT   
-+    echo "export OMP_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE
-+    echo "export KMP_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE
-+    echo "export MKL_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE  																			
-+    echo "export NSUP=128" >> $TMP_BATCH_FILE
-+    echo "export NREL=20" >> $TMP_BATCH_FILE
-+    echo "export OMP_PLACES=threads" >> $TMP_BATCH_FILE
-+    echo "export OMP_PROC_BIND=spread" >> $TMP_BATCH_FILE
-+    echo "export MPICH_MAX_THREAD_SAFETY=multiple" >> $TMP_BATCH_FILE
-+
-+    echo " " >> $TMP_BATCH_FILE
-+    echo "FILE=$FILE" >> $TMP_BATCH_FILE
-+    echo "FILEMAT=$INPUT_DIR/$MAT" >> $TMP_BATCH_FILE	
-+    echo " " >> $TMP_BATCH_FILE
-+    echo "CORE_VAL=$CORE_VAL" >> $TMP_BATCH_FILE
-+    echo "NCOL=$NCOL" >> $TMP_BATCH_FILE
-+    echo "NROW=$NROW" >> $TMP_BATCH_FILE
-+    # This should be computed individually for each script...
-+
-+    export OMP_NUM_THREADS=$OMP_NUM_THREADS
-+    export OMP_PLACES=threads
-+    export OMP_PROC_BIND=spread
-+    export MPICH_MAX_THREAD_SAFETY=multiple
-+    srun -n $CORE_VAL -N $NODE_VAL -c $TH_PER_RANK --cpu_bind=cores $FILE -c $NCOL -r $NROW $INPUT_DIR/$MAT | tee ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_${OMP_NUM_THREADS}
-+    # Add final line (srun line) to temporary slurm script
-+
-+    #cat $TMP_BATCH_FILE
-+    #echo " "
-+    # sbatch $TMP_BATCH_FILE
-+  done
-+#one
-+
-+done
-+done
-+
-+exit $EXIT_SUCCESS
-+
-diff --git a/build/batch_script_mpi_runit_pureOMP.sh b/build/batch_script_mpi_runit_pureOMP.sh
-new file mode 100644
-index 0000000..8b97ea1
---- /dev/null
-+++ b/build/batch_script_mpi_runit_pureOMP.sh
-@@ -0,0 +1,193 @@
-+#!/bin/bash
-+# Bash script to submit many files to Cori/Edison/Queue
-+
-+EXIT_SUCCESS=0
-+EXIT_HOST=1
-+EXIT_PARAM=2
-+
-+# MAX_PARAMS=1
-+# # ^^^ This should be fixed, as it should just loop through everything
-+# if [[ $# -eq 0 ]]; then
-+  # echo "Must have at least one parameter; exiting"
-+  # exit $EXIT_PARAM
-+# fi
-+# if [[ $# -gt $MAX_PARAMS ]]; then
-+  # echo "Too many parameters; exiting"
-+  # exit $EXIT_PARAM
-+# fi
-+
-+# INPUT_FILE=$1
-+# # ^^^ Get the input ile
-+
-+CUR_DIR=`pwd`
-+FILE_DIR=$CUR_DIR/EXAMPLE
-+INPUT_DIR=/project/projectdirs/sparse/liuyangz/my_research/matrix
-+FILE_NAME=pddrive
-+FILE=$FILE_DIR/$FILE_NAME
-+
-+TMP_BATCH_FILE=tmp_batch_file.slurm
-+# ^^^ Should check that this is not taken,
-+#     but I would never use this, so ...
-+
-+> $TMP_BATCH_FILE
-+
-+if [[ $NERSC_HOST == edison ]]; then
-+  CORES_PER_NODE=24
-+  THREADS_PER_NODE=48
-+elif [[ $NERSC_HOST == cori ]]; then
-+  CORES_PER_NODE=32
-+  THREADS_PER_NODE=64
-+  # This does not take hyperthreading into account
-+else
-+  # Host unknown; exiting
-+  exit $EXIT_HOST
-+fi
-+
-+
-+#nprows=(6 12 24)
-+#npcols=(6 12 24)
-+
-+
-+#nprows=(2048 1 32)
-+#npcols=(1 2048 64)
-+
-+# nprows=(32 )
-+# npcols=(64 )
-+
-+
-+
-+#nprows=(24 48 1 1 576 2304)
-+#npcols=(24 48 576 2304 1 1)
-+
-+
-+#nprows=(48  1  2304)
-+#npcols=(48  2304 1)
-+
-+#nprows=(6 12 24 48 )
-+#npcols=(6 12 24 48 )
-+
-+#nprows=(6 12 24 48 1 1 1 1 36 144 576 2304)
-+#npcols=(6 12 24 48 36 144 576 2304 1 1 1 1)
-+
-+#nprows=(32 128 512 1 1 1 4 8 16)
-+#npcols=(1 1 1 32 128 512 8 16 32)
-+
-+#nprows=(2048 1 32)
-+#npcols=(1 2048 64)
-+
-+
-+
-+
-+#nprows=(12 1 144)
-+#npcols=(12 144 1)
-+
-+
-+nprows=(1)
-+npcols=(1)
-+ 
-+for ((i = 0; i < ${#npcols[@]}; i++)); do
-+NROW=${nprows[i]}
-+NCOL=${npcols[i]}
-+
-+# NROW=36
-+CORE_VAL=`expr $NCOL \* $NROW`
-+NODE_VAL=`expr $CORE_VAL / $CORES_PER_NODE`
-+MOD_VAL=`expr $CORE_VAL % $CORES_PER_NODE`
-+if [[ $MOD_VAL -ne 0 ]]
-+then
-+  NODE_VAL=`expr $NODE_VAL + 1`
-+fi
-+#PARTITION=debug
-+PARTITION=regular
-+LICENSE=SCRATCH
-+TIME=00:20:00
-+
-+if [[ $NERSC_HOST == edison ]]
-+then
-+  CONSTRAINT=0
-+fi
-+if [[ $NERSC_HOST == cori ]]
-+then
-+  CONSTRAINT=haswell
-+fi
-+
-+
-+for NTH in 1 16 32
-+do
-+
-+OMP_NUM_THREADS=$NTH
-+TH_PER_RANK=`expr $NTH \* 2`
-+
-+
-+#for NSUP in 128 64 32 16 8
-+#do
-+  # for MAT in atmosmodl.rb nlpkkt80.mtx torso3.mtx Ga19As19H42.mtx A22.mtx cage13.rb 
-+  # for MAT in torso3.mtx
-+  # for MAT in matrix121.dat matrix211.dat tdr190k.dat tdr455k.dat nlpkkt80.mtx torso3.mtx helm2d03.mtx  
-+  # for MAT in tdr190k.dat Ga19As19H42.mtx
-+ for MAT in hvdc2.mtx torso3.mtx matrix121.dat helm2d03.mtx
-+   # for MAT in torso3.mtx
-+# for MAT in A22.bin DG_GrapheneDisorder_8192.bin DNA_715_64cell.bin LU_C_BN_C_4by2.bin Li4244.bin atmosmodj.bin Ga19As19H42.bin Geo_1438.bin StocF-1465.bin
-+
-+# for MAT in LU_C_BN_C_4by2.bin Li4244.bin atmosmodj.bin Ga19As19H42.bin Geo_1438.bin StocF-1465.bin
-+
-+
-+#for MAT in big.rua
-+#for MAT in torso3.bin    
-+# for MAT in Ga19As19H42.mtx   
-+  do
-+    # Start of looping stuff
-+    > $TMP_BATCH_FILE
-+    echo "#!/bin/bash -l" >> $TMP_BATCH_FILE
-+    echo " " >> $TMP_BATCH_FILE
-+    echo "#SBATCH -p $PARTITION" >> $TMP_BATCH_FILE
-+    echo "#SBATCH -N $NODE_VAL" >>  $TMP_BATCH_FILE
-+    echo "#SBATCH -t $TIME" >> $TMP_BATCH_FILE
-+    echo "#SBATCH -L $LICENSE" >> $TMP_BATCH_FILE
-+    echo "#SBATCH -J SLU_$MAT" >> $TMP_BATCH_FILE
-+    #echo "#SBATCH -o ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_async_simple_over_icollec_flat_mrhs" >> $TMP_BATCH_FILE
-+    #echo "#SBATCH -e ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_async_simple_over_icollec_flat_mrhs" >> $TMP_BATCH_FILE
-+    # echo "#SBATCH --mail-type=BEGIN" >> $TMP_BATCH_FILE
-+    # echo "#SBATCH --mail-type=END" >> $TMP_BATCH_FILE
-+    echo "#SBATCH --mail-user=liuyangzhuan@lbl.gov" >> $TMP_BATCH_FILE
-+    if [[ $NERSC_HOST == cori ]]
-+    then
-+      echo "#SBATCH -C $CONSTRAINT" >> $TMP_BATCH_FILE
-+    fi
-+    mkdir -p $MAT   
-+    echo "export OMP_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE
-+    echo "export KMP_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE
-+    echo "export MKL_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE  																			
-+    echo "export NSUP=128" >> $TMP_BATCH_FILE
-+    echo "export NREL=20" >> $TMP_BATCH_FILE
-+    echo "export OMP_PLACES=threads" >> $TMP_BATCH_FILE
-+    echo "export OMP_PROC_BIND=spread" >> $TMP_BATCH_FILE
-+    echo "export MPICH_MAX_THREAD_SAFETY=multiple" >> $TMP_BATCH_FILE
-+
-+    echo " " >> $TMP_BATCH_FILE
-+    echo "FILE=$FILE" >> $TMP_BATCH_FILE
-+    echo "FILEMAT=$INPUT_DIR/$MAT" >> $TMP_BATCH_FILE	
-+    echo " " >> $TMP_BATCH_FILE
-+    echo "CORE_VAL=$CORE_VAL" >> $TMP_BATCH_FILE
-+    echo "NCOL=$NCOL" >> $TMP_BATCH_FILE
-+    echo "NROW=$NROW" >> $TMP_BATCH_FILE
-+    # This should be computed individually for each script...
-+
-+    export OMP_NUM_THREADS=$OMP_NUM_THREADS
-+    export OMP_PLACES=threads
-+    export OMP_PROC_BIND=spread
-+    export MPICH_MAX_THREAD_SAFETY=multiple
-+    srun -n $CORE_VAL -c $TH_PER_RANK --cpu_bind=cores $FILE -c $NCOL -r $NROW $INPUT_DIR/$MAT | tee ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_OMP_${OMP_NUM_THREADS}
-+    # Add final line (srun line) to temporary slurm script
-+
-+    #cat $TMP_BATCH_FILE
-+    #echo " "
-+    # sbatch $TMP_BATCH_FILE
-+  done
-+#one
-+
-+done
-+done
-+
-+exit $EXIT_SUCCESS
-+
-diff --git a/build/batch_script_mpi_runit_pureOMP_1.sh b/build/batch_script_mpi_runit_pureOMP_1.sh
-new file mode 100644
-index 0000000..11667d7
---- /dev/null
-+++ b/build/batch_script_mpi_runit_pureOMP_1.sh
-@@ -0,0 +1,193 @@
-+#!/bin/bash
-+# Bash script to submit many files to Cori/Edison/Queue
-+
-+EXIT_SUCCESS=0
-+EXIT_HOST=1
-+EXIT_PARAM=2
-+
-+# MAX_PARAMS=1
-+# # ^^^ This should be fixed, as it should just loop through everything
-+# if [[ $# -eq 0 ]]; then
-+  # echo "Must have at least one parameter; exiting"
-+  # exit $EXIT_PARAM
-+# fi
-+# if [[ $# -gt $MAX_PARAMS ]]; then
-+  # echo "Too many parameters; exiting"
-+  # exit $EXIT_PARAM
-+# fi
-+
-+# INPUT_FILE=$1
-+# # ^^^ Get the input ile
-+
-+CUR_DIR=`pwd`
-+FILE_DIR=$CUR_DIR/EXAMPLE
-+INPUT_DIR=/project/projectdirs/sparse/liuyangz/my_research/matrix
-+FILE_NAME=pddrive
-+FILE=$FILE_DIR/$FILE_NAME
-+
-+TMP_BATCH_FILE=tmp_batch_file.slurm
-+# ^^^ Should check that this is not taken,
-+#     but I would never use this, so ...
-+
-+> $TMP_BATCH_FILE
-+
-+if [[ $NERSC_HOST == edison ]]; then
-+  CORES_PER_NODE=24
-+  THREADS_PER_NODE=48
-+elif [[ $NERSC_HOST == cori ]]; then
-+  CORES_PER_NODE=32
-+  THREADS_PER_NODE=64
-+  # This does not take hyperthreading into account
-+else
-+  # Host unknown; exiting
-+  exit $EXIT_HOST
-+fi
-+
-+
-+#nprows=(6 12 24)
-+#npcols=(6 12 24)
-+
-+
-+#nprows=(2048 1 32)
-+#npcols=(1 2048 64)
-+
-+# nprows=(32 )
-+# npcols=(64 )
-+
-+
-+
-+#nprows=(24 48 1 1 576 2304)
-+#npcols=(24 48 576 2304 1 1)
-+
-+
-+#nprows=(48  1  2304)
-+#npcols=(48  2304 1)
-+
-+#nprows=(6 12 24 48 )
-+#npcols=(6 12 24 48 )
-+
-+#nprows=(6 12 24 48 1 1 1 1 36 144 576 2304)
-+#npcols=(6 12 24 48 36 144 576 2304 1 1 1 1)
-+
-+#nprows=(32 128 512 1 1 1 4 8 16)
-+#npcols=(1 1 1 32 128 512 8 16 32)
-+
-+#nprows=(2048 1 32)
-+#npcols=(1 2048 64)
-+
-+
-+
-+
-+#nprows=(12 1 144)
-+#npcols=(12 144 1)
-+
-+
-+nprows=(1)
-+npcols=(1)
-+ 
-+for ((i = 0; i < ${#npcols[@]}; i++)); do
-+NROW=${nprows[i]}
-+NCOL=${npcols[i]}
-+
-+# NROW=36
-+CORE_VAL=`expr $NCOL \* $NROW`
-+NODE_VAL=`expr $CORE_VAL / $CORES_PER_NODE`
-+MOD_VAL=`expr $CORE_VAL % $CORES_PER_NODE`
-+if [[ $MOD_VAL -ne 0 ]]
-+then
-+  NODE_VAL=`expr $NODE_VAL + 1`
-+fi
-+#PARTITION=debug
-+PARTITION=regular
-+LICENSE=SCRATCH
-+TIME=00:20:00
-+
-+if [[ $NERSC_HOST == edison ]]
-+then
-+  CONSTRAINT=0
-+fi
-+if [[ $NERSC_HOST == cori ]]
-+then
-+  CONSTRAINT=haswell
-+fi
-+
-+
-+for NTH in 1  
-+do
-+
-+OMP_NUM_THREADS=$NTH
-+TH_PER_RANK=`expr $NTH \* 2`
-+
-+
-+#for NSUP in 128 64 32 16 8
-+#do
-+  # for MAT in atmosmodl.rb nlpkkt80.mtx torso3.mtx Ga19As19H42.mtx A22.mtx cage13.rb 
-+  # for MAT in torso3.mtx
-+  # for MAT in matrix121.dat matrix211.dat tdr190k.dat tdr455k.dat nlpkkt80.mtx torso3.mtx helm2d03.mtx  
-+  # for MAT in tdr190k.dat Ga19As19H42.mtx
-+ for MAT in torso3.bin 
-+   # for MAT in torso3.mtx
-+# for MAT in A22.bin DG_GrapheneDisorder_8192.bin DNA_715_64cell.bin LU_C_BN_C_4by2.bin Li4244.bin atmosmodj.bin Ga19As19H42.bin Geo_1438.bin StocF-1465.bin
-+
-+# for MAT in LU_C_BN_C_4by2.bin Li4244.bin atmosmodj.bin Ga19As19H42.bin Geo_1438.bin StocF-1465.bin
-+
-+
-+#for MAT in big.rua
-+#for MAT in torso3.bin    
-+# for MAT in Ga19As19H42.mtx   
-+  do
-+    # Start of looping stuff
-+    > $TMP_BATCH_FILE
-+    echo "#!/bin/bash -l" >> $TMP_BATCH_FILE
-+    echo " " >> $TMP_BATCH_FILE
-+    echo "#SBATCH -p $PARTITION" >> $TMP_BATCH_FILE
-+    echo "#SBATCH -N $NODE_VAL" >>  $TMP_BATCH_FILE
-+    echo "#SBATCH -t $TIME" >> $TMP_BATCH_FILE
-+    echo "#SBATCH -L $LICENSE" >> $TMP_BATCH_FILE
-+    echo "#SBATCH -J SLU_$MAT" >> $TMP_BATCH_FILE
-+    #echo "#SBATCH -o ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_async_simple_over_icollec_flat_mrhs" >> $TMP_BATCH_FILE
-+    #echo "#SBATCH -e ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_async_simple_over_icollec_flat_mrhs" >> $TMP_BATCH_FILE
-+    # echo "#SBATCH --mail-type=BEGIN" >> $TMP_BATCH_FILE
-+    # echo "#SBATCH --mail-type=END" >> $TMP_BATCH_FILE
-+    echo "#SBATCH --mail-user=liuyangzhuan@lbl.gov" >> $TMP_BATCH_FILE
-+    if [[ $NERSC_HOST == cori ]]
-+    then
-+      echo "#SBATCH -C $CONSTRAINT" >> $TMP_BATCH_FILE
-+    fi
-+    mkdir -p $MAT   
-+    echo "export OMP_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE
-+    echo "export KMP_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE
-+    echo "export MKL_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE  																			
-+    echo "export NSUP=128" >> $TMP_BATCH_FILE
-+    echo "export NREL=20" >> $TMP_BATCH_FILE
-+    echo "export OMP_PLACES=threads" >> $TMP_BATCH_FILE
-+    echo "export OMP_PROC_BIND=spread" >> $TMP_BATCH_FILE
-+    echo "export MPICH_MAX_THREAD_SAFETY=multiple" >> $TMP_BATCH_FILE
-+
-+    echo " " >> $TMP_BATCH_FILE
-+    echo "FILE=$FILE" >> $TMP_BATCH_FILE
-+    echo "FILEMAT=$INPUT_DIR/$MAT" >> $TMP_BATCH_FILE	
-+    echo " " >> $TMP_BATCH_FILE
-+    echo "CORE_VAL=$CORE_VAL" >> $TMP_BATCH_FILE
-+    echo "NCOL=$NCOL" >> $TMP_BATCH_FILE
-+    echo "NROW=$NROW" >> $TMP_BATCH_FILE
-+    # This should be computed individually for each script...
-+
-+    export OMP_NUM_THREADS=$OMP_NUM_THREADS
-+    export OMP_PLACES=threads
-+    export OMP_PROC_BIND=spread
-+    export MPICH_MAX_THREAD_SAFETY=multiple
-+    srun -n $CORE_VAL -c $TH_PER_RANK --cpu_bind=cores $FILE -c $NCOL -r $NROW $INPUT_DIR/$MAT | tee ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_OMP_${OMP_NUM_THREADS}
-+    # Add final line (srun line) to temporary slurm script
-+
-+    #cat $TMP_BATCH_FILE
-+    #echo " "
-+    # sbatch $TMP_BATCH_FILE
-+  done
-+#one
-+
-+done
-+done
-+
-+exit $EXIT_SUCCESS
-+
-diff --git a/build/batch_script_mpi_vtune.sh b/build/batch_script_mpi_vtune.sh
-new file mode 100644
-index 0000000..73fbd5e
---- /dev/null
-+++ b/build/batch_script_mpi_vtune.sh
-@@ -0,0 +1,165 @@
-+#!/bin/bash
-+# Bash script to submit many files to Cori/Edison/Queue
-+
-+EXIT_SUCCESS=0
-+EXIT_HOST=1
-+EXIT_PARAM=2
-+
-+# MAX_PARAMS=1
-+# # ^^^ This should be fixed, as it should just loop through everything
-+# if [[ $# -eq 0 ]]; then
-+  # echo "Must have at least one parameter; exiting"
-+  # exit $EXIT_PARAM
-+# fi
-+# if [[ $# -gt $MAX_PARAMS ]]; then
-+  # echo "Too many parameters; exiting"
-+  # exit $EXIT_PARAM
-+# fi
-+
-+# INPUT_FILE=$1
-+# # ^^^ Get the input ile
-+
-+CUR_DIR=`pwd`
-+#FILE_DIR=$CUR_DIR/EXAMPLE
-+#INPUT_DIR=~/Edison/my_research/SuperLU/SuperLUDIST_Begin/build/EXAMPLE
-+
-+FILE_DIR=./EXAMPLE
-+INPUT_DIR=./EXAMPLE
-+FILE_NAME=pddrive
-+FILE=$FILE_DIR/$FILE_NAME
-+
-+TMP_BATCH_FILE=tmp_batch_file.slurm
-+# ^^^ Should check that this is not taken,
-+#     but I would never use this, so ...
-+
-+> $TMP_BATCH_FILE
-+
-+if [[ $NERSC_HOST == edison ]]; then
-+  CORES_PER_NODE=24
-+elif [[ $NERSC_HOST == cori ]]; then
-+  CORES_PER_NODE=32
-+  # This does not take hyperthreading into account
-+else
-+  # Host unknown; exiting
-+  exit $EXIT_HOST
-+fi
-+
-+
-+for NROW in 1
-+do
-+
-+NCOL=1
-+# NROW=36
-+CORE_VAL=`expr $NCOL \* $NROW`
-+CORE_VALMAX=`expr $CORE_VAL - 1`
-+NODE_VAL=`expr $CORE_VAL / $CORES_PER_NODE`
-+MOD_VAL=`expr $CORE_VAL % $CORES_PER_NODE`
-+
-+if [[ $MOD_VAL -ne 0 ]]
-+then
-+  NODE_VAL=`expr $NODE_VAL + 1`
-+fi
-+PARTITION=debug
-+#PARTITION=regular
-+LICENSE=SCRATCH
-+TIME=00:30:00
-+
-+if [[ $NERSC_HOST == edison ]]
-+then
-+  CONSTRAINT=0
-+fi
-+if [[ $NERSC_HOST == cori ]]
-+then
-+  CONSTRAINT=haswell
-+fi
-+
-+OMP_NUM_THREADS=12
-+#COLLECT="-collect advanced-hotspots -knob sampling-interval=0.01"
-+COLLECT="-collect hotspots -start-paused -knob sampling-interval=1"
-+RES_DIR=Vtune_np${CORE_VAL}_mrhs
-+
-+#cat << EOF > mpmd.conf
-+#0 amplxe-cl -data-limit=0 ${COLLECT} -r ${RES_DIR} -trace-mpi -- ${RUNEXE}
-+#1-${CORE_VALMAX} ${RUNEXE}
-+#EOF
-+
-+
-+
-+#for NSUP in 128 64 32 16 8
-+#do
-+  # for MAT in atmosmodl.rb nlpkkt80.mtx torso3.mtx Ga19As19H42.mtx A22.mtx cage13.rb 
-+  for MAT in torso3.mtx
-+  # for MAT in matrix121.dat matrix211.dat tdr190k.dat tdr455k.dat nlpkkt80.mtx torso3.mtx helm2d03.mtx  
-+  # for MAT in tdr190k.dat Ga19As19H42.mtx
-+  # for MAT in nlpkkt80.mtx torso3.mtx Ga19As19H42.mtx A22.mtx matrix121.dat tdr190k.dat   
-+  # for MAT in Ga19As19H42.mtx   
-+  do
-+
-+
-+RUNEXE="$FILE -c $NCOL -r $NROW $INPUT_DIR/$MAT"
-+
-+if [[ ${CORE_VALMAX} -ne 0 ]]
-+then
-+
-+cat << EOF > mpmd.conf_np$CORE_VAL
-+0 amplxe-cl -data-limit=0 ${COLLECT} -r ${RES_DIR} -trace-mpi -- ${RUNEXE}
-+1-${CORE_VALMAX} ${RUNEXE}
-+EOF
-+
-+else
-+
-+cat << EOF > mpmd.conf_np$CORE_VAL
-+0 amplxe-cl -data-limit=0 ${COLLECT} -r ${RES_DIR} -trace-mpi -- ${RUNEXE}
-+EOF
-+
-+fi
-+
-+
-+
-+    # Start of looping stuff
-+    > $TMP_BATCH_FILE
-+    echo "#!/bin/bash -l" >> $TMP_BATCH_FILE
-+    echo " " >> $TMP_BATCH_FILE
-+    echo "#SBATCH -p $PARTITION" >> $TMP_BATCH_FILE
-+    echo "#SBATCH -N $NODE_VAL" >>  $TMP_BATCH_FILE
-+    echo "#SBATCH -t $TIME" >> $TMP_BATCH_FILE
-+    echo "#SBATCH --perf=vtune" >> $TMP_BATCH_FILE
-+    echo "#SBATCH -L $LICENSE" >> $TMP_BATCH_FILE
-+    echo "#SBATCH -J SLU_$MAT" >> $TMP_BATCH_FILE
-+    echo "#SBATCH -o ./$MAT/SLU.o_mpi_np${CORE_VAL}_vtune" >> $TMP_BATCH_FILE
-+    echo "#SBATCH -e ./$MAT/SLU.o_mpi_np${CORE_VAL}_vtune" >> $TMP_BATCH_FILE
-+    # echo "#SBATCH --mail-type=BEGIN" >> $TMP_BATCH_FILE
-+    # echo "#SBATCH --mail-type=END" >> $TMP_BATCH_FILE
-+    echo "#SBATCH --mail-user=liuyangzhuan@lbl.gov" >> $TMP_BATCH_FILE
-+    if [[ $NERSC_HOST == cori ]]
-+    then
-+      echo "#SBATCH -C $CONSTRAINT" >> $TMP_BATCH_FILE
-+    fi
-+    mkdir -p $MAT   
-+    echo "export OMP_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE
-+    echo "export KMP_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE
-+    echo "export MKL_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE  																			
-+    echo "export NSUP=128" >> $TMP_BATCH_FILE
-+    echo "export NREL=20" >> $TMP_BATCH_FILE
-+    
-+    echo " " >> $TMP_BATCH_FILE
-+    echo "FILE=$FILE" >> $TMP_BATCH_FILE
-+    echo "FILEMAT=$INPUT_DIR/$MAT" >> $TMP_BATCH_FILE	
-+    echo " " >> $TMP_BATCH_FILE
-+    echo "CORE_VAL=$CORE_VAL" >> $TMP_BATCH_FILE
-+    echo "NCOL=$NCOL" >> $TMP_BATCH_FILE
-+    echo "NROW=$NROW" >> $TMP_BATCH_FILE
-+    # This should be computed individually for each script...	
-+    echo "srun -n $CORE_VAL --multi-prog ./mpmd.conf_np$CORE_VAL" >> $TMP_BATCH_FILE
-+    # Add final line (srun line) to temporary slurm script
-+
-+    #cat $TMP_BATCH_FILE
-+    #echo " "
-+    sbatch $TMP_BATCH_FILE
-+  done
-+#one
-+
-+done
-+
-+exit $EXIT_SUCCESS
-+
-diff --git a/build/itacconf.txt b/build/itacconf.txt
-new file mode 100644
-index 0000000..201a3ea
---- /dev/null
-+++ b/build/itacconf.txt
-@@ -0,0 +1,28 @@
-+# module load itac/2018.beta
-+module swap intel/18.0.0.128 intel/17.0.3.191 
-+
-+module load itac/2017.up1
-+module rm cray-mpich/7.6.0
-+module load impi
-+export LD_PRELOAD=$VT_ROOT/slib/libVT.so
-+export VT_LOGFILE_FORMAT=STFSINGLE
-+export I_MPI_PMI_LIBRARY=/usr/lib64/slurmpmi/libpmi.so
-+export VT_PCTRACE=5
-+
-+export OMP_NUM_THREADS=1
-+export KMP_NUM_THREADS=1
-+export MKL_NUM_THREADS=1
-+export NSUP=128
-+export NREL=20
-+
-+srun -trace -n 32 ./EXAMPLE/pddrive -r 4 -c 8 ./EXAMPLE/torso3.mtx 
-+
-+traceanalyzer pddrive.single.stf
-+
-+
-+
-+# export I_MPI_FABRICS=ofi
-+# export I_MPI_OFI_PROVIDER=gni
-+# export I_MPI_OFI_LIBRARY=/usr/common/software/libfabric/1.5.0/gnu/lib/libfabric.so
-+
-+
-diff --git a/build/itacconf_crayMPI.txt b/build/itacconf_crayMPI.txt
-new file mode 100644
-index 0000000..a5c54b4
---- /dev/null
-+++ b/build/itacconf_crayMPI.txt
-@@ -0,0 +1,24 @@
-+module load itac/2017.up1
-+module swap intel/18.0.0.128 intel/17.0.3.191
-+export LD_PRELOAD=$VT_ROOT/slib/libVT.so
-+export VT_LOGFILE_FORMAT=STFSINGLE
-+export I_MPI_PMI_LIBRARY=/usr/lib64/slurmpmi/libpmi.so
-+
-+
-+export OMP_NUM_THREADS=1
-+export KMP_NUM_THREADS=1
-+export MKL_NUM_THREADS=1
-+export NSUP=128
-+export NREL=20
-+
-+srun -trace -n 32 ./EXAMPLE/pddrive -r 4 -c 8 ./EXAMPLE/torso3.mtx 
-+
-+traceanalyzer pddrive.single.stf
-+
-+
-+
-+# export I_MPI_FABRICS=ofi
-+# export I_MPI_OFI_PROVIDER=gni
-+# export I_MPI_OFI_LIBRARY=/usr/common/software/libfabric/1.5.0/gnu/lib/libfabric.so
-+
-+
-diff --git a/build/mpmd.conf_np1 b/build/mpmd.conf_np1
-new file mode 100644
-index 0000000..cb8abaf
---- /dev/null
-+++ b/build/mpmd.conf_np1
-@@ -0,0 +1 @@
-+0 amplxe-cl -data-limit=0 -collect hotspots -start-paused -knob sampling-interval=1 -r Vtune_np1_mrhs -trace-mpi -- ./EXAMPLE/pddrive -c 1 -r 1 ./EXAMPLE/torso3.mtx
-diff --git a/build/run_cmake_build.sh b/build/run_cmake_build.sh
-new file mode 100644
-index 0000000..80fefb8
---- /dev/null
-+++ b/build/run_cmake_build.sh
-@@ -0,0 +1,43 @@
-+#!/bin/bash
-+# Bash script to submit many files to Cori/Edison/Queue
-+Vtune=0
-+export CRAYPE_LINK_TYPE=dynamic
-+export PARMETIS_ROOT=~/Cori/my_software/parmetis-4.0.3_dynamic_longint
-+export PARMETIS_BUILD_DIR=${PARMETIS_ROOT}/build/Linux-x86_64 
-+rm -rf CMakeCache.txt
-+rm -rf CMakeFiles
-+rm -rf CTestTestfile.cmake
-+rm -rf cmake_install.cmake
-+rm -rf DartConfiguration.tcl 
-+if [[ ${Vtune} == 1 ]]; then
-+INC_VTUNE="-g -DVTUNE=1 -I$VTUNE_AMPLIFIER_XE_2017_DIR/include"
-+LIB_VTUNE="$VTUNE_AMPLIFIER_XE_2017_DIR/lib64/libittnotify.a"
-+fi
-+
-+cmake .. \
-+	-DTPL_PARMETIS_INCLUDE_DIRS="${PARMETIS_ROOT}/include;${PARMETIS_ROOT}/metis/include" \
-+	-DTPL_PARMETIS_LIBRARIES="${PARMETIS_BUILD_DIR}/libparmetis/libparmetis.so;${PARMETIS_BUILD_DIR}/libmetis/libmetis.so;${LIB_VTUNE}" \
-+	-Denable_blaslib=OFF \
-+	-DBUILD_SHARED_LIBS=ON \
-+	-DCMAKE_C_COMPILER=cc \
-+        -DCMAKE_CXX_COMPILER=CC \
-+	-DCMAKE_INSTALL_PREFIX=. \
-+	-DCMAKE_BUILD_TYPE=Release \
-+	-DCMAKE_VERBOSE_MAKEFILE:BOOL=ON \
-+	-DCMAKE_CXX_FLAGS="-Ofast -std=c++11 -DAdd_ -DRELEASE ${INC_VTUNE}" \
-+        -DCMAKE_C_FLAGS="-D_LONGINT -std=c11 -DPRNTlevel=1 -DPROFlevel=1 -DDEBUGlevel=0 ${INC_VTUNE}" \
-+	-DTPL_BLAS_LIBRARIES="/opt/intel/compilers_and_libraries_2017.2.174/linux/mkl/lib/intel64/libmkl_intel_lp64.so;/opt/intel/compilers_and_libraries_2017.2.174/linux/mkl/lib/intel64/libmkl_sequential.so;/opt/intel/compilers_and_libraries_2017.2.174/linux/mkl/lib/intel64/libmkl_core.so"
-+
-+#	-DTPL_BLAS_LIBRARIES="/opt/intel/compilers_and_libraries_2017.2.174/linux/mkl/lib/intel64/libmkl_intel_lp64.so;/opt/intel/compilers_and_libraries_2017.2.174/linux/mkl/lib/intel64/libmkl_sequential.so;/opt/intel/compilers_and_libraries_2017.2.174/linux/mkl/lib/intel64/libmkl_core.so" \
-+#        -DCMAKE_CXX_FLAGS="-g -trace -Ofast -std=c++11 -DAdd_ -DRELEASE -tcollect -L$VT_LIB_DIR -lVT $VT_ADD_LIBS" \
-+
-+
-+#	-DTPL_BLAS_LIBRARIES="/opt/intel/compilers_and_libraries_2017.2.174/linux/mkl/lib/intel64/libmkl_lapack95_lp64.a;/opt/intel/compilers_and_libraries_2017.2.174/linux/mkl/lib/intel64/libmkl_blas95_lp64.a"
-+
-+#	-DTPL_BLAS_LIBRARIES="/opt/intel/compilers_and_libraries_2017.2.174/linux/mkl/lib/intel64/libmkl_intel_lp64.a;/opt/intel/compilers_and_libraries_2017.2.174/linux/mkl/lib/intel64/libmkl_sequential.a;/opt/intel/compilers_and_libraries_2017.2.174/linux/mkl/lib/intel64/libmkl_core.a"  
-+
-+
-+# DCMAKE_BUILD_TYPE=Release or Debug compiler options set in CMAKELIST.txt
-+
-+#        -DCMAKE_C_FLAGS="-g -O0 -std=c99 -DPRNTlevel=2 -DPROFlevel=1 -DDEBUGlevel=0" \
-+#	-DCMAKE_C_FLAGS="-g -O0 -std=c11 -DPRNTlevel=1 -DPROFlevel=1 -DDEBUGlevel=0" \
-diff --git a/build/run_cmake_build_debug.sh b/build/run_cmake_build_debug.sh
-new file mode 100644
-index 0000000..5d5998e
---- /dev/null
-+++ b/build/run_cmake_build_debug.sh
-@@ -0,0 +1,43 @@
-+#!/bin/bash
-+# Bash script to submit many files to Cori/Edison/Queue
-+Vtune=0
-+export CRAYPE_LINK_TYPE=dynamic
-+export PARMETIS_ROOT=~/Cori/my_software/parmetis-4.0.3_dynamic
-+export PARMETIS_BUILD_DIR=${PARMETIS_ROOT}/build/Linux-x86_64 
-+rm -rf CMakeCache.txt
-+rm -rf CMakeFiles
-+rm -rf CTestTestfile.cmake
-+rm -rf cmake_install.cmake
-+rm -rf DartConfiguration.tcl 
-+if [[ ${Vtune} == 1 ]]; then
-+INC_VTUNE="-g -DVTUNE=1 -I$VTUNE_AMPLIFIER_XE_2017_DIR/include"
-+LIB_VTUNE="$VTUNE_AMPLIFIER_XE_2017_DIR/lib64/libittnotify.a"
-+fi
-+
-+cmake .. \
-+	-DTPL_PARMETIS_INCLUDE_DIRS="${PARMETIS_ROOT}/include;${PARMETIS_ROOT}/metis/include" \
-+	-DTPL_PARMETIS_LIBRARIES="${PARMETIS_BUILD_DIR}/libparmetis/libparmetis.so;${PARMETIS_BUILD_DIR}/libmetis/libmetis.so;${LIB_VTUNE}" \
-+	-Denable_blaslib=OFF \
-+	-DBUILD_SHARED_LIBS=ON \
-+	-DCMAKE_C_COMPILER=cc \
-+        -DCMAKE_CXX_COMPILER=CC \
-+	-DCMAKE_INSTALL_PREFIX=. \
-+	-DCMAKE_BUILD_TYPE=Debug \
-+	-DCMAKE_VERBOSE_MAKEFILE:BOOL=ON \
-+	-DCMAKE_C_FLAGS="-g -O0 -std=c11 -DPRNTlevel=1 -DPROFlevel=1 -DDEBUGlevel=0" \
-+	-DCMAKE_CXX_FLAGS="-Ofast -std=c++11 -DAdd_ -DRELEASE ${INC_VTUNE}" \
-+	-DTPL_BLAS_LIBRARIES="/opt/intel/compilers_and_libraries_2017.2.174/linux/mkl/lib/intel64/libmkl_intel_lp64.so;/opt/intel/compilers_and_libraries_2017.2.174/linux/mkl/lib/intel64/libmkl_sequential.so;/opt/intel/compilers_and_libraries_2017.2.174/linux/mkl/lib/intel64/libmkl_core.so"
-+
-+#	-DTPL_BLAS_LIBRARIES="/opt/intel/compilers_and_libraries_2017.2.174/linux/mkl/lib/intel64/libmkl_intel_lp64.so;/opt/intel/compilers_and_libraries_2017.2.174/linux/mkl/lib/intel64/libmkl_sequential.so;/opt/intel/compilers_and_libraries_2017.2.174/linux/mkl/lib/intel64/libmkl_core.so" \
-+#        -DCMAKE_CXX_FLAGS="-g -trace -Ofast -std=c++11 -DAdd_ -DRELEASE -tcollect -L$VT_LIB_DIR -lVT $VT_ADD_LIBS" \
-+
-+
-+#	-DTPL_BLAS_LIBRARIES="/opt/intel/compilers_and_libraries_2017.2.174/linux/mkl/lib/intel64/libmkl_lapack95_lp64.a;/opt/intel/compilers_and_libraries_2017.2.174/linux/mkl/lib/intel64/libmkl_blas95_lp64.a"
-+
-+#	-DTPL_BLAS_LIBRARIES="/opt/intel/compilers_and_libraries_2017.2.174/linux/mkl/lib/intel64/libmkl_intel_lp64.a;/opt/intel/compilers_and_libraries_2017.2.174/linux/mkl/lib/intel64/libmkl_sequential.a;/opt/intel/compilers_and_libraries_2017.2.174/linux/mkl/lib/intel64/libmkl_core.a"  
-+
-+
-+# DCMAKE_BUILD_TYPE=Release or Debug compiler options set in CMAKELIST.txt
-+
-+#        -DCMAKE_C_FLAGS="-g -O0 -std=c99 -DPRNTlevel=2 -DPROFlevel=1 -DDEBUGlevel=0" \
-+        #-DCMAKE_C_FLAGS="-std=c11 -DPRNTlevel=1 -DPROFlevel=1 -DDEBUGlevel=0 ${INC_VTUNE}" \
-diff --git a/build/simpleconf.txt b/build/simpleconf.txt
-new file mode 100644
-index 0000000..512e6ea
---- /dev/null
-+++ b/build/simpleconf.txt
-@@ -0,0 +1,9 @@
-+export MPICH_MAX_THREAD_SAFETY=multiple
-+export OMP_NUM_THREADS=1
-+export KMP_NUM_THREADS=1
-+export MKL_NUM_THREADS=1
-+export NSUP=1280
-+export NREL=1000
-+
-+srun -n 24 ./EXAMPLE/pddrive -c 24 -r 1 ../EXAMPLE/big.rua | tee a.out
-+
-diff --git a/build/vtuneconf.txt b/build/vtuneconf.txt
-new file mode 100644
-index 0000000..6307eee
---- /dev/null
-+++ b/build/vtuneconf.txt
-@@ -0,0 +1,2 @@
-+module unload darshan
-+module load vtune
-diff --git a/cmake/FindMETIS.cmake b/cmake/FindMETIS.cmake
-deleted file mode 100644
-index 527f991..0000000
---- a/cmake/FindMETIS.cmake
-+++ /dev/null
-@@ -1,54 +0,0 @@
--# - Try to find METIS
--# Once done this will define
--#
--#  METIS_FOUND        - system has METIS
--#  METIS_INCLUDE_DIRS - include directories for METIS
--#  METIS_LIBRARIES    - libraries for METIS
--#
--# and the imported target
--#
--#  METIS::METIS
--
--find_path(METIS_INCLUDE_DIR metis.h
--  DOC "Directory where the METIS header files are located"
--)
--mark_as_advanced(METIS_INCLUDE_DIR)
--set(METIS_INCLUDE_DIRS "${METIS_INCLUDE_DIR}")
--
--find_library(METIS_LIBRARY
--  NAMES metis
--  DOC "Directory where the METIS library is located"
--)
--mark_as_advanced(METIS_LIBRARY)
--set(METIS_LIBRARIES "${METIS_LIBRARY}")
--
--# Get METIS version
--if(NOT METIS_VERSION_STRING AND METIS_INCLUDE_DIR AND EXISTS "${METIS_INCLUDE_DIR}/metis.h")
--  set(version_pattern "^#define[\t ]+METIS_(MAJOR|MINOR)_VERSION[\t ]+([0-9\\.]+)$")
--  file(STRINGS "${METIS_INCLUDE_DIR}/metis.h" metis_version REGEX ${version_pattern})
--
--  foreach(match ${metis_version})
--    if(METIS_VERSION_STRING)
--      set(METIS_VERSION_STRING "${METIS_VERSION_STRING}.")
--    endif()
--    string(REGEX REPLACE ${version_pattern} "${METIS_VERSION_STRING}\\2" METIS_VERSION_STRING ${match})
--    set(METIS_VERSION_${CMAKE_MATCH_1} ${CMAKE_MATCH_2})
--  endforeach()
--  unset(metis_version)
--  unset(version_pattern)
--endif()
--
--# Standard package handling
--include(FindPackageHandleStandardArgs)
--find_package_handle_standard_args(METIS
--  REQUIRED_VARS METIS_LIBRARY METIS_INCLUDE_DIR
--  VERSION_VAR METIS_VERSION_STRING
--)
--
--if(METIS_FOUND)
--  if(NOT TARGET METIS::METIS)
--    add_library(METIS::METIS UNKNOWN IMPORTED)
--  endif()
--  set_property(TARGET METIS::METIS PROPERTY IMPORTED_LOCATION "${METIS_LIBRARY}")
--  set_property(TARGET METIS::METIS PROPERTY INTERFACE_INCLUDE_DIRECTORIES "${METIS_INCLUDE_DIRS}")
--endif()
-diff --git a/cmake/FindParMETIS.cmake b/cmake/FindParMETIS.cmake
-deleted file mode 100644
-index 5a88c73..0000000
---- a/cmake/FindParMETIS.cmake
-+++ /dev/null
-@@ -1,60 +0,0 @@
--# - Try to find ParMETIS
--# Once done this will define
--#
--#  PARMETIS_FOUND        - system has ParMETIS
--#  PARMETIS_INCLUDE_DIRS - include directories for ParMETIS
--#  PARMETIS_LIBRARIES    - libraries for ParMETIS
--#
--# and the imported target
--#
--#  ParMETIS::ParMETIS
--
--find_path(ParMETIS_INCLUDE_DIR parmetis.h
--  DOC "Directory where the ParMETIS header files are located"
--)
--mark_as_advanced(ParMETIS_INCLUDE_DIR)
--set(ParMETIS_INCLUDE_DIRS "${ParMETIS_INCLUDE_DIR}")
--
--find_library(ParMETIS_LIBRARY
--  NAMES parmetis
--  DOC "Directory where the ParMETIS library is located"
--)
--mark_as_advanced(ParMETIS_LIBRARY)
--set(ParMETIS_LIBRARIES "${ParMETIS_LIBRARY}")
--
--# Get ParMETIS version
--if(NOT PARMETIS_VERSION_STRING AND PARMETIS_INCLUDE_DIR AND EXISTS "${PARMETIS_INCLUDE_DIR}/parmetis.h")
--  set(version_pattern "^#define[\t ]+PARMETIS_(MAJOR|MINOR)_VERSION[\t ]+([0-9\\.]+)$")
--  file(STRINGS "${PARMETIS_INCLUDE_DIR}/parmetis.h" parmetis_version REGEX ${version_pattern})
--
--  foreach(match ${parmetis_version})
--    if(PARMETIS_VERSION_STRING)
--      set(PARMETIS_VERSION_STRING "${PARMETIS_VERSION_STRING}.")
--    endif()
--    string(REGEX REPLACE ${version_pattern} "${PARMETIS_VERSION_STRING}\\2" PARMETIS_VERSION_STRING ${match})
--    set(PARMETIS_VERSION_${CMAKE_MATCH_1} ${CMAKE_MATCH_2})
--  endforeach()
--  unset(parmetis_version)
--  unset(version_pattern)
--endif()
--
--# Standard package handling
--include(FindPackageHandleStandardArgs)
--find_package_handle_standard_args(ParMETIS
--  REQUIRED_VARS ParMETIS_LIBRARY ParMETIS_INCLUDE_DIR
--  VERSION_VAR PARMETIS_VERSION_STRING
--)
--
--# Dependencies
--include(CMakeFindDependencyMacro)
--#find_dependency(MPI)
--find_dependency(METIS)
--
--if(ParMETIS_FOUND)
--  if(NOT TARGET ParMETIS::ParMETIS)
--    add_library(ParMETIS::ParMETIS UNKNOWN IMPORTED)
--  endif()
--  set_property(TARGET ParMETIS::ParMETIS PROPERTY IMPORTED_LOCATION "${ParMETIS_LIBRARY}")
--  set_property(TARGET ParMETIS::ParMETIS PROPERTY INTERFACE_LINK_LIBRARIES METIS::METIS)
--  set_property(TARGET ParMETIS::ParMETIS PROPERTY INTERFACE_INCLUDE_DIRECTORIES "${ParMETIS_INCLUDE_DIRS}")
--endif()
-diff --git a/make.inc.in b/make.inc.in
-index c4526cb..3f06155 100644
---- a/make.inc.in
-+++ b/make.inc.in
-@@ -8,7 +8,7 @@
- #
- #  Creation date:   March 1, 2016	version 5.0.0
- #
--#  Modified:	    October 13, 2017    version 5.2.1
-+#  Modified:	    
- #		    
- #
- ############################################################################
-@@ -30,9 +30,9 @@ RANLIB       = @CMAKE_RANLIB@
- 
- CC           = @CMAKE_C_COMPILER@
- CFLAGS 	     = @CMAKE_C_FLAGS_RELEASE@ @CMAKE_C_FLAGS@
-+# CFLAGS       += -D_LONGINT   ## 64-bit integer
- # CFLAGS       += -D${DirDefs}
- # CFLAGS       += @COMPILE_DEFINITIONS@ 
--#XSDK_INDEX_SIZE = 64 ## 64-bit integer
- NOOPTS       = -O0
- FORTRAN	     = @CMAKE_Fortran_COMPILER@
- 
-diff --git a/make.inc_good_dynamic b/make.inc_good_dynamic
-new file mode 100644
-index 0000000..4a47aad
---- /dev/null
-+++ b/make.inc_good_dynamic
-@@ -0,0 +1,42 @@
-+############################################################################
-+#
-+#  Program:         SuperLU_DIST
-+#
-+#  Module:          make.inc
-+#
-+#  Purpose:         Top-level Definitions
-+#
-+#  Creation date:   March 1, 2016	version 5.0.0
-+#
-+#  Modified:	    
-+#		    
-+#
-+############################################################################
-+#
-+#  The name of the libraries to be created/linked to
-+#
-+SuperLUroot	= /global/homes/l/liuyangz/Cori/my_research/github/superlu_dist_task_hybrid_whypddistslow_01_27_2018/build
-+DSUPERLULIB   	= $(SuperLUroot)/SRC/libsuperlu_dist.so
-+
-+LIBS		= $(DSUPERLULIB) /opt/intel/compilers_and_libraries_2017.2.174/linux/mkl/lib/intel64/libmkl_intel_lp64.so /opt/intel/compilers_and_libraries_2017.2.174/linux/mkl/lib/intel64/libmkl_sequential.so /opt/intel/compilers_and_libraries_2017.2.174/linux/mkl/lib/intel64/libmkl_core.so /global/homes/l/liuyangz/Edison/my_software/parmetis-4.0.3_dynamic/build/Linux-x86_64/libparmetis/libparmetis.so /global/homes/l/liuyangz/Edison/my_software/parmetis-4.0.3_dynamic/build/Linux-x86_64/libmetis/libmetis.so
-+
-+#
-+#  The archiver and the flag(s) to use when building archive (library)
-+#  If your system has no ranlib, set RANLIB = echo.
-+#
-+ARCH         = /usr/bin/ar
-+ARCHFLAGS    = cr
-+RANLIB       = /usr/bin/ranlib
-+
-+CC           = /opt/cray/pe/craype/2.5.12/bin/cc
-+CFLAGS 	     = -O0 -g -I/global/homes/l/liuyangz/Edison/my_software/parmetis-4.0.3_dynamic/metis/include -I/global/homes/l/liuyangz/Edison/my_software/parmetis-4.0.3_dynamic/include  -DUSE_VENDOR_BLAS -qopenmp -std=c11 -DPRNTlevel=1 -DPROFlevel=1 -DDEBUGlevel=0
-+# CFLAGS       += -D_LONGINT   ## 64-bit integer
-+# CFLAGS       += -D
-+# CFLAGS       +=  
-+NOOPTS       = -O0
-+FORTRAN	     = /opt/cray/pe/craype/2.5.12/bin/ftn
-+CPP          =/opt/cray/pe/craype/2.5.12/bin/CC
-+CPPFLAGS     = -Ofast -std=c++11 -DAdd_ 
-+
-+LOADER       = $(CPP)
-+LOADOPTS     = $(LIBS) -Wl,-rpath,-qopenmp  -qopenmp -dynamic 
-diff --git a/make.inc_good_static b/make.inc_good_static
-new file mode 100644
-index 0000000..e11d889
---- /dev/null
-+++ b/make.inc_good_static
-@@ -0,0 +1,41 @@
-+############################################################################
-+#
-+#  Program:         SuperLU_DIST
-+#
-+#  Module:          make.inc
-+#
-+#  Purpose:         Top-level Definitions
-+#
-+#  Creation date:   March 1, 2016	version 5.0.0
-+#
-+#  Modified:	    
-+#		    
-+#
-+############################################################################
-+#
-+#  The name of the libraries to be created/linked to
-+#
-+SuperLUroot	= /global/homes/l/liuyangz/Cori/my_research/github/superlu_dist_task_hybrid_whypddistslow_01_27_2018/build
-+DSUPERLULIB   	= $(SuperLUroot)/SRC/libsuperlu_dist.a
-+
-+LIBS		= $(DSUPERLULIB) /opt/intel/compilers_and_libraries_2017.2.174/linux/mkl/lib/intel64/libmkl_intel_lp64.a /opt/intel/compilers_and_libraries_2017.2.174/linux/mkl/lib/intel64/libmkl_sequential.a /opt/intel/compilers_and_libraries_2017.2.174/linux/mkl/lib/intel64/libmkl_core.a /global/homes/l/liuyangz/Edison/my_software/parmetis-4.0.3/build/Linux-x86_64/libparmetis/libparmetis.a /global/homes/l/liuyangz/Edison/my_software/parmetis-4.0.3/build/Linux-x86_64/libmetis/libmetis.a
-+
-+#
-+#  The archiver and the flag(s) to use when building archive (library)
-+#  If your system has no ranlib, set RANLIB = echo.
-+#
-+ARCH         = /usr/bin/ar
-+ARCHFLAGS    = cr
-+RANLIB       = /usr/bin/ranlib
-+
-+CC           = /opt/cray/pe/craype/2.5.12/bin/cc
-+CFLAGS 	     = -O3 -DNDEBUG -I/global/homes/l/liuyangz/Edison/my_software/parmetis-4.0.3/metis/include -I/global/homes/l/liuyangz/Edison/my_software/parmetis-4.0.3/include  -DUSE_VENDOR_BLAS -qopenmp -std=c11 -DPRNTlevel=1 -DPROFlevel=1 -DDEBUGlevel=0
-+# CFLAGS       += -D_LONGINT   ## 64-bit integer
-+# CFLAGS       += -D
-+# CFLAGS       +=  
-+NOOPTS       = -O0
-+FORTRAN	     = /opt/cray/pe/craype/2.5.12/bin/ftn
-+CPP          =/opt/cray/pe/craype/2.5.12/bin/CC
-+CPPFLAGS     = -Ofast -std=c++11 -DAdd_ 
-+LOADER       = $(CPP)
-+LOADOPTS     = -Wl,-rpath,-qopenmp  -qopenmp 
-diff --git a/run_cmake_build.sh b/run_cmake_build.sh
-deleted file mode 100755
-index 450dd6f..0000000
---- a/run_cmake_build.sh
-+++ /dev/null
-@@ -1,60 +0,0 @@
--#!/bin/bash
--
--## if [ !$?NERSC_HOST ]
--if [ -z $NERSC_HOST ]
--then
--    echo "NERSC_HOST undefined"
--elif [ "$NERSC_HOST" == "edison" ]
--then
--    export PARMETIS_ROOT=~/Edison/lib/parmetis-4.0.3
--    export PARMETIS_BUILD_DIR=${PARMETIS_ROOT}/static-build/Linux-x86_64
--    cmake .. \
--    -DUSE_XSDK_DEFAULTS=FALSE\
--    -DTPL_PARMETIS_INCLUDE_DIRS="${PARMETIS_ROOT}/include;${PARMETIS_ROOT}/metis/include" \
--    -DTPL_PARMETIS_LIBRARIES="${PARMETIS_BUILD_DIR}/libparmetis/libparmetis.a;${PARMETIS_BUILD_DIR}/libmetis/libmetis.a" \
--    -DCMAKE_C_FLAGS="-std=c99 -fPIC" \
--    -DCMAKE_Fortran_COMPILER=ftn \
--    -Denable_blaslib=OFF \
--#    -DTPL_BLAS_LIBRARIES="-mkl" \
--    -DBUILD_SHARED_LIBS=OFF \
--    -DCMAKE_INSTALL_PREFIX=.
--#    -DCMAKE_EXE_LINKER_FLAGS="-shared"
--elif [ "$NERSC_HOST" == "cori" ]
--then
--    export PARMETIS_ROOT=~/Cori/lib/parmetis-4.0.3
--#    export PARMETIS_BUILD_DIR=${PARMETIS_ROOT}/shared-build
--    setenv PARMETIS_BUILD_DIR ${PARMETIS_ROOT}/static-build/Linux-x86_64
--    cmake .. \
--    -DUSE_XSDK_DEFAULTS=TRUE\
--    -DTPL_PARMETIS_INCLUDE_DIRS="${PARMETIS_ROOT}/include;${PARMETIS_ROOT}/metis/include" \
--    -DTPL_PARMETIS_LIBRARIES="${PARMETIS_BUILD_DIR}/libparmetis/libparmetis.a;${PARMETIS_BUILD_DIR}/libmetis/libmetis.a" \
--    -Denable_blaslib=OFF \
--    -DCMAKE_Fortran_COMPILER=ftn \
--    -DCMAKE_C_FLAGS="-std=c99 -fPIC" \
--#    -DCMAKE_EXE_LINKER_FLAGS="-shared" \
--    -DCMAKE_INSTALL_PREFIX=.
--fi
--
--THISHOST=`hostname -s`
--echo "host: $THISHOST"
--if [ "$THISHOST" == "ssg1" ]
--then
--  rm -fr ssg1-build; mkdir ssg1-build; cd ssg1-build;
--  export PARMETIS_ROOT=~/lib/static/64-bit/parmetis-4.0.3 
--#  export PARMETIS_ROOT=~/lib/static/parmetis-4.0.3 
--  export PARMETIS_BUILD_DIR=${PARMETIS_ROOT}/build/Linux-x86_64
--  echo "ParMetis root: $PARMETIS_ROOT"
--  cmake .. \
--    -DTPL_PARMETIS_INCLUDE_DIRS="${PARMETIS_ROOT}/include;${PARMETIS_ROOT}/metis/include" \
--    -DTPL_PARMETIS_LIBRARIES="${PARMETIS_BUILD_DIR}/libparmetis/libparmetis.a;${PARMETIS_BUILD_DIR}/libmetis/libmetis.a" \
--    -DCMAKE_C_FLAGS="-std=c99 -g -DPRNTlevel=0 -DDEBUGlevel=0" \
--    -Denable_blaslib=OFF \
--    -DBUILD_SHARED_LIBS=OFF \
--    -DCMAKE_C_COMPILER=mpicc \
--    -DXSDK_INDEX_SIZE=64 \
--    -DCMAKE_INSTALL_PREFIX=.
--#   -Denable_parmetislib=OFF
--fi
--
--# make VERBOSE=1
--# make test
diff --git a/build/batch_script_mpi.sh b/build/batch_script_mpi.sh
deleted file mode 100644
index 498386f..0000000
--- a/build/batch_script_mpi.sh
+++ /dev/null
@@ -1,146 +0,0 @@
-#!/bin/bash
-# Bash script to submit many files to Cori/Edison/Queue
-
-EXIT_SUCCESS=0
-EXIT_HOST=1
-EXIT_PARAM=2
-
-# MAX_PARAMS=1
-# # ^^^ This should be fixed, as it should just loop through everything
-# if [[ $# -eq 0 ]]; then
-  # echo "Must have at least one parameter; exiting"
-  # exit $EXIT_PARAM
-# fi
-# if [[ $# -gt $MAX_PARAMS ]]; then
-  # echo "Too many parameters; exiting"
-  # exit $EXIT_PARAM
-# fi
-
-# INPUT_FILE=$1
-# # ^^^ Get the input ile
-
-CUR_DIR=`pwd`
-FILE_DIR=$CUR_DIR/EXAMPLE
-INPUT_DIR=~/Edison/my_research/SuperLU/SuperLUDIST_Begin/build_bac/EXAMPLE
-FILE_NAME=pddrive
-FILE=$FILE_DIR/$FILE_NAME
-
-TMP_BATCH_FILE=tmp_batch_file.slurm
-# ^^^ Should check that this is not taken,
-#     but I would never use this, so ...
-
-> $TMP_BATCH_FILE
-
-if [[ $NERSC_HOST == edison ]]; then
-  CORES_PER_NODE=24
-elif [[ $NERSC_HOST == cori ]]; then
-  CORES_PER_NODE=32
-  # This does not take hyperthreading into account
-else
-  # Host unknown; exiting
-  exit $EXIT_HOST
-fi
-
-
-#nprows=(6 12 24 48 1 1 1 1 36 144 576 2304)
-#npcols=(6 12 24 48 36 144 576 2304 1 1 1 1)
-
-nprows=(6 12 24 48 )
-npcols=(6 12 24 48 )
-
-#nprows=(6 12 24 1 1 1  36 144 576 )
-#npcols=(6 12 24 36 144 576 1 1 1 )
-
-#nprows=(12 1 144)
-#npcols=(12 144 1)
-
-
-
-for ((i = 0; i < ${#npcols[@]}; i++)); do
-NROW=${nprows[i]}
-NCOL=${npcols[i]}
-
-# NROW=36
-CORE_VAL=`expr $NCOL \* $NROW`
-NODE_VAL=`expr $CORE_VAL / $CORES_PER_NODE`
-MOD_VAL=`expr $CORE_VAL % $CORES_PER_NODE`
-if [[ $MOD_VAL -ne 0 ]]
-then
-  NODE_VAL=`expr $NODE_VAL + 1`
-fi
-PARTITION=debug
-#PARTITION=regular
-LICENSE=SCRATCH
-TIME=00:20:00
-
-if [[ $NERSC_HOST == edison ]]
-then
-  CONSTRAINT=0
-fi
-if [[ $NERSC_HOST == cori ]]
-then
-  CONSTRAINT=haswell
-fi
-
-OMP_NUM_THREADS=1
-
-
-
-#for NSUP in 128 64 32 16 8
-#do
-  # for MAT in atmosmodl.rb nlpkkt80.mtx torso3.mtx Ga19As19H42.mtx A22.mtx cage13.rb 
-  for MAT in torso3.mtx
-  # for MAT in matrix121.dat matrix211.dat tdr190k.dat tdr455k.dat nlpkkt80.mtx torso3.mtx helm2d03.mtx  
-  # for MAT in tdr190k.dat Ga19As19H42.mtx
-  # for MAT in nlpkkt80.mtx torso3.mtx Ga19As19H42.mtx A22.mtx matrix121.dat tdr190k.dat   
-  # for MAT in Ga19As19H42.mtx   
-  do
-    # Start of looping stuff
-    > $TMP_BATCH_FILE
-    echo "#!/bin/bash -l" >> $TMP_BATCH_FILE
-    echo " " >> $TMP_BATCH_FILE
-    echo "#SBATCH -p $PARTITION" >> $TMP_BATCH_FILE
-    echo "#SBATCH -N $NODE_VAL" >>  $TMP_BATCH_FILE
-    echo "#SBATCH -t $TIME" >> $TMP_BATCH_FILE
-    echo "#SBATCH -L $LICENSE" >> $TMP_BATCH_FILE
-    echo "#SBATCH -J SLU_$MAT" >> $TMP_BATCH_FILE
-    echo "#SBATCH -o ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_async_simple_over_single" >> $TMP_BATCH_FILE
-    echo "#SBATCH -e ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_async_simple_over_single" >> $TMP_BATCH_FILE
-    # echo "#SBATCH --mail-type=BEGIN" >> $TMP_BATCH_FILE
-    # echo "#SBATCH --mail-type=END" >> $TMP_BATCH_FILE
-    echo "#SBATCH --mail-user=liuyangzhuan@lbl.gov" >> $TMP_BATCH_FILE
-    if [[ $NERSC_HOST == cori ]]
-    then
-      echo "#SBATCH -C $CONSTRAINT" >> $TMP_BATCH_FILE
-    fi
-    mkdir -p $MAT   
-    echo "export OMP_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE
-    echo "export KMP_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE
-    echo "export MKL_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE
-    echo "export MPICH_MAX_THREAD_SAFETY=multiple" >> $TMP_BATCH_FILE 
-    echo "export NSUP=128" >> $TMP_BATCH_FILE
-    echo "export NREL=20" >> $TMP_BATCH_FILE
-    
-    echo " " >> $TMP_BATCH_FILE
-    echo "FILE=$FILE" >> $TMP_BATCH_FILE
-    echo "FILEMAT=$INPUT_DIR/$MAT" >> $TMP_BATCH_FILE	
-    echo " " >> $TMP_BATCH_FILE
-    echo "CORE_VAL=$CORE_VAL" >> $TMP_BATCH_FILE
-    echo "NCOL=$NCOL" >> $TMP_BATCH_FILE
-    echo "NROW=$NROW" >> $TMP_BATCH_FILE
-    # This should be computed individually for each script...
-
-    echo "srun -n $CORE_VAL $FILE -c $NCOL -r $NROW $INPUT_DIR/$MAT" >> $TMP_BATCH_FILE
-    # Add final line (srun line) to temporary slurm script
-
-    #cat $TMP_BATCH_FILE
-    #echo " "
-    sbatch $TMP_BATCH_FILE
-  done
-#one
-
-done
-
-
-exit $EXIT_SUCCESS
-
diff --git a/build/batch_script_mpi_runit.sh b/build/batch_script_mpi_runit.sh
deleted file mode 100644
index 8c42251..0000000
--- a/build/batch_script_mpi_runit.sh
+++ /dev/null
@@ -1,194 +0,0 @@
-#!/bin/bash
-# Bash script to submit many files to Cori/Edison/Queue
-
-EXIT_SUCCESS=0
-EXIT_HOST=1
-EXIT_PARAM=2
-
-# MAX_PARAMS=1
-# # ^^^ This should be fixed, as it should just loop through everything
-# if [[ $# -eq 0 ]]; then
-  # echo "Must have at least one parameter; exiting"
-  # exit $EXIT_PARAM
-# fi
-# if [[ $# -gt $MAX_PARAMS ]]; then
-  # echo "Too many parameters; exiting"
-  # exit $EXIT_PARAM
-# fi
-
-# INPUT_FILE=$1
-# # ^^^ Get the input ile
-
-CUR_DIR=`pwd`
-FILE_DIR=$CUR_DIR/EXAMPLE
-INPUT_DIR=/project/projectdirs/sparse/liuyangz/my_research/matrix
-FILE_NAME=pddrive
-FILE=$FILE_DIR/$FILE_NAME
-
-TMP_BATCH_FILE=tmp_batch_file.slurm
-# ^^^ Should check that this is not taken,
-#     but I would never use this, so ...
-
-> $TMP_BATCH_FILE
-
-if [[ $NERSC_HOST == edison ]]; then
-  CORES_PER_NODE=24
-elif [[ $NERSC_HOST == cori ]]; then
-  CORES_PER_NODE=32
-  # This does not take hyperthreading into account
-else
-  # Host unknown; exiting
-  exit $EXIT_HOST
-fi
-
-
-#nprows=(6 12 24)
-#npcols=(6 12 24)
-
-
-#nprows=(2048 1 32)
-#npcols=(1 2048 64)
-
-# nprows=(32 )
-# npcols=(64 )
-
-
-
-#nprows=(24 48 1 1 576 2304)
-#npcols=(24 48 576 2304 1 1)
-
-
-#nprows=(48  1  2304)
-#npcols=(48  2304 1)
-
-#nprows=(6 12 24 48 )
-#npcols=(6 12 24 48 )
-
-#nprows=(6 12 24 48 1 1 1 1 36 144 576 2304)
-#npcols=(6 12 24 48 36 144 576 2304 1 1 1 1)
-
-# nprows=(32 128 512 1 1 1 4 8 16)
-# npcols=(1 1 1 32 128 512 8 16 32)
-
-#nprows=(2048 1 32)
-#npcols=(1 2048 64)
-
-
-
-
-#nprows=(12 1 144)
-#npcols=(12 144 1)
- 
-NREP=1	  
-
-#nprows=(4 8 16 32 45)
-#npcols=(4 8 16 32 45)
-#nprows=(32)
-#npcols=(48)
-
-nprows=(16)
-npcols=(16)
- 
-for ((i = 0; i < ${#npcols[@]}; i++)); do
-NROW=${nprows[i]}
-NCOL=${npcols[i]}
-
-# NROW=36
-CORE_VAL=`expr $NCOL \* $NROW`
-NODE_VAL=`expr $CORE_VAL / $CORES_PER_NODE`
-MOD_VAL=`expr $CORE_VAL % $CORES_PER_NODE`
-
-if [[ $MOD_VAL -ne 0 ]]
-then
-  NODE_VAL=`expr $NODE_VAL + 1`
-fi
-#PARTITION=debug
-PARTITION=regular
-ICENSE=SCRATCH
-TIME=00:20:00
-
-if [[ $NERSC_HOST == edison ]]
-then
-  CONSTRAINT=0
-fi
-if [[ $NERSC_HOST == cori ]]
-then
-  CONSTRAINT=haswell
-fi
-
-OMP_NUM_THREADS=1
-
-THREADS_PER_RANK=`expr 2 \* $OMP_NUM_THREADS`											 
-
-
-#for NSUP in 128 64 32 16 8
-#do
-  # for MAT in atmosmodl.rb nlpkkt80.mtx torso3.mtx Ga19As19H42.mtx A22.mtx cage13.rb 
-  #for MAT in torso3.mtx
-  # for MAT in matrix121.dat matrix211.dat tdr190k.dat tdr455k.dat nlpkkt80.mtx torso3.mtx helm2d03.mtx  
-  # for MAT in tdr190k.dat Ga19As19H42.mtx
-# for MAT in big.rua
-# for MAT in tdr455k.bin
-  # for MAT in A22.bin tdr455k.bin DG_GrapheneDisorder_8192.bin DNA_715_64cell.bin LU_C_BN_C_4by2.bin Li4244.bin atmosmodj.bin nlpkkt80.bin Ga19As19H42.bin Geo_1438.bin StocF-1465.bin cage13.bin																																							   
-#  for MAT in globalmat118_1536.bin								  
-#  for MAT in DG_PNF_14000.bin DG_GrapheneDisorder_32768.bin
- #  for MAT in DNA_715_64cell.mtx
- # for MAT in Ga19As19H42.mtx cage13.rb Geo_1438.mtx nlpkkt80.mtx torso3.mtx helm2d03.mtx gsm_106857.mtx atmosmodj.mtx StocF-1465.mtx hvdc2.mtx  
- for MAT in Geo_1438.bin
- do
-    # Start of looping stuff
-    > $TMP_BATCH_FILE
-    echo "#!/bin/bash -l" >> $TMP_BATCH_FILE
-    echo " " >> $TMP_BATCH_FILE
-    echo "#SBATCH -p $PARTITION" >> $TMP_BATCH_FILE
-    echo "#SBATCH -N $NODE_VAL" >>  $TMP_BATCH_FILE
-    echo "#SBATCH -t $TIME" >> $TMP_BATCH_FILE
-    echo "#SBATCH -L $LICENSE" >> $TMP_BATCH_FILE
-    echo "#SBATCH -J SLU_$MAT" >> $TMP_BATCH_FILE
-    #echo "#SBATCH -o ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_async_simple_over_icollec_mrhs" >> $TMP_BATCH_FILE
-    #echo "#SBATCH -e ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_async_simple_over_icollec_mrhs" >> $TMP_BATCH_FILE
-    # echo "#SBATCH --mail-type=BEGIN" >> $TMP_BATCH_FILE
-    # echo "#SBATCH --mail-type=END" >> $TMP_BATCH_FILE
-    echo "#SBATCH --mail-user=liuyangzhuan@lbl.gov" >> $TMP_BATCH_FILE
-    if [[ $NERSC_HOST == cori ]]
-    then
-      echo "#SBATCH -C $CONSTRAINT" >> $TMP_BATCH_FILE
-    fi
-    mkdir -p $MAT   
-    export OMP_NUM_THREADS=$OMP_NUM_THREADS
-    export KMP_NUM_THREADS=$OMP_NUM_THREADS
-    export MKL_NUM_THREADS=$OMP_NUM_THREADS  																			
-    export NSUP=128
-    export NREL=20
-
-    export OMP_PLACES=threads
-    export OMP_PROC_BIND=spread
-    export MPICH_MAX_THREAD_iSAFETY=multiple
-    
-    echo " " >> $TMP_BATCH_FILE
-    echo "FILE=$FILE" >> $TMP_BATCH_FILE
-    echo "FILEMAT=$INPUT_DIR/$MAT" >> $TMP_BATCH_FILE	
-    echo " " >> $TMP_BATCH_FILE
-    echo "CORE_VAL=$CORE_VAL" >> $TMP_BATCH_FILE
-    echo "NCOL=$NCOL" >> $TMP_BATCH_FILE
-    echo "NROW=$NROW" >> $TMP_BATCH_FILE
-    # This should be computed individually for each script...
-OUTPUT=./$MAT/SLU.o_mpi_${NROW}x${NCOL}_asyncU_cori	
-	rm -rf $OUTPUT
-	for ii in `seq 1 $NREP`
-    do
-    srun -n $CORE_VAL -N $NODE_VAL -c $THREADS_PER_RANK --cpu_bind=cores $FILE -c $NCOL -r $NROW $INPUT_DIR/$MAT | tee -a $OUTPUT
-	done	 
-    # Add final line (srun line) to temporary slurm script
-
-    #cat $TMP_BATCH_FILE
-    #echo " "
-    # sbatch $TMP_BATCH_FILE
-  done
-#one
-
-done
-
-
-exit $EXIT_SUCCESS
-
diff --git a/build/batch_script_mpi_runit10.sh b/build/batch_script_mpi_runit10.sh
deleted file mode 100644
index 3623c1d..0000000
--- a/build/batch_script_mpi_runit10.sh
+++ /dev/null
@@ -1,18 +0,0 @@
-#!/bin/bash
-# Bash script to submit many files to Cori/Edison/Queue
-
-
-EXIT_SUCCESS=0
-EXIT_HOST=1
-EXIT_PARAM=2
-
-export MPICH_MAX_THREAD_SAFETY=multiple
-
-for i in `seq 1 200`;
-do
-	srun -n 2 ./EXAMPLE/pddrive -r 1 -c 2 ../EXAMPLE/big.rua | tee -a a.out
-done
-
-
-exit $EXIT_SUCCESS
-
diff --git a/build/batch_script_mpi_runit11.sh b/build/batch_script_mpi_runit11.sh
deleted file mode 100644
index 6b4f2dd..0000000
--- a/build/batch_script_mpi_runit11.sh
+++ /dev/null
@@ -1,191 +0,0 @@
-#!/bin/bash
-# Bash script to submit many files to Cori/Edison/Queue
-
-EXIT_SUCCESS=0
-EXIT_HOST=1
-EXIT_PARAM=2
-
-# MAX_PARAMS=1
-# # ^^^ This should be fixed, as it should just loop through everything
-# if [[ $# -eq 0 ]]; then
-  # echo "Must have at least one parameter; exiting"
-  # exit $EXIT_PARAM
-# fi
-# if [[ $# -gt $MAX_PARAMS ]]; then
-  # echo "Too many parameters; exiting"
-  # exit $EXIT_PARAM
-# fi
-
-# INPUT_FILE=$1
-# # ^^^ Get the input ile
-
-CUR_DIR=`pwd`
-FILE_DIR=$CUR_DIR/EXAMPLE
-INPUT_DIR=/project/projectdirs/sparse/liuyangz/my_research/matrix
-FILE_NAME=pddrive
-FILE=$FILE_DIR/$FILE_NAME
-
-TMP_BATCH_FILE=tmp_batch_file.slurm
-# ^^^ Should check that this is not taken,
-#     but I would never use this, so ...
-
-> $TMP_BATCH_FILE
-
-if [[ $NERSC_HOST == edison ]]; then
-  CORES_PER_NODE=24
-elif [[ $NERSC_HOST == cori ]]; then
-  CORES_PER_NODE=32
-  # This does not take hyperthreading into account
-else
-  # Host unknown; exiting
-  exit $EXIT_HOST
-fi
-
-
-#nprows=(6 12 24)
-#npcols=(6 12 24)
-
-
-#nprows=(2048 1 32)
-#npcols=(1 2048 64)
-
-# nprows=(32 )
-# npcols=(64 )
-
-
-
-#nprows=(24 48 1 1 576 2304)
-#npcols=(24 48 576 2304 1 1)
-
-
-#nprows=(48  1  2304)
-#npcols=(48  2304 1)
-
-#nprows=(6 12 24 48 )
-#npcols=(6 12 24 48 )
-
-#nprows=(6 12 24 48 1 1 1 1 36 144 576 2304)
-#npcols=(6 12 24 48 36 144 576 2304 1 1 1 1)
-
-# nprows=(32 128 512 1 1 1 4 8 16)
-# npcols=(1 1 1 32 128 512 8 16 32)
-
-#nprows=(2048 1 32)
-#npcols=(1 2048 64)
-
-
-
-
-#nprows=(12 1 144)
-#npcols=(12 144 1)
- 
-NREP=5 
-
-nprows=(32 )
-npcols=(64)
-
-# nprows=(4 8)
-# npcols=(4 8)
- 
-for ((i = 0; i < ${#npcols[@]}; i++)); do
-NROW=${nprows[i]}
-NCOL=${npcols[i]}
-
-# NROW=36
-CORE_VAL=`expr $NCOL \* $NROW`
-NODE_VAL=`expr $CORE_VAL / $CORES_PER_NODE`
-MOD_VAL=`expr $CORE_VAL % $CORES_PER_NODE`
-
-if [[ $MOD_VAL -ne 0 ]]
-then
-  NODE_VAL=`expr $NODE_VAL + 1`
-fi
-#PARTITION=debug
-PARTITION=regular
-ICENSE=SCRATCH
-TIME=00:20:00
-
-if [[ $NERSC_HOST == edison ]]
-then
-  CONSTRAINT=0
-fi
-if [[ $NERSC_HOST == cori ]]
-then
-  CONSTRAINT=haswell
-fi
-
-OMP_NUM_THREADS=1
-
-THREADS_PER_RANK=`expr 2 \* $OMP_NUM_THREADS`											 
-
-
-#for NSUP in 128 64 32 16 8
-#do
-  # for MAT in atmosmodl.rb nlpkkt80.mtx torso3.mtx Ga19As19H42.mtx A22.mtx cage13.rb 
-  #for MAT in torso3.mtx
-  # for MAT in matrix121.dat matrix211.dat tdr190k.dat tdr455k.dat nlpkkt80.mtx torso3.mtx helm2d03.mtx  
-  # for MAT in tdr190k.dat Ga19As19H42.mtx
-# for MAT in big.rua
-# for MAT in tdr455k.bin
-  for MAT in torso3.mtx
-#  for MAT in A22.bin tdr455k.bin DG_GrapheneDisorder_8192.bin DNA_715_64cell.bin LU_C_BN_C_4by2.bin Li4244.bin atmosmodj.bin nlpkkt80.bin Ga19As19H42.bin Geo_1438.bin StocF-1465.bin cage13.bin																																							   
-#  for MAT in DG_PNF_14000.bin DG_GrapheneDisorder_32768.bin
- #  for MAT in DNA_715_64cell.mtx
- # for MAT in Ga19As19H42.mtx cage13.rb Geo_1438.mtx nlpkkt80.mtx torso3.mtx helm2d03.mtx gsm_106857.mtx atmosmodj.mtx StocF-1465.mtx hvdc2.mtx  
- do
-    # Start of looping stuff
-    > $TMP_BATCH_FILE
-    echo "#!/bin/bash -l" >> $TMP_BATCH_FILE
-    echo " " >> $TMP_BATCH_FILE
-    echo "#SBATCH -p $PARTITION" >> $TMP_BATCH_FILE
-    echo "#SBATCH -N $NODE_VAL" >>  $TMP_BATCH_FILE
-    echo "#SBATCH -t $TIME" >> $TMP_BATCH_FILE
-    echo "#SBATCH -L $LICENSE" >> $TMP_BATCH_FILE
-    echo "#SBATCH -J SLU_$MAT" >> $TMP_BATCH_FILE
-    #echo "#SBATCH -o ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_async_simple_over_icollec_mrhs" >> $TMP_BATCH_FILE
-    #echo "#SBATCH -e ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_async_simple_over_icollec_mrhs" >> $TMP_BATCH_FILE
-    # echo "#SBATCH --mail-type=BEGIN" >> $TMP_BATCH_FILE
-    # echo "#SBATCH --mail-type=END" >> $TMP_BATCH_FILE
-    echo "#SBATCH --mail-user=liuyangzhuan@lbl.gov" >> $TMP_BATCH_FILE
-    if [[ $NERSC_HOST == cori ]]
-    then
-      echo "#SBATCH -C $CONSTRAINT" >> $TMP_BATCH_FILE
-    fi
-    mkdir -p $MAT   
-    export OMP_NUM_THREADS=$OMP_NUM_THREADS
-    export KMP_NUM_THREADS=$OMP_NUM_THREADS
-    export MKL_NUM_THREADS=$OMP_NUM_THREADS  																			
-    echo "export NSUP=128" >> $TMP_BATCH_FILE
-    echo "export NREL=20" >> $TMP_BATCH_FILE
-
-    export OMP_PLACES=threads
-    export OMP_PROC_BIND=spread
-    export MPICH_MAX_THREAD_SAFETY=multiple
-    
-    echo " " >> $TMP_BATCH_FILE
-    echo "FILE=$FILE" >> $TMP_BATCH_FILE
-    echo "FILEMAT=$INPUT_DIR/$MAT" >> $TMP_BATCH_FILE	
-    echo " " >> $TMP_BATCH_FILE
-    echo "CORE_VAL=$CORE_VAL" >> $TMP_BATCH_FILE
-    echo "NCOL=$NCOL" >> $TMP_BATCH_FILE
-    echo "NROW=$NROW" >> $TMP_BATCH_FILE
-    # This should be computed individually for each script...
-OUTPUT=./$MAT/SLU.o_mpi_${NROW}x${NCOL}_groupgemm_cori	
-	rm -rf $OUTPUT
-	for ii in `seq 1 $NREP`
-    do
-    srun -n $CORE_VAL -N $NODE_VAL -c $THREADS_PER_RANK --cpu_bind=cores $FILE -c $NCOL -r $NROW $INPUT_DIR/$MAT | tee -a $OUTPUT
-	done	 
-    # Add final line (srun line) to temporary slurm script
-
-    #cat $TMP_BATCH_FILE
-    #echo " "
-    # sbatch $TMP_BATCH_FILE
-  done
-#one
-
-done
-
-
-exit $EXIT_SUCCESS
-
diff --git a/build/batch_script_mpi_runit4.sh b/build/batch_script_mpi_runit4.sh
deleted file mode 100644
index f2b7104..0000000
--- a/build/batch_script_mpi_runit4.sh
+++ /dev/null
@@ -1,172 +0,0 @@
-#!/bin/bash
-# Bash script to submit many files to Cori/Edison/Queue
-
-EXIT_SUCCESS=0
-EXIT_HOST=1
-EXIT_PARAM=2
-
-# MAX_PARAMS=1
-# # ^^^ This should be fixed, as it should just loop through everything
-# if [[ $# -eq 0 ]]; then
-  # echo "Must have at least one parameter; exiting"
-  # exit $EXIT_PARAM
-# fi
-# if [[ $# -gt $MAX_PARAMS ]]; then
-  # echo "Too many parameters; exiting"
-  # exit $EXIT_PARAM
-# fi
-
-# INPUT_FILE=$1
-# # ^^^ Get the input ile
-
-CUR_DIR=`pwd`
-FILE_DIR=$CUR_DIR/EXAMPLE
-INPUT_DIR=~/Edison/my_research/SuperLU/SuperLUDIST_Begin/build_bac/EXAMPLE
-FILE_NAME=pddrive
-FILE=$FILE_DIR/$FILE_NAME
-
-TMP_BATCH_FILE=tmp_batch_file.slurm
-# ^^^ Should check that this is not taken,
-#     but I would never use this, so ...
-
-> $TMP_BATCH_FILE
-
-if [[ $NERSC_HOST == edison ]]; then
-  CORES_PER_NODE=24
-elif [[ $NERSC_HOST == cori ]]; then
-  CORES_PER_NODE=32
-  # This does not take hyperthreading into account
-else
-  # Host unknown; exiting
-  exit $EXIT_HOST
-fi
-
-
-#nprows=(6 12 24)
-#npcols=(6 12 24)
-
-
-#nprows=(2048 1 32)
-#npcols=(1 2048 64)
-
-# nprows=(32 )
-# npcols=(64 )
-
-
-
-#nprows=(24 48 1 1 576 2304)
-#npcols=(24 48 576 2304 1 1)
-
-
-#nprows=(48  1  2304)
-#npcols=(48  2304 1)
-
-#nprows=(6 12 24 48 )
-#npcols=(6 12 24 48 )
-
-#nprows=(6 12 24 48 1 1 1 1 36 144 576 2304)
-#npcols=(6 12 24 48 36 144 576 2304 1 1 1 1)
-
-#nprows=(32 128 512 1 1 1 4 8 16)
-#npcols=(1 1 1 32 128 512 8 16 32)
-
-nprows=(2048 1 1 32)
-npcols=(1 512 2048 64)
-
-
-
-
-#nprows=(12 1 144)
-#npcols=(12 144 1)
-
-
-#nprows=(48)
-#npcols=(48)
- 
-for ((i = 0; i < ${#npcols[@]}; i++)); do
-NROW=${nprows[i]}
-NCOL=${npcols[i]}
-
-# NROW=36
-CORE_VAL=`expr $NCOL \* $NROW`
-NODE_VAL=`expr $CORE_VAL / $CORES_PER_NODE`
-MOD_VAL=`expr $CORE_VAL % $CORES_PER_NODE`
-if [[ $MOD_VAL -ne 0 ]]
-then
-  NODE_VAL=`expr $NODE_VAL + 1`
-fi
-#PARTITION=debug
-PARTITION=regular
-LICENSE=SCRATCH
-TIME=00:20:00
-
-if [[ $NERSC_HOST == edison ]]
-then
-  CONSTRAINT=0
-fi
-if [[ $NERSC_HOST == cori ]]
-then
-  CONSTRAINT=haswell
-fi
-
-OMP_NUM_THREADS=1
-
-
-
-#for NSUP in 128 64 32 16 8
-#do
-  # for MAT in atmosmodl.rb nlpkkt80.mtx torso3.mtx Ga19As19H42.mtx A22.mtx cage13.rb 
-  # for MAT in torso3.mtx
-  # for MAT in matrix121.dat matrix211.dat tdr190k.dat tdr455k.dat nlpkkt80.mtx torso3.mtx helm2d03.mtx  
-  # for MAT in tdr190k.dat Ga19As19H42.mtx
- for MAT in torso3.mtx 
- # for MAT in Ga19As19H42.mtx   
-  do
-    # Start of looping stuff
-    > $TMP_BATCH_FILE
-    echo "#!/bin/bash -l" >> $TMP_BATCH_FILE
-    echo " " >> $TMP_BATCH_FILE
-    echo "#SBATCH -p $PARTITION" >> $TMP_BATCH_FILE
-    echo "#SBATCH -N $NODE_VAL" >>  $TMP_BATCH_FILE
-    echo "#SBATCH -t $TIME" >> $TMP_BATCH_FILE
-    echo "#SBATCH -L $LICENSE" >> $TMP_BATCH_FILE
-    echo "#SBATCH -J SLU_$MAT" >> $TMP_BATCH_FILE
-    #echo "#SBATCH -o ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_async_simple_over_icollec_flat_mrhs" >> $TMP_BATCH_FILE
-    #echo "#SBATCH -e ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_async_simple_over_icollec_flat_mrhs" >> $TMP_BATCH_FILE
-    # echo "#SBATCH --mail-type=BEGIN" >> $TMP_BATCH_FILE
-    # echo "#SBATCH --mail-type=END" >> $TMP_BATCH_FILE
-    echo "#SBATCH --mail-user=liuyangzhuan@lbl.gov" >> $TMP_BATCH_FILE
-    if [[ $NERSC_HOST == cori ]]
-    then
-      echo "#SBATCH -C $CONSTRAINT" >> $TMP_BATCH_FILE
-    fi
-    mkdir -p $MAT   
-    echo "export OMP_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE
-    echo "export KMP_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE
-    echo "export MKL_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE  																			
-    echo "export NSUP=128" >> $TMP_BATCH_FILE
-    echo "export NREL=20" >> $TMP_BATCH_FILE
-    
-    echo " " >> $TMP_BATCH_FILE
-    echo "FILE=$FILE" >> $TMP_BATCH_FILE
-    echo "FILEMAT=$INPUT_DIR/$MAT" >> $TMP_BATCH_FILE	
-    echo " " >> $TMP_BATCH_FILE
-    echo "CORE_VAL=$CORE_VAL" >> $TMP_BATCH_FILE
-    echo "NCOL=$NCOL" >> $TMP_BATCH_FILE
-    echo "NROW=$NROW" >> $TMP_BATCH_FILE
-    # This should be computed individually for each script...
-
-    srun -n $CORE_VAL $FILE -c $NCOL -r $NROW $INPUT_DIR/$MAT | tee ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_async_simple_over_icollec_groupgemm_bettertree_cori_mrhs
-    # Add final line (srun line) to temporary slurm script
-
-    #cat $TMP_BATCH_FILE
-    #echo " "
-    # sbatch $TMP_BATCH_FILE
-  done
-#one
-
-done
-
-
-exit $EXIT_SUCCESS
-
diff --git a/build/batch_script_mpi_runit5.sh b/build/batch_script_mpi_runit5.sh
deleted file mode 100644
index eb50ec2..0000000
--- a/build/batch_script_mpi_runit5.sh
+++ /dev/null
@@ -1,168 +0,0 @@
-#!/bin/bash
-# Bash script to submit many files to Cori/Edison/Queue
-
-EXIT_SUCCESS=0
-EXIT_HOST=1
-EXIT_PARAM=2
-
-# MAX_PARAMS=1
-# # ^^^ This should be fixed, as it should just loop through everything
-# if [[ $# -eq 0 ]]; then
-  # echo "Must have at least one parameter; exiting"
-  # exit $EXIT_PARAM
-# fi
-# if [[ $# -gt $MAX_PARAMS ]]; then
-  # echo "Too many parameters; exiting"
-  # exit $EXIT_PARAM
-# fi
-
-# INPUT_FILE=$1
-# # ^^^ Get the input ile
-
-CUR_DIR=`pwd`
-FILE_DIR=$CUR_DIR/EXAMPLE
-INPUT_DIR=~/Edison/my_research/SuperLU/SuperLUDIST_Begin/build_bac/EXAMPLE
-FILE_NAME=pddrive
-FILE=$FILE_DIR/$FILE_NAME
-
-TMP_BATCH_FILE=tmp_batch_file.slurm
-# ^^^ Should check that this is not taken,
-#     but I would never use this, so ...
-
-> $TMP_BATCH_FILE
-
-if [[ $NERSC_HOST == edison ]]; then
-  CORES_PER_NODE=24
-elif [[ $NERSC_HOST == cori ]]; then
-  CORES_PER_NODE=32
-  # This does not take hyperthreading into account
-else
-  # Host unknown; exiting
-  exit $EXIT_HOST
-fi
-
-
-#nprows=(6 12 24)
-#npcols=(6 12 24)
-
-
-#nprows=(2048 1 32)
-#npcols=(1 2048 64)
-
-# nprows=(32 )
-# npcols=(64 )
-
-
-
-#nprows=(24 48 1 1 576 2304)
-#npcols=(24 48 576 2304 1 1)
-
-
-#nprows=(48  1  2304)
-#npcols=(48  2304 1)
-
-#nprows=(6 12 24 48 )
-#npcols=(6 12 24 48 )
-
-nprows=(6 12 24 48 1 1 1 1 36 144 576 2304)
-npcols=(6 12 24 48 36 144 576 2304 1 1 1 1)
-
-# nprows=( 576 2304)
-# npcols=( 1 1)
-
-
-#nprows=(12 1 144)
-#npcols=(12 144 1)
-
-
-#nprows=(48)
-#npcols=(48)
- 
-for ((i = 0; i < ${#npcols[@]}; i++)); do
-NROW=${nprows[i]}
-NCOL=${npcols[i]}
-
-# NROW=36
-CORE_VAL=`expr $NCOL \* $NROW`
-NODE_VAL=`expr $CORE_VAL / $CORES_PER_NODE`
-MOD_VAL=`expr $CORE_VAL % $CORES_PER_NODE`
-if [[ $MOD_VAL -ne 0 ]]
-then
-  NODE_VAL=`expr $NODE_VAL + 1`
-fi
-#PARTITION=debug
-PARTITION=regular
-LICENSE=SCRATCH
-TIME=00:20:00
-
-if [[ $NERSC_HOST == edison ]]
-then
-  CONSTRAINT=0
-fi
-if [[ $NERSC_HOST == cori ]]
-then
-  CONSTRAINT=haswell
-fi
-
-OMP_NUM_THREADS=1
-
-
-
-#for NSUP in 128 64 32 16 8
-#do
-  # for MAT in atmosmodl.rb nlpkkt80.mtx torso3.mtx Ga19As19H42.mtx A22.mtx cage13.rb 
-  # for MAT in torso3.mtx
-  # for MAT in matrix121.dat matrix211.dat tdr190k.dat tdr455k.dat nlpkkt80.mtx torso3.mtx helm2d03.mtx  
-  # for MAT in tdr190k.dat Ga19As19H42.mtx
- for MAT in helm2d03.mtx 
- # for MAT in torso3.mtx 
- # for MAT in Ga19As19H42.mtx   
-  do
-    # Start of looping stuff
-    > $TMP_BATCH_FILE
-    echo "#!/bin/bash -l" >> $TMP_BATCH_FILE
-    echo " " >> $TMP_BATCH_FILE
-    echo "#SBATCH -p $PARTITION" >> $TMP_BATCH_FILE
-    echo "#SBATCH -N $NODE_VAL" >>  $TMP_BATCH_FILE
-    echo "#SBATCH -t $TIME" >> $TMP_BATCH_FILE
-    echo "#SBATCH -L $LICENSE" >> $TMP_BATCH_FILE
-    echo "#SBATCH -J SLU_$MAT" >> $TMP_BATCH_FILE
-    #echo "#SBATCH -o ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_async_simple_over_icollec_flat_mrhs" >> $TMP_BATCH_FILE
-    #echo "#SBATCH -e ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_async_simple_over_icollec_flat_mrhs" >> $TMP_BATCH_FILE
-    # echo "#SBATCH --mail-type=BEGIN" >> $TMP_BATCH_FILE
-    # echo "#SBATCH --mail-type=END" >> $TMP_BATCH_FILE
-    echo "#SBATCH --mail-user=liuyangzhuan@lbl.gov" >> $TMP_BATCH_FILE
-    if [[ $NERSC_HOST == cori ]]
-    then
-      echo "#SBATCH -C $CONSTRAINT" >> $TMP_BATCH_FILE
-    fi
-    mkdir -p $MAT   
-    echo "export OMP_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE
-    echo "export KMP_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE
-    echo "export MKL_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE  																			
-    echo "export NSUP=128" >> $TMP_BATCH_FILE
-    echo "export NREL=20" >> $TMP_BATCH_FILE
-    
-    echo " " >> $TMP_BATCH_FILE
-    echo "FILE=$FILE" >> $TMP_BATCH_FILE
-    echo "FILEMAT=$INPUT_DIR/$MAT" >> $TMP_BATCH_FILE	
-    echo " " >> $TMP_BATCH_FILE
-    echo "CORE_VAL=$CORE_VAL" >> $TMP_BATCH_FILE
-    echo "NCOL=$NCOL" >> $TMP_BATCH_FILE
-    echo "NROW=$NROW" >> $TMP_BATCH_FILE
-    # This should be computed individually for each script...
-
-    srun -n $CORE_VAL $FILE -c $NCOL -r $NROW $INPUT_DIR/$MAT | tee ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_async_simple_over_icollec_groupgemm_bettertree_mrhs
-    # Add final line (srun line) to temporary slurm script
-
-    #cat $TMP_BATCH_FILE
-    #echo " "
-    # sbatch $TMP_BATCH_FILE
-  done
-#one
-
-done
-
-
-exit $EXIT_SUCCESS
-
diff --git a/build/batch_script_mpi_runit6.sh b/build/batch_script_mpi_runit6.sh
deleted file mode 100644
index 3105033..0000000
--- a/build/batch_script_mpi_runit6.sh
+++ /dev/null
@@ -1,172 +0,0 @@
-#!/bin/bash
-# Bash script to submit many files to Cori/Edison/Queue
-
-EXIT_SUCCESS=0
-EXIT_HOST=1
-EXIT_PARAM=2
-
-# MAX_PARAMS=1
-# # ^^^ This should be fixed, as it should just loop through everything
-# if [[ $# -eq 0 ]]; then
-  # echo "Must have at least one parameter; exiting"
-  # exit $EXIT_PARAM
-# fi
-# if [[ $# -gt $MAX_PARAMS ]]; then
-  # echo "Too many parameters; exiting"
-  # exit $EXIT_PARAM
-# fi
-
-# INPUT_FILE=$1
-# # ^^^ Get the input ile
-
-CUR_DIR=`pwd`
-FILE_DIR=$CUR_DIR/EXAMPLE
-INPUT_DIR=~/Edison/my_research/SuperLU/SuperLUDIST_Begin/build_bac/EXAMPLE
-FILE_NAME=pddrive
-FILE=$FILE_DIR/$FILE_NAME
-
-TMP_BATCH_FILE=tmp_batch_file.slurm
-# ^^^ Should check that this is not taken,
-#     but I would never use this, so ...
-
-> $TMP_BATCH_FILE
-
-if [[ $NERSC_HOST == edison ]]; then
-  CORES_PER_NODE=24
-elif [[ $NERSC_HOST == cori ]]; then
-  CORES_PER_NODE=32
-  # This does not take hyperthreading into account
-else
-  # Host unknown; exiting
-  exit $EXIT_HOST
-fi
-
-
-#nprows=(6 12 24)
-#npcols=(6 12 24)
-
-
-#nprows=(2048 1 32)
-#npcols=(1 2048 64)
-
-# nprows=(32 )
-# npcols=(64 )
-
-
-
-#nprows=(24 48 1 1 576 2304)
-#npcols=(24 48 576 2304 1 1)
-
-
-#nprows=(48  1  2304)
-#npcols=(48  2304 1)
-
-#nprows=(6 12 24 48 )
-#npcols=(6 12 24 48 )
-
-#nprows=(6 12 24 48 1 1 1 1 36 144 576 2304)
-#npcols=(6 12 24 48 36 144 576 2304 1 1 1 1)
-
-nprows=(1 2 2 4 6 )
-npcols=(1 1 2 4 6 )
-
-#nprows=(2048 1 32)
-#npcols=(1 2048 64)
-
-
-
-
-#nprows=(12 1 144)
-#npcols=(12 144 1)
-
-
-#nprows=(48)
-#npcols=(48)
- 
-for ((i = 0; i < ${#npcols[@]}; i++)); do
-NROW=${nprows[i]}
-NCOL=${npcols[i]}
-
-# NROW=36
-CORE_VAL=`expr $NCOL \* $NROW`
-NODE_VAL=`expr $CORE_VAL / $CORES_PER_NODE`
-MOD_VAL=`expr $CORE_VAL % $CORES_PER_NODE`
-if [[ $MOD_VAL -ne 0 ]]
-then
-  NODE_VAL=`expr $NODE_VAL + 1`
-fi
-#PARTITION=debug
-PARTITION=regular
-LICENSE=SCRATCH
-TIME=00:20:00
-
-if [[ $NERSC_HOST == edison ]]
-then
-  CONSTRAINT=0
-fi
-if [[ $NERSC_HOST == cori ]]
-then
-  CONSTRAINT=haswell
-fi
-
-OMP_NUM_THREADS=1
-
-
-
-#for NSUP in 128 64 32 16 8
-#do
-  # for MAT in atmosmodl.rb nlpkkt80.mtx torso3.mtx Ga19As19H42.mtx A22.mtx cage13.rb 
-  # for MAT in torso3.mtx
-  # for MAT in matrix121.dat matrix211.dat tdr190k.dat tdr455k.dat nlpkkt80.mtx torso3.mtx helm2d03.mtx  
-  # for MAT in tdr190k.dat Ga19As19H42.mtx
- for MAT in torso3.mtx 
- # for MAT in Ga19As19H42.mtx   
-  do
-    # Start of looping stuff
-    > $TMP_BATCH_FILE
-    echo "#!/bin/bash -l" >> $TMP_BATCH_FILE
-    echo " " >> $TMP_BATCH_FILE
-    echo "#SBATCH -p $PARTITION" >> $TMP_BATCH_FILE
-    echo "#SBATCH -N $NODE_VAL" >>  $TMP_BATCH_FILE
-    echo "#SBATCH -t $TIME" >> $TMP_BATCH_FILE
-    echo "#SBATCH -L $LICENSE" >> $TMP_BATCH_FILE
-    echo "#SBATCH -J SLU_$MAT" >> $TMP_BATCH_FILE
-    #echo "#SBATCH -o ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_async_simple_over_icollec_flat_mrhs" >> $TMP_BATCH_FILE
-    #echo "#SBATCH -e ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_async_simple_over_icollec_flat_mrhs" >> $TMP_BATCH_FILE
-    # echo "#SBATCH --mail-type=BEGIN" >> $TMP_BATCH_FILE
-    # echo "#SBATCH --mail-type=END" >> $TMP_BATCH_FILE
-    echo "#SBATCH --mail-user=liuyangzhuan@lbl.gov" >> $TMP_BATCH_FILE
-    if [[ $NERSC_HOST == cori ]]
-    then
-      echo "#SBATCH -C $CONSTRAINT" >> $TMP_BATCH_FILE
-    fi
-    mkdir -p $MAT   
-    echo "export OMP_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE
-    echo "export KMP_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE
-    echo "export MKL_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE  																			
-    echo "export NSUP=128" >> $TMP_BATCH_FILE
-    echo "export NREL=20" >> $TMP_BATCH_FILE
-    
-    echo " " >> $TMP_BATCH_FILE
-    echo "FILE=$FILE" >> $TMP_BATCH_FILE
-    echo "FILEMAT=$INPUT_DIR/$MAT" >> $TMP_BATCH_FILE	
-    echo " " >> $TMP_BATCH_FILE
-    echo "CORE_VAL=$CORE_VAL" >> $TMP_BATCH_FILE
-    echo "NCOL=$NCOL" >> $TMP_BATCH_FILE
-    echo "NROW=$NROW" >> $TMP_BATCH_FILE
-    # This should be computed individually for each script...
-
-    srun -n $CORE_VAL $FILE -c $NCOL -r $NROW $INPUT_DIR/$MAT | tee ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_async_simple_over_icollec_groupgemm_bettertree_norecur
-    # Add final line (srun line) to temporary slurm script
-
-    #cat $TMP_BATCH_FILE
-    #echo " "
-    # sbatch $TMP_BATCH_FILE
-  done
-#one
-
-done
-
-
-exit $EXIT_SUCCESS
-
diff --git a/build/batch_script_mpi_runit7.sh b/build/batch_script_mpi_runit7.sh
deleted file mode 100644
index 5b579a4..0000000
--- a/build/batch_script_mpi_runit7.sh
+++ /dev/null
@@ -1,16 +0,0 @@
-#!/bin/bash
-# Bash script to submit many files to Cori/Edison/Queue
-
-EXIT_SUCCESS=0
-EXIT_HOST=1
-EXIT_PARAM=2
-
-
-for i in `seq 1 200`;
-do
-	srun -n 1 ./EXAMPLE/pddrive -r 1 -c 1 ./EXAMPLE/torso3.mtx | tee -a a.out
-done
-
-
-exit $EXIT_SUCCESS
-
diff --git a/build/batch_script_mpi_runit8.sh b/build/batch_script_mpi_runit8.sh
deleted file mode 100644
index 4496147..0000000
--- a/build/batch_script_mpi_runit8.sh
+++ /dev/null
@@ -1,18 +0,0 @@
-#!/bin/bash
-# Bash script to submit many files to Cori/Edison/Queue
-
-
-EXIT_SUCCESS=0
-EXIT_HOST=1
-EXIT_PARAM=2
-
-export MPICH_MAX_THREAD_SAFETY=multiple
-
-for i in `seq 1 200`;
-do
-	srun -n 2 ./EXAMPLE/pddrive -r 2 -c 1 ../EXAMPLE/big.rua | tee -a a.out
-done
-
-
-exit $EXIT_SUCCESS
-
diff --git a/build/batch_script_mpi_runit9.sh b/build/batch_script_mpi_runit9.sh
deleted file mode 100644
index 5d4f5f9..0000000
--- a/build/batch_script_mpi_runit9.sh
+++ /dev/null
@@ -1,18 +0,0 @@
-#!/bin/bash
-# Bash script to submit many files to Cori/Edison/Queue
-
-
-EXIT_SUCCESS=0
-EXIT_HOST=1
-EXIT_PARAM=2
-
-export MPICH_MAX_THREAD_SAFETY=multiple
-
-for i in `seq 1 200`;
-do
-	srun -n 2 ./EXAMPLE/pddrive -r 2 -c 1 ./EXAMPLE/torso3.mtx | tee -a a.out
-done
-
-
-exit $EXIT_SUCCESS
-
diff --git a/build/batch_script_mpi_runit_hybridOMPMPI.sh b/build/batch_script_mpi_runit_hybridOMPMPI.sh
deleted file mode 100644
index 64abef1..0000000
--- a/build/batch_script_mpi_runit_hybridOMPMPI.sh
+++ /dev/null
@@ -1,187 +0,0 @@
-#!/bin/bash
-# Bash script to submit many files to Cori/Edison/Queue
-
-EXIT_SUCCESS=0
-EXIT_HOST=1
-EXIT_PARAM=2
-
-# MAX_PARAMS=1
-# # ^^^ This should be fixed, as it should just loop through everything
-# if [[ $# -eq 0 ]]; then
-  # echo "Must have at least one parameter; exiting"
-  # exit $EXIT_PARAM
-# fi
-# if [[ $# -gt $MAX_PARAMS ]]; then
-  # echo "Too many parameters; exiting"
-  # exit $EXIT_PARAM
-# fi
-
-# INPUT_FILE=$1
-# # ^^^ Get the input ile
-
-CUR_DIR=`pwd`
-FILE_DIR=$CUR_DIR/EXAMPLE
-INPUT_DIR=/project/projectdirs/sparse/liuyangz/my_research/matrix
-FILE_NAME=pddrive
-FILE=$FILE_DIR/$FILE_NAME
-
-TMP_BATCH_FILE=tmp_batch_file.slurm
-# ^^^ Should check that this is not taken,
-#     but I would never use this, so ...
-
-> $TMP_BATCH_FILE
-
-if [[ $NERSC_HOST == edison ]]; then
-  CORES_PER_NODE=24
-  THREADS_PER_NODE=48
-elif [[ $NERSC_HOST == cori ]]; then
-  CORES_PER_NODE=32
-  THREADS_PER_NODE=64
-  # This does not take hyperthreading into account
-else
-  # Host unknown; exiting
-  exit $EXIT_HOST
-fi
-
-
-#nprows=(6 12 24)
-#npcols=(6 12 24)
-
-
-#nprows=(2048 1 32)
-#npcols=(1 2048 64)
-
-# nprows=(32 )
-# npcols=(64 )
-
-
-
-#nprows=(24 48 1 1 576 2304)
-#npcols=(24 48 576 2304 1 1)
-
-
-#nprows=(48  1  2304)
-#npcols=(48  2304 1)
-
-#nprows=(6 12 24 48 )
-#npcols=(6 12 24 48 )
-
-#nprows=(6 12 24 48 1 1 1 1 36 144 576 2304)
-#npcols=(6 12 24 48 36 144 576 2304 1 1 1 1)
-
-#nprows=(32 128 512 1 1 1 4 8 16)
-#npcols=(1 1 1 32 128 512 8 16 32)
-
-#nprows=(2048 1 32)
-#npcols=(1 2048 64)
-
-
-
-
-#nprows=(12 1 144)
-#npcols=(12 144 1)
-
-
-nprows=(1 2 2 4)
-npcols=(1 1 2 4)
- 
-for ((i = 0; i < ${#npcols[@]}; i++)); do
-NROW=${nprows[i]}
-NCOL=${npcols[i]}
-
-# NROW=36
-CORE_VAL=`expr $NCOL \* $NROW`
-NODE_VAL=`expr $CORE_VAL / $CORES_PER_NODE`
-MOD_VAL=`expr $CORE_VAL % $CORES_PER_NODE`
-if [[ $MOD_VAL -ne 0 ]]
-then
-  NODE_VAL=`expr $NODE_VAL + 1`
-fi
-#PARTITION=debug
-PARTITION=regular
-LICENSE=SCRATCH
-TIME=00:20:00
-
-if [[ $NERSC_HOST == edison ]]
-then
-  CONSTRAINT=0
-fi
-if [[ $NERSC_HOST == cori ]]
-then
-  CONSTRAINT=haswell
-fi
-
-
-#for NTH in 8 
-for NTH in 1 2 4 8 
-do
-OMP_NUM_THREADS=$NTH
-TH_PER_RANK=`expr $NTH \* 2`
-
-
-#for NSUP in 128 64 32 16 8
-#do
-  # for MAT in atmosmodl.rb nlpkkt80.mtx torso3.mtx Ga19As19H42.mtx A22.mtx cage13.rb 
-  for MAT in torso3.bin
-  # for MAT in matrix121.dat matrix211.dat tdr190k.dat tdr455k.dat nlpkkt80.mtx torso3.mtx helm2d03.mtx  
-  # for MAT in tdr190k.dat Ga19As19H42.mtx
- # for MAT in torso3.mtx hvdc2.mtx matrix121.dat nlpkkt80.mtx helm2d03.mtx
-# for MAT in A22.bin DG_GrapheneDisorder_8192.bin DNA_715_64cell.bin LU_C_BN_C_4by2.bin Li4244.bin atmosmodj.bin Ga19As19H42.bin Geo_1438.bin StocF-1465.bin    
-  # for MAT in  A22.bin DNA_715_64cell.bin LU_C_BN_C_4by2.bin
- # for MAT in Ga19As19H42.mtx   
-  do
-    # Start of looping stuff
-    > $TMP_BATCH_FILE
-    echo "#!/bin/bash -l" >> $TMP_BATCH_FILE
-    echo " " >> $TMP_BATCH_FILE
-    echo "#SBATCH -p $PARTITION" >> $TMP_BATCH_FILE
-    echo "#SBATCH -N $NODE_VAL" >>  $TMP_BATCH_FILE
-    echo "#SBATCH -t $TIME" >> $TMP_BATCH_FILE
-    echo "#SBATCH -L $LICENSE" >> $TMP_BATCH_FILE
-    echo "#SBATCH -J SLU_$MAT" >> $TMP_BATCH_FILE
-    #echo "#SBATCH -o ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_async_simple_over_icollec_flat_mrhs" >> $TMP_BATCH_FILE
-    #echo "#SBATCH -e ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_async_simple_over_icollec_flat_mrhs" >> $TMP_BATCH_FILE
-    # echo "#SBATCH --mail-type=BEGIN" >> $TMP_BATCH_FILE
-    # echo "#SBATCH --mail-type=END" >> $TMP_BATCH_FILE
-    echo "#SBATCH --mail-user=liuyangzhuan@lbl.gov" >> $TMP_BATCH_FILE
-    if [[ $NERSC_HOST == cori ]]
-    then
-      echo "#SBATCH -C $CONSTRAINT" >> $TMP_BATCH_FILE
-    fi
-    mkdir -p $MAT   
-    echo "export OMP_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE
-    echo "export KMP_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE
-    echo "export MKL_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE  																			
-    echo "export NSUP=128" >> $TMP_BATCH_FILE
-    echo "export NREL=20" >> $TMP_BATCH_FILE
-    echo "export OMP_PLACES=threads" >> $TMP_BATCH_FILE
-    echo "export OMP_PROC_BIND=spread" >> $TMP_BATCH_FILE
-    echo "export MPICH_MAX_THREAD_SAFETY=multiple" >> $TMP_BATCH_FILE
-
-    echo " " >> $TMP_BATCH_FILE
-    echo "FILE=$FILE" >> $TMP_BATCH_FILE
-    echo "FILEMAT=$INPUT_DIR/$MAT" >> $TMP_BATCH_FILE	
-    echo " " >> $TMP_BATCH_FILE
-    echo "CORE_VAL=$CORE_VAL" >> $TMP_BATCH_FILE
-    echo "NCOL=$NCOL" >> $TMP_BATCH_FILE
-    echo "NROW=$NROW" >> $TMP_BATCH_FILE
-    # This should be computed individually for each script...
-
-    export OMP_NUM_THREADS=$OMP_NUM_THREADS
-    export OMP_PLACES=threads
-    export OMP_PROC_BIND=spread
-    export MPICH_MAX_THREAD_SAFETY=multiple
-    srun -n $CORE_VAL -N 4 -c $TH_PER_RANK --cpu_bind=cores $FILE -c $NCOL -r $NROW $INPUT_DIR/$MAT | tee ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_${OMP_NUM_THREADS}_mrhs
-    # Add final line (srun line) to temporary slurm script
-
-    #cat $TMP_BATCH_FILE
-    #echo " "
-    # sbatch $TMP_BATCH_FILE
-  done
-#one
-
-done
-done
-
-exit $EXIT_SUCCESS
-
diff --git a/build/batch_script_mpi_runit_hybridOMPMPI5.sh b/build/batch_script_mpi_runit_hybridOMPMPI5.sh
deleted file mode 100644
index b13017e..0000000
--- a/build/batch_script_mpi_runit_hybridOMPMPI5.sh
+++ /dev/null
@@ -1,187 +0,0 @@
-#!/bin/bash
-# Bash script to submit many files to Cori/Edison/Queue
-
-EXIT_SUCCESS=0
-EXIT_HOST=1
-EXIT_PARAM=2
-
-# MAX_PARAMS=1
-# # ^^^ This should be fixed, as it should just loop through everything
-# if [[ $# -eq 0 ]]; then
-  # echo "Must have at least one parameter; exiting"
-  # exit $EXIT_PARAM
-# fi
-# if [[ $# -gt $MAX_PARAMS ]]; then
-  # echo "Too many parameters; exiting"
-  # exit $EXIT_PARAM
-# fi
-
-# INPUT_FILE=$1
-# # ^^^ Get the input ile
-
-CUR_DIR=`pwd`
-FILE_DIR=$CUR_DIR/EXAMPLE
-INPUT_DIR=/project/projectdirs/sparse/liuyangz/my_research/matrix
-FILE_NAME=pddrive
-FILE=$FILE_DIR/$FILE_NAME
-
-TMP_BATCH_FILE=tmp_batch_file.slurm
-# ^^^ Should check that this is not taken,
-#     but I would never use this, so ...
-
-> $TMP_BATCH_FILE
-
-if [[ $NERSC_HOST == edison ]]; then
-  CORES_PER_NODE=24
-  THREADS_PER_NODE=48
-elif [[ $NERSC_HOST == cori ]]; then
-  CORES_PER_NODE=32
-  THREADS_PER_NODE=64
-  # This does not take hyperthreading into account
-else
-  # Host unknown; exiting
-  exit $EXIT_HOST
-fi
-
-
-#nprows=(6 12 24)
-#npcols=(6 12 24)
-
-
-#nprows=(2048 1 32)
-#npcols=(1 2048 64)
-
-# nprows=(32 )
-# npcols=(64 )
-
-
-
-#nprows=(24 48 1 1 576 2304)
-#npcols=(24 48 576 2304 1 1)
-
-
-#nprows=(48  1  2304)
-#npcols=(48  2304 1)
-
-#nprows=(6 12 24 48 )
-#npcols=(6 12 24 48 )
-
-#nprows=(6 12 24 48 1 1 1 1 36 144 576 2304)
-#npcols=(6 12 24 48 36 144 576 2304 1 1 1 1)
-
-#nprows=(32 128 512 1 1 1 4 8 16)
-#npcols=(1 1 1 32 128 512 8 16 32)
-
-#nprows=(2048 1 32)
-#npcols=(1 2048 64)
-
-
-
-
-#nprows=(12 1 144)
-#npcols=(12 144 1)
-
-
-nprows=(4)
-npcols=(8)
- 
-for ((i = 0; i < ${#npcols[@]}; i++)); do
-NROW=${nprows[i]}
-NCOL=${npcols[i]}
-
-# NROW=36
-CORE_VAL=`expr $NCOL \* $NROW`
-NODE_VAL=`expr $CORE_VAL / $CORES_PER_NODE`
-MOD_VAL=`expr $CORE_VAL % $CORES_PER_NODE`
-if [[ $MOD_VAL -ne 0 ]]
-then
-  NODE_VAL=`expr $NODE_VAL + 1`
-fi
-#PARTITION=debug
-PARTITION=regular
-LICENSE=SCRATCH
-TIME=00:20:00
-
-if [[ $NERSC_HOST == edison ]]
-then
-  CONSTRAINT=0
-fi
-if [[ $NERSC_HOST == cori ]]
-then
-  CONSTRAINT=haswell
-fi
-
-
-for NTH in 1  
-do
-
-OMP_NUM_THREADS=$NTH
-TH_PER_RANK=`expr $NTH \* 2`
-
-
-#for NSUP in 128 64 32 16 8
-#do
-  # for MAT in atmosmodl.rb nlpkkt80.mtx torso3.mtx Ga19As19H42.mtx A22.mtx cage13.rb 
-  for MAT in torso3.bin
-  # for MAT in matrix121.dat matrix211.dat tdr190k.dat tdr455k.dat nlpkkt80.mtx torso3.mtx helm2d03.mtx  
-  # for MAT in tdr190k.dat Ga19As19H42.mtx
- # for MAT in torso3.mtx hvdc2.mtx matrix121.dat nlpkkt80.mtx helm2d03.mtx
-# for MAT in A22.bin DG_GrapheneDisorder_8192.bin DNA_715_64cell.bin LU_C_BN_C_4by2.bin Li4244.bin atmosmodj.bin Ga19As19H42.bin Geo_1438.bin StocF-1465.bin    
-  # for MAT in  A22.bin DNA_715_64cell.bin LU_C_BN_C_4by2.bin
- # for MAT in Ga19As19H42.mtx   
-  do
-    # Start of looping stuff
-    > $TMP_BATCH_FILE
-    echo "#!/bin/bash -l" >> $TMP_BATCH_FILE
-    echo " " >> $TMP_BATCH_FILE
-    echo "#SBATCH -p $PARTITION" >> $TMP_BATCH_FILE
-    echo "#SBATCH -N $NODE_VAL" >>  $TMP_BATCH_FILE
-    echo "#SBATCH -t $TIME" >> $TMP_BATCH_FILE
-    echo "#SBATCH -L $LICENSE" >> $TMP_BATCH_FILE
-    echo "#SBATCH -J SLU_$MAT" >> $TMP_BATCH_FILE
-    #echo "#SBATCH -o ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_async_simple_over_icollec_flat_mrhs" >> $TMP_BATCH_FILE
-    #echo "#SBATCH -e ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_async_simple_over_icollec_flat_mrhs" >> $TMP_BATCH_FILE
-    # echo "#SBATCH --mail-type=BEGIN" >> $TMP_BATCH_FILE
-    # echo "#SBATCH --mail-type=END" >> $TMP_BATCH_FILE
-    echo "#SBATCH --mail-user=liuyangzhuan@lbl.gov" >> $TMP_BATCH_FILE
-    if [[ $NERSC_HOST == cori ]]
-    then
-      echo "#SBATCH -C $CONSTRAINT" >> $TMP_BATCH_FILE
-    fi
-    mkdir -p $MAT   
-    echo "export OMP_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE
-    echo "export KMP_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE
-    echo "export MKL_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE  																			
-    echo "export NSUP=128" >> $TMP_BATCH_FILE
-    echo "export NREL=20" >> $TMP_BATCH_FILE
-    echo "export OMP_PLACES=threads" >> $TMP_BATCH_FILE
-    echo "export OMP_PROC_BIND=spread" >> $TMP_BATCH_FILE
-    echo "export MPICH_MAX_THREAD_SAFETY=multiple" >> $TMP_BATCH_FILE
-
-    echo " " >> $TMP_BATCH_FILE
-    echo "FILE=$FILE" >> $TMP_BATCH_FILE
-    echo "FILEMAT=$INPUT_DIR/$MAT" >> $TMP_BATCH_FILE	
-    echo " " >> $TMP_BATCH_FILE
-    echo "CORE_VAL=$CORE_VAL" >> $TMP_BATCH_FILE
-    echo "NCOL=$NCOL" >> $TMP_BATCH_FILE
-    echo "NROW=$NROW" >> $TMP_BATCH_FILE
-    # This should be computed individually for each script...
-
-    export OMP_NUM_THREADS=$OMP_NUM_THREADS
-    export OMP_PLACES=threads
-    export OMP_PROC_BIND=spread
-    export MPICH_MAX_THREAD_SAFETY=multiple
-    srun -n $CORE_VAL -N 2 -c $TH_PER_RANK --cpu_bind=cores $FILE -c $NCOL -r $NROW $INPUT_DIR/$MAT | tee ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_${OMP_NUM_THREADS}_mrhs
-    # Add final line (srun line) to temporary slurm script
-
-    #cat $TMP_BATCH_FILE
-    #echo " "
-    # sbatch $TMP_BATCH_FILE
-  done
-#one
-
-done
-done
-
-exit $EXIT_SUCCESS
-
diff --git a/build/batch_script_mpi_runit_hybridOMPMPI_2.sh b/build/batch_script_mpi_runit_hybridOMPMPI_2.sh
deleted file mode 100644
index cde2b97..0000000
--- a/build/batch_script_mpi_runit_hybridOMPMPI_2.sh
+++ /dev/null
@@ -1,186 +0,0 @@
-#!/bin/bash
-# Bash script to submit many files to Cori/Edison/Queue
-
-EXIT_SUCCESS=0
-EXIT_HOST=1
-EXIT_PARAM=2
-
-# MAX_PARAMS=1
-# # ^^^ This should be fixed, as it should just loop through everything
-# if [[ $# -eq 0 ]]; then
-  # echo "Must have at least one parameter; exiting"
-  # exit $EXIT_PARAM
-# fi
-# if [[ $# -gt $MAX_PARAMS ]]; then
-  # echo "Too many parameters; exiting"
-  # exit $EXIT_PARAM
-# fi
-
-# INPUT_FILE=$1
-# # ^^^ Get the input ile
-
-CUR_DIR=`pwd`
-FILE_DIR=$CUR_DIR/EXAMPLE
-INPUT_DIR=/project/projectdirs/sparse/liuyangz/my_research/matrix
-FILE_NAME=pddrive
-FILE=$FILE_DIR/$FILE_NAME
-
-TMP_BATCH_FILE=tmp_batch_file.slurm
-# ^^^ Should check that this is not taken,
-#     but I would never use this, so ...
-
-> $TMP_BATCH_FILE
-
-if [[ $NERSC_HOST == edison ]]; then
-  CORES_PER_NODE=24
-  THREADS_PER_NODE=48
-elif [[ $NERSC_HOST == cori ]]; then
-  CORES_PER_NODE=32
-  THREADS_PER_NODE=64
-  # This does not take hyperthreading into account
-else
-  # Host unknown; exiting
-  exit $EXIT_HOST
-fi
-
-
-#nprows=(6 12 24)
-#npcols=(6 12 24)
-
-
-#nprows=(2048 1 32)
-#npcols=(1 2048 64)
-
-# nprows=(32 )
-# npcols=(64 )
-
-
-
-#nprows=(24 48 1 1 576 2304)
-#npcols=(24 48 576 2304 1 1)
-
-
-#nprows=(48  1  2304)
-#npcols=(48  2304 1)
-
-#nprows=(6 12 24 48 )
-#npcols=(6 12 24 48 )
-
-#nprows=(6 12 24 48 1 1 1 1 36 144 576 2304)
-#npcols=(6 12 24 48 36 144 576 2304 1 1 1 1)
-
-#nprows=(32 128 512 1 1 1 4 8 16)
-#npcols=(1 1 1 32 128 512 8 16 32)
-
-#nprows=(2048 1 32)
-#npcols=(1 2048 64)
-
-
-
-
-#nprows=(12 1 144)
-#npcols=(12 144 1)
-
-
-nprows=(4)
-npcols=(4)
- 
-for ((i = 0; i < ${#npcols[@]}; i++)); do
-NROW=${nprows[i]}
-NCOL=${npcols[i]}
-
-# NROW=36
-CORE_VAL=`expr $NCOL \* $NROW`
-NODE_VAL=`expr $CORE_VAL / $CORES_PER_NODE`
-MOD_VAL=`expr $CORE_VAL % $CORES_PER_NODE`
-if [[ $MOD_VAL -ne 0 ]]
-then
-  NODE_VAL=`expr $NODE_VAL + 1`
-fi
-#PARTITION=debug
-PARTITION=regular
-LICENSE=SCRATCH
-TIME=00:20:00
-
-if [[ $NERSC_HOST == edison ]]
-then
-  CONSTRAINT=0
-fi
-if [[ $NERSC_HOST == cori ]]
-then
-  CONSTRAINT=haswell
-fi
-
-
-for NTH in 1 2 4 
-do
-
-OMP_NUM_THREADS=$NTH
-TH_PER_RANK=`expr $NTH \* 2`
-
-
-#for NSUP in 128 64 32 16 8
-#do
-  # for MAT in atmosmodl.rb nlpkkt80.mtx torso3.mtx Ga19As19H42.mtx A22.mtx cage13.rb 
-  # for MAT in torso3.mtx
-  # for MAT in matrix121.dat matrix211.dat tdr190k.dat tdr455k.dat nlpkkt80.mtx torso3.mtx helm2d03.mtx  
-  # for MAT in tdr190k.dat Ga19As19H42.mtx
- # for MAT in torso3.mtx hvdc2.mtx matrix121.dat nlpkkt80.mtx helm2d03.mtx
-   for MAT in DG_GrapheneDisorder_8192.bin Li4244.bin torso3.bin
- # for MAT in Ga19As19H42.mtx   
-  do
-    # Start of looping stuff
-    > $TMP_BATCH_FILE
-    echo "#!/bin/bash -l" >> $TMP_BATCH_FILE
-    echo " " >> $TMP_BATCH_FILE
-    echo "#SBATCH -p $PARTITION" >> $TMP_BATCH_FILE
-    echo "#SBATCH -N $NODE_VAL" >>  $TMP_BATCH_FILE
-    echo "#SBATCH -t $TIME" >> $TMP_BATCH_FILE
-    echo "#SBATCH -L $LICENSE" >> $TMP_BATCH_FILE
-    echo "#SBATCH -J SLU_$MAT" >> $TMP_BATCH_FILE
-    #echo "#SBATCH -o ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_async_simple_over_icollec_flat_mrhs" >> $TMP_BATCH_FILE
-    #echo "#SBATCH -e ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_async_simple_over_icollec_flat_mrhs" >> $TMP_BATCH_FILE
-    # echo "#SBATCH --mail-type=BEGIN" >> $TMP_BATCH_FILE
-    # echo "#SBATCH --mail-type=END" >> $TMP_BATCH_FILE
-    echo "#SBATCH --mail-user=liuyangzhuan@lbl.gov" >> $TMP_BATCH_FILE
-    if [[ $NERSC_HOST == cori ]]
-    then
-      echo "#SBATCH -C $CONSTRAINT" >> $TMP_BATCH_FILE
-    fi
-    mkdir -p $MAT   
-    echo "export OMP_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE
-    echo "export KMP_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE
-    echo "export MKL_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE  																			
-    echo "export NSUP=128" >> $TMP_BATCH_FILE
-    echo "export NREL=20" >> $TMP_BATCH_FILE
-    echo "export OMP_PLACES=threads" >> $TMP_BATCH_FILE
-    echo "export OMP_PROC_BIND=spread" >> $TMP_BATCH_FILE
-    echo "export MPICH_MAX_THREAD_SAFETY=multiple" >> $TMP_BATCH_FILE
-
-    echo " " >> $TMP_BATCH_FILE
-    echo "FILE=$FILE" >> $TMP_BATCH_FILE
-    echo "FILEMAT=$INPUT_DIR/$MAT" >> $TMP_BATCH_FILE	
-    echo " " >> $TMP_BATCH_FILE
-    echo "CORE_VAL=$CORE_VAL" >> $TMP_BATCH_FILE
-    echo "NCOL=$NCOL" >> $TMP_BATCH_FILE
-    echo "NROW=$NROW" >> $TMP_BATCH_FILE
-    # This should be computed individually for each script...
-
-    export OMP_NUM_THREADS=$OMP_NUM_THREADS
-    export OMP_PLACES=threads
-    export OMP_PROC_BIND=spread
-    export MPICH_MAX_THREAD_SAFETY=multiple
-    srun -n $CORE_VAL -N 64 -c $TH_PER_RANK --cpu_bind=cores $FILE -c $NCOL -r $NROW $INPUT_DIR/$MAT | tee ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_hybridOMPMPI_${OMP_NUM_THREADS}_cori
-    # Add final line (srun line) to temporary slurm script
-
-    #cat $TMP_BATCH_FILE
-    #echo " "
-    # sbatch $TMP_BATCH_FILE
-  done
-#one
-
-done
-done
-
-exit $EXIT_SUCCESS
-
diff --git a/build/batch_script_mpi_runit_hybridOMPMPI_3.sh b/build/batch_script_mpi_runit_hybridOMPMPI_3.sh
deleted file mode 100644
index d3a087d..0000000
--- a/build/batch_script_mpi_runit_hybridOMPMPI_3.sh
+++ /dev/null
@@ -1,187 +0,0 @@
-#!/bin/bash
-# Bash script to submit many files to Cori/Edison/Queue
-
-EXIT_SUCCESS=0
-EXIT_HOST=1
-EXIT_PARAM=2
-
-# MAX_PARAMS=1
-# # ^^^ This should be fixed, as it should just loop through everything
-# if [[ $# -eq 0 ]]; then
-  # echo "Must have at least one parameter; exiting"
-  # exit $EXIT_PARAM
-# fi
-# if [[ $# -gt $MAX_PARAMS ]]; then
-  # echo "Too many parameters; exiting"
-  # exit $EXIT_PARAM
-# fi
-
-# INPUT_FILE=$1
-# # ^^^ Get the input ile
-
-CUR_DIR=`pwd`
-FILE_DIR=$CUR_DIR/EXAMPLE
-INPUT_DIR=/project/projectdirs/sparse/liuyangz/my_research/matrix
-FILE_NAME=pddrive
-FILE=$FILE_DIR/$FILE_NAME
-
-TMP_BATCH_FILE=tmp_batch_file.slurm
-# ^^^ Should check that this is not taken,
-#     but I would never use this, so ...
-
-> $TMP_BATCH_FILE
-
-if [[ $NERSC_HOST == edison ]]; then
-  CORES_PER_NODE=24
-  THREADS_PER_NODE=48
-elif [[ $NERSC_HOST == cori ]]; then
-  CORES_PER_NODE=32
-  THREADS_PER_NODE=64
-  # This does not take hyperthreading into account
-else
-  # Host unknown; exiting
-  exit $EXIT_HOST
-fi
-
-
-#nprows=(6 12 24)
-#npcols=(6 12 24)
-
-
-#nprows=(2048 1 32)
-#npcols=(1 2048 64)
-
-# nprows=(32 )
-# npcols=(64 )
-
-
-
-#nprows=(24 48 1 1 576 2304)
-#npcols=(24 48 576 2304 1 1)
-
-
-#nprows=(48  1  2304)
-#npcols=(48  2304 1)
-
-#nprows=(6 12 24 48 )
-#npcols=(6 12 24 48 )
-
-#nprows=(6 12 24 48 1 1 1 1 36 144 576 2304)
-#npcols=(6 12 24 48 36 144 576 2304 1 1 1 1)
-
-#nprows=(32 128 512 1 1 1 4 8 16)
-#npcols=(1 1 1 32 128 512 8 16 32)
-
-#nprows=(2048 1 32)
-#npcols=(1 2048 64)
-
-
-
-
-#nprows=(12 1 144)
-#npcols=(12 144 1)
-
-
-nprows=(16)
-npcols=(16)
- 
-for ((i = 0; i < ${#npcols[@]}; i++)); do
-NROW=${nprows[i]}
-NCOL=${npcols[i]}
-
-# NROW=36
-CORE_VAL=`expr $NCOL \* $NROW`
-NODE_VAL=`expr $CORE_VAL / $CORES_PER_NODE`
-MOD_VAL=`expr $CORE_VAL % $CORES_PER_NODE`
-if [[ $MOD_VAL -ne 0 ]]
-then
-  NODE_VAL=`expr $NODE_VAL + 1`
-fi
-#PARTITION=debug
-PARTITION=regular
-LICENSE=SCRATCH
-TIME=00:20:00
-
-if [[ $NERSC_HOST == edison ]]
-then
-  CONSTRAINT=0
-fi
-if [[ $NERSC_HOST == cori ]]
-then
-  CONSTRAINT=haswell
-fi
-
-
-for NTH in 1 2 4 
-do
-
-OMP_NUM_THREADS=$NTH
-TH_PER_RANK=`expr $NTH \* 2`
-
-
-#for NSUP in 128 64 32 16 8
-#do
-  # for MAT in atmosmodl.rb nlpkkt80.mtx torso3.mtx Ga19As19H42.mtx A22.mtx cage13.rb 
-  # for MAT in torso3.mtx
-  # for MAT in matrix121.dat matrix211.dat tdr190k.dat tdr455k.dat nlpkkt80.mtx torso3.mtx helm2d03.mtx  
-  # for MAT in tdr190k.dat Ga19As19H42.mtx
- for MAT in torso3.bin
-# for MAT in A22.bin DG_GrapheneDisorder_8192.bin DNA_715_64cell.bin LU_C_BN_C_4by2.bin Li4244.bin atmosmodj.bin Ga19As19H42.bin Geo_1438.bin StocF-1465.bin    
-  # for MAT in  A22.bin DNA_715_64cell.bin LU_C_BN_C_4by2.bin
- # for MAT in Ga19As19H42.mtx   
-  do
-    # Start of looping stuff
-    > $TMP_BATCH_FILE
-    echo "#!/bin/bash -l" >> $TMP_BATCH_FILE
-    echo " " >> $TMP_BATCH_FILE
-    echo "#SBATCH -p $PARTITION" >> $TMP_BATCH_FILE
-    echo "#SBATCH -N $NODE_VAL" >>  $TMP_BATCH_FILE
-    echo "#SBATCH -t $TIME" >> $TMP_BATCH_FILE
-    echo "#SBATCH -L $LICENSE" >> $TMP_BATCH_FILE
-    echo "#SBATCH -J SLU_$MAT" >> $TMP_BATCH_FILE
-    #echo "#SBATCH -o ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_async_simple_over_icollec_flat_mrhs" >> $TMP_BATCH_FILE
-    #echo "#SBATCH -e ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_async_simple_over_icollec_flat_mrhs" >> $TMP_BATCH_FILE
-    # echo "#SBATCH --mail-type=BEGIN" >> $TMP_BATCH_FILE
-    # echo "#SBATCH --mail-type=END" >> $TMP_BATCH_FILE
-    echo "#SBATCH --mail-user=liuyangzhuan@lbl.gov" >> $TMP_BATCH_FILE
-    if [[ $NERSC_HOST == cori ]]
-    then
-      echo "#SBATCH -C $CONSTRAINT" >> $TMP_BATCH_FILE
-    fi
-    mkdir -p $MAT   
-    echo "export OMP_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE
-    echo "export KMP_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE
-    echo "export MKL_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE  																			
-    echo "export NSUP=128" >> $TMP_BATCH_FILE
-    echo "export NREL=20" >> $TMP_BATCH_FILE
-    echo "export OMP_PLACES=threads" >> $TMP_BATCH_FILE
-    echo "export OMP_PROC_BIND=spread" >> $TMP_BATCH_FILE
-    echo "export MPICH_MAX_THREAD_SAFETY=multiple" >> $TMP_BATCH_FILE
-
-    echo " " >> $TMP_BATCH_FILE
-    echo "FILE=$FILE" >> $TMP_BATCH_FILE
-    echo "FILEMAT=$INPUT_DIR/$MAT" >> $TMP_BATCH_FILE	
-    echo " " >> $TMP_BATCH_FILE
-    echo "CORE_VAL=$CORE_VAL" >> $TMP_BATCH_FILE
-    echo "NCOL=$NCOL" >> $TMP_BATCH_FILE
-    echo "NROW=$NROW" >> $TMP_BATCH_FILE
-    # This should be computed individually for each script...
-
-    export OMP_NUM_THREADS=$OMP_NUM_THREADS
-    export OMP_PLACES=threads
-    export OMP_PROC_BIND=spread
-    export MPICH_MAX_THREAD_SAFETY=multiple
-    srun -n $CORE_VAL -N 32 -c $TH_PER_RANK --cpu_bind=cores $FILE -c $NCOL -r $NROW $INPUT_DIR/$MAT | tee ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_${OMP_NUM_THREADS}
-    # Add final line (srun line) to temporary slurm script
-
-    #cat $TMP_BATCH_FILE
-    #echo " "
-    # sbatch $TMP_BATCH_FILE
-  done
-#one
-
-done
-done
-
-exit $EXIT_SUCCESS
-
diff --git a/build/batch_script_mpi_runit_hybridOMPMPI_4.sh b/build/batch_script_mpi_runit_hybridOMPMPI_4.sh
deleted file mode 100644
index 28109df..0000000
--- a/build/batch_script_mpi_runit_hybridOMPMPI_4.sh
+++ /dev/null
@@ -1,187 +0,0 @@
-#!/bin/bash
-# Bash script to submit many files to Cori/Edison/Queue
-
-EXIT_SUCCESS=0
-EXIT_HOST=1
-EXIT_PARAM=2
-
-# MAX_PARAMS=1
-# # ^^^ This should be fixed, as it should just loop through everything
-# if [[ $# -eq 0 ]]; then
-  # echo "Must have at least one parameter; exiting"
-  # exit $EXIT_PARAM
-# fi
-# if [[ $# -gt $MAX_PARAMS ]]; then
-  # echo "Too many parameters; exiting"
-  # exit $EXIT_PARAM
-# fi
-
-# INPUT_FILE=$1
-# # ^^^ Get the input ile
-
-CUR_DIR=`pwd`
-FILE_DIR=$CUR_DIR/EXAMPLE
-INPUT_DIR=/project/projectdirs/sparse/liuyangz/my_research/matrix
-FILE_NAME=pddrive
-FILE=$FILE_DIR/$FILE_NAME
-
-TMP_BATCH_FILE=tmp_batch_file.slurm
-# ^^^ Should check that this is not taken,
-#     but I would never use this, so ...
-
-> $TMP_BATCH_FILE
-
-if [[ $NERSC_HOST == edison ]]; then
-  CORES_PER_NODE=24
-  THREADS_PER_NODE=48
-elif [[ $NERSC_HOST == cori ]]; then
-  CORES_PER_NODE=32
-  THREADS_PER_NODE=64
-  # This does not take hyperthreading into account
-else
-  # Host unknown; exiting
-  exit $EXIT_HOST
-fi
-
-
-#nprows=(6 12 24)
-#npcols=(6 12 24)
-
-
-#nprows=(2048 1 32)
-#npcols=(1 2048 64)
-
-# nprows=(32 )
-# npcols=(64 )
-
-
-
-#nprows=(24 48 1 1 576 2304)
-#npcols=(24 48 576 2304 1 1)
-
-
-#nprows=(48  1  2304)
-#npcols=(48  2304 1)
-
-#nprows=(6 12 24 48 )
-#npcols=(6 12 24 48 )
-
-#nprows=(6 12 24 48 1 1 1 1 36 144 576 2304)
-#npcols=(6 12 24 48 36 144 576 2304 1 1 1 1)
-
-#nprows=(32 128 512 1 1 1 4 8 16)
-#npcols=(1 1 1 32 128 512 8 16 32)
-
-#nprows=(2048 1 32)
-#npcols=(1 2048 64)
-
-
-
-
-#nprows=(12 1 144)
-#npcols=(12 144 1)
-
-
-nprows=(4)
-npcols=(4)
- 
-for ((i = 0; i < ${#npcols[@]}; i++)); do
-NROW=${nprows[i]}
-NCOL=${npcols[i]}
-
-# NROW=36
-CORE_VAL=`expr $NCOL \* $NROW`
-NODE_VAL=`expr $CORE_VAL / $CORES_PER_NODE`
-MOD_VAL=`expr $CORE_VAL % $CORES_PER_NODE`
-if [[ $MOD_VAL -ne 0 ]]
-then
-  NODE_VAL=`expr $NODE_VAL + 1`
-fi
-#PARTITION=debug
-PARTITION=regular
-LICENSE=SCRATCH
-TIME=00:20:00
-
-if [[ $NERSC_HOST == edison ]]
-then
-  CONSTRAINT=0
-fi
-if [[ $NERSC_HOST == cori ]]
-then
-  CONSTRAINT=haswell
-fi
-
-
-for NTH in 1 2 4 
-do
-
-OMP_NUM_THREADS=$NTH
-TH_PER_RANK=`expr $NTH \* 2`
-
-
-#for NSUP in 128 64 32 16 8
-#do
-  # for MAT in atmosmodl.rb nlpkkt80.mtx torso3.mtx Ga19As19H42.mtx A22.mtx cage13.rb 
-  # for MAT in torso3.mtx
-  # for MAT in matrix121.dat matrix211.dat tdr190k.dat tdr455k.dat nlpkkt80.mtx torso3.mtx helm2d03.mtx  
-  # for MAT in tdr190k.dat Ga19As19H42.mtx
- for MAT in torso3.bin
-# for MAT in A22.bin DG_GrapheneDisorder_8192.bin DNA_715_64cell.bin LU_C_BN_C_4by2.bin Li4244.bin atmosmodj.bin Ga19As19H42.bin Geo_1438.bin StocF-1465.bin    
-  # for MAT in  A22.bin DNA_715_64cell.bin LU_C_BN_C_4by2.bin
- # for MAT in Ga19As19H42.mtx   
-  do
-    # Start of looping stuff
-    > $TMP_BATCH_FILE
-    echo "#!/bin/bash -l" >> $TMP_BATCH_FILE
-    echo " " >> $TMP_BATCH_FILE
-    echo "#SBATCH -p $PARTITION" >> $TMP_BATCH_FILE
-    echo "#SBATCH -N $NODE_VAL" >>  $TMP_BATCH_FILE
-    echo "#SBATCH -t $TIME" >> $TMP_BATCH_FILE
-    echo "#SBATCH -L $LICENSE" >> $TMP_BATCH_FILE
-    echo "#SBATCH -J SLU_$MAT" >> $TMP_BATCH_FILE
-    #echo "#SBATCH -o ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_async_simple_over_icollec_flat_mrhs" >> $TMP_BATCH_FILE
-    #echo "#SBATCH -e ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_async_simple_over_icollec_flat_mrhs" >> $TMP_BATCH_FILE
-    # echo "#SBATCH --mail-type=BEGIN" >> $TMP_BATCH_FILE
-    # echo "#SBATCH --mail-type=END" >> $TMP_BATCH_FILE
-    echo "#SBATCH --mail-user=liuyangzhuan@lbl.gov" >> $TMP_BATCH_FILE
-    if [[ $NERSC_HOST == cori ]]
-    then
-      echo "#SBATCH -C $CONSTRAINT" >> $TMP_BATCH_FILE
-    fi
-    mkdir -p $MAT   
-    echo "export OMP_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE
-    echo "export KMP_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE
-    echo "export MKL_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE  																			
-    echo "export NSUP=128" >> $TMP_BATCH_FILE
-    echo "export NREL=20" >> $TMP_BATCH_FILE
-    echo "export OMP_PLACES=threads" >> $TMP_BATCH_FILE
-    echo "export OMP_PROC_BIND=spread" >> $TMP_BATCH_FILE
-    echo "export MPICH_MAX_THREAD_SAFETY=multiple" >> $TMP_BATCH_FILE
-
-    echo " " >> $TMP_BATCH_FILE
-    echo "FILE=$FILE" >> $TMP_BATCH_FILE
-    echo "FILEMAT=$INPUT_DIR/$MAT" >> $TMP_BATCH_FILE	
-    echo " " >> $TMP_BATCH_FILE
-    echo "CORE_VAL=$CORE_VAL" >> $TMP_BATCH_FILE
-    echo "NCOL=$NCOL" >> $TMP_BATCH_FILE
-    echo "NROW=$NROW" >> $TMP_BATCH_FILE
-    # This should be computed individually for each script...
-
-    export OMP_NUM_THREADS=$OMP_NUM_THREADS
-    export OMP_PLACES=threads
-    export OMP_PROC_BIND=spread
-    export MPICH_MAX_THREAD_SAFETY=multiple
-    srun -n $CORE_VAL -N 4 -c $TH_PER_RANK --cpu_bind=cores $FILE -c $NCOL -r $NROW $INPUT_DIR/$MAT | tee ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_${OMP_NUM_THREADS}
-    # Add final line (srun line) to temporary slurm script
-
-    #cat $TMP_BATCH_FILE
-    #echo " "
-    # sbatch $TMP_BATCH_FILE
-  done
-#one
-
-done
-done
-
-exit $EXIT_SUCCESS
-
diff --git a/build/batch_script_mpi_runit_hybridOMPMPI_6.sh b/build/batch_script_mpi_runit_hybridOMPMPI_6.sh
deleted file mode 100644
index 379a8e4..0000000
--- a/build/batch_script_mpi_runit_hybridOMPMPI_6.sh
+++ /dev/null
@@ -1,189 +0,0 @@
-#!/bin/bash
-# Bash script to submit many files to Cori/Edison/Queue
-
-EXIT_SUCCESS=0
-EXIT_HOST=1
-EXIT_PARAM=2
-
-# MAX_PARAMS=1
-# # ^^^ This should be fixed, as it should just loop through everything
-# if [[ $# -eq 0 ]]; then
-  # echo "Must have at least one parameter; exiting"
-  # exit $EXIT_PARAM
-# fi
-# if [[ $# -gt $MAX_PARAMS ]]; then
-  # echo "Too many parameters; exiting"
-  # exit $EXIT_PARAM
-# fi
-
-# INPUT_FILE=$1
-# # ^^^ Get the input ile
-
-CUR_DIR=`pwd`
-FILE_DIR=$CUR_DIR/EXAMPLE
-INPUT_DIR=/project/projectdirs/sparse/liuyangz/my_research/matrix
-FILE_NAME=pddrive
-FILE=$FILE_DIR/$FILE_NAME
-
-TMP_BATCH_FILE=tmp_batch_file.slurm
-# ^^^ Should check that this is not taken,
-#     but I would never use this, so ...
-
-> $TMP_BATCH_FILE
-
-if [[ $NERSC_HOST == edison ]]; then
-  CORES_PER_NODE=24
-  THREADS_PER_NODE=48
-elif [[ $NERSC_HOST == cori ]]; then
-  CORES_PER_NODE=32
-  THREADS_PER_NODE=64
-  # This does not take hyperthreading into account
-else
-  # Host unknown; exiting
-  exit $EXIT_HOST
-fi
-
-
-#nprows=(6 12 24)
-#npcols=(6 12 24)
-
-
-#nprows=(2048 1 32)
-#npcols=(1 2048 64)
-
-# nprows=(32 )
-# npcols=(64 )
-
-
-
-#nprows=(24 48 1 1 576 2304)
-#npcols=(24 48 576 2304 1 1)
-
-
-#nprows=(48  1  2304)
-#npcols=(48  2304 1)
-
-#nprows=(6 12 24 48 )
-#npcols=(6 12 24 48 )
-
-#nprows=(6 12 24 48 1 1 1 1 36 144 576 2304)
-#npcols=(6 12 24 48 36 144 576 2304 1 1 1 1)
-
-#nprows=(32 128 512 1 1 1 4 8 16)
-#npcols=(1 1 1 32 128 512 8 16 32)
-
-#nprows=(2048 1 32)
-#npcols=(1 2048 64)
-
-
-
-
-#nprows=(12 1 144)
-#npcols=(12 144 1)
-
-
-nprows=(16 )
-npcols=(16 )
- 
-for ((i = 0; i < ${#npcols[@]}; i++)); do
-NROW=${nprows[i]}
-NCOL=${npcols[i]}
-
-# NROW=36
-
-#PARTITION=debug
-PARTITION=regular
-LICENSE=SCRATCH
-TIME=00:20:00
-
-if [[ $NERSC_HOST == edison ]]
-then
-  CONSTRAINT=0
-fi
-if [[ $NERSC_HOST == cori ]]
-then
-  CONSTRAINT=haswell
-fi
-
-
-#for NTH in 8 
-for NTH in 1 2 4 8 
-do
-OMP_NUM_THREADS=$NTH
-TH_PER_RANK=`expr $NTH \* 2`
-
-CORE_VAL=`expr $NCOL \* $NROW`
-NODE_VAL=`expr $CORE_VAL / $CORES_PER_NODE \* $NTH`
-MOD_VAL=`expr $CORE_VAL % $CORES_PER_NODE`
-if [[ $MOD_VAL -ne 0 ]]
-then
-  NODE_VAL=`expr $NODE_VAL + 1`
-fi
-
-
-#for NSUP in 128 64 32 16 8
-#do
-  # for MAT in atmosmodl.rb nlpkkt80.mtx torso3.mtx Ga19As19H42.mtx A22.mtx cage13.rb 
-  for MAT in torso3.bin
-  # for MAT in matrix121.dat matrix211.dat tdr190k.dat tdr455k.dat nlpkkt80.mtx torso3.mtx helm2d03.mtx  
-  # for MAT in tdr190k.dat Ga19As19H42.mtx
- # for MAT in torso3.mtx hvdc2.mtx matrix121.dat nlpkkt80.mtx helm2d03.mtx
-# for MAT in A22.bin DG_GrapheneDisorder_8192.bin DNA_715_64cell.bin LU_C_BN_C_4by2.bin Li4244.bin atmosmodj.bin Ga19As19H42.bin Geo_1438.bin StocF-1465.bin    
-  # for MAT in  A22.bin DNA_715_64cell.bin LU_C_BN_C_4by2.bin
- # for MAT in Ga19As19H42.mtx   
-  do
-    # Start of looping stuff
-    > $TMP_BATCH_FILE
-    echo "#!/bin/bash -l" >> $TMP_BATCH_FILE
-    echo " " >> $TMP_BATCH_FILE
-    echo "#SBATCH -p $PARTITION" >> $TMP_BATCH_FILE
-    echo "#SBATCH -N $NODE_VAL" >>  $TMP_BATCH_FILE
-    echo "#SBATCH -t $TIME" >> $TMP_BATCH_FILE
-    echo "#SBATCH -L $LICENSE" >> $TMP_BATCH_FILE
-    echo "#SBATCH -J SLU_$MAT" >> $TMP_BATCH_FILE
-    #echo "#SBATCH -o ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_async_simple_over_icollec_flat_mrhs" >> $TMP_BATCH_FILE
-    #echo "#SBATCH -e ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_async_simple_over_icollec_flat_mrhs" >> $TMP_BATCH_FILE
-    # echo "#SBATCH --mail-type=BEGIN" >> $TMP_BATCH_FILE
-    # echo "#SBATCH --mail-type=END" >> $TMP_BATCH_FILE
-    echo "#SBATCH --mail-user=liuyangzhuan@lbl.gov" >> $TMP_BATCH_FILE
-    if [[ $NERSC_HOST == cori ]]
-    then
-      echo "#SBATCH -C $CONSTRAINT" >> $TMP_BATCH_FILE
-    fi
-    mkdir -p $MAT   
-    echo "export OMP_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE
-    echo "export KMP_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE
-    echo "export MKL_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE  																			
-    echo "export NSUP=128" >> $TMP_BATCH_FILE
-    echo "export NREL=20" >> $TMP_BATCH_FILE
-    echo "export OMP_PLACES=threads" >> $TMP_BATCH_FILE
-    echo "export OMP_PROC_BIND=spread" >> $TMP_BATCH_FILE
-    echo "export MPICH_MAX_THREAD_SAFETY=multiple" >> $TMP_BATCH_FILE
-
-    echo " " >> $TMP_BATCH_FILE
-    echo "FILE=$FILE" >> $TMP_BATCH_FILE
-    echo "FILEMAT=$INPUT_DIR/$MAT" >> $TMP_BATCH_FILE	
-    echo " " >> $TMP_BATCH_FILE
-    echo "CORE_VAL=$CORE_VAL" >> $TMP_BATCH_FILE
-    echo "NCOL=$NCOL" >> $TMP_BATCH_FILE
-    echo "NROW=$NROW" >> $TMP_BATCH_FILE
-    # This should be computed individually for each script...
-
-    export OMP_NUM_THREADS=$OMP_NUM_THREADS
-    export OMP_PLACES=threads
-    export OMP_PROC_BIND=spread
-    export MPICH_MAX_THREAD_SAFETY=multiple
-    srun -n $CORE_VAL -N $NODE_VAL -c $TH_PER_RANK --cpu_bind=cores $FILE -c $NCOL -r $NROW $INPUT_DIR/$MAT | tee ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_${OMP_NUM_THREADS}
-    # Add final line (srun line) to temporary slurm script
-
-    #cat $TMP_BATCH_FILE
-    #echo " "
-    # sbatch $TMP_BATCH_FILE
-  done
-#one
-
-done
-done
-
-exit $EXIT_SUCCESS
-
diff --git a/build/batch_script_mpi_runit_pureOMP.sh b/build/batch_script_mpi_runit_pureOMP.sh
deleted file mode 100644
index 8b97ea1..0000000
--- a/build/batch_script_mpi_runit_pureOMP.sh
+++ /dev/null
@@ -1,193 +0,0 @@
-#!/bin/bash
-# Bash script to submit many files to Cori/Edison/Queue
-
-EXIT_SUCCESS=0
-EXIT_HOST=1
-EXIT_PARAM=2
-
-# MAX_PARAMS=1
-# # ^^^ This should be fixed, as it should just loop through everything
-# if [[ $# -eq 0 ]]; then
-  # echo "Must have at least one parameter; exiting"
-  # exit $EXIT_PARAM
-# fi
-# if [[ $# -gt $MAX_PARAMS ]]; then
-  # echo "Too many parameters; exiting"
-  # exit $EXIT_PARAM
-# fi
-
-# INPUT_FILE=$1
-# # ^^^ Get the input ile
-
-CUR_DIR=`pwd`
-FILE_DIR=$CUR_DIR/EXAMPLE
-INPUT_DIR=/project/projectdirs/sparse/liuyangz/my_research/matrix
-FILE_NAME=pddrive
-FILE=$FILE_DIR/$FILE_NAME
-
-TMP_BATCH_FILE=tmp_batch_file.slurm
-# ^^^ Should check that this is not taken,
-#     but I would never use this, so ...
-
-> $TMP_BATCH_FILE
-
-if [[ $NERSC_HOST == edison ]]; then
-  CORES_PER_NODE=24
-  THREADS_PER_NODE=48
-elif [[ $NERSC_HOST == cori ]]; then
-  CORES_PER_NODE=32
-  THREADS_PER_NODE=64
-  # This does not take hyperthreading into account
-else
-  # Host unknown; exiting
-  exit $EXIT_HOST
-fi
-
-
-#nprows=(6 12 24)
-#npcols=(6 12 24)
-
-
-#nprows=(2048 1 32)
-#npcols=(1 2048 64)
-
-# nprows=(32 )
-# npcols=(64 )
-
-
-
-#nprows=(24 48 1 1 576 2304)
-#npcols=(24 48 576 2304 1 1)
-
-
-#nprows=(48  1  2304)
-#npcols=(48  2304 1)
-
-#nprows=(6 12 24 48 )
-#npcols=(6 12 24 48 )
-
-#nprows=(6 12 24 48 1 1 1 1 36 144 576 2304)
-#npcols=(6 12 24 48 36 144 576 2304 1 1 1 1)
-
-#nprows=(32 128 512 1 1 1 4 8 16)
-#npcols=(1 1 1 32 128 512 8 16 32)
-
-#nprows=(2048 1 32)
-#npcols=(1 2048 64)
-
-
-
-
-#nprows=(12 1 144)
-#npcols=(12 144 1)
-
-
-nprows=(1)
-npcols=(1)
- 
-for ((i = 0; i < ${#npcols[@]}; i++)); do
-NROW=${nprows[i]}
-NCOL=${npcols[i]}
-
-# NROW=36
-CORE_VAL=`expr $NCOL \* $NROW`
-NODE_VAL=`expr $CORE_VAL / $CORES_PER_NODE`
-MOD_VAL=`expr $CORE_VAL % $CORES_PER_NODE`
-if [[ $MOD_VAL -ne 0 ]]
-then
-  NODE_VAL=`expr $NODE_VAL + 1`
-fi
-#PARTITION=debug
-PARTITION=regular
-LICENSE=SCRATCH
-TIME=00:20:00
-
-if [[ $NERSC_HOST == edison ]]
-then
-  CONSTRAINT=0
-fi
-if [[ $NERSC_HOST == cori ]]
-then
-  CONSTRAINT=haswell
-fi
-
-
-for NTH in 1 16 32
-do
-
-OMP_NUM_THREADS=$NTH
-TH_PER_RANK=`expr $NTH \* 2`
-
-
-#for NSUP in 128 64 32 16 8
-#do
-  # for MAT in atmosmodl.rb nlpkkt80.mtx torso3.mtx Ga19As19H42.mtx A22.mtx cage13.rb 
-  # for MAT in torso3.mtx
-  # for MAT in matrix121.dat matrix211.dat tdr190k.dat tdr455k.dat nlpkkt80.mtx torso3.mtx helm2d03.mtx  
-  # for MAT in tdr190k.dat Ga19As19H42.mtx
- for MAT in hvdc2.mtx torso3.mtx matrix121.dat helm2d03.mtx
-   # for MAT in torso3.mtx
-# for MAT in A22.bin DG_GrapheneDisorder_8192.bin DNA_715_64cell.bin LU_C_BN_C_4by2.bin Li4244.bin atmosmodj.bin Ga19As19H42.bin Geo_1438.bin StocF-1465.bin
-
-# for MAT in LU_C_BN_C_4by2.bin Li4244.bin atmosmodj.bin Ga19As19H42.bin Geo_1438.bin StocF-1465.bin
-
-
-#for MAT in big.rua
-#for MAT in torso3.bin    
-# for MAT in Ga19As19H42.mtx   
-  do
-    # Start of looping stuff
-    > $TMP_BATCH_FILE
-    echo "#!/bin/bash -l" >> $TMP_BATCH_FILE
-    echo " " >> $TMP_BATCH_FILE
-    echo "#SBATCH -p $PARTITION" >> $TMP_BATCH_FILE
-    echo "#SBATCH -N $NODE_VAL" >>  $TMP_BATCH_FILE
-    echo "#SBATCH -t $TIME" >> $TMP_BATCH_FILE
-    echo "#SBATCH -L $LICENSE" >> $TMP_BATCH_FILE
-    echo "#SBATCH -J SLU_$MAT" >> $TMP_BATCH_FILE
-    #echo "#SBATCH -o ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_async_simple_over_icollec_flat_mrhs" >> $TMP_BATCH_FILE
-    #echo "#SBATCH -e ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_async_simple_over_icollec_flat_mrhs" >> $TMP_BATCH_FILE
-    # echo "#SBATCH --mail-type=BEGIN" >> $TMP_BATCH_FILE
-    # echo "#SBATCH --mail-type=END" >> $TMP_BATCH_FILE
-    echo "#SBATCH --mail-user=liuyangzhuan@lbl.gov" >> $TMP_BATCH_FILE
-    if [[ $NERSC_HOST == cori ]]
-    then
-      echo "#SBATCH -C $CONSTRAINT" >> $TMP_BATCH_FILE
-    fi
-    mkdir -p $MAT   
-    echo "export OMP_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE
-    echo "export KMP_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE
-    echo "export MKL_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE  																			
-    echo "export NSUP=128" >> $TMP_BATCH_FILE
-    echo "export NREL=20" >> $TMP_BATCH_FILE
-    echo "export OMP_PLACES=threads" >> $TMP_BATCH_FILE
-    echo "export OMP_PROC_BIND=spread" >> $TMP_BATCH_FILE
-    echo "export MPICH_MAX_THREAD_SAFETY=multiple" >> $TMP_BATCH_FILE
-
-    echo " " >> $TMP_BATCH_FILE
-    echo "FILE=$FILE" >> $TMP_BATCH_FILE
-    echo "FILEMAT=$INPUT_DIR/$MAT" >> $TMP_BATCH_FILE	
-    echo " " >> $TMP_BATCH_FILE
-    echo "CORE_VAL=$CORE_VAL" >> $TMP_BATCH_FILE
-    echo "NCOL=$NCOL" >> $TMP_BATCH_FILE
-    echo "NROW=$NROW" >> $TMP_BATCH_FILE
-    # This should be computed individually for each script...
-
-    export OMP_NUM_THREADS=$OMP_NUM_THREADS
-    export OMP_PLACES=threads
-    export OMP_PROC_BIND=spread
-    export MPICH_MAX_THREAD_SAFETY=multiple
-    srun -n $CORE_VAL -c $TH_PER_RANK --cpu_bind=cores $FILE -c $NCOL -r $NROW $INPUT_DIR/$MAT | tee ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_OMP_${OMP_NUM_THREADS}
-    # Add final line (srun line) to temporary slurm script
-
-    #cat $TMP_BATCH_FILE
-    #echo " "
-    # sbatch $TMP_BATCH_FILE
-  done
-#one
-
-done
-done
-
-exit $EXIT_SUCCESS
-
diff --git a/build/batch_script_mpi_runit_pureOMP_1.sh b/build/batch_script_mpi_runit_pureOMP_1.sh
deleted file mode 100644
index 11667d7..0000000
--- a/build/batch_script_mpi_runit_pureOMP_1.sh
+++ /dev/null
@@ -1,193 +0,0 @@
-#!/bin/bash
-# Bash script to submit many files to Cori/Edison/Queue
-
-EXIT_SUCCESS=0
-EXIT_HOST=1
-EXIT_PARAM=2
-
-# MAX_PARAMS=1
-# # ^^^ This should be fixed, as it should just loop through everything
-# if [[ $# -eq 0 ]]; then
-  # echo "Must have at least one parameter; exiting"
-  # exit $EXIT_PARAM
-# fi
-# if [[ $# -gt $MAX_PARAMS ]]; then
-  # echo "Too many parameters; exiting"
-  # exit $EXIT_PARAM
-# fi
-
-# INPUT_FILE=$1
-# # ^^^ Get the input ile
-
-CUR_DIR=`pwd`
-FILE_DIR=$CUR_DIR/EXAMPLE
-INPUT_DIR=/project/projectdirs/sparse/liuyangz/my_research/matrix
-FILE_NAME=pddrive
-FILE=$FILE_DIR/$FILE_NAME
-
-TMP_BATCH_FILE=tmp_batch_file.slurm
-# ^^^ Should check that this is not taken,
-#     but I would never use this, so ...
-
-> $TMP_BATCH_FILE
-
-if [[ $NERSC_HOST == edison ]]; then
-  CORES_PER_NODE=24
-  THREADS_PER_NODE=48
-elif [[ $NERSC_HOST == cori ]]; then
-  CORES_PER_NODE=32
-  THREADS_PER_NODE=64
-  # This does not take hyperthreading into account
-else
-  # Host unknown; exiting
-  exit $EXIT_HOST
-fi
-
-
-#nprows=(6 12 24)
-#npcols=(6 12 24)
-
-
-#nprows=(2048 1 32)
-#npcols=(1 2048 64)
-
-# nprows=(32 )
-# npcols=(64 )
-
-
-
-#nprows=(24 48 1 1 576 2304)
-#npcols=(24 48 576 2304 1 1)
-
-
-#nprows=(48  1  2304)
-#npcols=(48  2304 1)
-
-#nprows=(6 12 24 48 )
-#npcols=(6 12 24 48 )
-
-#nprows=(6 12 24 48 1 1 1 1 36 144 576 2304)
-#npcols=(6 12 24 48 36 144 576 2304 1 1 1 1)
-
-#nprows=(32 128 512 1 1 1 4 8 16)
-#npcols=(1 1 1 32 128 512 8 16 32)
-
-#nprows=(2048 1 32)
-#npcols=(1 2048 64)
-
-
-
-
-#nprows=(12 1 144)
-#npcols=(12 144 1)
-
-
-nprows=(1)
-npcols=(1)
- 
-for ((i = 0; i < ${#npcols[@]}; i++)); do
-NROW=${nprows[i]}
-NCOL=${npcols[i]}
-
-# NROW=36
-CORE_VAL=`expr $NCOL \* $NROW`
-NODE_VAL=`expr $CORE_VAL / $CORES_PER_NODE`
-MOD_VAL=`expr $CORE_VAL % $CORES_PER_NODE`
-if [[ $MOD_VAL -ne 0 ]]
-then
-  NODE_VAL=`expr $NODE_VAL + 1`
-fi
-#PARTITION=debug
-PARTITION=regular
-LICENSE=SCRATCH
-TIME=00:20:00
-
-if [[ $NERSC_HOST == edison ]]
-then
-  CONSTRAINT=0
-fi
-if [[ $NERSC_HOST == cori ]]
-then
-  CONSTRAINT=haswell
-fi
-
-
-for NTH in 1  
-do
-
-OMP_NUM_THREADS=$NTH
-TH_PER_RANK=`expr $NTH \* 2`
-
-
-#for NSUP in 128 64 32 16 8
-#do
-  # for MAT in atmosmodl.rb nlpkkt80.mtx torso3.mtx Ga19As19H42.mtx A22.mtx cage13.rb 
-  # for MAT in torso3.mtx
-  # for MAT in matrix121.dat matrix211.dat tdr190k.dat tdr455k.dat nlpkkt80.mtx torso3.mtx helm2d03.mtx  
-  # for MAT in tdr190k.dat Ga19As19H42.mtx
- for MAT in torso3.bin 
-   # for MAT in torso3.mtx
-# for MAT in A22.bin DG_GrapheneDisorder_8192.bin DNA_715_64cell.bin LU_C_BN_C_4by2.bin Li4244.bin atmosmodj.bin Ga19As19H42.bin Geo_1438.bin StocF-1465.bin
-
-# for MAT in LU_C_BN_C_4by2.bin Li4244.bin atmosmodj.bin Ga19As19H42.bin Geo_1438.bin StocF-1465.bin
-
-
-#for MAT in big.rua
-#for MAT in torso3.bin    
-# for MAT in Ga19As19H42.mtx   
-  do
-    # Start of looping stuff
-    > $TMP_BATCH_FILE
-    echo "#!/bin/bash -l" >> $TMP_BATCH_FILE
-    echo " " >> $TMP_BATCH_FILE
-    echo "#SBATCH -p $PARTITION" >> $TMP_BATCH_FILE
-    echo "#SBATCH -N $NODE_VAL" >>  $TMP_BATCH_FILE
-    echo "#SBATCH -t $TIME" >> $TMP_BATCH_FILE
-    echo "#SBATCH -L $LICENSE" >> $TMP_BATCH_FILE
-    echo "#SBATCH -J SLU_$MAT" >> $TMP_BATCH_FILE
-    #echo "#SBATCH -o ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_async_simple_over_icollec_flat_mrhs" >> $TMP_BATCH_FILE
-    #echo "#SBATCH -e ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_async_simple_over_icollec_flat_mrhs" >> $TMP_BATCH_FILE
-    # echo "#SBATCH --mail-type=BEGIN" >> $TMP_BATCH_FILE
-    # echo "#SBATCH --mail-type=END" >> $TMP_BATCH_FILE
-    echo "#SBATCH --mail-user=liuyangzhuan@lbl.gov" >> $TMP_BATCH_FILE
-    if [[ $NERSC_HOST == cori ]]
-    then
-      echo "#SBATCH -C $CONSTRAINT" >> $TMP_BATCH_FILE
-    fi
-    mkdir -p $MAT   
-    echo "export OMP_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE
-    echo "export KMP_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE
-    echo "export MKL_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE  																			
-    echo "export NSUP=128" >> $TMP_BATCH_FILE
-    echo "export NREL=20" >> $TMP_BATCH_FILE
-    echo "export OMP_PLACES=threads" >> $TMP_BATCH_FILE
-    echo "export OMP_PROC_BIND=spread" >> $TMP_BATCH_FILE
-    echo "export MPICH_MAX_THREAD_SAFETY=multiple" >> $TMP_BATCH_FILE
-
-    echo " " >> $TMP_BATCH_FILE
-    echo "FILE=$FILE" >> $TMP_BATCH_FILE
-    echo "FILEMAT=$INPUT_DIR/$MAT" >> $TMP_BATCH_FILE	
-    echo " " >> $TMP_BATCH_FILE
-    echo "CORE_VAL=$CORE_VAL" >> $TMP_BATCH_FILE
-    echo "NCOL=$NCOL" >> $TMP_BATCH_FILE
-    echo "NROW=$NROW" >> $TMP_BATCH_FILE
-    # This should be computed individually for each script...
-
-    export OMP_NUM_THREADS=$OMP_NUM_THREADS
-    export OMP_PLACES=threads
-    export OMP_PROC_BIND=spread
-    export MPICH_MAX_THREAD_SAFETY=multiple
-    srun -n $CORE_VAL -c $TH_PER_RANK --cpu_bind=cores $FILE -c $NCOL -r $NROW $INPUT_DIR/$MAT | tee ./$MAT/SLU.o_mpi_${NROW}x${NCOL}_OMP_${OMP_NUM_THREADS}
-    # Add final line (srun line) to temporary slurm script
-
-    #cat $TMP_BATCH_FILE
-    #echo " "
-    # sbatch $TMP_BATCH_FILE
-  done
-#one
-
-done
-done
-
-exit $EXIT_SUCCESS
-
diff --git a/build/batch_script_mpi_vtune.sh b/build/batch_script_mpi_vtune.sh
deleted file mode 100644
index 73fbd5e..0000000
--- a/build/batch_script_mpi_vtune.sh
+++ /dev/null
@@ -1,165 +0,0 @@
-#!/bin/bash
-# Bash script to submit many files to Cori/Edison/Queue
-
-EXIT_SUCCESS=0
-EXIT_HOST=1
-EXIT_PARAM=2
-
-# MAX_PARAMS=1
-# # ^^^ This should be fixed, as it should just loop through everything
-# if [[ $# -eq 0 ]]; then
-  # echo "Must have at least one parameter; exiting"
-  # exit $EXIT_PARAM
-# fi
-# if [[ $# -gt $MAX_PARAMS ]]; then
-  # echo "Too many parameters; exiting"
-  # exit $EXIT_PARAM
-# fi
-
-# INPUT_FILE=$1
-# # ^^^ Get the input ile
-
-CUR_DIR=`pwd`
-#FILE_DIR=$CUR_DIR/EXAMPLE
-#INPUT_DIR=~/Edison/my_research/SuperLU/SuperLUDIST_Begin/build/EXAMPLE
-
-FILE_DIR=./EXAMPLE
-INPUT_DIR=./EXAMPLE
-FILE_NAME=pddrive
-FILE=$FILE_DIR/$FILE_NAME
-
-TMP_BATCH_FILE=tmp_batch_file.slurm
-# ^^^ Should check that this is not taken,
-#     but I would never use this, so ...
-
-> $TMP_BATCH_FILE
-
-if [[ $NERSC_HOST == edison ]]; then
-  CORES_PER_NODE=24
-elif [[ $NERSC_HOST == cori ]]; then
-  CORES_PER_NODE=32
-  # This does not take hyperthreading into account
-else
-  # Host unknown; exiting
-  exit $EXIT_HOST
-fi
-
-
-for NROW in 1
-do
-
-NCOL=1
-# NROW=36
-CORE_VAL=`expr $NCOL \* $NROW`
-CORE_VALMAX=`expr $CORE_VAL - 1`
-NODE_VAL=`expr $CORE_VAL / $CORES_PER_NODE`
-MOD_VAL=`expr $CORE_VAL % $CORES_PER_NODE`
-
-if [[ $MOD_VAL -ne 0 ]]
-then
-  NODE_VAL=`expr $NODE_VAL + 1`
-fi
-PARTITION=debug
-#PARTITION=regular
-LICENSE=SCRATCH
-TIME=00:30:00
-
-if [[ $NERSC_HOST == edison ]]
-then
-  CONSTRAINT=0
-fi
-if [[ $NERSC_HOST == cori ]]
-then
-  CONSTRAINT=haswell
-fi
-
-OMP_NUM_THREADS=12
-#COLLECT="-collect advanced-hotspots -knob sampling-interval=0.01"
-COLLECT="-collect hotspots -start-paused -knob sampling-interval=1"
-RES_DIR=Vtune_np${CORE_VAL}_mrhs
-
-#cat << EOF > mpmd.conf
-#0 amplxe-cl -data-limit=0 ${COLLECT} -r ${RES_DIR} -trace-mpi -- ${RUNEXE}
-#1-${CORE_VALMAX} ${RUNEXE}
-#EOF
-
-
-
-#for NSUP in 128 64 32 16 8
-#do
-  # for MAT in atmosmodl.rb nlpkkt80.mtx torso3.mtx Ga19As19H42.mtx A22.mtx cage13.rb 
-  for MAT in torso3.mtx
-  # for MAT in matrix121.dat matrix211.dat tdr190k.dat tdr455k.dat nlpkkt80.mtx torso3.mtx helm2d03.mtx  
-  # for MAT in tdr190k.dat Ga19As19H42.mtx
-  # for MAT in nlpkkt80.mtx torso3.mtx Ga19As19H42.mtx A22.mtx matrix121.dat tdr190k.dat   
-  # for MAT in Ga19As19H42.mtx   
-  do
-
-
-RUNEXE="$FILE -c $NCOL -r $NROW $INPUT_DIR/$MAT"
-
-if [[ ${CORE_VALMAX} -ne 0 ]]
-then
-
-cat << EOF > mpmd.conf_np$CORE_VAL
-0 amplxe-cl -data-limit=0 ${COLLECT} -r ${RES_DIR} -trace-mpi -- ${RUNEXE}
-1-${CORE_VALMAX} ${RUNEXE}
-EOF
-
-else
-
-cat << EOF > mpmd.conf_np$CORE_VAL
-0 amplxe-cl -data-limit=0 ${COLLECT} -r ${RES_DIR} -trace-mpi -- ${RUNEXE}
-EOF
-
-fi
-
-
-
-    # Start of looping stuff
-    > $TMP_BATCH_FILE
-    echo "#!/bin/bash -l" >> $TMP_BATCH_FILE
-    echo " " >> $TMP_BATCH_FILE
-    echo "#SBATCH -p $PARTITION" >> $TMP_BATCH_FILE
-    echo "#SBATCH -N $NODE_VAL" >>  $TMP_BATCH_FILE
-    echo "#SBATCH -t $TIME" >> $TMP_BATCH_FILE
-    echo "#SBATCH --perf=vtune" >> $TMP_BATCH_FILE
-    echo "#SBATCH -L $LICENSE" >> $TMP_BATCH_FILE
-    echo "#SBATCH -J SLU_$MAT" >> $TMP_BATCH_FILE
-    echo "#SBATCH -o ./$MAT/SLU.o_mpi_np${CORE_VAL}_vtune" >> $TMP_BATCH_FILE
-    echo "#SBATCH -e ./$MAT/SLU.o_mpi_np${CORE_VAL}_vtune" >> $TMP_BATCH_FILE
-    # echo "#SBATCH --mail-type=BEGIN" >> $TMP_BATCH_FILE
-    # echo "#SBATCH --mail-type=END" >> $TMP_BATCH_FILE
-    echo "#SBATCH --mail-user=liuyangzhuan@lbl.gov" >> $TMP_BATCH_FILE
-    if [[ $NERSC_HOST == cori ]]
-    then
-      echo "#SBATCH -C $CONSTRAINT" >> $TMP_BATCH_FILE
-    fi
-    mkdir -p $MAT   
-    echo "export OMP_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE
-    echo "export KMP_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE
-    echo "export MKL_NUM_THREADS=$OMP_NUM_THREADS" >> $TMP_BATCH_FILE  																			
-    echo "export NSUP=128" >> $TMP_BATCH_FILE
-    echo "export NREL=20" >> $TMP_BATCH_FILE
-    
-    echo " " >> $TMP_BATCH_FILE
-    echo "FILE=$FILE" >> $TMP_BATCH_FILE
-    echo "FILEMAT=$INPUT_DIR/$MAT" >> $TMP_BATCH_FILE	
-    echo " " >> $TMP_BATCH_FILE
-    echo "CORE_VAL=$CORE_VAL" >> $TMP_BATCH_FILE
-    echo "NCOL=$NCOL" >> $TMP_BATCH_FILE
-    echo "NROW=$NROW" >> $TMP_BATCH_FILE
-    # This should be computed individually for each script...	
-    echo "srun -n $CORE_VAL --multi-prog ./mpmd.conf_np$CORE_VAL" >> $TMP_BATCH_FILE
-    # Add final line (srun line) to temporary slurm script
-
-    #cat $TMP_BATCH_FILE
-    #echo " "
-    sbatch $TMP_BATCH_FILE
-  done
-#one
-
-done
-
-exit $EXIT_SUCCESS
-
diff --git a/build/itacconf.txt b/build/itacconf.txt
deleted file mode 100644
index 201a3ea..0000000
--- a/build/itacconf.txt
+++ /dev/null
@@ -1,28 +0,0 @@
-# module load itac/2018.beta
-module swap intel/18.0.0.128 intel/17.0.3.191 
-
-module load itac/2017.up1
-module rm cray-mpich/7.6.0
-module load impi
-export LD_PRELOAD=$VT_ROOT/slib/libVT.so
-export VT_LOGFILE_FORMAT=STFSINGLE
-export I_MPI_PMI_LIBRARY=/usr/lib64/slurmpmi/libpmi.so
-export VT_PCTRACE=5
-
-export OMP_NUM_THREADS=1
-export KMP_NUM_THREADS=1
-export MKL_NUM_THREADS=1
-export NSUP=128
-export NREL=20
-
-srun -trace -n 32 ./EXAMPLE/pddrive -r 4 -c 8 ./EXAMPLE/torso3.mtx 
-
-traceanalyzer pddrive.single.stf
-
-
-
-# export I_MPI_FABRICS=ofi
-# export I_MPI_OFI_PROVIDER=gni
-# export I_MPI_OFI_LIBRARY=/usr/common/software/libfabric/1.5.0/gnu/lib/libfabric.so
-
-
diff --git a/build/itacconf_crayMPI.txt b/build/itacconf_crayMPI.txt
deleted file mode 100644
index a5c54b4..0000000
--- a/build/itacconf_crayMPI.txt
+++ /dev/null
@@ -1,24 +0,0 @@
-module load itac/2017.up1
-module swap intel/18.0.0.128 intel/17.0.3.191
-export LD_PRELOAD=$VT_ROOT/slib/libVT.so
-export VT_LOGFILE_FORMAT=STFSINGLE
-export I_MPI_PMI_LIBRARY=/usr/lib64/slurmpmi/libpmi.so
-
-
-export OMP_NUM_THREADS=1
-export KMP_NUM_THREADS=1
-export MKL_NUM_THREADS=1
-export NSUP=128
-export NREL=20
-
-srun -trace -n 32 ./EXAMPLE/pddrive -r 4 -c 8 ./EXAMPLE/torso3.mtx 
-
-traceanalyzer pddrive.single.stf
-
-
-
-# export I_MPI_FABRICS=ofi
-# export I_MPI_OFI_PROVIDER=gni
-# export I_MPI_OFI_LIBRARY=/usr/common/software/libfabric/1.5.0/gnu/lib/libfabric.so
-
-
diff --git a/build/mpmd.conf_np1 b/build/mpmd.conf_np1
deleted file mode 100644
index cb8abaf..0000000
--- a/build/mpmd.conf_np1
+++ /dev/null
@@ -1 +0,0 @@
-0 amplxe-cl -data-limit=0 -collect hotspots -start-paused -knob sampling-interval=1 -r Vtune_np1_mrhs -trace-mpi -- ./EXAMPLE/pddrive -c 1 -r 1 ./EXAMPLE/torso3.mtx
diff --git a/build/run_cmake_build.sh b/build/run_cmake_build.sh
deleted file mode 100644
index 80fefb8..0000000
--- a/build/run_cmake_build.sh
+++ /dev/null
@@ -1,43 +0,0 @@
-#!/bin/bash
-# Bash script to submit many files to Cori/Edison/Queue
-Vtune=0
-export CRAYPE_LINK_TYPE=dynamic
-export PARMETIS_ROOT=~/Cori/my_software/parmetis-4.0.3_dynamic_longint
-export PARMETIS_BUILD_DIR=${PARMETIS_ROOT}/build/Linux-x86_64 
-rm -rf CMakeCache.txt
-rm -rf CMakeFiles
-rm -rf CTestTestfile.cmake
-rm -rf cmake_install.cmake
-rm -rf DartConfiguration.tcl 
-if [[ ${Vtune} == 1 ]]; then
-INC_VTUNE="-g -DVTUNE=1 -I$VTUNE_AMPLIFIER_XE_2017_DIR/include"
-LIB_VTUNE="$VTUNE_AMPLIFIER_XE_2017_DIR/lib64/libittnotify.a"
-fi
-
-cmake .. \
-	-DTPL_PARMETIS_INCLUDE_DIRS="${PARMETIS_ROOT}/include;${PARMETIS_ROOT}/metis/include" \
-	-DTPL_PARMETIS_LIBRARIES="${PARMETIS_BUILD_DIR}/libparmetis/libparmetis.so;${PARMETIS_BUILD_DIR}/libmetis/libmetis.so;${LIB_VTUNE}" \
-	-Denable_blaslib=OFF \
-	-DBUILD_SHARED_LIBS=ON \
-	-DCMAKE_C_COMPILER=cc \
-        -DCMAKE_CXX_COMPILER=CC \
-	-DCMAKE_INSTALL_PREFIX=. \
-	-DCMAKE_BUILD_TYPE=Release \
-	-DCMAKE_VERBOSE_MAKEFILE:BOOL=ON \
-	-DCMAKE_CXX_FLAGS="-Ofast -std=c++11 -DAdd_ -DRELEASE ${INC_VTUNE}" \
-        -DCMAKE_C_FLAGS="-D_LONGINT -std=c11 -DPRNTlevel=1 -DPROFlevel=1 -DDEBUGlevel=0 ${INC_VTUNE}" \
-	-DTPL_BLAS_LIBRARIES="/opt/intel/compilers_and_libraries_2017.2.174/linux/mkl/lib/intel64/libmkl_intel_lp64.so;/opt/intel/compilers_and_libraries_2017.2.174/linux/mkl/lib/intel64/libmkl_sequential.so;/opt/intel/compilers_and_libraries_2017.2.174/linux/mkl/lib/intel64/libmkl_core.so"
-
-#	-DTPL_BLAS_LIBRARIES="/opt/intel/compilers_and_libraries_2017.2.174/linux/mkl/lib/intel64/libmkl_intel_lp64.so;/opt/intel/compilers_and_libraries_2017.2.174/linux/mkl/lib/intel64/libmkl_sequential.so;/opt/intel/compilers_and_libraries_2017.2.174/linux/mkl/lib/intel64/libmkl_core.so" \
-#        -DCMAKE_CXX_FLAGS="-g -trace -Ofast -std=c++11 -DAdd_ -DRELEASE -tcollect -L$VT_LIB_DIR -lVT $VT_ADD_LIBS" \
-
-
-#	-DTPL_BLAS_LIBRARIES="/opt/intel/compilers_and_libraries_2017.2.174/linux/mkl/lib/intel64/libmkl_lapack95_lp64.a;/opt/intel/compilers_and_libraries_2017.2.174/linux/mkl/lib/intel64/libmkl_blas95_lp64.a"
-
-#	-DTPL_BLAS_LIBRARIES="/opt/intel/compilers_and_libraries_2017.2.174/linux/mkl/lib/intel64/libmkl_intel_lp64.a;/opt/intel/compilers_and_libraries_2017.2.174/linux/mkl/lib/intel64/libmkl_sequential.a;/opt/intel/compilers_and_libraries_2017.2.174/linux/mkl/lib/intel64/libmkl_core.a"  
-
-
-# DCMAKE_BUILD_TYPE=Release or Debug compiler options set in CMAKELIST.txt
-
-#        -DCMAKE_C_FLAGS="-g -O0 -std=c99 -DPRNTlevel=2 -DPROFlevel=1 -DDEBUGlevel=0" \
-#	-DCMAKE_C_FLAGS="-g -O0 -std=c11 -DPRNTlevel=1 -DPROFlevel=1 -DDEBUGlevel=0" \
diff --git a/build/run_cmake_build_debug.sh b/build/run_cmake_build_debug.sh
deleted file mode 100644
index 5d5998e..0000000
--- a/build/run_cmake_build_debug.sh
+++ /dev/null
@@ -1,43 +0,0 @@
-#!/bin/bash
-# Bash script to submit many files to Cori/Edison/Queue
-Vtune=0
-export CRAYPE_LINK_TYPE=dynamic
-export PARMETIS_ROOT=~/Cori/my_software/parmetis-4.0.3_dynamic
-export PARMETIS_BUILD_DIR=${PARMETIS_ROOT}/build/Linux-x86_64 
-rm -rf CMakeCache.txt
-rm -rf CMakeFiles
-rm -rf CTestTestfile.cmake
-rm -rf cmake_install.cmake
-rm -rf DartConfiguration.tcl 
-if [[ ${Vtune} == 1 ]]; then
-INC_VTUNE="-g -DVTUNE=1 -I$VTUNE_AMPLIFIER_XE_2017_DIR/include"
-LIB_VTUNE="$VTUNE_AMPLIFIER_XE_2017_DIR/lib64/libittnotify.a"
-fi
-
-cmake .. \
-	-DTPL_PARMETIS_INCLUDE_DIRS="${PARMETIS_ROOT}/include;${PARMETIS_ROOT}/metis/include" \
-	-DTPL_PARMETIS_LIBRARIES="${PARMETIS_BUILD_DIR}/libparmetis/libparmetis.so;${PARMETIS_BUILD_DIR}/libmetis/libmetis.so;${LIB_VTUNE}" \
-	-Denable_blaslib=OFF \
-	-DBUILD_SHARED_LIBS=ON \
-	-DCMAKE_C_COMPILER=cc \
-        -DCMAKE_CXX_COMPILER=CC \
-	-DCMAKE_INSTALL_PREFIX=. \
-	-DCMAKE_BUILD_TYPE=Debug \
-	-DCMAKE_VERBOSE_MAKEFILE:BOOL=ON \
-	-DCMAKE_C_FLAGS="-g -O0 -std=c11 -DPRNTlevel=1 -DPROFlevel=1 -DDEBUGlevel=0" \
-	-DCMAKE_CXX_FLAGS="-Ofast -std=c++11 -DAdd_ -DRELEASE ${INC_VTUNE}" \
-	-DTPL_BLAS_LIBRARIES="/opt/intel/compilers_and_libraries_2017.2.174/linux/mkl/lib/intel64/libmkl_intel_lp64.so;/opt/intel/compilers_and_libraries_2017.2.174/linux/mkl/lib/intel64/libmkl_sequential.so;/opt/intel/compilers_and_libraries_2017.2.174/linux/mkl/lib/intel64/libmkl_core.so"
-
-#	-DTPL_BLAS_LIBRARIES="/opt/intel/compilers_and_libraries_2017.2.174/linux/mkl/lib/intel64/libmkl_intel_lp64.so;/opt/intel/compilers_and_libraries_2017.2.174/linux/mkl/lib/intel64/libmkl_sequential.so;/opt/intel/compilers_and_libraries_2017.2.174/linux/mkl/lib/intel64/libmkl_core.so" \
-#        -DCMAKE_CXX_FLAGS="-g -trace -Ofast -std=c++11 -DAdd_ -DRELEASE -tcollect -L$VT_LIB_DIR -lVT $VT_ADD_LIBS" \
-
-
-#	-DTPL_BLAS_LIBRARIES="/opt/intel/compilers_and_libraries_2017.2.174/linux/mkl/lib/intel64/libmkl_lapack95_lp64.a;/opt/intel/compilers_and_libraries_2017.2.174/linux/mkl/lib/intel64/libmkl_blas95_lp64.a"
-
-#	-DTPL_BLAS_LIBRARIES="/opt/intel/compilers_and_libraries_2017.2.174/linux/mkl/lib/intel64/libmkl_intel_lp64.a;/opt/intel/compilers_and_libraries_2017.2.174/linux/mkl/lib/intel64/libmkl_sequential.a;/opt/intel/compilers_and_libraries_2017.2.174/linux/mkl/lib/intel64/libmkl_core.a"  
-
-
-# DCMAKE_BUILD_TYPE=Release or Debug compiler options set in CMAKELIST.txt
-
-#        -DCMAKE_C_FLAGS="-g -O0 -std=c99 -DPRNTlevel=2 -DPROFlevel=1 -DDEBUGlevel=0" \
-        #-DCMAKE_C_FLAGS="-std=c11 -DPRNTlevel=1 -DPROFlevel=1 -DDEBUGlevel=0 ${INC_VTUNE}" \
diff --git a/build/simpleconf.txt b/build/simpleconf.txt
deleted file mode 100644
index 512e6ea..0000000
--- a/build/simpleconf.txt
+++ /dev/null
@@ -1,9 +0,0 @@
-export MPICH_MAX_THREAD_SAFETY=multiple
-export OMP_NUM_THREADS=1
-export KMP_NUM_THREADS=1
-export MKL_NUM_THREADS=1
-export NSUP=1280
-export NREL=1000
-
-srun -n 24 ./EXAMPLE/pddrive -c 24 -r 1 ../EXAMPLE/big.rua | tee a.out
-
diff --git a/build/vtuneconf.txt b/build/vtuneconf.txt
deleted file mode 100644
index 6307eee..0000000
--- a/build/vtuneconf.txt
+++ /dev/null
@@ -1,2 +0,0 @@
-module unload darshan
-module load vtune
diff --git a/make.inc.in b/make.inc.in
index 3f06155..c4526cb 100644
--- a/make.inc.in
+++ b/make.inc.in
@@ -8,7 +8,7 @@
 #
 #  Creation date:   March 1, 2016	version 5.0.0
 #
-#  Modified:	    
+#  Modified:	    October 13, 2017    version 5.2.1
 #		    
 #
 ############################################################################
@@ -30,9 +30,9 @@ RANLIB       = @CMAKE_RANLIB@
 
 CC           = @CMAKE_C_COMPILER@
 CFLAGS 	     = @CMAKE_C_FLAGS_RELEASE@ @CMAKE_C_FLAGS@
-# CFLAGS       += -D_LONGINT   ## 64-bit integer
 # CFLAGS       += -D${DirDefs}
 # CFLAGS       += @COMPILE_DEFINITIONS@ 
+#XSDK_INDEX_SIZE = 64 ## 64-bit integer
 NOOPTS       = -O0
 FORTRAN	     = @CMAKE_Fortran_COMPILER@
 
diff --git a/make.inc_good_dynamic b/make.inc_good_dynamic
deleted file mode 100644
index 4a47aad..0000000
--- a/make.inc_good_dynamic
+++ /dev/null
@@ -1,42 +0,0 @@
-############################################################################
-#
-#  Program:         SuperLU_DIST
-#
-#  Module:          make.inc
-#
-#  Purpose:         Top-level Definitions
-#
-#  Creation date:   March 1, 2016	version 5.0.0
-#
-#  Modified:	    
-#		    
-#
-############################################################################
-#
-#  The name of the libraries to be created/linked to
-#
-SuperLUroot	= /global/homes/l/liuyangz/Cori/my_research/github/superlu_dist_task_hybrid_whypddistslow_01_27_2018/build
-DSUPERLULIB   	= $(SuperLUroot)/SRC/libsuperlu_dist.so
-
-LIBS		= $(DSUPERLULIB) /opt/intel/compilers_and_libraries_2017.2.174/linux/mkl/lib/intel64/libmkl_intel_lp64.so /opt/intel/compilers_and_libraries_2017.2.174/linux/mkl/lib/intel64/libmkl_sequential.so /opt/intel/compilers_and_libraries_2017.2.174/linux/mkl/lib/intel64/libmkl_core.so /global/homes/l/liuyangz/Edison/my_software/parmetis-4.0.3_dynamic/build/Linux-x86_64/libparmetis/libparmetis.so /global/homes/l/liuyangz/Edison/my_software/parmetis-4.0.3_dynamic/build/Linux-x86_64/libmetis/libmetis.so
-
-#
-#  The archiver and the flag(s) to use when building archive (library)
-#  If your system has no ranlib, set RANLIB = echo.
-#
-ARCH         = /usr/bin/ar
-ARCHFLAGS    = cr
-RANLIB       = /usr/bin/ranlib
-
-CC           = /opt/cray/pe/craype/2.5.12/bin/cc
-CFLAGS 	     = -O0 -g -I/global/homes/l/liuyangz/Edison/my_software/parmetis-4.0.3_dynamic/metis/include -I/global/homes/l/liuyangz/Edison/my_software/parmetis-4.0.3_dynamic/include  -DUSE_VENDOR_BLAS -qopenmp -std=c11 -DPRNTlevel=1 -DPROFlevel=1 -DDEBUGlevel=0
-# CFLAGS       += -D_LONGINT   ## 64-bit integer
-# CFLAGS       += -D
-# CFLAGS       +=  
-NOOPTS       = -O0
-FORTRAN	     = /opt/cray/pe/craype/2.5.12/bin/ftn
-CPP          =/opt/cray/pe/craype/2.5.12/bin/CC
-CPPFLAGS     = -Ofast -std=c++11 -DAdd_ 
-
-LOADER       = $(CPP)
-LOADOPTS     = $(LIBS) -Wl,-rpath,-qopenmp  -qopenmp -dynamic 
diff --git a/make.inc_good_static b/make.inc_good_static
deleted file mode 100644
index e11d889..0000000
--- a/make.inc_good_static
+++ /dev/null
@@ -1,41 +0,0 @@
-############################################################################
-#
-#  Program:         SuperLU_DIST
-#
-#  Module:          make.inc
-#
-#  Purpose:         Top-level Definitions
-#
-#  Creation date:   March 1, 2016	version 5.0.0
-#
-#  Modified:	    
-#		    
-#
-############################################################################
-#
-#  The name of the libraries to be created/linked to
-#
-SuperLUroot	= /global/homes/l/liuyangz/Cori/my_research/github/superlu_dist_task_hybrid_whypddistslow_01_27_2018/build
-DSUPERLULIB   	= $(SuperLUroot)/SRC/libsuperlu_dist.a
-
-LIBS		= $(DSUPERLULIB) /opt/intel/compilers_and_libraries_2017.2.174/linux/mkl/lib/intel64/libmkl_intel_lp64.a /opt/intel/compilers_and_libraries_2017.2.174/linux/mkl/lib/intel64/libmkl_sequential.a /opt/intel/compilers_and_libraries_2017.2.174/linux/mkl/lib/intel64/libmkl_core.a /global/homes/l/liuyangz/Edison/my_software/parmetis-4.0.3/build/Linux-x86_64/libparmetis/libparmetis.a /global/homes/l/liuyangz/Edison/my_software/parmetis-4.0.3/build/Linux-x86_64/libmetis/libmetis.a
-
-#
-#  The archiver and the flag(s) to use when building archive (library)
-#  If your system has no ranlib, set RANLIB = echo.
-#
-ARCH         = /usr/bin/ar
-ARCHFLAGS    = cr
-RANLIB       = /usr/bin/ranlib
-
-CC           = /opt/cray/pe/craype/2.5.12/bin/cc
-CFLAGS 	     = -O3 -DNDEBUG -I/global/homes/l/liuyangz/Edison/my_software/parmetis-4.0.3/metis/include -I/global/homes/l/liuyangz/Edison/my_software/parmetis-4.0.3/include  -DUSE_VENDOR_BLAS -qopenmp -std=c11 -DPRNTlevel=1 -DPROFlevel=1 -DDEBUGlevel=0
-# CFLAGS       += -D_LONGINT   ## 64-bit integer
-# CFLAGS       += -D
-# CFLAGS       +=  
-NOOPTS       = -O0
-FORTRAN	     = /opt/cray/pe/craype/2.5.12/bin/ftn
-CPP          =/opt/cray/pe/craype/2.5.12/bin/CC
-CPPFLAGS     = -Ofast -std=c++11 -DAdd_ 
-LOADER       = $(CPP)
-LOADOPTS     = -Wl,-rpath,-qopenmp  -qopenmp 
diff --git a/run_cmake_build.sh b/run_cmake_build.sh
new file mode 100755
index 0000000..450dd6f
--- /dev/null
+++ b/run_cmake_build.sh
@@ -0,0 +1,60 @@
+#!/bin/bash
+
+## if [ !$?NERSC_HOST ]
+if [ -z $NERSC_HOST ]
+then
+    echo "NERSC_HOST undefined"
+elif [ "$NERSC_HOST" == "edison" ]
+then
+    export PARMETIS_ROOT=~/Edison/lib/parmetis-4.0.3
+    export PARMETIS_BUILD_DIR=${PARMETIS_ROOT}/static-build/Linux-x86_64
+    cmake .. \
+    -DUSE_XSDK_DEFAULTS=FALSE\
+    -DTPL_PARMETIS_INCLUDE_DIRS="${PARMETIS_ROOT}/include;${PARMETIS_ROOT}/metis/include" \
+    -DTPL_PARMETIS_LIBRARIES="${PARMETIS_BUILD_DIR}/libparmetis/libparmetis.a;${PARMETIS_BUILD_DIR}/libmetis/libmetis.a" \
+    -DCMAKE_C_FLAGS="-std=c99 -fPIC" \
+    -DCMAKE_Fortran_COMPILER=ftn \
+    -Denable_blaslib=OFF \
+#    -DTPL_BLAS_LIBRARIES="-mkl" \
+    -DBUILD_SHARED_LIBS=OFF \
+    -DCMAKE_INSTALL_PREFIX=.
+#    -DCMAKE_EXE_LINKER_FLAGS="-shared"
+elif [ "$NERSC_HOST" == "cori" ]
+then
+    export PARMETIS_ROOT=~/Cori/lib/parmetis-4.0.3
+#    export PARMETIS_BUILD_DIR=${PARMETIS_ROOT}/shared-build
+    setenv PARMETIS_BUILD_DIR ${PARMETIS_ROOT}/static-build/Linux-x86_64
+    cmake .. \
+    -DUSE_XSDK_DEFAULTS=TRUE\
+    -DTPL_PARMETIS_INCLUDE_DIRS="${PARMETIS_ROOT}/include;${PARMETIS_ROOT}/metis/include" \
+    -DTPL_PARMETIS_LIBRARIES="${PARMETIS_BUILD_DIR}/libparmetis/libparmetis.a;${PARMETIS_BUILD_DIR}/libmetis/libmetis.a" \
+    -Denable_blaslib=OFF \
+    -DCMAKE_Fortran_COMPILER=ftn \
+    -DCMAKE_C_FLAGS="-std=c99 -fPIC" \
+#    -DCMAKE_EXE_LINKER_FLAGS="-shared" \
+    -DCMAKE_INSTALL_PREFIX=.
+fi
+
+THISHOST=`hostname -s`
+echo "host: $THISHOST"
+if [ "$THISHOST" == "ssg1" ]
+then
+  rm -fr ssg1-build; mkdir ssg1-build; cd ssg1-build;
+  export PARMETIS_ROOT=~/lib/static/64-bit/parmetis-4.0.3 
+#  export PARMETIS_ROOT=~/lib/static/parmetis-4.0.3 
+  export PARMETIS_BUILD_DIR=${PARMETIS_ROOT}/build/Linux-x86_64
+  echo "ParMetis root: $PARMETIS_ROOT"
+  cmake .. \
+    -DTPL_PARMETIS_INCLUDE_DIRS="${PARMETIS_ROOT}/include;${PARMETIS_ROOT}/metis/include" \
+    -DTPL_PARMETIS_LIBRARIES="${PARMETIS_BUILD_DIR}/libparmetis/libparmetis.a;${PARMETIS_BUILD_DIR}/libmetis/libmetis.a" \
+    -DCMAKE_C_FLAGS="-std=c99 -g -DPRNTlevel=0 -DDEBUGlevel=0" \
+    -Denable_blaslib=OFF \
+    -DBUILD_SHARED_LIBS=OFF \
+    -DCMAKE_C_COMPILER=mpicc \
+    -DXSDK_INDEX_SIZE=64 \
+    -DCMAKE_INSTALL_PREFIX=.
+#   -Denable_parmetislib=OFF
+fi
+
+# make VERBOSE=1
+# make test
